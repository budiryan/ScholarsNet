[{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9809125v1", 
    "other_authors": "Michael Manthey", 
    "title": "Distributed Computation, the Twisted Isomorphism, and Auto-Poiesis", 
    "arxiv-id": "cs/9809125v1", 
    "author": "Michael Manthey", 
    "publish": "1998-09-14T14:42:26Z", 
    "summary": "This paper presents a synchronization-based, multi-process computational\nmodel of anticipatory systems called the Phase Web. It describes a\nself-organizing paradigm that explicitly recognizes and exploits the existence\nof a boundary between inside and outside, accepts and exploits intentionality,\nand uses explicit self-reference to describe eg. auto-poiesis. The model\nexplicitly connects computation to a discrete Clifford algebraic formalization\nthat is in turn extended into homology and co-homology, wherein the recursive\nnature of objects and boundaries becomes apparent and itself subject to\nhierarchical recursion. Topsy, a computer program embodying the Phase Web, is\navailable at www.cs.auc.dk/topsy."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9810019v1", 
    "other_authors": "Robert Strom, Guruduth Banavar, Tushar Chandra, Marc Kaplan, Kevan Miller, Bodhi Mukherjee, Daniel Sturman, Michael Ward", 
    "title": "Gryphon: An Information Flow Based Approach to Message Brokering", 
    "arxiv-id": "cs/9810019v1", 
    "author": "Michael Ward", 
    "publish": "1998-10-21T18:43:47Z", 
    "summary": "Gryphon is a distributed computing paradigm for message brokering, which is\nthe transferring of information in the form of streams of events from\ninformation providers to information consumers. This extended abstract outlines\nthe major problems in message brokering and Gryphon's approach to solving them."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9909013v1", 
    "other_authors": "Jaap-Henk Hoepman", 
    "title": "Self-stabilizing mutual exclusion on a ring, even if K=N", 
    "arxiv-id": "cs/9909013v1", 
    "author": "Jaap-Henk Hoepman", 
    "publish": "1999-09-21T13:39:03Z", 
    "summary": "We show that, contrary to common belief, Dijkstra's self-stabilizing mutual\nexclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of\nstates per node is one less than the number of nodes on the ring."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9909015v1", 
    "other_authors": "Francis C. Chu, Joseph Y. Halpern", 
    "title": "A decision-theoretic approach to reliable message delivery", 
    "arxiv-id": "cs/9909015v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "1999-09-21T20:51:37Z", 
    "summary": "We argue that the tools of decision theory need to be taken more seriously in\nthe specification and analysis of systems. We illustrate this by considering a\nsimple problem involving reliable communication, showing how considerations of\nutility and probability can be used to decide when it is worth sending\nheartbeat messages and, if they are sent, how often they should be sent."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0002008v1", 
    "other_authors": "R. Gates, P. Katis, N. Sabadini, R. F. C. Walters", 
    "title": "On Automata with Boundary", 
    "arxiv-id": "cs/0002008v1", 
    "author": "R. F. C. Walters", 
    "publish": "2000-02-16T02:45:39Z", 
    "summary": "We present a theory of automata with boundary for designing, modelling and\nanalysing distributed systems. Notions of behaviour, design and simulation\nappropriate to the theory are defined. The problem of model checking for\ndeadlock detection is discussed, and an algorithm for state space reduction in\nexhaustive search, based on the theory presented here, is described. Three\nexamples of the application of the theory are given, one in the course of the\ndevelopment of the ideas and two as illustrative examples of the use of the\ntheory."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0003054v1", 
    "other_authors": "Adriana Iamnitchi, Ian Foster", 
    "title": "A Problem-Specific Fault-Tolerance Mechanism for Asynchronous,   Distributed Systems", 
    "arxiv-id": "cs/0003054v1", 
    "author": "Ian Foster", 
    "publish": "2000-03-12T00:19:24Z", 
    "summary": "The idle computers on a local area, campus area, or even wide area network\nrepresent a significant computational resource---one that is, however, also\nunreliable, heterogeneous, and opportunistic. This type of resource has been\nused effectively for embarrassingly parallel problems but not for more tightly\ncoupled problems. We describe an algorithm that allows branch-and-bound\nproblems to be solved in such environments. In designing this algorithm, we\nfaced two challenges: (1) scalability, to effectively exploit the variably\nsized pools of resources available, and (2) fault tolerance, to ensure the\nreliability of services. We achieve scalability through a fully decentralized\nalgorithm, by using a membership protocol for managing dynamically available\nresources. However, this fully decentralized design makes achieving reliability\neven more challenging. We guarantee fault tolerance in the sense that the loss\nof up to all but one resource will not affect the quality of the solution. For\npropagating information efficiently, we use epidemic communication for both the\nmembership protocol and the fault-tolerance mechanism. We have developed a\nsimulation framework that allows us to evaluate design alternatives. Results\nobtained in this framework suggest that our techniques can execute scalably and\nreliably."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0004013v1", 
    "other_authors": "Lex Weaver, Andrew Lynes", 
    "title": "Sorting Integers on the AP1000", 
    "arxiv-id": "cs/0004013v1", 
    "author": "Andrew Lynes", 
    "publish": "2000-04-23T04:32:18Z", 
    "summary": "Sorting is one of the classic problems of computer science. Whilst well\nunderstood on sequential machines, the diversity of architectures amongst\nparallel systems means that algorithms do not perform uniformly on all\nplatforms. This document describes the implementation of a radix based\nalgorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer,\nwhich was constructed as an entry in the Joint Symposium on Parallel Processing\n(JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also\ngiven to a full radix sort conducted in parallel across the machine."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0006004v1", 
    "other_authors": "S. A. Mondal", 
    "title": "A Note on \"Optimal Static Load Balancing in Distributed Computer   Systems\"", 
    "arxiv-id": "cs/0006004v1", 
    "author": "S. A. Mondal", 
    "publish": "2000-06-02T03:14:07Z", 
    "summary": "The problem of minimizing mean response time of generic jobs submitted to a\nheterogenous distributed computer systems is considered in this paper. A static\nload balancing strategy, in which decision of redistribution of loads does not\ndepend on the state of the system, is used for this purpose. The article is\nclosely related to a previous article on the same topic. The present article\npoints out number of inconsistencies in the previous article, provides a new\nformulation, and discusses the impact of new findings, based on the improved\nformulation, on the results of the previous article."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0006008v1", 
    "other_authors": "Cynthia Dwork, Joseph Y. Halpern, O. Waarts", 
    "title": "Performing work efficiently in the presence of faults", 
    "arxiv-id": "cs/0006008v1", 
    "author": "O. Waarts", 
    "publish": "2000-06-02T18:35:55Z", 
    "summary": "We consider a system of t synchronous processes that communicate only by\nsending messages to one another, and that together must perform $n$ independent\nunits of work. Processes may fail by crashing; we want to guarantee that in\nevery execution of the protocol in which at least one process survives, all n\nunits of work will be performed. We consider three parameters: the number of\nmessages sent, the total number of units of work performed (including\nmultiplicities), and time. We present three protocols for solving the problem.\nAll three are work-optimal, doing O(n+t) work. The first has moderate costs in\nthe remaining two parameters, sending O(t\\sqrt{t}) messages, and taking O(n+t)\ntime. This protocol can be easily modified to run in any completely\nasynchronous system equipped with a failure detection mechanism. The second\nsends only O(t log{t}) messages, but its running time is large (exponential in\nn and t). The third is essentially time-optimal in the (usual) case in which\nthere are no failures, and its time complexity degrades gracefully as the\nnumber of failures increases."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0007015v1", 
    "other_authors": "Ted Herman", 
    "title": "Phase Clocks for Transient Fault Repair", 
    "arxiv-id": "cs/0007015v1", 
    "author": "Ted Herman", 
    "publish": "2000-07-10T15:59:03Z", 
    "summary": "Phase clocks are synchronization tools that implement a form of logical time\nin distributed systems. For systems tolerating transient faults by self-repair\nof damaged data, phase clocks can enable reasoning about the progress of\ndistributed repair procedures. This paper presents a phase clock algorithm\nsuited to the model of transient memory faults in asynchronous systems with\nread/write registers. The algorithm is self-stabilizing and guarantees accuracy\nof phase clocks within O(k) time following an initial state that is k-faulty.\nComposition theorems show how the algorithm can be used for the timing of\ndistributed procedures that repair system outputs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0009021v1", 
    "other_authors": "Rajkumar Buyya, David Abramson, Jon Giddy", 
    "title": "Nimrod/G: An Architecture of a Resource Management and Scheduling System   in a Global Computational Grid", 
    "arxiv-id": "cs/0009021v1", 
    "author": "Jon Giddy", 
    "publish": "2000-09-22T10:44:16Z", 
    "summary": "The availability of powerful microprocessors and high-speed networks as\ncommodity components has enabled high performance computing on distributed\nsystems (wide-area cluster computing). In this environment, as the resources\nare usually distributed geographically at various levels (department,\nenterprise, or worldwide) there is a great challenge in integrating,\ncoordinating and presenting them as a single resource to the user; thus forming\na computational grid. Another challenge comes from the distributed ownership of\nresources with each resource having its own access policy, cost, and mechanism.\n  The proposed Nimrod/G grid-enabled resource management and scheduling system\nbuilds on our earlier work on Nimrod and follows a modular and component-based\narchitecture enabling extensibility, portability, ease of development, and\ninteroperability of independently developed components. It uses the Globus\ntoolkit services and can be easily extended to operate with any other emerging\ngrid middleware services. It focuses on the management and scheduling of\ncomputations over dynamic resources scattered geographically across the\nInternet at department, enterprise, or global level with particular emphasis on\ndeveloping scheduling schemes based on the concept of computational economy for\na real test bed, namely, the Globus testbed (GUSTO)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0012024v4", 
    "other_authors": "Jeffrey Considine, Leonid A. Levin, David Metcalf", 
    "title": "Byzantine Agreement with Faulty Majority using Bounded Broadcast", 
    "arxiv-id": "cs/0012024v4", 
    "author": "David Metcalf", 
    "publish": "2000-12-26T23:05:58Z", 
    "summary": "Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely\nused building block of reliable distributed protocols. It simulates broadcast\ndespite the presence of faulty parties within the network, traditionally using\nonly private unicast links. Under such conditions, Byzantine Agreement requires\nmore than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed\na Byzantine Agreement protocol for any compliant majority based on an\nadditional primitive allowing transmission to any two parties simultaneously.\nThey proposed a problem of generalizing these results to wider channels and\nfewer compliant parties. We prove that 2f < kh condition is necessary and\nsufficient for implementing broadcast with h compliant and f faulty parties\nusing k-cast channels."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0102016v1", 
    "other_authors": "Jaechun No, Rajeev Thakur, Dinesh Kaushik, Lori Freitag, Alok Choudhary", 
    "title": "A Scientific Data Management System for Irregular Applications", 
    "arxiv-id": "cs/0102016v1", 
    "author": "Alok Choudhary", 
    "publish": "2001-02-20T20:17:22Z", 
    "summary": "Many scientific applications are I/O intensive and generate or access large\ndata sets, spanning hundreds or thousands of \"files.\" Management, storage,\nefficient access, and analysis of this data present an extremely challenging\ntask. We have developed a software system, called Scientific Data Manager\n(SDM), that uses a combination of parallel file I/O and database support for\nhigh-performance scientific data management. SDM provides a high-level API to\nthe user and internally, uses a parallel file system to store real data and a\ndatabase to store application-related metadata. In this paper, we describe how\nwe designed and implemented SDM to support irregular applications. SDM can\nefficiently handle the reading and writing of data in an irregular mesh as well\nas the distribution of index values. We describe the SDM user interface and how\nwe implemented it to achieve high performance. SDM makes extensive use of\nMPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a\nhistory file to optimize the cost of the index distribution using the metadata\nstored in the database. We present performance results with two irregular\napplications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,\non the SGI Origin2000 at Argonne National Laboratory."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0102017v1", 
    "other_authors": "Ralph Butler, William Gropp, Ewing Lusk", 
    "title": "Components and Interfaces of a Process Management System for Parallel   Programs", 
    "arxiv-id": "cs/0102017v1", 
    "author": "Ewing Lusk", 
    "publish": "2001-02-21T16:04:21Z", 
    "summary": "Parallel jobs are different from sequential jobs and require a different type\nof process management. We present here a process management system for parallel\nprograms such as those written using MPI. A primary goal of the system, which\nwe call MPD (for multipurpose daemon), is to be scalable. By this we mean that\nstartup of interactive parallel jobs comprising thousands of processes is\nquick, that signals can be quickly delivered to processes, and that stdin,\nstdout, and stderr are managed intuitively. Our primary target is parallel\nmachines made up of clusters of SMPs, but the system is also useful in more\ntightly integrated environments. We describe how MPD enables much faster\nstartup and better runtime management of parallel jobs. We show how close\ncontrol of stdio can support the easy implementation of a number of convenient\nsystem utilities, even a parallel debugger. We describe a simple but general\ninterface that can be used to separate any process manager from a parallel\nlibrary, which we use to keep MPD separate from MPICH."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0104002v1", 
    "other_authors": "Sudharshan Vazhkudai, Steven Tuecke, Ian Foster", 
    "title": "Replica Selection in the Globus Data Grid", 
    "arxiv-id": "cs/0104002v1", 
    "author": "Ian Foster", 
    "publish": "2001-04-02T18:19:06Z", 
    "summary": "The Globus Data Grid architecture provides a scalable infrastructure for the\nmanagement of storage resources and data that are distributed across Grid\nenvironments. These services are designed to support a variety of scientific\napplications, ranging from high-energy physics to computational genomics, that\nrequire access to large amounts of data (terabytes or even petabytes) with\nvaried quality of service requirements. By layering on a set of core services,\nsuch as data transport, security, and replica cataloging, one can construct\nvarious higher-level services. In this paper, we discuss the design and\nimplementation of a high-level replica selection service that uses information\nregarding replica location and user preferences to guide selection from among\nstorage replica alternatives. We first present a basic replica selection\nservice design, then show how dynamic information collected using Globus\ninformation service capabilities concerning storage system properties can help\nimprove and optimize the selection process. We demonstrate the use of Condor's\nClassAds resource description and matchmaking mechanism as an efficient tool\nfor representing and matching storage resource capabilities and policies\nagainst application requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0105013v1", 
    "other_authors": "Shlomi Dolev, Ted Herman", 
    "title": "Dijkstra's Self-Stabilizing Algorithm in Unsupportive Environments", 
    "arxiv-id": "cs/0105013v1", 
    "author": "Ted Herman", 
    "publish": "2001-05-07T14:29:59Z", 
    "summary": "The first self-stabilizing algorithm [Dij73] assumed the existence of a\ncentral daemon, that activates one processor at time to change state as a\nfunction of its own state and the state of a neighbor. Subsequent research has\nreconsidered this algorithm without the assumption of a central daemon, and\nunder different forms of communication, such as the model of link registers. In\nall of these investigations, one common feature is the atomicity of\ncommunication, whether by shared variables or read/write registers. This paper\nweakens the atomicity assumptions for the communication model, proposing\nversions of [Dij73] that tolerate various weaker forms of atomicity. First, a\nsolution for the case of regular registers is presented. Then the case of safe\nregisters is considered, with both negative and positive results presented. The\npaper also presents an implementation of [Dij73] based on registers that have\nprobabilistically correct behavior, which requires a notion of weak\nstabilization."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0105034v1", 
    "other_authors": "Ronald I. Greenberg, Lee Guan", 
    "title": "On the Area of Hypercube Layouts", 
    "arxiv-id": "cs/0105034v1", 
    "author": "Lee Guan", 
    "publish": "2001-05-29T21:59:18Z", 
    "summary": "This paper precisely analyzes the wire density and required area in standard\nlayout styles for the hypercube. The most natural, regular layout of a\nhypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses\nfloor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of\ntracks per row can be reduced by 1 with a less regular design.) This paper also\ngives a simple formula for the wire density at any cut position and a full\ncharacterization of all places where the wire density is maximized (which does\nnot occur at the bisection)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106020v1", 
    "other_authors": "Rajkumar Buyya, Heinz Stockinger, Jonathan Giddy, David Abrams", 
    "title": "Economic Models for Management of Resources in Grid Computing", 
    "arxiv-id": "cs/0106020v1", 
    "author": "David Abrams", 
    "publish": "2001-06-11T07:46:56Z", 
    "summary": "The accelerated development in Grid and peer-to-peer computing has positioned\nthem as promising next generation computing platforms. They enable the creation\nof Virtual Enterprises (VE) for sharing resources distributed across the world.\nHowever, resource management, application development and usage models in these\nenvironments is a complex undertaking. This is due to the geographic\ndistribution of resources that are owned by different organizations. The\nresource owners of each of these resources have different usage or access\npolicies and cost models, and varying loads and availability. In order to\naddress complex resource management issues, we have proposed a computational\neconomy framework for resource allocation and for regulating supply and demand\nin Grid computing environments. The framework provides mechanisms for\noptimizing resource provider and consumer objective functions through trading\nand brokering services. In a real world market, there exist various economic\nmodels for setting the price for goods based on supply-and-demand and their\nvalue to the user. They include commodity market, posted price, tenders and\nauctions. In this paper, we discuss the use of these models for interaction\nbetween Grid components in deciding resource value and the necessary\ninfrastructure to realize them. In addition to normal services offered by Grid\ncomputing systems, we need an infrastructure to support interaction protocols,\nallocation mechanisms, currency, secure banking, and enforcement services.\nFurthermore, we demonstrate the usage of some of these economic models in\nresource brokering through Nimrod/G deadline and cost-based scheduling for two\ndifferent optimization strategies on the World Wide Grid (WWG) testbed."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106038v1", 
    "other_authors": "David M. Mackie", 
    "title": "Simple and Effective Distributed Computing with a Scheduling Service", 
    "arxiv-id": "cs/0106038v1", 
    "author": "David M. Mackie", 
    "publish": "2001-06-15T17:09:49Z", 
    "summary": "High-throughput computing projects require the solution of large numbers of\nproblems. In many cases, these problems can be solved on desktop PCs, or can be\nbroken down into independent \"PC-solvable\" sub-problems. In such cases, the\nprojects are high-performance computing projects, but only because of the sheer\nnumber of the needed calculations. We briefly describe our efforts to increase\nthe throughput of one such project. We then explain how to easily set up a\ndistributed computing facility composed of standard networked PCs running\nWindows 95, 98, 2000, or NT. The facility requires no special software or\nhardware, involves little or no re-coding of application software, and operates\nalmost invisibly to the owners of the PCs. Depending on the number and quality\nof PCs recruited, performance can rival that of supercomputers."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106056v2", 
    "other_authors": "John Tromp, Paul Vitanyi", 
    "title": "Randomized Two-Process Wait-Free Test-and-Set", 
    "arxiv-id": "cs/0106056v2", 
    "author": "Paul Vitanyi", 
    "publish": "2001-06-28T15:45:35Z", 
    "summary": "We present the first explicit, and currently simplest, randomized algorithm\nfor 2-process wait-free test-and-set. It is implemented with two 4-valued\nsingle writer single reader atomic variables. A test-and-set takes at most 11\nexpected elementary steps, while a reset takes exactly 1 elementary step. Based\non a finite-state analysis, the proofs of correctness and expected length are\ncompressed into one table."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0107034v1", 
    "other_authors": "Elizabeth D. Dolan", 
    "title": "NEOS Server 4.0 Administrative Guide", 
    "arxiv-id": "cs/0107034v1", 
    "author": "Elizabeth D. Dolan", 
    "publish": "2001-07-26T15:19:00Z", 
    "summary": "The NEOS Server 4.0 provides a general Internet-based client/server as a link\nbetween users and software applications. The administrative guide covers the\nfundamental principals behind the operation of the NEOS Server, installation\nand trouble-shooting of the Server software, and implementation details of\npotential interest to a NEOS Server administrator. The guide also discusses\nmaking new software applications available through the Server, including areas\nof concern to remote solver administrators such as maintaining security,\nproviding usage instructions, and enforcing reasonable restrictions on jobs.\nThe administrative guide is intended both as an introduction to the NEOS Server\nand as a reference for use when running the Server."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108001v1", 
    "other_authors": "Gabrielle Allen, David Angulo, Ian Foster, Gerd Lanfermann, Chuang Liu, Thomas Radke, Ed Seidel, John Shalf", 
    "title": "The Cactus Worm: Experiments with Dynamic Resource Discovery and   Allocation in a Grid Environment", 
    "arxiv-id": "cs/0108001v1", 
    "author": "John Shalf", 
    "publish": "2001-08-01T23:14:34Z", 
    "summary": "The ability to harness heterogeneous, dynamically available \"Grid\" resources\nis attractive to typically resource-starved computational scientists and\nengineers, as in principle it can increase, by significant factors, the number\nof cycles that can be delivered to applications. However, new adaptive\napplication structures and dynamic runtime system mechanisms are required if we\nare to operate effectively in Grid environments. In order to explore some of\nthese issues in a practical setting, we are developing an experimental\nframework, called Cactus, that incorporates both adaptive application\nstructures for dealing with changing resource characteristics and adaptive\nresource selection mechanisms that allow applications to change their resource\nallocations (e.g., via migration) when performance falls outside specified\nlimits. We describe here the adaptive resource selection mechanisms and\ndescribe how they are used to achieve automatic application migration to\n\"better\" resources following performance degradation. Our results provide\ninsights into the architectural structures required to support adaptive\nresource selection. In addition, we suggest that this \"Cactus Worm\" is an\ninteresting challenge problem for Grid computing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108002v1", 
    "other_authors": "Sibsankar Haldar, Paul Vitanyi", 
    "title": "Bounded Concurrent Timestamp Systems Using Vector Clocks", 
    "arxiv-id": "cs/0108002v1", 
    "author": "Paul Vitanyi", 
    "publish": "2001-08-02T17:13:48Z", 
    "summary": "Shared registers are basic objects used as communication mediums in\nasynchronous concurrent computation. A concurrent timestamp system is a higher\ntyped communication object, and has been shown to be a powerful tool to solve\nmany concurrency control problems. It has turned out to be possible to\nconstruct such higher typed objects from primitive lower typed ones. The next\nstep is to find efficient constructions. We propose a very efficient wait-free\nconstruction of bounded concurrent timestamp systems from 1-writer multireader\nregisters. This finalizes, corrects, and extends, a preliminary bounded\nmultiwriter construction proposed by the second author in 1986. That work\npartially initiated the current interest in wait-free concurrent objects, and\nintroduced a notion of discrete vector clocks in distributed algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108019v1", 
    "other_authors": "E. Ong, E. Lusk, W. Gropp", 
    "title": "Scalable Unix Commands for Parallel Processors: A High-Performance   Implementation", 
    "arxiv-id": "cs/0108019v1", 
    "author": "W. Gropp", 
    "publish": "2001-08-27T15:54:41Z", 
    "summary": "We describe a family of MPI applications we call the Parallel Unix Commands.\nThese commands are natural parallel versions of common Unix user commands such\nas ls, ps, and find, together with a few similar commands particular to the\nparallel environment. We describe the design and implementation of these\nprograms and present some performance results on a 256-node Linux cluster. The\nParallel Unix Commands are open source and freely available."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0109017v1", 
    "other_authors": "William D. Gropp", 
    "title": "Learning from the Success of MPI", 
    "arxiv-id": "cs/0109017v1", 
    "author": "William D. Gropp", 
    "publish": "2001-09-13T21:14:21Z", 
    "summary": "The Message Passing Interface (MPI) has been extremely successful as a\nportable way to program high-performance parallel computers. This success has\noccurred in spite of the view of many that message passing is difficult and\nthat other approaches, including automatic parallelization and directive-based\nparallelism, are easier to use. This paper argues that MPI has succeeded\nbecause it addresses all of the important issues in providing a parallel\nprogramming model."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0110058v1", 
    "other_authors": "Yi Pan", 
    "title": "Teaching Parallel Programming Using Both High-Level and Low-Level   Languages", 
    "arxiv-id": "cs/0110058v1", 
    "author": "Yi Pan", 
    "publish": "2001-10-29T19:00:15Z", 
    "summary": "We discuss the use of both MPI and OpenMP in the teaching of senior\nundergraduate and junior graduate classes in parallel programming. We briefly\nintroduce the OpenMP standard and discuss why we have chosen to use it in\nparallel programming classes. Advantages of using OpenMP over message passing\nmethods are discussed. We also include a brief enumeration of some of the\ndrawbacks of using OpenMP and how these drawbacks are being addressed by\nsupplementing OpenMP with additional MPI codes and projects. Several projects\ngiven in my class are also described in this paper."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111028v1", 
    "other_authors": "JM. Chaize, A. Goetz, WD. Klotz, J. Meyer, M. Perez, E. Taurel, P. Verdier", 
    "title": "The ESRF TANGO control system status", 
    "arxiv-id": "cs/0111028v1", 
    "author": "P. Verdier", 
    "publish": "2001-11-09T14:29:16Z", 
    "summary": "TANGO is an object oriented control system toolkit based on CORBA presently\nunder development at the ESRF. IN this paper, the TANGO philosophy is briefly\npresented. All the existing tools developed around TANGO will also be\npresented. This include a code genrator, a WEB interface to TANGO objects, an\nadministration tool and an interface to LabView. Finally, an xample of a TANGO\ndevice server for OPC device is given."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111031v1", 
    "other_authors": "Robert W. Carey, Kirby W. Fong, Randy J. Sanchez, Joseph D. Tappero, John P. Woodruff", 
    "title": "Large-Scale Corba-Distributed Software Framework for Nif Controls", 
    "arxiv-id": "cs/0111031v1", 
    "author": "John P. Woodruff", 
    "publish": "2001-11-09T16:40:09Z", 
    "summary": "The Integrated Computer Control System (ICCS) is based on a scalable software\nframework that is distributed over some 325 computers throughout the NIF\nfacility. The framework provides templates and services at multiple levels of\nabstraction for the construction of software applications that communicate via\nCORBA (Common Object Request Broker Architecture). Various forms of\nobject-oriented software design patterns are implemented as templates to be\nextended by application software. Developers extend the framework base classes\nto model the numerous physical control points, thereby sharing the\nfunctionality defined by the base classes. About 56,000 software objects each\nindividually addressed through CORBA are to be created in the complete ICCS.\nMost objects have a persistent state that is initialized at system start-up and\nstored in a database. Additional framework services are provided by centralized\nserver programs that implement events, alerts, reservations, message logging,\ndatabase/file persistence, name services, and process management. The ICCS\nsoftware framework approach allows for efficient construction of a software\nsystem that supports a large number of distributed control points representing\na complex control application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111033v1", 
    "other_authors": "A. Gotz, A. Homs, B. Regad, M. Perez, P. Makijarvi, W. D. Klotz", 
    "title": "Modernising the ESRF control system with GNU/Linux", 
    "arxiv-id": "cs/0111033v1", 
    "author": "W. D. Klotz", 
    "publish": "2001-11-09T20:08:18Z", 
    "summary": "he ESRF control system is in the process of being modernised. The present\ncontrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,\nMotif and C. The new control system will be based on compact PCI, 100 MHz\nEthernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main\nfrontend operating system will be GNU/Linux running on Intel/x86 and\nMotorola/68k. Linux will also be used on handheld devices for mobile control.\nThis poster describes how GNU/Linux is being used to modernise the control\nsystem and what problems have been encountered so far"
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111047v1", 
    "other_authors": "Rajkumar Buyya, Kim Branson, Jon Giddy, David Abramson", 
    "title": "Virtual Laboratory: Enabling On-Demand Drug Design with the World Wide   Grid", 
    "arxiv-id": "cs/0111047v1", 
    "author": "David Abramson", 
    "publish": "2001-11-17T07:20:58Z", 
    "summary": "Computational Grids are emerging as a popular paradigm for solving\nlarge-scale compute and data intensive problems in science, engineering, and\ncommerce. However, application composition, resource management and scheduling\nin these environments is a complex undertaking. In this paper, we illustrate\nthe creation of a virtual laboratory environment by leveraging existing Grid\ntechnologies to enable molecular modeling for drug design on distributed\nresources. It involves screening millions of molecules of chemical compounds\nagainst a protein target, chemical database (CDB) to identify those with\npotential use for drug design. We have grid-enabled the molecular docking\nprocess by composing it as a parameter sweep application using the Nimrod-G\ntools. We then developed new tools for remote access to molecules in CDB small\nmolecule database. The Nimrod-G resource broker along with molecule CDB data\nbroker is used for scheduling and on-demand processing of jobs on distributed\ngrid resources. The results demonstrate the ease of use and suitability of the\nNimrod-G and virtual laboratory tools."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111048v1", 
    "other_authors": "David Abramson, Rajkumar Buyya, Jonathan Giddy", 
    "title": "A Computational Economy for Grid Computing and its Implementation in the   Nimrod-G Resource Brok", 
    "arxiv-id": "cs/0111048v1", 
    "author": "Jonathan Giddy", 
    "publish": "2001-11-17T07:32:44Z", 
    "summary": "Computational Grids, coupling geographically distributed resources such as\nPCs, workstations, clusters, and scientific instruments, have emerged as a next\ngeneration computing platform for solving large-scale problems in science,\nengineering, and commerce. However, application development, resource\nmanagement, and scheduling in these environments continue to be a complex\nundertaking. In this article, we discuss our efforts in developing a resource\nmanagement system for scheduling computations on resources distributed across\nthe world with varying quality of service. Our service-oriented grid computing\nsystem called Nimrod-G manages all operations associated with remote execution\nincluding resource discovery, trading, scheduling based on economic principles\nand a user defined quality of service requirement. The Nimrod-G resource broker\nis implemented by leveraging existing technologies such as Globus, and provides\nnew services that are essential for constructing industrial-strength Grids. We\ndiscuss results of preliminary experiments on scheduling some parametric\ncomputations using the Nimrod-G resource broker on a world-wide grid testbed\nthat spans five continents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0202003v3", 
    "other_authors": "Paul Vitanyi", 
    "title": "Simple Optimal Wait-free Multireader Registers", 
    "arxiv-id": "cs/0202003v3", 
    "author": "Paul Vitanyi", 
    "publish": "2002-02-04T17:29:15Z", 
    "summary": "Multireader shared registers are basic objects used as communication medium\nin asynchronous concurrent computation. We propose a surprisingly simple and\nnatural scheme to obtain several wait-free constructions of bounded 1-writer\nmultireader registers from atomic 1-writer 1-reader registers, that is easier\nto prove correct than any previous construction. Our main construction is the\nfirst symmetric pure timestamp one that is optimal with respect to the\nworst-case local use of control bits; the other one is optimal with respect to\nglobal use of control bits; both are optimal in time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0203019v1", 
    "other_authors": "Rajkumar Buyya, Manzur Murshed", 
    "title": "GridSim: A Toolkit for the Modeling and Simulation of Distributed   Resource Management and Scheduling for Grid Computing", 
    "arxiv-id": "cs/0203019v1", 
    "author": "Manzur Murshed", 
    "publish": "2002-03-14T03:44:18Z", 
    "summary": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular\nparadigms for next generation parallel and distributed computing. The\nmanagement of resources and scheduling of applications in such large-scale\ndistributed systems is a complex undertaking. In order to prove the\neffectiveness of resource brokers and associated scheduling algorithms, their\nperformance needs to be evaluated under different scenarios such as varying\nnumber of resources and users with different requirements. In a grid\nenvironment, it is hard and even impossible to perform scheduler performance\nevaluation in a repeatable and controllable manner as resources and users are\ndistributed across multiple organizations with their own policies. To overcome\nthis limitation, we have developed a Java-based discrete-event grid simulation\ntoolkit called GridSim. The toolkit supports modeling and simulation of\nheterogeneous grid resources (both time- and space-shared), users and\napplication models. It provides primitives for creation of application tasks,\nmapping of tasks to resources, and their management. To demonstrate suitability\nof the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker\nand evaluated the performance of deadline and budget constrained cost- and\ntime-minimization scheduling algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0203020v1", 
    "other_authors": "Rajkumar Buyya, Manzur Murshed", 
    "title": "A Deadline and Budget Constrained Cost-Time Optimisation Algorithm for   Scheduling Task Farming Applications on Global Grids", 
    "arxiv-id": "cs/0203020v1", 
    "author": "Manzur Murshed", 
    "publish": "2002-03-14T03:50:19Z", 
    "summary": "Computational Grids and peer-to-peer (P2P) networks enable the sharing,\nselection, and aggregation of geographically distributed resources for solving\nlarge-scale problems in science, engineering, and commerce. The management and\ncomposition of resources and services for scheduling applications, however,\nbecomes a complex undertaking. We have proposed a computational economy\nframework for regulating the supply and demand for resources and allocating\nthem for applications based on the users quality of services requirements. The\nframework requires economy driven deadline and budget constrained (DBC)\nscheduling algorithms for allocating resources to application jobs in such a\nway that the users requirements are met. In this paper, we propose a new\nscheduling algorithm, called DBC cost-time optimisation, which extends the DBC\ncost-optimisation algorithm to optimise for time, keeping the cost of\ncomputation at the minimum. The superiority of this new scheduling algorithm,\nin achieving lower job completion time, is demonstrated by simulating the\nWorld-Wide Grid and scheduling task-farming applications for different deadline\nand budget scenarios using both this new and the cost optimisation scheduling\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0204048v1", 
    "other_authors": "Rajkumar Buyya", 
    "title": "Economic-based Distributed Resource Management and Scheduling for Grid   Computing", 
    "arxiv-id": "cs/0204048v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2002-04-22T20:06:42Z", 
    "summary": "Computational Grids, emerging as an infrastructure for next generation\ncomputing, enable the sharing, selection, and aggregation of geographically\ndistributed resources for solving large-scale problems in science, engineering,\nand commerce. As the resources in the Grid are heterogeneous and geographically\ndistributed with varying availability and a variety of usage and cost policies\nfor diverse users at different times and, priorities as well as goals that vary\nwith time. The management of resources and application scheduling in such a\nlarge and distributed environment is a complex task. This thesis proposes a\ndistributed computational economy as an effective metaphor for the management\nof resources and application scheduling. It proposes an architectural framework\nthat supports resource trading and quality of services based scheduling. It\nenables the regulation of supply and demand for resources and provides an\nincentive for resource owners for participating in the Grid and motives the\nusers to trade-off between the deadline, budget, and the required level of\nquality of service. The thesis demonstrates the capability of economic-based\nsystems for peer-to-peer distributed computing by developing users'\nquality-of-service requirements driven scheduling strategies and algorithms. It\ndemonstrates their effectiveness by performing scheduling experiments on the\nWorld-Wide Grid for solving parameter sweep applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0205021v1", 
    "other_authors": "A. Waananen, M. Ellert, A. Konstantinov, B. Konya, O. Smirnova", 
    "title": "An Overview of a Grid Architecture for Scientific Computing", 
    "arxiv-id": "cs/0205021v1", 
    "author": "O. Smirnova", 
    "publish": "2002-05-14T19:22:00Z", 
    "summary": "This document gives an overview of a Grid testbed architecture proposal for\nthe NorduGrid project. The aim of the project is to establish an inter-Nordic\ntestbed facility for implementation of wide area computing and data handling.\nThe architecture is supposed to define a Grid system suitable for solving data\nintensive problems at the Large Hadron Collider at CERN. We present the various\narchitecture components needed for such a system. After that we go on to give a\ndescription of the dynamics by showing the task flow."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0205023v2", 
    "other_authors": "M. Ellert, A. Konstantinov, B. Konya, O. Smirnova, A. Waananen", 
    "title": "Performance evaluation of the GridFTP within the NorduGrid project", 
    "arxiv-id": "cs/0205023v2", 
    "author": "A. Waananen", 
    "publish": "2002-05-14T19:55:37Z", 
    "summary": "This report presents results of the tests measuring the performance of\nmulti-threaded file transfers, using the GridFTP implementation of the Globus\nproject over the NorduGrid network resources. Point to point WAN tests, carried\nout between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. It\nwas found that multiple threaded download via the high performance GridFTP\nprotocol can significantly improve file transfer performance, and can serve as\na reliable data"
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0206038v1", 
    "other_authors": "N. T. Karonis, B. de Supinski, I. Foster, W. Gropp, E. Lusk", 
    "title": "A Multilevel Approach to Topology-Aware Collective Operations in   Computational Grids", 
    "arxiv-id": "cs/0206038v1", 
    "author": "E. Lusk", 
    "publish": "2002-06-24T18:28:21Z", 
    "summary": "The efficient implementation of collective communiction operations has\nreceived much attention. Initial efforts produced \"optimal\" trees based on\nnetwork communication models that assumed equal point-to-point latencies\nbetween any two processes. This assumption is violated in most practical\nsettings, however, particularly in heterogeneous systems such as clusters of\nSMPs and wide-area \"computational Grids,\" with the result that collective\noperations perform suboptimally. In response, more recent work has focused on\ncreating topology-aware trees for collective operations that minimize\ncommunication across slower channels (e.g., a wide-area network). While these\nefforts have significant communication benefits, they all limit their view of\nthe network to only two layers. We present a strategy based upon a multilayer\nview of the network. By creating multilevel topology-aware trees we take\nadvantage of communication cost differences at every level in the network. We\nused this strategy to implement topology-aware versions of several MPI\ncollective operations in MPICH-G2, the Globus Toolkit[tm]-enabled version of\nthe popular MPICH implementation of the MPI standard. Using information about\ntopology provided by MPICH-G2, we construct these multilevel topology-aware\ntrees automatically during execution. We present results demonstrating the\nadvantages of our multilevel approach by comparing it to the default\n(topology-unaware) implementation provided by MPICH and a topology-aware\ntwo-layer implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0206040v2", 
    "other_authors": "N. T. Karonis, B. Toonen, I. Foster", 
    "title": "MPICH-G2: A Grid-Enabled Implementation of the Message Passing Interface", 
    "arxiv-id": "cs/0206040v2", 
    "author": "I. Foster", 
    "publish": "2002-06-25T19:55:45Z", 
    "summary": "Application development for distributed computing \"Grids\" can benefit from\ntools that variously hide or enable application-level management of critical\naspects of the heterogeneous environment. As part of an investigation of these\nissues, we have developed MPICH-G2, a Grid-enabled implementation of the\nMessage Passing Interface (MPI) that allows a user to run MPI programs across\nmultiple computers, at the same or different sites, using the same commands\nthat would be used on a parallel computer. This library extends the Argonne\nMPICH implementation of MPI to use services provided by the Globus Toolkit for\nauthentication, authorization, resource allocation, executable staging, and\nI/O, as well as for process creation, monitoring, and control. Various\nperformance-critical operations, including startup and collective operations,\nare configured to exploit network topology information. The library also\nexploits MPI constructs for performance management; for example, the MPI\ncommunicator construct is used for application-level discovery of, and\nadaptation to, both network topology and network quality-of-service mechanisms.\nWe describe the MPICH-G2 design and implementation, present performance\nresults, and review application experiences, including record-setting\ndistributed simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0207036v1", 
    "other_authors": "N. Allen, T. Terriberry", 
    "title": "System Description for a Scalable, Fault-Tolerant, Distributed Garbage   Collector", 
    "arxiv-id": "cs/0207036v1", 
    "author": "T. Terriberry", 
    "publish": "2002-07-10T00:53:23Z", 
    "summary": "We describe an efficient and fault-tolerant algorithm for distributed cyclic\ngarbage collection. The algorithm imposes few requirements on the local\nmachines and allows for flexibility in the choice of local collector and\ndistributed acyclic garbage collector to use with it. We have emphasized\nreducing the number and size of network messages without sacrificing the\npromptness of collection throughout the algorithm. Our proposed collector is a\nvariant of back tracing to avoid extensive synchronization between machines. We\nhave added an explicit forward tracing stage to the standard back tracing stage\nand designed a tuned heuristic to reduce the total amount of work done by the\ncollector. Of particular note is the development of fault-tolerant cooperation\nbetween traces and a heuristic that aggressively reduces the set of suspect\nobjects."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0207096v1", 
    "other_authors": "Avery Ching, Alok Choudhary, Wei-keng Liao, Rob Ross, William Gropp", 
    "title": "Noncontiguous I/O through PVFS", 
    "arxiv-id": "cs/0207096v1", 
    "author": "William Gropp", 
    "publish": "2002-07-29T16:20:25Z", 
    "summary": "With the tremendous advances in processor and memory technology, I/O has\nrisen to become the bottleneck in high-performance computing for many\napplications. The development of parallel file systems has helped to ease the\nperformance gap, but I/O still remains an area needing significant performance\nimprovement. Research has found that noncontiguous I/O access patterns in\nscientific applications combined with current file system methods to perform\nthese accesses lead to unacceptable performance for large data sets. To enhance\nperformance of noncontiguous I/O we have created list I/O, a native version of\nnoncontiguous I/O. We have used the Parallel Virtual File System (PVFS) to\nimplement our ideas. Our research and experimentation shows that list I/O\noutperforms current noncontiguous I/O access methods in most I/O situations and\ncan substantially enhance the performance of real-world scientific\napplications."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0208021v1", 
    "other_authors": "G. A. Kohring", 
    "title": "Implicit Simulations using Messaging Protocols", 
    "arxiv-id": "cs/0208021v1", 
    "author": "G. A. Kohring", 
    "publish": "2002-08-14T09:11:20Z", 
    "summary": "A novel algorithm for performing parallel, distributed computer simulations\non the Internet using IP control messages is introduced. The algorithm employs\ncarefully constructed ICMP packets which enable the required computations to be\ncompleted as part of the standard IP communication protocol. After providing a\ndetailed description of the algorithm, experimental applications in the areas\nof stochastic neural networks and deterministic cellular automata are\ndiscussed. As an example of the algorithms potential power, a simulation of a\ndeterministic cellular automaton involving 10^5 Internet connected devices was\nperformed."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0208027v1", 
    "other_authors": "Robert C. Steinke, Gary J. Nutt", 
    "title": "A Unified Theory of Shared Memory Consistency", 
    "arxiv-id": "cs/0208027v1", 
    "author": "Gary J. Nutt", 
    "publish": "2002-08-19T19:09:04Z", 
    "summary": "Memory consistency models have been developed to specify what values may be\nreturned by a read given that, in a distributed system, memory operations may\nonly be partially ordered. Before this work, consistency models were defined\nindependently. Each model followed a set of rules which was separate from the\nrules of every other model. In our work we have defined a set of four\nconsistency properties. Any subset of the four properties yields a set of rules\nwhich constitute a consistency model. Every consistency model previously\ndescribed in the literature can be defined based on our four properties.\nTherefore, we present these properties as a unfied theory of shared memory\nconsistency."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0210002v1", 
    "other_authors": "Alexander Barmouta Rajkumar Buyya", 
    "title": "GridBank: A Grid Accounting Services Architecture (GASA) for Distributed   Systems Sharing and Integration", 
    "arxiv-id": "cs/0210002v1", 
    "author": "Alexander Barmouta Rajkumar Buyya", 
    "publish": "2002-10-01T10:41:52Z", 
    "summary": "Computational Grids are emerging as new infrastructure for Internet-based\nparallel and distributed computing. They enable the sharing, exchange,\ndiscovery, and aggregation of resources distributed across multiple\nadministrative domains, organizations and enterprises. To accomplish this,\nGrids need infrastructure that supports various services: security, uniform\naccess, resource management, scheduling, application composition, computational\neconomy, and accountability. Many Grid projects have developed technologies\nthat provide many of these services with an exception of accountability. To\novercome this limitation, we propose a new infrastructure called Grid Bank that\nprovides services for accounting. This paper presents requirements of Grid\naccountability and different models within which it can operate and proposes\nGrid Bank Services Architecture that meets them. The paper highlights\nimplementation issues with detailed discussion on format for various\nrecords/database that the GridBank need to maintain. It also presents protocols\nfor interaction between GridBank and various components within Grid computing\nenvironments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0301033v1", 
    "other_authors": "K. Keahey, T. Fredian, Q. Peng, D. P. Schissel, M. Thompson, I. Foster, M. Greenwald, D. McCune", 
    "title": "Computational Grids in Action: The Natinal Fusion Collaboratory", 
    "arxiv-id": "cs/0301033v1", 
    "author": "D. McCune", 
    "publish": "2003-01-28T20:11:16Z", 
    "summary": "The National Fusion Collaboratory (NFC) project was created to advance\nscientific understanding and innovation in magnetic fusion research by enabling\nmore efficient use of existing experimental facilities through more effective\nintegration of experiment, theory, and modeling. To achieve this objective, NFC\nintroduced the concept of \"network services\", which build on top of\ncomputational Grids, and provide Fusion codes, together with their maintenance\nand hardware resources as a service to the community. This mode of operation\nrequires the development of new authorization and enforcement capabilities. In\naddition, the nature of Fusion experiments places strident quality of service\nrequirements on codes run during the experimental cycle. In this paper, we\ndescribe Grid computing requirements of the Fusion community, and present our\nfirst experiments in meeting those requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0301035v1", 
    "other_authors": "Alex Brodsky, Jan B. Pedersen, Alan Wagner", 
    "title": "On the Complexity of Buffer Allocation in Message Passing Systems", 
    "arxiv-id": "cs/0301035v1", 
    "author": "Alan Wagner", 
    "publish": "2003-01-31T01:21:21Z", 
    "summary": "Message passing programs commonly use buffers to avoid unnecessary\nsynchronizations and to improve performance by overlapping communication with\ncomputation. Unfortunately, using buffers makes the program no longer portable,\npotentially unable to complete on systems without a sufficient number of\nbuffers. Effective buffer use entails that the minimum number needed for a safe\nexecution be allocated.\n  We explore a variety of problems related to buffer allocation for safe and\nefficient execution of message passing programs. We show that determining the\nminimum number of buffers or verifying a buffer assignment are intractable\nproblems. However, we give a polynomial time algorithm to determine the minimum\nnumber of buffers needed to allow for asynchronous execution. We extend these\nresults to several different buffering schemes, which in some cases make the\nproblems tractable."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302006v1", 
    "other_authors": "Jia Yu, Srikumar Venugopal, Rajkumar Buyya", 
    "title": "Grid Market Directory: A Web Services based Grid Service Publication   Directory", 
    "arxiv-id": "cs/0302006v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T03:31:02Z", 
    "summary": "As Grids are emerging as the next generation service-oriented computing\nplatforms, they need to support Grid economy that helps in the management of\nsupply and demand for resources and offers an economic incentive for Grid\nresource providers. To enable this Grid economy, a market-like Grid environment\nincluding an infrastructure that supports the publication of services and their\ndiscovery is needed. As part of the Gridbus project, we proposed and have\ndeveloped a Grid Market Directory (GMD) that serves as a registry for\nhigh-level service publication and discovery in Virtual Organisations."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302007v1", 
    "other_authors": "Martin Placek, Rajkumar Buyya", 
    "title": "G-Monitor: Gridbus web portal for monitoring and steering application   execution on global grids", 
    "arxiv-id": "cs/0302007v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T03:36:29Z", 
    "summary": "Grids are experiencing a rapid growth in their application and along with\nthis there is a requirement for a portal which is easy to use and scalable. We\nhave responded to this requirement by developing an easy to use, scalable,\nweb-based portal called G-Monitor. This paper proposes a generic architecture\nfor a web portal into a grid environment and discusses our implementation and\nits application."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302008v1", 
    "other_authors": "Shoib Burq, Steve Melnikoff, Kim Branson, Rajkumar Buyya", 
    "title": "Visual Environment for Rapid Composition of Parameter-Sweep Applications   for Distributed Processing on Global Grids", 
    "arxiv-id": "cs/0302008v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T04:06:02Z", 
    "summary": "Computational Grids are emerging as a platform for next-generation parallel\nand distributed computing. Large-scale parametric studies and parameter sweep\napplications find a natural place in the Grid?s distribution model. There is\nlittle or no communication between jobs. The task of parallelizing and\ndistributing existing applications is conceptually trivial. These properties of\nparametric studies make it an ideal place to start developing integrated\ndevelopment environments (IDEs) for rapidly Grid-enabling applications.\nHowever, the availability of IDEs for scientists to Grid-enable their\napplications, without the need of developing them as parallel applications\nexplicitly, is still lacking. This paper presents a Java based IDE called\nVisual Parametric Tool (VPT), developed as part of the Gridbus project, for\nrapid creation of parameter sweep (data parallel/SPMD) applications. It\nsupports automatic creation of parameter script and parameterisation of the\ninput data files, which is compatible with the Nimrod-G parameter specification\nlanguage. The usefulness of VPT is demonstrated by a case study on composition\nof molecular docking application as a parameter sweep application. Such\napplications can be deployed on clusters using the Nimrod/enFuzion system and\non global Grids using the Nimrod-G grid resource broker."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302018v1", 
    "other_authors": "Ding Choon Hoong, Rajkumar Buyya", 
    "title": "Guided Google: A Meta Search Engine and its Implementation using the   Google Distributed Web Services", 
    "arxiv-id": "cs/0302018v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-13T04:49:26Z", 
    "summary": "With the advent of the Internet, search engines have begun sprouting like\nmushrooms after a rainfall. Only in recent years, have developers become more\ninnovative, and came up with guided searching facilities online. The goals of\nthese applications are to help ease and guide the searching efforts of a novice\nweb user toward their desired objectives. A number of implementations of such\nservices are emerging. This paper proposes a guided meta-search engine, called\n\"Guided Google\", as it serves as an interface to the actual Google.com search\nengine, using the Google Web Services."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302019v1", 
    "other_authors": "R. Buyya, S. Date, Y. Mizuno-Matsumoto, S. Venugopal, D. Abramson", 
    "title": "Economic and On Demand Brain Activity Analysis on Global Grids", 
    "arxiv-id": "cs/0302019v1", 
    "author": "D. Abramson", 
    "publish": "2003-02-13T04:53:10Z", 
    "summary": "The lack of computational power within an organization for analyzing\nscientific data, and the distribution of knowledge (by scientists) and\ntechnologies (advanced scientific devices) are two major problems commonly\nobserved in scientific disciplines. One such scientific discipline is brain\nscience. The analysis of brain activity data gathered from the MEG\n(Magnetoencephalography) instrument is an important research topic in medical\nscience since it helps doctors in identifying symptoms of diseases. The data\nneeds to be analyzed exhaustively to efficiently diagnose and analyze brain\nfunctions and requires access to large-scale computational resources. The\npotential platform for solving such resource intensive applications is the\nGrid. This paper describes a MEG data analysis system developed by us,\nleveraging Grid technologies, primarily Nimrod-G, Gridbus, and Globus. This\npaper explains the application of economy-based grid scheduling algorithms to\nthe problem domain for on-demand processing of analysis jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0303005v1", 
    "other_authors": "H. Ballhausen", 
    "title": "Fair Solution to the Reader-Writer-Problem with Semaphores only", 
    "arxiv-id": "cs/0303005v1", 
    "author": "H. Ballhausen", 
    "publish": "2003-03-08T17:15:00Z", 
    "summary": "The reader-writer-problem is a standard problem in concurrent programming. A\nresource is shared by several processes which need either inclusive reading or\nexclusive writing access. The known solutions to this problem typically involve\na number of global counters and queues. Here a very simple algorithm is\npresented which needs only two semaphores for synchronisation and no other\nglobal objects. The approach yields a fair solution without starving."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0304037v1", 
    "other_authors": "Sudharshan Vazhkudai, Jennifer M. Schopf", 
    "title": "Using Regression Techniques to Predict Large Data Transfers", 
    "arxiv-id": "cs/0304037v1", 
    "author": "Jennifer M. Schopf", 
    "publish": "2003-04-23T20:36:09Z", 
    "summary": "The recent proliferation of Data Grids and the increasingly common practice\nof using resources as distributed data stores provide a convenient environment\nfor communities of researchers to share, replicate, and manage access to copies\nof large datasets. This has led to the question of which replica can be\naccessed most efficiently. In such environments, fetching data from one of the\nseveral replica locations requires accurate predictions of end-to-end transfer\ntimes. The answer to this question can depend on many factors, including\nphysical characteristics of the resources and the load behavior on the CPUs,\nnetworks, and storage devices that are part of the end-to-end data path linking\npossible sources and sinks. Our approach combines end-to-end application\nthroughput observations with network and disk load variations and captures\nwhole-system performance and variations in load patterns. Our predictions\ncharacterize the effect of load variations of several shared devices (network\nand disk) on file transfer times. We develop a suite of univariate and\nmultivariate predictors that can use multiple data sources to improve the\naccuracy of the predictions as well as address Data Grid variations\n(availability of data and sporadic nature of transfers). We ran a large set of\ndata transfer experiments using GridFTP and observed performance predictions\nwithin 15% error for our testbed sites, which is quite promising for a\npragmatic system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305050v2", 
    "other_authors": "Olof Barring", 
    "title": "Towards automation of computing fabrics using tools from the fabric   management workpackage of the EU DataGrid project", 
    "arxiv-id": "cs/0305050v2", 
    "author": "Olof Barring", 
    "publish": "2003-05-28T16:25:45Z", 
    "summary": "The EU DataGrid project workpackage 4 has as an objective to provide the\nnecessary tools for automating the management of medium size to very large\ncomputing fabrics. At the end of the second project year subsystems for\ncentralized configuration management (presented at LISA'02) and\nperformance/exception monitoring have been delivered. This will soon be\naugmented with a subsystem for node installation and service configuration,\nwhich is based on existing widely used standards where available (e.g. rpm,\nkickstart, init.d scripts) and clean interfaces to OS dependent components\n(e.g. base installation and service management). The three subsystems together\nallow for centralized management of very large computer farms. Finally, a fault\ntolerance system is being developed for tying together the above subsystems to\nform a complete framework for automated enterprise computing management by\n3Q03. All software developed is open source covered by the EU DataGrid project\nlicense agreements. This article describes the architecture behind the designed\nfabric management system and the status of the different developments. It also\ncovers the experience with an existing tool for automated configuration and\ninstallation that have been adapted and used from the beginning to manage the\nEU DataGrid testbed, which is now used for LHC data challenges."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305058v2", 
    "other_authors": "V. Ciaschini, F. Donno, A. Fanfani, F. Fanzago, V. Garbellotto, M. Verlato, L. Vaccarossa", 
    "title": "ATLAS and CMS applications on the WorldGrid testbed", 
    "arxiv-id": "cs/0305058v2", 
    "author": "L. Vaccarossa", 
    "publish": "2003-05-30T09:18:55Z", 
    "summary": "WorldGrid is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. It has been developed in the context of the DataTAG and iVDGL\nprojects, and successfully demonstrated during the WorldGrid demos at IST2002\n(Copenhagen) and SC2002 (Baltimore). Two HEP experiments, ATLAS and CMS,\nsuccessful exploited the WorldGrid testbed for executing jobs simulating the\nresponse of their detectors to physics eve nts produced by real collisions\nexpected at the LHC accelerator starting from 2007. This data intensive\nactivity has been run since many years on local dedicated computing farms\nconsisting of hundreds of nodes and Terabytes of disk and tape storage. Within\nthe WorldGrid testbed, for the first time HEP simulation jobs were submitted\nand run indifferently on US and European resources, despite of their underlying\ndifferent Grid implementations, and produced data which could be retrieved and\nfurther analysed on the submitting machine, or simply stored on the remote\nresources and registered on a Replica Catalogue which made them available to\nthe Grid for further processing. In this contribution we describe the job\nsubmission from Europe for both ATLAS and CMS applications, performed through\nthe GENIUS portal operating on top of an EDG User Interface submitting to an\nEDG Resource Broker, pointing out the chosen interoperability solutions which\nmade US and European resources equivalent from the applications point of view,\nthe data management in the WorldGrid environment, and the CMS specific\nproduction tools which were interfaced to the GENIUS portal."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305059v1", 
    "other_authors": "E. Leonardi, M. W. Schulz", 
    "title": "EU DataGRID testbed management and support at CERN", 
    "arxiv-id": "cs/0305059v1", 
    "author": "M. W. Schulz", 
    "publish": "2003-05-30T10:07:12Z", 
    "summary": "In this paper we report on the first two years of running the CERN testbed\nsite for the EU DataGRID project. The site consists of about 120 dual-processor\nPCs distributed over several testbeds used for different purposes: software\ndevelopment, system integration, and application tests. Activities at the site\nincluded test productions of MonteCarlo data for LHC experiments, tutorials and\ndemonstrations of GRID technologies, and support for individual users analysis.\nThis paper focuses on node installation and configuration techniques, service\nmanagement, user support in a gridified environment, and includes\nconsiderations on scalability and security issues and comparisons with\n\"traditional\" production systems, as seen from the administrator point of view."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305061v1", 
    "other_authors": "Andras Horvath, Emanuele Leonardi, Markus Schulz", 
    "title": "A Secure Infrastructure For System Console and Reset Access", 
    "arxiv-id": "cs/0305061v1", 
    "author": "Markus Schulz", 
    "publish": "2003-05-30T11:42:37Z", 
    "summary": "During the last years large farms have been built using commodity hardware.\nThis hardware lacks components for remote and automated administration.\nProducts that can be retrofitted to these systems are either costly or\ninherently insecure. We present a system based on serial ports and simple\nmachine controlled relays. We report on experience gained by setting up a\n50-machine test environment as well as current work in progress in the area."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305062v2", 
    "other_authors": "Aamir Shafi, Umer Farooq, Saad Kiani, Maria Riaz, Anjum Shehzad, Arshad Ali, Iosif Legrand, Harvey Newman", 
    "title": "DIAMOnDS - DIstributed Agents for MObile & Dynamic Services", 
    "arxiv-id": "cs/0305062v2", 
    "author": "Harvey Newman", 
    "publish": "2003-05-30T11:48:57Z", 
    "summary": "Distributed Services Architecture with support for mobile agents between\nservices, offer significantly improved communication and computational\nflexibility. The uses of agents allow execution of complex operations that\ninvolve large amounts of data to be processed effectively using distributed\nresources. The prototype system Distributed Agents for Mobile and Dynamic\nServices (DIAMOnDS), allows a service to send agents on its behalf, to other\nservices, to perform data manipulation and processing. Agents have been\nimplemented as mobile services that are discovered using the Jini Lookup\nmechanism and used by other services for task management and communication.\nAgents provide proxies for interaction with other services as well as specific\nGUI to monitor and control the agent activity. Thus agents acting on behalf of\none service cooperate with other services to carry out a job, providing\ninter-operation of loosely coupled services in a semi-autonomous way. Remote\nfile system access functionality has been incorporated by the agent framework\nand allows services to dynamically share and browse the file system resources\nof hosts, running the services. Generic database access functionality has been\nimplemented in the mobile agent framework that allows performing complex data\nmining and processing operations efficiently in distributed system. A basic\ndata searching agent is also implemented that performs a query based search in\na file system. The testing of the framework was carried out on WAN by moving\nConnectivity Test agents between AgentStations in CERN, Switzerland and NUST,\nPakistan."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305063v2", 
    "other_authors": "Gregory E. Graham, Dave Evans, Iain Bertram", 
    "title": "McRunjob: A High Energy Physics Workflow Planner for Grid Production   Processing", 
    "arxiv-id": "cs/0305063v2", 
    "author": "Iain Bertram", 
    "publish": "2003-05-30T19:53:14Z", 
    "summary": "McRunjob is a powerful grid workflow manager used to manage the generation of\nlarge numbers of production processing jobs in High Energy Physics. In use at\nboth the DZero and CMS experiments, McRunjob has been used to manage large\nMonte Carlo production processing since 1999 and is being extended to uses in\nregular production processing for analysis and reconstruction. Described at\nCHEP 2001, McRunjob converts core metadata into jobs submittable in a variety\nof environments. The powerful core metadata description language includes\nmethods for converting the metadata into persistent forms, job descriptions,\nmulti-step workflows, and data provenance information. The language features\nallow for structure in the metadata by including full expressions, namespaces,\nfunctional dependencies, site specific parameters in a grid environment, and\nontological definitions. It also has simple control structures for\nparallelization of large jobs. McRunjob features a modular design which allows\nfor easy expansion to new job description languages or new application level\ntasks."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305065v1", 
    "other_authors": "James A. Hamilton, Gregory P. Dubois-Felsmann, Rainer Bartoldus", 
    "title": "A Generic Multi-node State Monitoring Subsystem", 
    "arxiv-id": "cs/0305065v1", 
    "author": "Rainer Bartoldus", 
    "publish": "2003-05-30T17:43:03Z", 
    "summary": "The BaBar online data acquisition (DAQ) system includes approximately fifty\nUnix systems that collectively implement the level-three trigger. These systems\nall run the same code. Each of these systems has its own state, and this state\nis expected to change in response to changes in the overall DAQ system. A\nspecialized subsystem has been developed to initiate processing on this\ncollection of systems, and to monitor them both for error conditions and to\nensure that they all follow the same state trajectory within a specifiable\nperiod of time. This subsystem receives start commands from the main DAQ run\ncontrol system, and reports major coherent state changes, as well as error\nconditions, back to the run control system. This state monitoring subsystem has\nthe novel feature that it does not know anything about the state machines that\nit is monitoring, and hence does not introduce any fundamentally new state\nmachine into the overall system. This feature makes it trivially applicable to\nother multi-node subsystems. Indeed it has already found a second application\nbeyond the level-three trigger, within the BaBar experiment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305066v2", 
    "other_authors": "Gregory E. Graham, M. Anzar Afaq, Shafqat Aziz, L. A. T. Bauerdick, Michael Ernst, Joseph Kaiser, Natalia Ratnikova, Hans Wenzel, Yujun Wu, Erik Aslakson, Julian Bunn, Saima Iqbal, Iosif Legrand, Harvey Newman, Suresh Singh, Conrad Steenberg, James Branson, Ian Fisk, James Letts, Adam Arbree, Paul Avery, Dimitri Bourilkov, Richard Cavanaugh, Jorge Rodriguez, Suchindra Kategari, Peter Couvares, Alan DeSmet, Miron Livny, Alain Roy, Todd Tannenbaum", 
    "title": "The CMS Integration Grid Testbed", 
    "arxiv-id": "cs/0305066v2", 
    "author": "Todd Tannenbaum", 
    "publish": "2003-05-30T19:45:44Z", 
    "summary": "The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2\nhardware at the following sites: the California Institute of Technology, Fermi\nNational Accelerator Laboratory, the University of California at San Diego, and\nthe University of Florida at Gainesville. The IGT runs jobs using the Globus\nToolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is\nmanaged using VO management scripts from the European Data Grid (EDG). Gridwide\nmonitoring is accomplished using local tools such as Ganglia interfaced into\nthe Globus Metadata Directory Service (MDS) and the agent based Mona Lisa.\nDomain specific software is packaged and installed using the Distrib ution\nAfter Release (DAR) tool of CMS, while middleware under the auspices of the\nVirtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us\ntwo month span in Fall of 2002, over 1 million official CMS GEANT based Monte\nCarlo events were generated and returned to CERN for analysis while being\ndemonstrated at SC2002. In this paper, we describe the process that led to one\nof the world's first continuously available, functioning grids."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306001v2", 
    "other_authors": "Conrad D. Steenberg, Eric Aslakson, Julian J. Bunn, Harvey B. Newman, Michael Thomas, Frank van Lingen", 
    "title": "Clarens Client and Server Applications", 
    "arxiv-id": "cs/0306001v2", 
    "author": "Frank van Lingen", 
    "publish": "2003-05-30T20:25:50Z", 
    "summary": "Several applications have been implemented with access via the Clarens web\nservice infrastructure, including virtual organization management, JetMET\nphysics data analysis using relational databases, and Storage Resource Broker\n(SRB) access. This functionality is accessible transparently from Python\nscripts, the Root analysis framework and from Java applications and browser\napplets."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306002v2", 
    "other_authors": "Conrad D. Steenberg, Eric Aslakson, Julian J. Bunn, Harvey B. Newman, Michael Thomas, Frank van Lingen", 
    "title": "The Clarens web services architecture", 
    "arxiv-id": "cs/0306002v2", 
    "author": "Frank van Lingen", 
    "publish": "2003-05-30T20:34:05Z", 
    "summary": "Clarens is a uniquely flexible web services infrastructure providing a\nunified access protocol to a diverse set of functions useful to the HEP\ncommunity. It uses the standard HTTP protocol combined with application layer,\ncertificate based authentication to provide single sign-on to individuals,\norganizations and hosts, with fine-grained access control to services, files\nand virtual organization (VO) management. This contribution describes the\nserver functionality, while client applications are described in a subsequent\ntalk."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306003v2", 
    "other_authors": "Rob Byrom, Brian Coghlan, Andrew W Cooke, Roney Cordenonsi, Linda Cornwall, Ari Datta, Abdeslem Djaoui, Laurence Field, Steve Fisher, Steve Hicks, Stuart Kenny, James Magowan, Werner Nutt, David O'Callaghan, Manfred Oevers, Norbert Podhorszki, John Ryan, Manish Soni, Paul Taylor, Antony J. Wilson, Xiaomei Zhu", 
    "title": "R-GMA: First results after deployment", 
    "arxiv-id": "cs/0306003v2", 
    "author": "Xiaomei Zhu", 
    "publish": "2003-05-30T20:39:27Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which is being\ndeveloped within the European DataGrid Project as an Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each VO had one large relational database. We provide a number of different\nProducer types with different characteristics; for example some support\nstreaming of information. We also provide combined Consumer/Producers, which\nare able to combine information and republish it. At the heart of the system is\nthe mediator, which for any query is able to find and connect to the best\nProducers to do the job. We are able to invoke MDS info-provider scripts and\npublish the resulting information via R-GMA in addition to having some of our\nown sensors. APIs are available which allow the user to deploy monitoring and\ninformation services for any application that may be needed in the future. We\nhave used it both for information about the grid (primarily to find what\nservices are available at any one time) and for application monitoring. R-GMA\nhas been deployed in Grid testbeds, we describe the results and experiences of\nthis deployment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306004v2", 
    "other_authors": "R. Alfieri, R. Cecchini, V. Ciaschini, L. dell'Agnello, A. Gianoli, F. Spataro, F. Bonnassieux, P. Broadfoot, G. Lowe, L. Cornwall, J. Jensen, D. Kelsey, A. Frohner, D. L. Groep, W. Som de Cerff, M. Steenbakkers, G. Venekamp, D. Kouril, A. McNab, O. Mulmo, M. Silander, J. Hahkala, K. Lhorentey", 
    "title": "Managing Dynamic User Communities in a Grid of Autonomous Resources", 
    "arxiv-id": "cs/0306004v2", 
    "author": "K. Lhorentey", 
    "publish": "2003-05-30T21:04:49Z", 
    "summary": "One of the fundamental concepts in Grid computing is the creation of Virtual\nOrganizations (VO's): a set of resource consumers and providers that join\nforces to solve a common problem. Typical examples of Virtual Organizations\ninclude collaborations formed around the Large Hadron Collider (LHC)\nexperiments. To date, Grid computing has been applied on a relatively small\nscale, linking dozens of users to a dozen resources, and management of these\nVO's was a largely manual operation. With the advance of large collaboration,\nlinking more than 10000 users with a 1000 sites in 150 counties, a\ncomprehensive, automated management system is required. It should be simple\nenough not to deter users, while at the same time ensuring local site autonomy.\nThe VO Management Service (VOMS), developed by the EU DataGrid and DataTAG\nprojects[1, 2], is a secured system for managing authorization for users and\nresources in virtual organizations. It extends the existing Grid Security\nInfrastructure[3] architecture with embedded VO affiliation assertions that can\nbe independently verified by all VO members and resource providers. Within the\nEU DataGrid project, Grid services for job submission, file- and database\naccess are being equipped with fine- grained authorization systems that take VO\nmembership into account. These also give resource owners the ability to ensure\nsite security and enforce local access policies. This paper will describe the\nEU DataGrid security architecture, the VO membership service and the local site\nenforcement mechanisms Local Centre Authorization Service (LCAS), Local\nCredential Mapping Service(LCMAPS) and the Java Trust and Authorization\nManager."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306007v1", 
    "other_authors": "G. Avellino, S. Beco, B. Cantalupo, F. Pacini, A. Terracina, A. Maraschini, D. Colling, S. Monforte, M. Pappalardo, L. Salconi, F. Giacomini, E. Ronchieri, D. Kouril, A. Krenek, L. Matyska, M. Mulac, J. Pospisil, M. Ruda, Z. Salvet, J. Sitera, M. Vocu, M. Mezzadri, F. Prelz, A. Gianelle, R. Peluso, M. Sgaravatto, S. Barale, A. Guarise, A. Werbrouck", 
    "title": "The first deployment of workload management services on the EU DataGrid   Testbed: feedback on design and implementation", 
    "arxiv-id": "cs/0306007v1", 
    "author": "A. Werbrouck", 
    "publish": "2003-05-31T19:39:54Z", 
    "summary": "Application users have now been experiencing for about a year with the\nstandardized resource brokering services provided by the 'workload management'\npackage of the EU DataGrid project (WP1). Understanding, shaping and pushing\nthe limits of the system has provided valuable feedback on both its design and\nimplementation. A digest of the lessons, and \"better practices\", that were\nlearned, and that were applied towards the second major release of the\nsoftware, is given."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306008v3", 
    "other_authors": "A. Ceseracciu, M. Piemontese, F. Safai Tehrani, P. Elmer, D. Johnson, T. M. Pulliam", 
    "title": "The new BaBar Data Reconstruction Control System", 
    "arxiv-id": "cs/0306008v3", 
    "author": "T. M. Pulliam", 
    "publish": "2003-05-31T20:48:53Z", 
    "summary": "The BaBar experiment is characterized by extremely high luminosity, a complex\ndetector, and a huge data volume, with increasing requirements each year. To\nfulfill these requirements a new control system has been designed and developed\nfor the offline data reconstruction system. The new control system described in\nthis paper provides the performance and flexibility needed to manage a large\nnumber of small computing farms, and takes full benefit of OO design. The\ninfrastructure is well isolated from the processing layer, it is generic and\nflexible, based on a light framework providing message passing and cooperative\nmultitasking. The system is actively distributed, enforces the separation\nbetween different processing tiers by using different naming domains, and glues\nthem together by dedicated brokers. It provides a powerful Finite State Machine\nframework to describe custom processing models in a simple regular language.\nThis paper describes this new control system, currently in use at SLAC and\nPadova on ~450 CPUs organized in 12 farms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306011v1", 
    "other_authors": "Heinz Stockinger, Flavia Donno, Erwin Laure, Shahzad Muzaffar, Peter Kunszt, Giuseppe Andronico, Paul Millar", 
    "title": "Grid Data Management in Action: Experience in Running and Supporting   Data Management Services in the EU DataGrid Project", 
    "arxiv-id": "cs/0306011v1", 
    "author": "Paul Millar", 
    "publish": "2003-06-02T08:50:48Z", 
    "summary": "In the first phase of the EU DataGrid (EDG) project, a Data Management System\nhas been implemented and provided for deployment. The components of the current\nEDG Testbed are: a prototype of a Replica Manager Service built around the\nbasic services provided by Globus, a centralised Replica Catalogue to store\ninformation about physical locations of files, and the Grid Data Mirroring\nPackage (GDMP) that is widely used in various HEP collaborations in Europe and\nthe US for data mirroring. During this year these services have been refined\nand made more robust so that they are fit to be used in a pre-production\nenvironment. Application users have been using this first release of the Data\nManagement Services for more than a year. In the paper we present the\ncomponents and their interaction, our implementation and experience as well as\nthe feedback received from our user communities. We have resolved not only\nissues regarding integration with other EDG service components but also many of\nthe interoperability issues with components of our partner projects in Europe\nand the U.S. The paper concludes with the basic lessons learned during this\noperation. These conclusions provide the motivation for the architecture of the\nnext generation of Data Management Services that will be deployed in EDG during\n2003."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306018v1", 
    "other_authors": "S. Andreozzi, S. Fantinel, D. Rebatto, L. Vaccarossa, G. Tortone", 
    "title": "A monitoring tool for a GRID operation center", 
    "arxiv-id": "cs/0306018v1", 
    "author": "G. Tortone", 
    "publish": "2003-06-03T17:22:07Z", 
    "summary": "WorldGRID is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. The WorldGRID testbed has been successfully demonstrated during the\nWorldGRID demos at SuperComputing 2002 (Baltimore) and IST2002 (Copenhagen)\nwhere real HEP application jobs were transparently submitted from US and Europe\nusing \"native\" mechanisms and run where resources were available, independently\nof their location. To monitor the behavior and performance of such testbed and\nspot problems as soon as they arise, DataTAG has developed the EDT-Monitor tool\nbased on the Nagios package that allows for Virtual Organization centric views\nof the Grid through dynamic geographical maps. The tool has been used to spot\nseveral problems during the WorldGRID operations, such as malfunctioning\nResource Brokers or Information Servers, sites not correctly configured, job\ndispatching problems, etc. In this paper we give an overview of the package,\nits features and scalability solutions and we report on the experience acquired\nand the benefit that a GRID operation center would gain from such a tool."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306027v1", 
    "other_authors": "I. Augustin, F. Carminati, J. Closier, E. van Herwijnen, J. J. Blaising, D. Boutigny, C. Charlot, V. Garonne, A. Tsaregorodtsev, K. Bos, J. Templon, P. Capiluppi, A. Fanfani, R. Barbera, G. Negri, L. Perini, S. Resconi, M. Sitta, M. Reale, D. Vicinanza, S. Bagnasco, P. Cerello, A. Sciaba, O. Smirnova, D. Colling, F. Harris, S. Burke", 
    "title": "HEP Applications Evaluation of the EDG Testbed and Middleware", 
    "arxiv-id": "cs/0306027v1", 
    "author": "S. Burke", 
    "publish": "2003-06-05T15:35:20Z", 
    "summary": "Workpackage 8 of the European Datagrid project was formed in January 2001\nwith representatives from the four LHC experiments, and with experiment\nindependent people from five of the six main EDG partners. In September 2002\nWP8 was strengthened by the addition of effort from BaBar and D0. The original\nmandate of WP8 was, following the definition of short- and long-term\nrequirements, to port experiment software to the EDG middleware and testbed\nenvironment. A major additional activity has been testing the basic\nfunctionality and performance of this environment. This paper reviews\nexperiences and evaluations in the areas of job submission, data management,\nmass storage handling, information systems and monitoring. It also comments on\nthe problems of remote debugging, the portability of code, and scaling problems\nwith increasing numbers of jobs, sites and nodes. Reference is made to the\npioneeering work of Atlas and CMS in integrating the use of the EDG Testbed\ninto their data challenges. A forward look is made to essential software\ndevelopments within EDG and to the necessary cooperation between EDG and LCG\nfor the LCG prototype due in mid 2003."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306029v1", 
    "other_authors": "Timm M. Steinbeck, Volker Lindenstruth, Heinz Tilsner", 
    "title": "A Software Data Transport Framework for Trigger Applications on Clusters", 
    "arxiv-id": "cs/0306029v1", 
    "author": "Heinz Tilsner", 
    "publish": "2003-06-06T07:02:25Z", 
    "summary": "In the future ALICE heavy ion experiment at CERN's Large Hadron Collider\ninput data rates of up to 25 GB/s have to be handled by the High Level Trigger\n(HLT) system, which has to scale them down to at most 1.25 GB/s before being\nwritten to permanent storage. The HLT system that is being designed to cope\nwith these data rates consists of a large PC cluster, up to the order of a 1000\nnodes, connected by a fast network. For the software that will run on these\nnodes a flexible data transport and distribution software framework has been\ndeveloped. This framework consists of a set of separate components, that can be\nconnected via a common interface, allowing to construct different\nconfigurations for the HLT, that are even changeable at runtime. To ensure a\nfault-tolerant operation of the HLT, the framework includes a basic fail-over\nmechanism that will be further expanded in the future, utilizing the runtime\nreconnection feature of the framework's component interface. First performance\ntests show very promising results for the software, indicating that it can\nachieve an event rate for the data transport sufficiently high to satisfy\nALICE's requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306030v1", 
    "other_authors": "A. McNab", 
    "title": "Grid-based access control for Unix environments, Filesystems and Web   Sites", 
    "arxiv-id": "cs/0306030v1", 
    "author": "A. McNab", 
    "publish": "2003-06-06T10:45:43Z", 
    "summary": "The EU DataGrid has deployed a grid testbed at approximately 20 sites across\nEurope, with several hundred registered users. This paper describes\nauthorisation systems produced by GridPP and currently used on the EU DataGrid\nTestbed, including local Unix pool accounts and fine-grained access control\nwith Access Control Lists and Grid-aware filesystems, fileservers and web\ndevelopement environments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306045v1", 
    "other_authors": "Flavia Donno, Vincenzo Ciaschini, David Rebatto, Luca Vaccarossa, Marco Verlato", 
    "title": "The WorldGrid transatlantic testbed: a successful example of Grid   interoperability across EU and U.S. domains", 
    "arxiv-id": "cs/0306045v1", 
    "author": "Marco Verlato", 
    "publish": "2003-06-11T16:47:17Z", 
    "summary": "The European DataTAG project has taken a major step towards making the\nconcept of a worldwide computing Grid a reality. In collaboration with the\ncompanion U.S. project iVDGL, DataTAG has realized an intercontinental testbed\nspanning Europe and the U.S. integrating architecturally different Grid\nimplementations based on the Globus toolkit. The WorldGrid testbed has been\nsuccessfully demonstrated at SuperComputing 2002 and IST2002 where real HEP\napplication jobs were transparently submitted from U.S. and Europe using native\nmechanisms and run where resources were available, independently of their\nlocation. In this paper we describe the architecture of the WorldGrid testbed,\nthe problems encountered and the solutions taken in realizing such a testbed.\nWith our work we present an important step towards interoperability of Grid\nmiddleware developed and deployed in Europe and the U.S.. Some of the solutions\ndeveloped in WorldGrid will be adopted by the LHC Computing Grid first service.\nTo the best of our knowledge, this is the first large-scale testbed that\ncombines middleware components and makes them work together."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306048v1", 
    "other_authors": "Jianwei Li, Wei-keng Liao, Alok Choudhary, Robert Ross, Rajeev Thakur, William Gropp, Rob Latham", 
    "title": "Parallel netCDF: A Scientific High-Performance I/O Interface", 
    "arxiv-id": "cs/0306048v1", 
    "author": "Rob Latham", 
    "publish": "2003-06-11T20:25:52Z", 
    "summary": "Dataset storage, exchange, and access play a critical role in scientific\napplications. For such purposes netCDF serves as a portable and efficient file\nformat and programming interface, which is popular in numerous scientific\napplication domains. However, the original interface does not provide an\nefficient mechanism for parallel data storage and access. In this work, we\npresent a new parallel interface for writing and reading netCDF datasets. This\ninterface is derived with minimum changes from the serial netCDF interface but\ndefines semantics for parallel access and is tailored for high performance. The\nunderlying parallel I/O is achieved through MPI-IO, allowing for dramatic\nperformance gains through the use of collective I/O optimizations. We compare\nthe implementation strategies with HDF5 and analyze both. Our tests indicate\nprogramming convenience and significant I/O performance improvement with this\nparallel netCDF interface."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306051v2", 
    "other_authors": "Atsushi Manabe, Kohki Ishikawa, Yoshihiko Itoh, Setsuya Kawabata, Tetsuro Mashimo, Youhei Morita, Hiroshi Sakamoto, Takashi Sasaki, Hiroyuki Sato, Junichi Tanaka, Ikuo Ueda, Yoshiyuki Watase, Satomi Yamamoto, Shigeo Yashiro", 
    "title": "A data Grid testbed environment in Gigabit WAN with HPSS", 
    "arxiv-id": "cs/0306051v2", 
    "author": "Shigeo Yashiro", 
    "publish": "2003-06-12T08:48:16Z", 
    "summary": "For data analysis of large-scale experiments such as LHC Atlas and other\nJapanese high energy and nuclear physics projects, we have constructed a Grid\ntest bed at ICEPP and KEK. These institutes are connected to national\nscientific gigabit network backbone called SuperSINET. In our test bed, we have\ninstalled NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK\nas a large scale data store. Atlas simulation data at ICEPP has been\ntransferred and accessed using SuperSINET. We have tested various performances\nand characteristics of HPSS through this high speed WAN. The measurement\nincludes comparison between computing and storage resources are tightly coupled\nwith low latency LAN and long distant WAN."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306052v1", 
    "other_authors": "Gilbert Poulard", 
    "title": "ATLAS Data Challenge 1", 
    "arxiv-id": "cs/0306052v1", 
    "author": "Gilbert Poulard", 
    "publish": "2003-06-12T12:59:39Z", 
    "summary": "In 2002 the ATLAS experiment started a series of Data Challenges (DC) of\nwhich the goals are the validation of the Computing Model, of the complete\nsoftware suite, of the data model, and to ensure the correctness of the\ntechnical choices to be made. A major feature of the first Data Challenge (DC1)\nwas the preparation and the deployment of the software required for the\nproduction of large event samples for the High Level Trigger (HLT) and physics\ncommunities, and the production of those samples as a world-wide distributed\nactivity. The first phase of DC1 was run during summer 2002, and involved 39\ninstitutes in 18 countries. More than 10 million physics events and 30 million\nsingle particle events were fully simulated. Over a period of about 40 calendar\ndays 71000 CPU-days were used producing 30 Tbytes of data in about 35000\npartitions. In the second phase the next processing step was performed with the\nparticipation of 56 institutes in 21 countries (~ 4000 processors used in\nparallel). The basic elements of the ATLAS Monte Carlo production system are\ndescribed. We also present how the software suite was validated and the\nparticipating sites were certified. These productions were already partly\nperformed by using different flavours of Grid middleware at ~ 20 sites."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306055v1", 
    "other_authors": "Jeremiah Mans, David Bengali", 
    "title": "BlueOx: A Java Framework for Distributed Data Analysis", 
    "arxiv-id": "cs/0306055v1", 
    "author": "David Bengali", 
    "publish": "2003-06-12T15:53:17Z", 
    "summary": "High energy physics experiments including those at the Tevatron and the\nupcoming LHC require analysis of large data sets which are best handled by\ndistributed computation. We present the design and development of a distributed\ndata analysis framework based on Java. Analysis jobs run through three phases:\ndiscovery of data sets available, brokering/assignment of data sets to analysis\nservers, and job execution. Each phase is represented by a set of abstract\ninterfaces. These interfaces allow different techniques to be used without\nmodification to the framework. For example, the communications interface has\nbeen implemented by both a packet protocol and a SOAP-based scheme. User\nauthentication can be provided either through simple passwords or through a GSI\ncertificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a\nhypothetical high-energy linear collider experiment have been interfaced with\nthe framework."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306058v1", 
    "other_authors": "Vladimir Bahyl, Benjamin Chardi, Jan van Eldik, Ulrich Fuchs, Thorsten Kleinwort, Martin Murth, Tim Smith", 
    "title": "Installing, Running and Maintaining Large Linux Clusters at CERN", 
    "arxiv-id": "cs/0306058v1", 
    "author": "Tim Smith", 
    "publish": "2003-06-12T18:45:34Z", 
    "summary": "Having built up Linux clusters to more than 1000 nodes over the past five\nyears, we already have practical experience confronting some of the LHC scale\ncomputing challenges: scalability, automation, hardware diversity, security,\nand rolling OS upgrades. This paper describes the tools and processes we have\nimplemented, working in close collaboration with the EDG project [1],\nespecially with the WP4 subtask, to improve the manageability of our clusters,\nin particular in the areas of system installation, configuration, and\nmonitoring. In addition to the purely technical issues, providing shared\ninteractive and batch services which can adapt to meet the diverse and changing\nrequirements of our users is a significant challenge. We describe the\ndevelopments and tuning that we have introduced on our LSF based systems to\nmaximise both responsiveness to users and overall system utilisation. Finally,\nthis paper will describe the problems we are facing in enlarging our\nheterogeneous Linux clusters, the progress we have made in dealing with the\ncurrent issues and the steps we are taking to gridify the clusters"
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306060v1", 
    "other_authors": "N. Brook, A. Bogdanchikov, A. Buckley, J. Closier, U. Egede, M. Frank, D. Galli, M. Gandelman, V. Garonne, C. Gaspar, R. Graciani Diaz, K. Harrison, E. van Herwijnen, A. Khan, S. Klous, I. Korolko, G. Kuznetsov, F. Loverre, U. Marconi, J. P. Palacios, G. N. Patrick, A. Pickford, S. Ponce, V. Romanovski, J. J. Saborido, M. Schmelling, A. Soroko, A. Tsaregorodtsev, V. Vagnoni, A. Washbrook", 
    "title": "DIRAC - Distributed Infrastructure with Remote Agent Control", 
    "arxiv-id": "cs/0306060v1", 
    "author": "A. Washbrook", 
    "publish": "2003-06-12T23:54:24Z", 
    "summary": "This paper describes DIRAC, the LHCb Monte Carlo production system. DIRAC has\na client/server architecture based on: Compute elements distributed among the\ncollaborating institutes; Databases for production management, bookkeeping (the\nmetadata catalogue) and software configuration; Monitoring and cataloguing\nservices for updating and accessing the databases. Locally installed software\nagents implemented in Python monitor the local batch queue, interrogate the\nproduction database for any outstanding production requests using the XML-RPC\nprotocol and initiate the job submission. The agent checks and, if necessary,\ninstalls any required software automatically. After the job has processed the\nevents, the agent transfers the output data and updates the metadata catalogue.\nDIRAC has been successfully installed at 18 collaborating institutes, including\nthe DataGRID, and has been used in recent Physics Data Challenges. In the near\nto medium term future we must use a mixed environment with different types of\ngrid middleware or no middleware. We describe how this flexibility has been\nachieved and how ubiquitously available grid middleware would improve DIRAC."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306063v1", 
    "other_authors": "Richard Baker, Dantong Yu, Tomasz Wlodek", 
    "title": "A Model for Grid User Management", 
    "arxiv-id": "cs/0306063v1", 
    "author": "Tomasz Wlodek", 
    "publish": "2003-06-13T17:01:45Z", 
    "summary": "Registration and management of users in a large scale Grid computing\nenvironment presents new challenges that are not well addressed by existing\nprotocols. Within a single Virtual Organization (VO), thousands of users will\npotentially need access to hundreds of computing sites, and the traditional\nmodel where users register for local accounts at each site will present\nsignificant scaling problems. However, computing sites must maintain control\nover access to the site and site policies generally require individual local\naccounts for every user. We present here a model that allows users to register\nonce with a VO and yet still provides all of the computing sites the\ninformation they require with the required level of trust. We have developed\ntools to allow sites to automate the management of local accounts and the\nmappings between Grid identities and local accounts."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306064v2", 
    "other_authors": "Muhammad Asif Jan, Fahd Ali Zahid, Mohammad Moazam Fraz, Arshad Ali", 
    "title": "Exploiting peer group concept for adaptive and highly available services", 
    "arxiv-id": "cs/0306064v2", 
    "author": "Arshad Ali", 
    "publish": "2003-06-13T13:38:01Z", 
    "summary": "This paper presents a prototype for redundant, highly available and fault\ntolerant peer to peer framework for data management. Peer to peer computing is\ngaining importance due to its flexible organization, lack of central authority,\ndistribution of functionality to participating nodes and ability to utilize\nunused computational resources. Emergence of GRID computing has provided much\nneeded infrastructure and administrative domain for peer to peer computing. The\ncomponents of this framework exploit peer group concept to scope service and\ninformation search, arrange services and information in a coherent manner,\nprovide selective redundancy and ensure availability in face of failure and\nhigh load conditions. A prototype system has been implemented using JXTA peer\nto peer technology and XML is used for service description and interfaces,\nallowing peers to communicate with services implemented in various platforms\nincluding web services and JINI services. It utilizes code mobility to achieve\nrole interchange among services and ensure dynamic group membership. Security\nis ensured by using Public Key Infrastructure (PKI) to implement group level\nsecurity policies for membership and service access."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306067v1", 
    "other_authors": "P. Buncic, P. Saiz, A. J. Peters", 
    "title": "The AliEn system, status and perspectives", 
    "arxiv-id": "cs/0306067v1", 
    "author": "A. J. Peters", 
    "publish": "2003-06-13T15:43:15Z", 
    "summary": "AliEn is a production environment that implements several components of the\nGrid paradigm needed to simulate, reconstruct and analyse HEP data in a\ndistributed way. The system is built around Open Source components, uses the\nWeb Services model and standard network protocols to implement the computing\nplatform that is currently being used to produce and analyse Monte Carlo data\nat over 30 sites on four continents. The aim of this paper is to present the\ncurrent AliEn architecture and outline its future developments in the light of\nemerging standards."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306068v1", 
    "other_authors": "Pablo Saiz, Predrag Buncic, Andreas J. Peters", 
    "title": "AliEn Resource Brokers", 
    "arxiv-id": "cs/0306068v1", 
    "author": "Andreas J. Peters", 
    "publish": "2003-06-13T16:00:45Z", 
    "summary": "AliEn (ALICE Environment) is a lightweight GRID framework developed by the\nAlice Collaboration. When the experiment starts running, it will collect data\nat a rate of approximately 2 PB per year, producing O(109) files per year. All\nthese files, including all simulated events generated during the preparation\nphase of the experiment, must be accounted and reliably tracked in the GRID\nenvironment. The backbone of AliEn is a distributed file catalogue, which\nassociates universal logical file name to physical file names for each dataset\nand provides transparent access to datasets independently of physical location.\nThe file replication and transport is carried out under the control of the File\nTransport Broker. In addition, the file catalogue maintains information about\nevery job running in the system. The jobs are distributed by the Job Resource\nBroker that is implemented using a simplified pull (as opposed to traditional\npush) architecture. This paper describes the Job and File Transport Resource\nBrokers and shows that a similar architecture can be applied to solve both\nproblems."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306069v1", 
    "other_authors": "Teela Pulliam, Peter Elmer, Alvise Dorigo", 
    "title": "Distributed Offline Data Reconstruction in BaBar", 
    "arxiv-id": "cs/0306069v1", 
    "author": "Alvise Dorigo", 
    "publish": "2003-06-13T16:16:44Z", 
    "summary": "The BaBar experiment at SLAC is in its fourth year of running. The data\nprocessing system has been continuously evolving to meet the challenges of\nhigher luminosity running and the increasing bulk of data to re-process each\nyear. To meet these goals a two-pass processing architecture has been adopted,\nwhere 'rolling calibrations' are quickly calculated on a small fraction of the\nevents in the first pass and the bulk data reconstruction done in the second.\nThis allows for quick detector feedback in the first pass and allows for the\nparallelization of the second pass over two or more separate farms. This\ntwo-pass system allows also for distribution of processing farms off-site. The\nfirst such site has been setup at INFN Padova. The challenges met here were\nmany. The software was ported to a full Linux-based, commodity hardware system.\nThe raw dataset, 90 TB, was imported from SLAC utilizing a 155 Mbps network\nlink. A system for quality control and export of the processed data back to\nSLAC was developed. Between SLAC and Padova we are currently running three\npass-one farms, with 32 CPUs each, and nine pass-two farms with 64 to 80 CPUs\neach. The pass-two farms can process between 2 and 4 million events per day.\nDetails about the implementation and performance of the system will be\npresented."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306071v1", 
    "other_authors": "Andreas J. Peters, P. Saiz, P. Buncic", 
    "title": "AliEnFS - a Linux File System for the AliEn Grid Services", 
    "arxiv-id": "cs/0306071v1", 
    "author": "P. Buncic", 
    "publish": "2003-06-13T18:18:59Z", 
    "summary": "Among the services offered by the AliEn (ALICE Environment\nhttp://alien.cern.ch) Grid framework there is a virtual file catalogue to allow\ntransparent access to distributed data-sets using various file transfer\nprotocols. $alienfs$ (AliEn File System) integrates the AliEn file catalogue as\na new file system type into the Linux kernel using LUFS, a hybrid user space\nfile system framework (Open Source http://lufs.sourceforge.net). LUFS uses a\nspecial kernel interface level called VFS (Virtual File System Switch) to\ncommunicate via a generalised file system interface to the AliEn file system\ndaemon. The AliEn framework is used for authentication, catalogue browsing,\nfile registration and read/write transfer operations. A C++ API implements the\ngeneric file system operations. The goal of AliEnFS is to allow users easy\ninteractive access to a worldwide distributed virtual file system using\nfamiliar shell commands (f.e. cp,ls,rm ...) The paper discusses general aspects\nof Grid File Systems, the AliEn implementation and present and future\ndevelopments for the AliEn Grid File System."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306072v1", 
    "other_authors": "G. Avellino, S. Barale, S. Beco, B. Cantalupo, D. Colling, F. Giacomini, A. Gianelle, A. Guarise, A. Krenek, D. Kouril, A. Maraschini, L. Matyska, M. Mezzadri, S. Monforte, M. Mulac, F. Pacini, M. Pappalardo, R. Peluso, J. Pospisil, F. Prelz, E. Ronchieri, M. Ruda, L. Salconi, Z. Salvet, M. Sgaravatto, J. Sitera, A. Terracina, M. Vocu, A. Werbrouck", 
    "title": "The EU DataGrid Workload Management System: towards the second major   release", 
    "arxiv-id": "cs/0306072v1", 
    "author": "A. Werbrouck", 
    "publish": "2003-06-13T18:57:35Z", 
    "summary": "In the first phase of the European DataGrid project, the 'workload\nmanagement' package (WP1) implemented a working prototype, providing users with\nan environment allowing to define and submit jobs to the Grid, and able to find\nand use the ``best'' resources for these jobs. Application users have now been\nexperiencing for about a year now with this first release of the workload\nmanagement system. The experiences acquired, the feedback received by the user\nand the need to plug new components implementing new functionalities, triggered\nan update of the existing architecture. A description of this revised and\ncomplemented workload management system is given."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306074v1", 
    "other_authors": "Jim Kowalkowski", 
    "title": "Understanding and Coping with Hardware and Software Failures in a Very   Large Trigger Farm", 
    "arxiv-id": "cs/0306074v1", 
    "author": "Jim Kowalkowski", 
    "publish": "2003-06-13T21:24:05Z", 
    "summary": "When thousands of processors are involved in performing event filtering on a\ntrigger farm, there is likely to be a large number of failures within the\nsoftware and hardware systems. BTeV, a proton/antiproton collider experiment at\nFermi National Accelerator Laboratory, has designed a trigger, which includes\nseveral thousand processors. If fault conditions are not given proper\ntreatment, it is conceivable that this trigger system will experience failures\nat a high enough rate to have a negative impact on its effectiveness. The RTES\n(Real Time Embedded Systems) collaboration is a group of physicists, engineers,\nand computer scientists working to address the problem of reliability in\nlarge-scale clusters with real-time constraints such as this. Resulting\ninfrastructure must be highly scalable, verifiable, extensible by users, and\ndynamically changeable."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306075v1", 
    "other_authors": "Barbara Jacak, Roy Lacey, Dave Morrison, Irina Sourikova, Andrey Shevel, Qiu Zhiping", 
    "title": "Data Management for Physics Analysis in Phenix (BNL, RHIC)", 
    "arxiv-id": "cs/0306075v1", 
    "author": "Qiu Zhiping", 
    "publish": "2003-06-13T21:24:55Z", 
    "summary": "Every year the PHENIX collaboration deals with increasing volume of data (now\nabout 1/4 PB/year). Apparently the more data the more questions how to process\nall the data in most efficient way. In recent past many developments in HEP\ncomputing were dedicated to the production environment. Now we need more tools\nto help to obtain physics results from the analysis of distributed simulated\nand experimental data. Developments in Grid architectures gave many examples\nhow distributed computing facilities can be organized to meet physics analysis\nneeds. We feel that our main task in this area is to try to use already\ndeveloped systems or system components in PHENIX environment.\n  We are concentrating here on the followed problems: file/replica catalog\nwhich keep names of our files, data moving over WAN, job submission in\nmulticluster environment.\n  PHENIX is a running experiment and this fact narrowed our ability to test new\nsoftware on the collaboration computer facilities. We are experimenting with\nsystem prototypes at State University of New York at Stony Brook (SUNYSB) where\nwe run midrange computing cluster for physics analysis. The talk is dedicated\nto discuss some experience with Grid software and achieved results."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306084v1", 
    "other_authors": "R. J. Barlow, A. Forti, A. McNab, S. Salih, D. Smith, T. Adye", 
    "title": "BaBar Web job submission with Globus authentication and AFS access", 
    "arxiv-id": "cs/0306084v1", 
    "author": "T. Adye", 
    "publish": "2003-06-14T02:39:07Z", 
    "summary": "We present two versions of a grid job submission system produced for the\nBaBar experiment. Both use globus job submission to process data spread across\nvarious sites, producing output which can be combined for analysis. The\nproblems encountered with authorisation and authentication, data location, job\nsubmission, and the input and output sandboxes are described, as are the\nsolutions. The total system is still some way short of the aims of enterprises\nsuch as the EDG, but represent a significant step along the way."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306092v1", 
    "other_authors": "Y. Morita, H. Sato, Y. Watase, O. Tatebe, S. Sekiguchi, S. Matsuoka, N. Soda, A. Dell'Acqua", 
    "title": "Building A High Performance Parallel File System Using Grid Datafarm and   ROOT I/O", 
    "arxiv-id": "cs/0306092v1", 
    "author": "A. Dell'Acqua", 
    "publish": "2003-06-14T16:29:16Z", 
    "summary": "Sheer amount of petabyte scale data foreseen in the LHC experiments require a\ncareful consideration of the persistency design and the system design in the\nworld-wide distributed computing. Event parallelism of the HENP data analysis\nenables us to take maximum advantage of the high performance cluster computing\nand networking when we keep the parallelism both in the data processing phase,\nin the data management phase, and in the data transfer phase. A modular\narchitecture of FADS/ Goofy, a versatile detector simulation framework for\nGeant4, enables an easy choice of plug-in facilities for persistency\ntechnologies such as Objectivity/DB and ROOT I/O. The framework is designed to\nwork naturally with the parallel file system of Grid Datafarm (Gfarm).\nFADS/Goofy is proven to generate 10^6 Geant4-simulated Atlas Mockup events\nusing a 512 CPU PC cluster. The data in ROOT I/O files is replicated using\nGfarm file system. The histogram information is collected from the distributed\nROOT files. During the data replication it has been demonstrated to achieve\nmore than 2.3 Gbps data transfer rate between the PC clusters over seven\nparticipating PC clusters in the United States and in Japan."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306093v1", 
    "other_authors": "Antonio Amorim, Luis Pedro, Han Fei, Nuno Almeida, Paulo Trezentos, Jaime E. Villate", 
    "title": "Grid-Brick Event Processing Framework in GEPS", 
    "arxiv-id": "cs/0306093v1", 
    "author": "Jaime E. Villate", 
    "publish": "2003-06-14T22:33:35Z", 
    "summary": "Experiments like ATLAS at LHC involve a scale of computing and data\nmanagement that greatly exceeds the capability of existing systems, making it\nnecessary to resort to Grid-based Parallel Event Processing Systems (GEPS).\nTraditional Grid systems concentrate the data in central data servers which\nhave to be accessed by many nodes each time an analysis or processing job\nstarts. These systems require very powerful central data servers and make\nlittle use of the distributed disk space that is available in commodity\ncomputers. The Grid-Brick system, which is described in this paper, follows a\ndifferent approach. The data storage is split among all grid nodes having each\none a piece of the whole information. Users submit queries and the system will\ndistribute the tasks through all the nodes and retrieve the result, merging\nthem together in the Job Submit Server. The main advantage of using this system\nis the huge scalability it provides, while its biggest disadvantage appears in\nthe case of failure of one of the nodes. A workaround for this problem involves\ndata replication or backup."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306096v1", 
    "other_authors": "H. B. Newman, I. C. Legrand, P. Galvez, R. Voicu, C. Cirstoiu", 
    "title": "MonALISA : A Distributed Monitoring Service Architecture", 
    "arxiv-id": "cs/0306096v1", 
    "author": "C. Cirstoiu", 
    "publish": "2003-06-16T08:33:44Z", 
    "summary": "The MonALISA (Monitoring Agents in A Large Integrated Services Architecture)\nsystem provides a distributed monitoring service. MonALISA is based on a\nscalable Dynamic Distributed Services Architecture which is designed to meet\nthe needs of physics collaborations for monitoring global Grid systems, and is\nimplemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the\nsystem derives from the use of multithreaded Station Servers to host a variety\nof loosely coupled self-describing dynamic services, the ability of each\nservice to register itself and then to be discovered and used by any other\nservices, or clients that require such information, and the ability of all\nservices and clients subscribing to a set of events (state changes) in the\nsystem to be notified automatically. The framework integrates several existing\nmonitoring tools and procedures to collect parameters describing computational\nnodes, applications and network performance. It has built-in SNMP support and\nnetwork-performance monitoring algorithms that enable it to monitor end-to-end\nnetwork performance as well as the performance and state of site facilities in\na Grid. MonALISA is currently running around the clock on the US CMS test Grid\nas well as an increasing number of other sites. It is also being used to\nmonitor the performance and optimize the interconnections among the reflectors\nin the VRVS system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306100v1", 
    "other_authors": "Vijay Sehkri, Igor Mandrichenko, Dane Skow", 
    "title": "Site Authorization Service (SAZ)", 
    "arxiv-id": "cs/0306100v1", 
    "author": "Dane Skow", 
    "publish": "2003-06-16T16:07:24Z", 
    "summary": "In this paper we present a methodology to provide an additional level of\ncentralized control for the grid resources. This centralized control is applied\nto site-wide distribution of various grids and thus providing an upper hand in\nthe maintenance."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306110v1", 
    "other_authors": "M. Bellato, L. Berti, V. Brigljevic, G. Bruno, E. Cano, S. Cittolin, A. Csilling, S. Erhan, D. Gigi, F. Glege, R. Gomez-Reino, M. Gulmini, J. Gutleber, C. Jacobs, M. Kozlovszky, H. Larsen, I. Magrans, G. Maron, F. Meijers, E. Meschi, S. Murray, A. Oh, L. Orsini, L. Pollet, A. Racz, G. Rorato, D. Samyn, P. Scharff-Hansen, C. Schwick, P. Sphicas, N. Toniolo, S. Ventura, L. Zangrando", 
    "title": "Run Control and Monitor System for the CMS Experiment", 
    "arxiv-id": "cs/0306110v1", 
    "author": "L. Zangrando", 
    "publish": "2003-06-18T16:34:11Z", 
    "summary": "The Run Control and Monitor System (RCMS) of the CMS experiment is the set of\nhardware and software components responsible for controlling and monitoring the\nexperiment during data-taking. It provides users with a \"virtual counting\nroom\", enabling them to operate the experiment and to monitor detector status\nand data quality from any point in the world. This paper describes the\narchitecture of the RCMS with particular emphasis on its scalability through a\ndistributed collection of nodes arranged in a tree-based hierarchy. The current\nimplementation of the architecture in a prototype RCMS used in test beam\nsetups, detector validations and DAQ demonstrators is documented. A discussion\nof the key technologies used, including Web Services, and the results of tests\nperformed with a 128-node system are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306111v1", 
    "other_authors": "Sergio Andreozzi, Massimo Sgaravatto, Cristina Vistoli", 
    "title": "Sharing a conceptual model of grid resources and services", 
    "arxiv-id": "cs/0306111v1", 
    "author": "Cristina Vistoli", 
    "publish": "2003-06-18T14:55:40Z", 
    "summary": "Grid technologies aim at enabling a coordinated resource-sharing and\nproblem-solving capabilities over local and wide area networks and span\nlocations, organizations, machine architectures and software boundaries. The\nheterogeneity of involved resources and the need for interoperability among\ndifferent grid middlewares require the sharing of a common information model.\nAbstractions of different flavors of resources and services and conceptual\nschemas of domain specific entities require a collaboration effort in order to\nenable a coherent information services cooperation.\n  With this paper, we present the result of our experience in grid resources\nand services modelling carried out within the Grid Laboratory Uniform\nEnvironment (GLUE) effort, a joint US and EU High Energy Physics projects\ncollaboration towards grid interoperability. The first implementation-neutral\nagreement on services such as batch computing and storage manager, resources\nsuch as the hierarchy cluster, sub-cluster, host and the storage library are\npresented. Design guidelines and operational results are depicted together with\nopen issues and future evolutions."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306112v1", 
    "other_authors": "D. Bonham, G. Garzoglio, R. Herber, J. Kowalkowski, D. Litvintsev, L. Lueking, M. Paterno, D. Petravick, L. Piccoli, R. Pordes, N. Stanfield, I. Terekhov, J. Trumbo, J. Tseng, S. Veseli, M. Votava, V. White, T. Huffman, S. Stonjek, K. Waltkins, P. Crosby, D. Waters, R. St. Denis", 
    "title": "Adapting SAM for CDF", 
    "arxiv-id": "cs/0306112v1", 
    "author": "R. St. Denis", 
    "publish": "2003-06-18T16:28:02Z", 
    "summary": "The CDF and D0 experiments probe the high-energy frontier and as they do so\nhave accumulated hundreds of Terabytes of data on the way to petabytes of data\nover the next two years. The experiments have made a commitment to use the\ndeveloping Grid based on the SAM system to handle these data. The D0 SAM has\nbeen extended for use in CDF as common patterns of design emerged to meet the\nsimilar requirements of these experiments. The process by which the merger was\nachieved is explained with particular emphasis on lessons learned concerning\nthe database design patterns plus realization of the use cases."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306115v1", 
    "other_authors": "L. Lueking, representing the D0 Remote Analysis Task Force", 
    "title": "D0 Regional Analysis Center Concepts", 
    "arxiv-id": "cs/0306115v1", 
    "author": "representing the D0 Remote Analysis Task Force", 
    "publish": "2003-06-19T19:58:07Z", 
    "summary": "The D0 experiment is facing many exciting challenges providing a computing\nenvironment for its worldwide collaboration. Transparent access to data for\nprocessing and analysis has been enabled through deployment of its SAM system\nto collaborating sites and additional functionality will be provided soon with\nSAMGrid components. In order to maximize access to global storage,\ncomputational and intellectual resources, and to enable the system to scale to\nthe large demands soon to be realized, several strategic sites have been\nidentified as Regional Analysis Centers (RAC's). These sites play an expanded\nrole within the system. The philosophy and function of these centers is\ndiscussed and details of their composition and operation are outlined. The plan\nfor future additional centers is also addressed."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306133v1", 
    "other_authors": "D. Engh, S. Smallen, J. Gieraltowski, L. Fang, R. Gardner, D. Gannon, R. Bramley", 
    "title": "GRAPPA: Grid Access Portal for Physics Applications", 
    "arxiv-id": "cs/0306133v1", 
    "author": "R. Bramley", 
    "publish": "2003-06-26T17:09:07Z", 
    "summary": "Grappa is a Grid portal effort designed to provide physicists convenient\naccess to Grid tools and services. The ATLAS analysis and control framework,\nAthena, was used as the target application. Grappa provides basic Grid\nfunctionality such as resource configuration, credential testing, job\nsubmission, job monitoring, results monitoring, and preliminary integration\nwith the ATLAS replica catalog system, MAGDA. Grappa uses Jython to combine the\nease of scripting with the power of java-based toolkits. This provides a\npowerful framework for accessing diverse Grid resources with uniform\ninterfaces. The initial prototype system was based on the XCAT Science Portal\ndeveloped at the Indiana University Extreme Computing Lab and was demonstrated\nby running Monte Carlo production on the U.S. ATLAS test-bed. The portal also\ncommunicated with a European resource broker on WorldGrid as part of the joint\niVDGL-DataTAG interoperability project for the IST2002 and SC2002\ndemonstrations. The current prototype replaces the XCAT Science Portal with an\nxbooks jetspeed portlet for managing user scripts."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307007v2", 
    "other_authors": "A. Baranovski, G. Garzoglio, A. Kreymer, L. Lueking, S. Stonjek, I. Terekhov, F. Wuerthwein, A. Roy, P. Mhashikar, V. Murthi, T. Tannenbaum, R. Walker, F. Ratnikov, T. Rockwell", 
    "title": "Management of Grid Jobs and Information within SAMGrid", 
    "arxiv-id": "cs/0307007v2", 
    "author": "T. Rockwell", 
    "publish": "2003-07-03T20:26:13Z", 
    "summary": "We describe some of the key aspects of the SAMGrid system, used by the D0 and\nCDF experiments at Fermilab. Having sustained success of the data handling part\nof SAMGrid, we have developed new services for job and information services.\nOur job management is rooted in \\CondorG and uses enhancements that are general\napplicability for HEP grids. Our information system is based on a uniform\nframework for configuration management based on XML data representation and\nprocessing."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307019v1", 
    "other_authors": "D. Holmgren, A. Singh, P. Mackenzie, J. Simone", 
    "title": "Lattice QCD Production on Commodity Clusters at Fermilab", 
    "arxiv-id": "cs/0307019v1", 
    "author": "J. Simone", 
    "publish": "2003-07-08T15:36:56Z", 
    "summary": "We describe the construction and results to date of Fermilab's three\nMyrinet-networked lattice QCD production clusters (an 80-node dual Pentium III\ncluster, a 48-node dual Xeon cluster, and a 128-node dual Xeon cluster). We\nexamine a number of aspects of performance of the MILC lattice QCD code running\non these clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307021v1", 
    "other_authors": "A. Singh, D. Holmgren, R. Rechenmacher, S. Epsteyn", 
    "title": "Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at   Fermilab", 
    "arxiv-id": "cs/0307021v1", 
    "author": "S. Epsteyn", 
    "publish": "2003-07-08T16:58:57Z", 
    "summary": "Fermilab operates several clusters for lattice gauge computing. Minimal\nmanpower is available to manage these clusters. We have written a number of\ntools and developed techniques to cope with this task. We describe our tools\nwhich use the IPMI facilities of our systems for hardware management tasks such\nas remote power control, remote system resets, and health monitoring. We\ndiscuss our techniques involving network booting for installation and upgrades\nof the operating system on these computers, and for reloading BIOS and other\nfirmware. Finally, we discuss our tools for parallel command processing and\ntheir use in monitoring and administrating the PBS batch queue system used on\nour clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307052v1", 
    "other_authors": "Hussein Gibbins, Rajkumar Buyya", 
    "title": "Gridscape: A Tool for the Creation of Interactive and Dynamic Grid   Testbed Web Portals", 
    "arxiv-id": "cs/0307052v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-07-22T12:27:28Z", 
    "summary": "The notion of grid computing has gained an increasing popularity recently as\na realistic solution to many of our large-scale data storage and processing\nneeds. It enables the sharing, selection and aggregation of resources\ngeographically distributed across collaborative organisations. Now more and\nmore people are beginning to embrace grid computing and thus are seeing the\nneed to set up their own grids and grid testbeds. With this comes the need to\nhave some means to enable them to view and monitor the status of the resources\nin these testbeds (eg. Web based Grid portal). Generally developers invest a\nsubstantial amount of time and effort developing custom monitoring software. To\novercome this limitation, this paper proposes Gridscape ? a tool that enables\nthe rapid creation of interactive and dynamic testbed portals (without any\nprogramming effort). Gridscape primarily aims to provide a solution for those\nusers who need to be able to create a grid testbed portal but don?t necessarily\nhave the time or resources to build a system of their own from scratch."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307066v1", 
    "other_authors": "Oleg Lodygensky, Gilles Fedak, Vincent Neri, Alain Cordier, Franck Cappello", 
    "title": "Augernome & XtremWeb: Monte Carlos computation on a global computing   platform", 
    "arxiv-id": "cs/0307066v1", 
    "author": "Franck Cappello", 
    "publish": "2003-07-29T14:12:07Z", 
    "summary": "In this paper, we present XtremWeb, a Global Computing platform used to\ngenerate monte carlos showers in Auger, an HEP experiment to study the highest\nenergy cosmic rays at Mallargue-Mendoza, Argentina.\n  XtremWeb main goal, as a Global Computing platform, is to compute distributed\napplications using idle time of widely interconnected machines. It is\nespecially dedicated to -but not limited to- multi-parameters applications such\nas monte carlos computations; its security mechanisms ensuring not only hosts\nintegrity but also results certification and its fault tolerant features,\nencouraged us to test it and, finally, to deploy it as to support our CPU needs\nto simulate showers.\n  We first introduce Auger computing needs and how Global Computing could help.\nWe then detail XtremWeb architecture and goals. The fourth and last part\npresents the profits we have gained to choose this platform. We conclude on\nwhat could be done next."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0308024v1", 
    "other_authors": "Rob Byrom, Brian Coghlan, Andrew W Cooke, Roney Cordenonsi, Linda Cornwall, Abdeslem Djaoui, Laurence Field, Steve Fisher, Steve Hicks, Stuart Kenny, Jason Leake, James Magowan, Werner Nutt, David O'Callaghan, Norbert Podhorszki, John Ryan, Manish Soni, Paul Taylor, Antony J Wilson", 
    "title": "Relational Grid Monitoring Architecture (R-GMA)", 
    "arxiv-id": "cs/0308024v1", 
    "author": "Antony J Wilson", 
    "publish": "2003-08-15T23:53:49Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which has been\ndeveloped within the European DataGrid Project as a Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each Virtual Organisation had one large relational database. We provide a\nnumber of different Producer types with different characteristics; for example\nsome support streaming of information. We also provide combined\nConsumer/Producers, which are able to combine information and republish it. At\nthe heart of the system is the mediator, which for any query is able to find\nand connect to the best Producers for the job. We have developed components to\nallow a measure of inter-working between MDS and R-GMA. We have used it both\nfor information about the grid (primarily to find out about what services are\navailable at any one time) and for application monitoring. R-GMA has been\ndeployed in various testbeds; we describe some preliminary results and\nexperiences of this deployment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0309026v1", 
    "other_authors": "Mark C. Little", 
    "title": "A thought experiment on Quantum Mechanics and Distributed Failure   Detection", 
    "arxiv-id": "cs/0309026v1", 
    "author": "Mark C. Little", 
    "publish": "2003-09-15T10:43:47Z", 
    "summary": "One of the biggest problems in current distributed systems is that presented\nby one machine attempting to determine the liveness of another in a timely\nmanner. Unfortunately, the symptoms exhibited by a failed machine can also be\nthe result of other causes, e.g., an overloaded machine or network which drops\nmessages, making it impossible to detect a machine failure with cetainty until\nthat machine recovers. This is a well understood problem and one which has led\nto a large body of research into failure suspectors: since it is not possible\nto detect a failure, the best one can do is suspect a failure and program\naccordingly. However, one machine's suspicions may not be the same as\nanother's; therefore, these algorithms spend a considerable effort in ensuring\na consistent view among all available machines of who is suspects of being\nfailed. This paper describes a thought experiment on how quantum mechanics may\nbe used to provide a failure detector that is guaranteed to give both accurate\nand instantaneous information about the liveness of machines, no matter the\ndistances involved."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0166-218X(03)00368-8", 
    "link": "http://arxiv.org/pdf/cs/0309040v1", 
    "other_authors": "L. D. Penso, V. C. Barbosa", 
    "title": "A distributed algorithm to find k-dominating sets", 
    "arxiv-id": "cs/0309040v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T01:14:43Z", 
    "summary": "We consider a connected undirected graph $G(n,m)$ with $n$ nodes and $m$\nedges. A $k$-dominating set $D$ in $G$ is a set of nodes having the property\nthat every node in $G$ is at most $k$ edges away from at least one node in $D$.\nFinding a $k$-dominating set of minimum size is NP-hard. We give a new\nsynchronous distributed algorithm to find a $k$-dominating set in $G$ of size\nno greater than $\\lfloor n/(k+1)\\rfloor$. Our algorithm requires $O(k\\log^*n)$\ntime and $O(m\\log k+n\\log k\\log^*n)$ messages to run. It has the same time\ncomplexity as the best currently known algorithm, but improves on that\nalgorithm's message complexity and is, in addition, conceptually simpler."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0309042v1", 
    "other_authors": "L. M. A. Drummond, V. C. Barbosa", 
    "title": "On reducing the complexity of matrix clocks", 
    "arxiv-id": "cs/0309042v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T12:57:59Z", 
    "summary": "Matrix clocks are a generalization of the notion of vector clocks that allows\nthe local representation of causal precedence to reach into an asynchronous\ndistributed computation's past with depth $x$, where $x\\ge 1$ is an integer.\nMaintaining matrix clocks correctly in a system of $n$ nodes requires that\neverymessage be accompanied by $O(n^x)$ numbers, which reflects an exponential\ndependency of the complexity of matrix clocks upon the desired depth $x$. We\nintroduce a novel type of matrix clock, one that requires only $nx$ numbers to\nbe attached to each message while maintaining what for many applications may be\nthe most significant portion of the information that the original matrix clock\ncarries. In order to illustrate the new clock's applicability, we demonstrate\nits use in the monitoring of certain resource-sharing computations."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0309049v1", 
    "other_authors": "Joao Lourenco, Jose C. Cunha, Vitor Moreira", 
    "title": "Control and Debugging of Distributed Programs Using Fiddle", 
    "arxiv-id": "cs/0309049v1", 
    "author": "Vitor Moreira", 
    "publish": "2003-09-26T11:49:10Z", 
    "summary": "The main goal of Fiddle, a distributed debugging engine, is to provide a\nflexible platform for developing debugging tools. Fiddle provides a layered set\nof interfaces with a minimal set of debugging functionalities, for the\ninspection and control of distributed and multi-threaded applications.\n  This paper illustrates how Fiddle is used to support integrated testing and\ndebugging. The approach described is based on a tool, called Deipa, that\ninterprets sequences of commands read from an input file, generated by an\nindependent testing tool. Deipa acts as a Fiddle client, in order to enforce\nspecific execution paths in a distributed PVM program. Other Fiddle clients may\nbe used along with Deipa for the fine debugging at process level. Fiddle and\nDeipa functionalities and architectures are described, and a working example\nshows a step-by-step application of these tools."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0310029v1", 
    "other_authors": "Rajeev Thakur, William Gropp, Ewing Lusk", 
    "title": "Optimizing Noncontiguous Accesses in MPI-IO", 
    "arxiv-id": "cs/0310029v1", 
    "author": "Ewing Lusk", 
    "publish": "2003-10-15T19:35:00Z", 
    "summary": "The I/O access patterns of many parallel applications consist of accesses to\na large number of small, noncontiguous pieces of data. If an application's I/O\nneeds are met by making many small, distinct I/O requests, however, the I/O\nperformance degrades drastically. To avoid this problem, MPI-IO allows users to\naccess noncontiguous data with a single I/O function call, unlike in Unix I/O.\nIn this paper, we explain how critical this feature of MPI-IO is for high\nperformance and how it enables implementations to perform optimizations. We\nfirst provide a classification of the different ways of expressing an\napplication's I/O needs in MPI-IO--we classify them into four levels, called\nlevel~0 through level~3. We demonstrate that, for applications with\nnoncontiguous access patterns, the I/O performance improves dramatically if\nusers write their applications to make level-3 requests (noncontiguous,\ncollective) rather than level-0 requests (Unix style). We then describe how our\nMPI-IO implementation, ROMIO, delivers high performance for noncontiguous\nrequests. We explain in detail the two key optimizations ROMIO performs: data\nsieving for noncontiguous requests from one process and collective I/O for\nnoncontiguous requests from multiple processes. We describe how we have\nimplemented these optimizations portably on multiple machines and file systems,\ncontrolled their memory requirements, and also achieved high performance. We\ndemonstrate the performance and portability with performance results for three\napplications--an astrophysics-application template (DIST3D), the NAS BTIO\nbenchmark, and an unstructured code (UNSTRUC)--on five different parallel\nmachines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0310030v1", 
    "other_authors": "Oliver Oppitz", 
    "title": "A Particular Bug Trap: Execution Replay Using Virtual Machines", 
    "arxiv-id": "cs/0310030v1", 
    "author": "Oliver Oppitz", 
    "publish": "2003-10-15T20:54:14Z", 
    "summary": "Execution-replay (ER) is well known in the literature but has been restricted\nto special system architectures for many years. Improved hardware resources and\nthe maturity of virtual machine technology promise to make ER useful for a\nbroader range of development projects.\n  This paper describes an approach to create a practical, generic ER\ninfrastructure for desktop PC systems using virtual machine technology. In the\ncreated VM environment arbitrary application programs will run and be replayed\nunmodified, neither instrumentation nor recompilation are required."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311009v1", 
    "other_authors": "A. Demichev, D. Foster, V. Kalyaev, A. Kryukov, M. Lamanna, V. Pose, R. B. Da Rocha, C. Wang", 
    "title": "OGSA/Globus Evaluation for Data Intensive Applications", 
    "arxiv-id": "cs/0311009v1", 
    "author": "C. Wang", 
    "publish": "2003-11-10T11:12:26Z", 
    "summary": "We present an architecture of Globus Toolkit 3 based testbed intended for\nevaluation of applicability of the Open Grid Service Architecture (OGSA) for\nData Intensive Applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311010v1", 
    "other_authors": "V. Kalyaev, A. Kryukov", 
    "title": "Problem of Application Job Monitoring in GRID Systems", 
    "arxiv-id": "cs/0311010v1", 
    "author": "A. Kryukov", 
    "publish": "2003-11-10T11:39:04Z", 
    "summary": "We present a new approach to monitoring of the execution process of an\napplication job in the GRID environment. The main point of the approach is use\nof GRID ervices to access monitoring information with the security level\navailable in GRID."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311021v1", 
    "other_authors": "L. Shamardin", 
    "title": "LCG-1 Deployment and usage experience", 
    "arxiv-id": "cs/0311021v1", 
    "author": "L. Shamardin", 
    "publish": "2003-11-17T13:19:31Z", 
    "summary": "LCG-1 is the second release of the software framework for the LHC Computing\nGrid project. In our work we describe the installation process, arising\nproblems and their solutions, and configuration tuning details of the complete\nLCG-1 site, including all LCG elements required for the self-sufficient site."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0312022v1", 
    "other_authors": "Manjuka Soysa, Rajkumar Buyya, Baikunth Nath", 
    "title": "GridEmail: A Case for Economically Regulated Internet-based   Interpersonal Communications", 
    "arxiv-id": "cs/0312022v1", 
    "author": "Baikunth Nath", 
    "publish": "2003-12-12T11:42:17Z", 
    "summary": "Email has emerged as a dominant form of electronic communication between\npeople. Spam is a major problem for email users, with estimates of up to 56% of\nemail falling into that category. Control of Spam is being attempted with\ntechnical and legislative methods. In this paper we look at email and spam from\na supply-demand perspective. We propose Gridemail, an email system based on an\neconomy of communicating parties, where participants? motivations are\nrepresented as pricing policies and profiles. This system is expected to help\npeople regulate their personal communications to suit their conditions, and\nhelp in removing unwanted messages."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0312049v1", 
    "other_authors": "Gianluca Argentini", 
    "title": "Using virtual processors for SPMD parallel programs", 
    "arxiv-id": "cs/0312049v1", 
    "author": "Gianluca Argentini", 
    "publish": "2003-12-21T14:37:24Z", 
    "summary": "In this paper I describe some results on the use of virtual processors\ntechnology for parallelize some SPMD computational programs. The tested\ntechnology is the INTEL Hyper Threading on real processors, and the programs\nare MATLAB scripts for floating points computation. The conclusions of the work\nconcern on the utility and limits of the used approach. The main result is that\nusing virtual processors is a good technique for improving parallel programs\nnot only for memory-based computations, but in the case of massive disk-storage\noperations too."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0401027v1", 
    "other_authors": "Russell K. Standish, Duraid Madina", 
    "title": "ClassdescMP: Easy MPI programming in C++", 
    "arxiv-id": "cs/0401027v1", 
    "author": "Duraid Madina", 
    "publish": "2004-01-27T03:55:27Z", 
    "summary": "ClassdescMP is a distributed memory parallel programming system for use with\nC++ and MPI. It uses the Classdesc reflection system to ease the task of\nbuilding complicated messages to be sent between processes. It doesn't hide the\nunderlying MPI API, so it is an augmentation of MPI capabilities. Users can\nstill call standard MPI function calls if needed for performance reasons."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402012v1", 
    "other_authors": "Joseph Y. Halpern, Aleta Ricciardi", 
    "title": "A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and   Failure Detectors", 
    "arxiv-id": "cs/0402012v1", 
    "author": "Aleta Ricciardi", 
    "publish": "2004-02-05T17:15:08Z", 
    "summary": "It is shown that, in a precise sense, if there is no bound on the number of\nfaulty processes in a system with unreliable but fair communication, Uniform\nDistributed Coordination (UDC) can be attained if and only if a system has\nperfect failure detectors. This result is generalized to the case where there\nis a bound t on the number of faulty processes. It is shown that a certain type\nof generalized failure detector is necessary and sufficient for achieving UDC\nin a context with at most t faulty processes. Reasoning about processes'\nknowledge as to which other processes are faulty plays a key role in the\nanalysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402017v1", 
    "other_authors": "Akshay Luther, Rajkumar Buyya, Rajiv Ranjan, Srikumar Venugopal", 
    "title": "Alchemi: A .NET-based Grid Computing Framework and its Integration into   Global Grids", 
    "arxiv-id": "cs/0402017v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-02-10T09:18:07Z", 
    "summary": "Computational grids that couple geographically distributed resources are\nbecoming the de-facto computing platform for solving large-scale problems in\nscience, engineering, and commerce. Software to enable grid computing has been\nprimarily written for Unix-class operating systems, thus severely limiting the\nability to effectively utilize the computing resources of the vast majority of\ndesktop computers i.e. those running variants of the Microsoft Windows\noperating system. Addressing Windows-based grid computing is particularly\nimportant from the software industry's viewpoint where interest in grids is\nemerging rapidly. Microsoft's .NET Framework has become near-ubiquitous for\nimplementing commercial distributed systems for Windows-based platforms,\npositioning it as the ideal platform for grid computing in this context. In\nthis paper we present Alchemi, a .NET-based grid computing framework that\nprovides the runtime machinery and programming environment required to\nconstruct desktop grids and develop grid applications. It allows flexible\napplication composition by supporting an object-oriented grid application\nprogramming model in addition to a grid job model. Cross-platform support is\nprovided via a web services interface and a flexible execution model supports\ndedicated and non-dedicated (voluntary) execution by grid nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402018v1", 
    "other_authors": "Choon Hoong Ding, Sarana Nutanong, Rajkumar Buyya", 
    "title": "P2P Networks for Content Sharing", 
    "arxiv-id": "cs/0402018v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2004-02-10T14:24:48Z", 
    "summary": "Peer-to-peer (P2P) technologies have been widely used for content sharing,\npopularly called \"file-swapping\" networks. This chapter gives a broad overview\nof content sharing P2P technologies. It starts with the fundamental concept of\nP2P computing followed by the analysis of network topologies used in\npeer-to-peer systems. Next, three milestone peer-to-peer technologies: Napster,\nGnutella, and Fasttrack are explored in details, and they are finally concluded\nwith the comparison table in the last section."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0403015v2", 
    "other_authors": "Ichiro Adachi, Taisuke Hibino, Luc Hinz, Ryosuke Itoh, Nobu Katayama, Shohei Nishida, Frederic Ronga, Toshifumi Tsukamoto, Masahiko Yokoyama", 
    "title": "Belle Computing System", 
    "arxiv-id": "cs/0403015v2", 
    "author": "Masahiko Yokoyama", 
    "publish": "2004-03-11T09:20:20Z", 
    "summary": "We describe the present status of the computing system in the Belle\nexperiment at the KEKB $e^+e^-$ asymmetric-energy collider. So far, we have\nlogged more than 160 fb$^{-1}$ of data, corresponding to the world's largest\ndata sample of 170M $B\\bar{B}$ pairs at the $\\Upsilon(4S)$ energy region. A\nlarge amount of event data has to be processed to produce an analysis event\nsample in a timely fashion. In addition, Monte Carlo events have to be created\nto control systematic errors accurately. This requires stable and efficient\nusage of computing resources. Here we review our computing model and then\ndescribe how we efficiently proceed DST/MC productions in our system."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404014v1", 
    "other_authors": "Timm M. Steinbeck", 
    "title": "A Modular and Fault-Tolerant Data Transport Framework", 
    "arxiv-id": "cs/0404014v1", 
    "author": "Timm M. Steinbeck", 
    "publish": "2004-04-06T13:35:55Z", 
    "summary": "The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to\nreduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output\nbefore the data is written to permanent storage. To cope with these data rates\na large PC cluster system is being designed to scale to several 1000 nodes,\nconnected by a fast network. For the software that will run on these nodes a\nflexible data transport and distribution software framework, described in this\nthesis, has been developed. The framework consists of a set of separate\ncomponents, that can be connected via a common interface. This allows to\nconstruct different configurations for the HLT, that are even changeable at\nruntime. To ensure a fault-tolerant operation of the HLT, the framework\nincludes a basic fail-over mechanism that allows to replace whole nodes after a\nfailure. The mechanism will be further expanded in the future, utilizing the\nruntime reconnection feature of the framework's component interface. To connect\ncluster nodes a communication class library is used that abstracts from the\nactual network technology and protocol used to retain flexibility in the\nhardware choice. It contains already two working prototype versions for the TCP\nprotocol as well as SCI network adapters. Extensions can be added to the\nlibrary without modifications to other parts of the framework. Extensive tests\nand measurements have been performed with the framework. Their results as well\nas conclusions drawn from them are also presented in this thesis. Performance\ntests show very promising results for the system, indicating that it can\nfulfill ALICE's requirements concerning the data transport."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404015v2", 
    "other_authors": "Ahmet A. Husainov", 
    "title": "The study of distributed computing algorithms by multithread   applications", 
    "arxiv-id": "cs/0404015v2", 
    "author": "Ahmet A. Husainov", 
    "publish": "2004-04-07T01:27:32Z", 
    "summary": "The material in this note is used as an introduction to distributed\nalgorithms in a four year course on software and automatic control system in\nthe computer technology department of the Komsomolsk-on-Amur state technical\nuniversity. All our the program examples are written in Borland C/C++ 5.02 for\nWindows 95/98/2000/NT/XP, and hence suit to compile and execute by Visual\nC/C++. We consider the following approaches of the distributed computing: the\nconversion of recursive algorithms to multithread applications, a realization\nof the pairing algorithm, the building of wave systems by Petri nets and object\noriented programming."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404027v1", 
    "other_authors": "Rajkumar Buyya, Srikumar Venugopal", 
    "title": "The Gridbus Toolkit for Service Oriented Grid and Utility Computing: An   Overview and Status Report", 
    "arxiv-id": "cs/0404027v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-04-13T09:16:54Z", 
    "summary": "Grids aim at exploiting synergies that result from cooperation of autonomous\ndistributed entities. The synergies that result from grid cooperation include\nthe sharing, exchange, selection, and aggregation of geographically distributed\nresources such as computers, data bases, software, and scientific instruments\nfor solving large-scale problems in science, engineering, and commerce. For\nthis cooperation to be sustainable, participants need to have economic\nincentive. Therefore, \"incentive\" mechanisms should be considered as one of key\ndesign parameters of Grid architectures. In this article, we present an\noverview and status of an open source Grid toolkit, called Gridbus, whose\narchitecture is fundamentally driven by the requirements of Grid economy.\nGridbus technologies provide services for both computational and data grids\nthat power the emerging eScience and eBusiness applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0405023v1", 
    "other_authors": "Srikumar Venugopal, Rajkumar Buyya, Lyle Winton", 
    "title": "A Grid Service Broker for Scheduling Distributed Data-Oriented   Applications on Global Grids", 
    "arxiv-id": "cs/0405023v1", 
    "author": "Lyle Winton", 
    "publish": "2004-05-06T10:15:26Z", 
    "summary": "The next generation of scientific experiments and studies, popularly called\nas e-Science, is carried out by large collaborations of researchers distributed\naround the world engaged in analysis of huge collections of data generated by\nscientific instruments. Grid computing has emerged as an enabler for e-Science\nas it permits the creation of virtual organizations that bring together\ncommunities with common objectives. Within a community, data collections are\nstored or replicated on distributed resources to enhance storage capability or\nefficiency of access. In such an environment, scientists need to have the\nability to carry out their studies by transparently accessing distributed data\nand computational resources. In this paper, we propose and develop a Grid\nbroker that mediates access to distributed resources by (a) discovering\nsuitable data sources for a given analysis scenario, (b) suitable computational\nresources, (c) optimally mapping analysis jobs to resources, (d) deploying and\nmonitoring job execution on selected resources, (e) accessing data from local\nor remote data source during job execution and (f) collating and presenting\nresults. The broker supports a declarative and dynamic parametric programming\nmodel for creating grid applications. We have used this model in grid-enabling\na high energy physics analysis application (Belle Analysis Software Framework).\nThe broker has been used in deploying Belle experiment data analysis jobs on a\ngrid testbed, called Belle Analysis Data Grid, having resources distributed\nacross Australia interconnected through GrangeNet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0405086v1", 
    "other_authors": "Vasilii Zhakhovskii, Katsunobu Nishihara, Yuko Fukuda, Shinji Shimojo", 
    "title": "A New Dynamical Domain Decomposition Method for Parallel Molecular   Dynamics Simulation on Grid", 
    "arxiv-id": "cs/0405086v1", 
    "author": "Shinji Shimojo", 
    "publish": "2004-05-24T07:29:13Z", 
    "summary": "We develop a new Lagrangian material particle -- dynamical domain\ndecomposition method (MPD^3) for large scale parallel molecular dynamics (MD)\nsimulation of nonstationary heterogeneous systems on a heterogeneous computing\nnet. MPD^3 is based on Voronoi decomposition of simulated matter. The map of\nVoronoi polygons is known as the Dirichlet tessellation and used for grid\ngeneration in computational fluid dynamics. From the hydrodynamics point of\nview the moving Voronoi polygon looks as a material particle (MP). MPs can\nexchange particles and information. To balance heterogeneous computing\nconditions the MP centers should be dependent on timing data. We propose a\nsimple and efficient iterative algorithm which based on definition of the\ntiming-dependent balancing displacement of MP center for next simulation step.\n  The MPD^3 program was tested in various computing environments and physical\nproblems. We have demonstrated that MPD^3 is a high-adaptive decomposition\nalgorithm for MD simulation. It was shown that the well-balanced decomposition\ncan result from dynamical Voronoi polygon tessellation. One would expect the\nsimilar approach can be successfully applied for other particle methods like\nMonte Carlo, particle-in-cell, and smooth-particle-hydrodynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407001v1", 
    "other_authors": "Parvin Asadzadeh, Rajkumar Buyya, Chun Ling Kei, Deepa Nayar, Srikumar Venugopal", 
    "title": "Global Grids and Software Toolkits: A Study of Four Grid Middleware   Technologies", 
    "arxiv-id": "cs/0407001v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-07-01T11:54:06Z", 
    "summary": "Grid is an infrastructure that involves the integrated and collaborative use\nof computers, networks, databases and scientific instruments owned and managed\nby multiple organizations. Grid applications often involve large amounts of\ndata and/or computing resources that require secure resource sharing across\norganizational boundaries. This makes Grid application management and\ndeployment a complex undertaking. Grid middlewares provide users with seamless\ncomputing ability and uniform access to resources in the heterogeneous Grid\nenvironment. Several software toolkits and systems have been developed, most of\nwhich are results of academic research projects, all over the world. This\nchapter will focus on four of these middlewares--UNICORE, Globus, Legion and\nGridbus. It also presents our implementation of a resource broker for UNICORE\nas this functionality was not supported in it. A comparison of these systems on\nthe basis of the architecture, implementation model and several other features\nis included."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407012v1", 
    "other_authors": "Arshad Ali, Ashiq Anjum, Atif Mehmood, Richard McClatchey, Ian Willers, Julian Bunn, Harvey Newman, Michael Thomas, Conrad Steenberg", 
    "title": "A Taxonomy and Survey of Grid Resource Planning and Reservation Systems   for Grid Enabled Analysis Environment", 
    "arxiv-id": "cs/0407012v1", 
    "author": "Conrad Steenberg", 
    "publish": "2004-07-05T15:48:43Z", 
    "summary": "The concept of coupling geographically distributed resources for solving\nlarge scale problems is becoming increasingly popular forming what is popularly\ncalled grid computing. Management of resources in the Grid environment becomes\ncomplex as the resources are geographically distributed, heterogeneous in\nnature and owned by different individuals and organizations each having their\nown resource management policies and different access and cost models. There\nhave been many projects that have designed and implemented the resource\nmanagement systems with a variety of architectures and services. In this paper\nwe have presented the general requirements that a Resource Management system\nshould satisfy. The taxonomy has also been defined based on which survey of\nresource management systems in different existing Grid projects has been\nconducted to identify the key areas where these systems lack the desired\nfunctionality."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407013v1", 
    "other_authors": "Naveed Ahmad, Arshad Ali, Ashiq Anjum, Tahir Azim, Julian Bunn, Ali Hassan, Ahsan Ikram, Frank van Lingen, Richard McClatchey, Harvey Newman, Conrad Steenberg, Michael Thomas, Ian Willers", 
    "title": "Distributed Analysis and Load Balancing System for Grid Enabled Analysis   on Hand-held devices using Multi-Agents Systems", 
    "arxiv-id": "cs/0407013v1", 
    "author": "Ian Willers", 
    "publish": "2004-07-05T16:08:36Z", 
    "summary": "Handheld devices, while growing rapidly, are inherently constrained and lack\nthe capability of executing resource hungry applications. This paper presents\nthe design and implementation of distributed analysis and load-balancing system\nfor hand-held devices using multi-agents system. This system enables low\nresource mobile handheld devices to act as potential clients for Grid enabled\napplications and analysis environments. We propose a system, in which mobile\nagents will transport, schedule, execute and return results for heavy\ncomputational jobs submitted by handheld devices. Moreover, in this way, our\nsystem provides high throughput computing environment for hand-held devices."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407014v2", 
    "other_authors": "Arshad Ali, Ashiq Anjum, Tahir Azim, Julian Bunn, Ahsan Ikram, Richard McClatchey, Harvey Newman, Conrad Steenberg, Michael Thomas, Ian Willers", 
    "title": "A Grid-enabled Interface to Condor for Interactive Analysis on Handheld   and Resource-limited Devices", 
    "arxiv-id": "cs/0407014v2", 
    "author": "Ian Willers", 
    "publish": "2004-07-05T19:40:00Z", 
    "summary": "This paper was withdrawn by the authors."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407017v1", 
    "other_authors": "Andrew Cantino, Fronefield Crawford, Saurav Dhital, John P. Dougherty, Reid Sherman", 
    "title": "A Low Cost Distributed Computing Approach to Pulsar Searches at a Small   College", 
    "arxiv-id": "cs/0407017v1", 
    "author": "Reid Sherman", 
    "publish": "2004-07-07T18:55:20Z", 
    "summary": "We describe a distributed processing cluster of inexpensive Linux machines\ndeveloped jointly by the Astronomy and Computer Science departments at\nHaverford College which has been successfully used to search a large volume of\ndata from a recent radio pulsar survey. Analysis of radio pulsar surveys\nrequires significant computational resources to handle the demanding data\nstorage and processing needs. One goal of this project was to explore issues\nencountered when processing a large amount of pulsar survey data with limited\ncomputational resources. This cluster, which was developed and activated in\nonly a few weeks by supervised undergraduate summer research students, used\nexisting decommissioned computers, the campus network, and a script-based,\nclient-oriented, self-scheduled data distribution approach to process the data.\nThis setup provided simplicity, efficiency, and \"on-the-fly\" scalability at low\ncost. The entire 570 GB data set from the pulsar survey was processed at\nHaverford over the course of a ten-week summer period using this cluster. We\nconclude that this cluster can serve as a useful computational model in cases\nwhere data processing must be carried out on a limited budget. We have also\nconstructed a DVD archive of the raw survey data in order to investigate the\nfeasibility of using DVD as an inexpensive and easily accessible raw data\nstorage format for pulsar surveys. DVD-based storage has not been widely\nexplored in the pulsar community, but it has several advantages. The DVD\narchive we have constructed is reliable, portable, inexpensive, and can be\neasily read by any standard modern machine."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408032v2", 
    "other_authors": "Luiz Angelo Barchet-Estefanel, Gregory Mounie", 
    "title": "Performance Characterisation of Intra-Cluster Collective Communications", 
    "arxiv-id": "cs/0408032v2", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T06:31:10Z", 
    "summary": "Although recent works try to improve collective communication in grid systems\nby separating intra and inter-cluster communication, the optimisation of\ncommunications focus only on inter-cluster communications. We believe, instead,\nthat the overall performance of the application may be improved if\nintra-cluster collective communications performance is known in advance. Hence,\nit is important to have an accurate model of the intra-cluster collective\ncommunications, which provides the necessary evidences to tune and to predict\ntheir performance correctly. In this paper we present our experience on\nmodelling such communication strategies. We describe and compare different\nimplementation strategies with their communication models, evaluating the\nmodels' accuracy and describing the practical challenges that can be found when\nmodelling collective communications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408033v3", 
    "other_authors": "Luiz Angelo Barchet-Estefanel, Gregory Mounie", 
    "title": "Identifying Logical Homogeneous Clusters for Efficient Wide-area   Communications", 
    "arxiv-id": "cs/0408033v3", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T07:39:57Z", 
    "summary": "Recently, many works focus on the implementation of collective communication\noperations adapted to wide area computational systems, like computational Grids\nor global-computing. Due to the inherently heterogeneity of such environments,\nmost works separate \"clusters\" in different hierarchy levels. to better model\nthe communication. However, in our opinion, such works do not give enough\nattention to the delimitation of such clusters, as they normally use the\nlocality or the IP subnet from the machines to delimit a cluster without\nverifying the \"homogeneity\" of such clusters. In this paper, we describe a\nstrategy to gather network information from different local-area networks and\nto construct \"logical homogeneous clusters\", better suited to the performance\nmodelling."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408034v3", 
    "other_authors": "Luiz Angelo Barchet-Estefanel, Gregory Mounie", 
    "title": "Fast Tuning of Intra-Cluster Collective Communications", 
    "arxiv-id": "cs/0408034v3", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T07:40:33Z", 
    "summary": "Recent works try to optimise collective communication in grid systems\nfocusing mostly on the optimisation of communications among different clusters.\nWe believe that intra-cluster collective communications should also be\noptimised, as a way to improve the overall efficiency and to allow the\nconstruction of multi-level collective operations. Indeed, inside homogeneous\nclusters, a simple optimisation approach rely on the comparison from different\nimplementation strategies, through their communication models. In this paper we\nevaluate this approach, comparing different implementation strategies with\ntheir predicted performances. As a result, we are able to choose the\ncommunication strategy that better adapts to each network environment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410009v1", 
    "other_authors": "Scott Douglas, Aaron Harwood", 
    "title": "Diffusive Load Balancing of Loosely-Synchronous Parallel Programs over   Peer-to-Peer Networks", 
    "arxiv-id": "cs/0410009v1", 
    "author": "Aaron Harwood", 
    "publish": "2004-10-05T08:23:45Z", 
    "summary": "The use of under-utilized Internet resources is widely recognized as a viable\nform of high performance computing. Sustained processing power of roughly 40T\nFLOPS using 4 million volunteered Internet hosts has been reported for\nembarrassingly parallel problems. At the same time, peer-to-peer (P2P) file\nsharing networks, with more than 50 million participants, have demonstrated the\ncapacity for scale in distributed systems. This paper contributes a study of\nload balancing techniques for a general class of loosely-synchronous parallel\nalgorithms when executed over a P2P network. We show that decentralized,\ndiffusive load balancing can be effective at balancing load and is facilitated\nby the dynamic properties of P2P. While a moderate degree of dynamicity can\nbenefit load balancing, significant dynamicity hinders the parallel program\nperformance due to the need for increased load migration. To the best of our\nknowledge this study provides new insight into the performance of\nloosely-synchronous parallel programs over the Internet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410016v1", 
    "other_authors": "Antonio Amorim, Jaime Villate, Pedro Andrade", 
    "title": "HEP@Home - A distributed computing system based on BOINC", 
    "arxiv-id": "cs/0410016v1", 
    "author": "Pedro Andrade", 
    "publish": "2004-10-07T12:42:10Z", 
    "summary": "Project SETI@HOME has proven to be one of the biggest successes of\ndistributed computing during the last years. With a quite simple approach SETI\nmanages to process large volumes of data using a vast amount of distributed\ncomputer power.\n  To extend the generic usage of this kind of distributed computing tools,\nBOINC is being developed. In this paper we propose HEP@HOME, a BOINC version\ntailored to the specific requirements of the High Energy Physics (HEP)\ncommunity.\n  The HEP@HOME will be able to process large amounts of data using virtually\nunlimited computing power, as BOINC does, and it should be able to work\naccording to HEP specifications. In HEP the amounts of data to be analyzed or\nreconstructed are of central importance. Therefore, one of the design\nprinciples of this tool is to avoid data transfer. This will allow scientists\nto run their analysis applications and taking advantage of a large number of\nCPUs. This tool also satisfies other important requirements in HEP, namely,\nsecurity, fault-tolerance and monitoring."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410074v2", 
    "other_authors": "Jianyang Zeng, Wen-Jing Hsu", 
    "title": "ReCord: A Distributed Hash Table with Recursive Structure", 
    "arxiv-id": "cs/0410074v2", 
    "author": "Wen-Jing Hsu", 
    "publish": "2004-10-29T02:56:21Z", 
    "summary": "We propose a simple distributed hash table called ReCord, which is a\ngeneralized version of Randomized-Chord and offers improved tradeoffs in\nperformance and topology maintenance over existing P2P systems. ReCord is\nscalable and can be easily implemented as an overlay network, and offers a good\ntradeoff between the node degree and query latency. For instance, an $n$-node\nReCord with $O(\\log n)$ node degree has an expected latency of $\\Theta(\\log n)$\nhops. Alternatively, it can also offer $\\Theta(\\frac{\\log n}{\\log \\log n})$\nhops latency at a higher cost of $O(\\frac{\\log^2 n}{\\log\n  \\log n})$ node degree. Meanwhile, simulations of the dynamic behaviors of\nReCord are studied."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0411045v1", 
    "other_authors": "Catalin Dumitrescu, Ian Foster", 
    "title": "Usage Policy-based CPU Sharing in VOs", 
    "arxiv-id": "cs/0411045v1", 
    "author": "Ian Foster", 
    "publish": "2004-11-12T22:48:00Z", 
    "summary": "Resource sharing within Grid collaborations usually implies specific sharing\nmechanisms at participating sites. Challenging policy issues can arise within\nvirtual organizations (VOs) that integrate participants and resources spanning\nmultiple physical institutions. Resource owners may wish to grant to one or\nmore VOs the right to use certain resources subject to local policy and service\nlevel agreements, and each VO may then wish to use those resources subject to\nVO policy. Thus, we must address the question of what usage policies (UPs)\nshould be considered for resource sharing in VOs. As a first step in addressing\nthis question, we develop and evaluate different UP scenarios within a\nspecialized context that mimics scientific Grids within which the resources to\nbe shared are computers. We also present a UP architecture and define roles and\nfunctions for scheduling resources in such grid environments while satisfying\nresource owner policies."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0411046v2", 
    "other_authors": "Jesse S. A. Bridgewater, P. Oscar Boykin, Vwani P. Roychowdhury", 
    "title": "Balanced Overlay Networks (BON): Decentralized Load Balancing via   Self-Organized Random Networks", 
    "arxiv-id": "cs/0411046v2", 
    "author": "Vwani P. Roychowdhury", 
    "publish": "2004-11-15T20:05:00Z", 
    "summary": "We present a novel framework, called balanced overlay networks (BON), that\nprovides scalable, decentralized load balancing for distributed computing using\nlarge-scale pools of heterogeneous computers. Fundamentally, BON encodes the\ninformation about each node's available computational resources in the\nstructure of the links connecting the nodes in the network. This distributed\nencoding is self-organized, with each node managing its in-degree and local\nconnectivity via random-walk sampling. Assignment of incoming jobs to nodes\nwith the most free resources is also accomplished by sampling the nodes via\nshort random walks. Extensive simulations show that the resulting highly\ndynamic and self-organized graph structure can efficiently balance\ncomputational load throughout large-scale networks. These simulations cover a\nwide spectrum of cases, including significant heterogeneity in available\ncomputing resources and high burstiness in incoming load. We provide analytical\nresults that prove BON's scalability for truly large-scale networks: in\nparticular we show that under certain ideal conditions, the network structure\nconverges to Erdos-Renyi (ER) random graphs; our simulation results, however,\nshow that the algorithm does much better, and the structures seem to approach\nthe ideal case of d-regular random graphs. We also make a connection between\nhighly-loaded BONs and the well-known ball-bin randomized load balancing\nframework."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412115v1", 
    "other_authors": "Bernadette Charron-Bost", 
    "title": "Reductions in Distributed Computing Part I: Consensus and Atomic   Commitment Tasks", 
    "arxiv-id": "cs/0412115v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2004-12-29T19:50:21Z", 
    "summary": "We introduce several notions of reduction in distributed computing, and\ninvestigate reduction properties of two fundamental agreement tasks, namely\nConsensus and Atomic Commitment.\n  We first propose the notion of reduction \"a la Karp'', an analog for\ndistributed computing of the classical Karp reduction. We then define a weaker\nreduction which is the analog of Cook reduction. These two reductions are\ncalled K-reduction and C-reduction, respectively.\n  We also introduce the notion of C*-reduction which has no counterpart in\nclassical (namely, non distributed) systems, and which naturally arises when\ndealing with symmetric tasks.\n  We establish various reducibility and irreducibility theorems with respect to\nthese three reductions. Our main result is an incomparability statement for\nConsensus and Atomic Commitment tasks: we show that they are incomparable with\nrespect to the C-reduction, except when the resiliency degree is 1, in which\ncase Atomic Commitment is strictly harder than Consensus. A side consequence of\nthese results is that our notion of C-reduction is strictly weaker than the one\nof K-reduction, even for unsolvable tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412116v1", 
    "other_authors": "Bernadette Charron-Bost", 
    "title": "Reductions in Distributed Computing Part II: k-Threshold Agreement Tasks", 
    "arxiv-id": "cs/0412116v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2004-12-29T20:51:58Z", 
    "summary": "We extend the results of Part I by considering a new class of agreement\ntasks, the so-called k-Threshold Agreement tasks (previously introduced by\nCharron-Bost and Le Fessant). These tasks naturally interpolate between Atomic\nCommitment and Consensus. Moreover, they constitute a valuable tool to derive\nirreducibility results between Consensus tasks only. In particular, they allow\nus to show that (A) for a fixed set of processes, the higher the resiliency\ndegree is, the harder the Consensus task is, and (B) for a fixed resiliency\ndegree, the smaller the set of processes is, the harder the Consensus task is.\n  The proofs of these results lead us to consider new oracle-based reductions,\ninvolving a weaker variant of the C-reduction introduced in Part I. We also\ndiscuss the relationship between our results and previous ones relating\nf-resiliency and wait-freedom."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412121v1", 
    "other_authors": "Michael Treaster, Nadir Kiyanclar, Gregory A. Koenig, William Yurcik", 
    "title": "A Distributed Economics-based Infrastructure for Utility Computing", 
    "arxiv-id": "cs/0412121v1", 
    "author": "William Yurcik", 
    "publish": "2004-12-31T19:16:38Z", 
    "summary": "Existing attempts at utility computing revolve around two approaches. The\nfirst consists of proprietary solutions involving renting time on dedicated\nutility computing machines. The second requires the use of heavy, monolithic\napplications that are difficult to deploy, maintain, and use.\n  We propose a distributed, community-oriented approach to utility computing.\nOur approach provides an infrastructure built on Web Services in which modular\ncomponents are combined to create a seemingly simple, yet powerful system. The\ncommunity-oriented nature generates an economic environment which results in\nfair transactions between consumers and providers of computing cycles while\nsimultaneously encouraging improvements in the infrastructure of the\ncomputational grid itself."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0501002v1", 
    "other_authors": "Michael Treaster", 
    "title": "A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel   Systems", 
    "arxiv-id": "cs/0501002v1", 
    "author": "Michael Treaster", 
    "publish": "2005-01-01T03:32:51Z", 
    "summary": "Supercomputing systems today often come in the form of large numbers of\ncommodity systems linked together into a computing cluster. These systems, like\nany distributed system, can have large numbers of independent hardware\ncomponents cooperating or collaborating on a computation. Unfortunately, any of\nthis vast number of components can fail at any time, resulting in potentially\nerroneous output. In order to improve the robustness of supercomputing\napplications in the presence of failures, many techniques have been developed\nto provide resilience to these kinds of system faults. This survey provides an\noverview of these various fault-tolerance techniques."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502018v1", 
    "other_authors": "Maria A. Nieto-Santisteban, Alexander S. Szalay, Aniruddha R. Thakar, William J. O'Mullane, Jim Gray, James Annis", 
    "title": "When Database Systems Meet the Grid", 
    "arxiv-id": "cs/0502018v1", 
    "author": "James Annis", 
    "publish": "2005-02-03T21:43:50Z", 
    "summary": "We illustrate the benefits of combining database systems and Grid\ntechnologies for data-intensive applications. Using a cluster of SQL servers,\nwe reimplemented an existing Grid application that finds galaxy clusters in a\nlarge astronomical database. The SQL implementation runs an order of magnitude\nfaster than the earlier Tcl-C-file-based implementation. We discuss why and how\nGrid applications can take advantage of database systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502051v1", 
    "other_authors": "Zaheer Abbas, Muhammad Umer, Mohammed Odeh, Richard McClatchey, Arshad Ali, Farooq Ahmad", 
    "title": "A Semantic Grid-based E-Learning Framework (SELF)", 
    "arxiv-id": "cs/0502051v1", 
    "author": "Farooq Ahmad", 
    "publish": "2005-02-09T17:22:21Z", 
    "summary": "E-learning can be loosely defined as a wide set of applications and\nprocesses, which uses available electronic media (and tools) to deliver\nvocational education and training. With its increasing recognition as an\nubiquitous mode of instruction and interaction in the academic as well as\ncorporate world, the need for a scaleable and realistic model is becoming\nimportant. In this paper we introduce SELF; a Semantic grid-based E-Learning\nFramework. SELF aims to identify the key-enablers in a practical grid-based\nE-learning environment and to minimize technological reworking by proposing a\nwell-defined interaction plan among currently available tools and technologies.\nWe define a dichotomy with E-learning specific application layers on top and\nsemantic grid-based support layers underneath. We also map the latest open and\nfreeware technologies with various components in SELF."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502069v1", 
    "other_authors": "Alexander Kroeller, Sandor P. Fekete, Carsten Buschmann, Stefan Fischer, Dennis Pfisterer", 
    "title": "Koordinatenfreies Lokationsbewusstsein (Localization without   Coordinates)", 
    "arxiv-id": "cs/0502069v1", 
    "author": "Dennis Pfisterer", 
    "publish": "2005-02-15T16:35:13Z", 
    "summary": "Localization is one of the fundamental issues in sensor networks. It is\nalmost always assumed that it must be solved by assigning coordinates to the\nnodes. This article discusses positioning algorithms from a theoretical,\npractical and simulative point of view, and identifies difficulties and\nlimitations. Ideas for more abstract means of location awareness are presented\nand the resulting possible improvements for applications are shown. Nodes with\ncertain topological or environmental properties are clustered, and the\nneighborhood structure of the clusters is modeled as a graph. Eines der\nfundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer\ndie Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon\nausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In\ndiesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein\nkritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen\naufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine\nabstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird\ngezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf\neiner graphenbasierten Modellierung des Netzes: Knoten mit bestimmten\ntopologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und\nClusternachbarschaften dann als Graphen modelliert."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502093v1", 
    "other_authors": "Alessandro Mei, Romeo Rizzi", 
    "title": "Online Permutation Routing in Partitioned Optical Passive Star Networks", 
    "arxiv-id": "cs/0502093v1", 
    "author": "Romeo Rizzi", 
    "publish": "2005-02-26T00:07:43Z", 
    "summary": "This paper establishes the state of the art in both deterministic and\nrandomized online permutation routing in the POPS network. Indeed, we show that\nany permutation can be routed online on a POPS network either with\n$O(\\frac{d}{g}\\log g)$ deterministic slots, or, with high probability, with\n$5c\\lceil d/g\\rceil+o(d/g)+O(\\log\\log g)$ randomized slots, where constant\n$c=\\exp (1+e^{-1})\\approx 3.927$. When $d=\\Theta(g)$, that we claim to be the\n\"interesting\" case, the randomized algorithm is exponentially faster than any\nother algorithm in the literature, both deterministic and randomized ones. This\nis true in practice as well. Indeed, experiments show that it outperforms its\nrivals even starting from as small a network as a POPS(2,2), and the gap grows\nexponentially with the size of the network. We can also show that, under proper\nhypothesis, no deterministic algorithm can asymptotically match its\nperformance."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503013v1", 
    "other_authors": "Luiz Angelo Barchet-Estefanel, Gr\u00e9gory Mouni\u00e9", 
    "title": "Pr\u00e9diction de Performances pour les Communications Collectives", 
    "arxiv-id": "cs/0503013v1", 
    "author": "Gr\u00e9gory Mouni\u00e9", 
    "publish": "2005-03-04T17:09:23Z", 
    "summary": "Des travaux r\\'{e}cents visent l'optimisation des op\\'{e}rations de\ncommunication collective dans les environnements de type grille de calcul. La\nsolution la plus r\\'{e}pandue est la s\\'{e}paration des communications internes\net externes \\`{a} chaque grappe, mais cela n'exclut pas le d\\'{e}coupage des\ncommunications en plusieurs couches, pratique efficace d\\'{e}montr\\'{e}e par\nKaronis et al. [10]. Dans les deux cas, la pr\\'{e}diction des performances est\nun facteur essentiel, soit pour le r\\'{e}glage fin des param\\`{e}tres de\ncommunication, soit pour le calcul de la distribution et de la hi\\'{e}rarchie\ndes communications. Pour cela, il est tr\\`{e}s important d'avoir des\nmod\\`{e}les pr\\'{e}cis des communications collectives, lesquels seront\nutilis\\'{e}s pour pr\\'{e}dire ces performances. Cet article d\\'{e}crit notre\nexp\\'{e}rience sur la mod\\'{e}lisation des op\\'{e}rations de communication\ncollective. Nous pr\\'{e}sentons des mod\\`{e}les de communication pour\ndiff\\'{e}rents patrons de communication collective comme un vers plusieurs, un\nvers plusieurs personnalis\\'{e} et plusieurs vers plusieurs. Pour \\'{e}valuer\nla pr\\'{e}cision des mod\\`{e}les, nous comparons les pr\\'{e}dictions obtenues\navec les r\\'{e}sultats des exp\\'{e}rimentations effectu\\'{e}es sur deux\nenvironnements r\\'{e}seaux diff\\'{e}rents, Fast Ethernet et Myrinet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503025v2", 
    "other_authors": "Jia Yu, Rajkumar Buyya", 
    "title": "A Taxonomy of Workflow Management Systems for Grid Computing", 
    "arxiv-id": "cs/0503025v2", 
    "author": "Rajkumar Buyya", 
    "publish": "2005-03-11T01:13:07Z", 
    "summary": "With the advent of Grid and application technologies, scientists and\nengineers are building more and more complex applications to manage and process\nlarge data sets, and execute scientific experiments on distributed resources.\nSuch application scenarios require means for composing and executing complex\nworkflows. Therefore, many efforts have been made towards the development of\nworkflow management systems for Grid computing. In this paper, we propose a\ntaxonomy that characterizes and classifies various approaches for building and\nexecuting workflows on Grids. We also survey several representative Grid\nworkflow systems developed by various projects world-wide to demonstrate the\ncomprehensiveness of the taxonomy. The taxonomy not only highlights the design\nand engineering similarities and differences of state-of-the-art in Grid\nworkflow systems, but also identifies the areas that need further research."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503045v1", 
    "other_authors": "G. E. Graham, M. Anzar Afaq, David Evans, Gerald Guglielmo, Eric Wicklund, Peter Love", 
    "title": "Contextual Constraint Modeling in Grid Application Workflows", 
    "arxiv-id": "cs/0503045v1", 
    "author": "Peter Love", 
    "publish": "2005-03-19T01:28:48Z", 
    "summary": "This paper introduces a new mechanism for specifying constraints in\ndistributed workflows. By introducing constraints in a contextual form, it is\nshown how different people and groups within collaborative communities can\ncooperatively constrain workflows. A comparison with existing state-of-the-art\nworkflow systems is made. These ideas are explored in practice with an\nillustrative example from High Energy Physics."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504033v1", 
    "other_authors": "Arshad Ali, Ashiq Anjum, Tahir Azim, Julian Bunn, Atif Mehmood, Richard McClatchey, Harvey Newman, Waqas ur Rehman, Conrad Steenberg, Michael Thomas, Frank van Lingen, Ian Willers, Muhammad Adeel Zafar", 
    "title": "Resource Management Services for a Grid Analysis Environment", 
    "arxiv-id": "cs/0504033v1", 
    "author": "Muhammad Adeel Zafar", 
    "publish": "2005-04-10T11:59:25Z", 
    "summary": "Selecting optimal resources for submitting jobs on a computational Grid or\naccessing data from a data grid is one of the most important tasks of any Grid\nmiddleware. Most modern Grid software today satisfies this responsibility and\ngives a best-effort performance to solve this problem. Almost all decisions\nregarding scheduling and data access are made by the software automatically,\ngiving users little or no control over the entire process. To solve this\nproblem, a more interactive set of services and middleware is desired that\nprovides users more information about Grid weather, and gives them more control\nover the decision making process. This paper presents a set of services that\nhave been developed to provide more interactive resource management\ncapabilities within the Grid Analysis Environment (GAE) being developed\ncollaboratively by Caltech, NUST and several other institutes. These include a\nsteering service, a job monitoring service and an estimator service that have\nbeen designed and written using a common Grid-enabled Web Services framework\nnamed Clarens. The paper also presents a performance analysis of the developed\nservices to show that they have indeed resulted in a more interactive and\npowerful system for user-centric Grid-enabled physics analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504034v1", 
    "other_authors": "Arshad Ali, Ashiq Anjum, Tahir Azim, Julian Bunn, Saima Iqbal, Richard McClatchey, Harvey Newman, S. Yousaf Shah, Tony Solomonides, Conrad Steenberg, Michael Thomas, Frank van Lingen, Ian Willers", 
    "title": "Heterogeneous Relational Databases for a Grid-enabled Analysis   Environment", 
    "arxiv-id": "cs/0504034v1", 
    "author": "Ian Willers", 
    "publish": "2005-04-10T12:05:03Z", 
    "summary": "Grid based systems require a database access mechanism that can provide\nseamless homogeneous access to the requested data through a virtual data access\nsystem, i.e. a system which can take care of tracking the data that is stored\nin geographically distributed heterogeneous databases. This system should\nprovide an integrated view of the data that is stored in the different\nrepositories by using a virtual data access mechanism, i.e. a mechanism which\ncan hide the heterogeneity of the backend databases from the client\napplications. This paper focuses on accessing data stored in disparate\nrelational databases through a web service interface, and exploits the features\nof a Data Warehouse and Data Marts. We present a middleware that enables\napplications to access data stored in geographically distributed relational\ndatabases without being aware of their physical locations and underlying\nschema. A web service interface is provided to enable applications to access\nthis middleware in a language and platform independent way. A prototype\nimplementation was created based on Clarens [4], Unity [7] and POOL [8]. This\nability to access the data stored in the distributed relational databases\ntransparently is likely to be a very powerful one for Grid users, especially\nthe scientific community wishing to collate and analyze data distributed over\nthe Grid."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504044v1", 
    "other_authors": "Michael Thomas, Conrad Steenberg, Frank van Lingen, Harvey Newman, Julian Bunn, Arshad Ali, Richard McClatchey, Ashiq Anjum, Tahir Azim, Waqas ur Rehman, Faisal Khan, Jang Uk In", 
    "title": "JClarens: A Java Framework for Developing and Deploying Web Services for   Grid Computing", 
    "arxiv-id": "cs/0504044v1", 
    "author": "Jang Uk In", 
    "publish": "2005-04-11T21:45:07Z", 
    "summary": "High Energy Physics (HEP) and other scientific communities have adopted\nService Oriented Architectures (SOA) as part of a larger Grid computing effort.\nThis effort involves the integration of many legacy applications and\nprogramming libraries into a SOA framework. The Grid Analysis Environment (GAE)\nis such a service oriented architecture based on the Clarens Grid Services\nFramework and is being developed as part of the Compact Muon Solenoid (CMS)\nexperiment at the Large Hadron Collider (LHC) at European Laboratory for\nParticle Physics (CERN). Clarens provides a set of authorization, access\ncontrol, and discovery services, as well as XMLRPC and SOAP access to all\ndeployed services. Two implementations of the Clarens Web Services Framework\n(Python and Java) offer integration possibilities for a wide range of\nprogramming languages. This paper describes the Java implementation of the\nClarens Web Services Framework called JClarens. and several web services of\ninterest to the scientific and Grid community that have been deployed using\nJClarens."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506001v1", 
    "other_authors": "Dmitry Mogilevsky, Sean Keller", 
    "title": "SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters", 
    "arxiv-id": "cs/0506001v1", 
    "author": "Sean Keller", 
    "publish": "2005-05-31T21:05:03Z", 
    "summary": "Modern high-performance computing relies heavily on the use of commodity\nprocessors arranged together in clusters. These clusters consist of individual\nnodes (typically off-the-shelf single or dual processor machines) connected\ntogether with a high speed interconnect. Using cluster computation has many\nbenefits, but also carries the liability of being failure prone due to the\nsheer number of components involved. Many effective solutions have been\nproposed to aid failure recovery in clusters, their one significant downside\nbeing the failure models they support. Most of the work in the area has focused\non detecting and correcting fail-stop errors. We propose a system that will\nalso detect more general error models, such as Byzantine errors, thus allowing\nexisting failure recovery methods to handle them correctly."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506006v1", 
    "other_authors": "Nicolas Capit, Georges Da Costa, Yiannis Georgiou, Guillaume Huard, Cyrille Martin, Gr\u00e9gory Mouni\u00e9, Pierre Neyron, Olivier Richard", 
    "title": "A batch scheduler with high level components", 
    "arxiv-id": "cs/0506006v1", 
    "author": "Olivier Richard", 
    "publish": "2005-06-02T13:04:14Z", 
    "summary": "In this article we present the design choices and the evaluation of a batch\nscheduler for large clusters, named OAR. This batch scheduler is based upon an\noriginal design that emphasizes on low software complexity by using high level\ntools. The global architecture is built upon the scripting language Perl and\nthe relational database engine Mysql. The goal of the project OAR is to prove\nthat it is possible today to build a complex system for ressource management\nusing such tools without sacrificing efficiency and scalability. Currently, our\nsystem offers most of the important features implemented by other batch\nschedulers such as priority scheduling (by queues), reservations, backfilling\nand some global computing support. Despite the use of high level tools, our\nexperiments show that our system has performances close to other systems.\nFurthermore, OAR is currently exploited for the management of 700 nodes (a\nmetropolitan GRID) and has shown good efficiency and robustness."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506097v1", 
    "other_authors": "Samuel Thibault", 
    "title": "A Flexible Thread Scheduler for Hierarchical Multiprocessor Machines", 
    "arxiv-id": "cs/0506097v1", 
    "author": "Samuel Thibault", 
    "publish": "2005-06-27T14:32:50Z", 
    "summary": "With the current trend of multiprocessor machines towards more and more\nhierarchical architectures, exploiting the full computational power requires\ncareful distribution of execution threads and data so as to limit expensive\nremote memory accesses. Existing multi-threaded libraries provide only limited\nfacilities to let applications express distribution indications, so that\nprogrammers end up with explicitly distributing tasks according to the\nunderlying architecture, which is difficult and not portable. In this article,\nwe present: (1) a model for dynamically expressing the structure of the\ncomputation; (2) a scheduler interpreting this model so as to make judicious\nhierarchical distribution decisions; (3) an implementation within the Marcel\nuser-level thread library. We experimented our proposal on a scientific\napplication running on a ccNUMA Bull NovaScale with 16 Intel Itanium II\nprocessors; results show a 30% gain compared to a classical scheduler, and are\nsimilar to what a handmade scheduler achieves in a non-portable way."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0508052v1", 
    "other_authors": "Pierre Leone, Olivier Powell, Jose Rolim", 
    "title": "Energy Optimal Data Propagation in Wireless Sensor Networks", 
    "arxiv-id": "cs/0508052v1", 
    "author": "Jose Rolim", 
    "publish": "2005-08-10T15:54:13Z", 
    "summary": "We propose an algorithm which produces a randomized strategy reaching optimal\ndata propagation in wireless sensor networks (WSN).In [6] and [8], an energy\nbalanced solution is sought using an approximation algorithm. Our algorithm\nimproves by (a) when an energy-balanced solution does not exist, it still finds\nan optimal solution (whereas previous algorithms did not consider this case and\nprovide no useful solution) (b) instead of being an approximation algorithm, it\nfinds the exact solution in one pass. We also provide a rigorous proof of the\noptimality of our solution."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0510081v1", 
    "other_authors": "Toan Nguyen, Lizhe Wang, Vittorio Selmin", 
    "title": "Virtual Environments for multiphysics code validation on Computing Grids", 
    "arxiv-id": "cs/0510081v1", 
    "author": "Vittorio Selmin", 
    "publish": "2005-10-26T12:26:57Z", 
    "summary": "We advocate in this paper the use of grid-based infrastructures that are\ndesigned for seamless approaches to the numerical expert users, i.e., the\nmultiphysics applications designers. It relies on sophisticated computing\nenvironments based on computing grids, connecting heterogeneous computing\nresources: mainframes, PC-clusters and workstations running multiphysics codes\nand utility software, e.g., visualization tools. The approach is based on\nconcepts defined by the HEAVEN* consortium. HEAVEN is a European scientific\nconsortium including industrial partners from the aerospace, telecommunication\nand software industries, as well as academic research institutes. Currently,\nthe HEAVEN consortium works on a project that aims to create advanced services\nplatforms. It is intended to enable \"virtual private grids\" supporting various\nenvironments for users manipulating a suitable high-level interface. This will\nbecome the basis for future generalized services allowing the integration of\nvarious services without the need to deploy specific grid infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0511083v1", 
    "other_authors": "Olivier Powell, Aubin Jarry, Pierre Leone, Jose Rolim", 
    "title": "Gradient Based Routing in Wireless Sensor Networks: a Mixed Strategy", 
    "arxiv-id": "cs/0511083v1", 
    "author": "Jose Rolim", 
    "publish": "2005-11-23T18:13:18Z", 
    "summary": "We show how recent theoretical advances for data-propagation in Wireless\nSensor Networks (WSNs) can be combined to improve gradient-based routing (GBR)\nin Wireless Sensor Networks. We propose a mixed-strategy of direct transmission\nand multi-hop propagation of data which improves the lifespan of WSNs by\nreaching better energy-load-balancing amongst sensor nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0512005v1", 
    "other_authors": "Vitorino Ramos, Carlos Fernandes, Agostinho C. Rosa", 
    "title": "On Ants, Bacteria and Dynamic Environments", 
    "arxiv-id": "cs/0512005v1", 
    "author": "Agostinho C. Rosa", 
    "publish": "2005-12-01T04:52:00Z", 
    "summary": "Wasps, bees, ants and termites all make effective use of their environment\nand resources by displaying collective swarm intelligence. Termite colonies -\nfor instance - build nests with a complexity far beyond the comprehension of\nthe individual termite, while ant colonies dynamically allocate labor to\nvarious vital tasks such as foraging or defense without any central\ndecision-making ability. Recent research suggests that microbial life can be\neven richer: highly social, intricately networked, and teeming with\ninteractions, as found in bacteria. What strikes from these observations is\nthat both ant colonies and bacteria have similar natural mechanisms based on\nStigmergy and Self-Organization in order to emerge coherent and sophisticated\npatterns of global behaviour. Keeping in mind the above characteristics we will\npresent a simple model to tackle the collective adaptation of a social swarm\nbased on real ant colony behaviors (SSA algorithm) for tracking extrema in\ndynamic environments and highly multimodal complex functions described in the\nwell-know De Jong test suite. Then, for the purpose of comparison, a recent\nmodel of artificial bacterial foraging (BFOA algorithm) based on similar\nstigmergic features is described and analyzed. Final results indicate that the\nSSA collective intelligence is able to cope and quickly adapt to unforeseen\nsituations even when over the same cooperative foraging period, the community\nis requested to deal with two different and contradictory purposes, while\noutperforming BFOA in adaptive speed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0601078v1", 
    "other_authors": "Benjamin Gaidioz, Birger Koblitz, Nuno Santos", 
    "title": "Exploring high performance distributed file storage using LDPC codes", 
    "arxiv-id": "cs/0601078v1", 
    "author": "Nuno Santos", 
    "publish": "2006-01-17T13:30:47Z", 
    "summary": "We explore the feasibility of implementing a reliable, high performance,\ndistributed storage system on a commodity computing cluster. Files are\ndistributed across storage nodes using erasure coding with small Low-Density\nParity-Check (LDPC) codes which provide high reliability while keeping the\nstorage and performance overhead small. We present performance measurements\ndone on a prototype system comprising 50 nodes which are self organised using a\npeer-to-peer overlay."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0601111v1", 
    "other_authors": "Chen Zhang, Ted Herman", 
    "title": "Localization in Wireless Sensor Grids", 
    "arxiv-id": "cs/0601111v1", 
    "author": "Ted Herman", 
    "publish": "2006-01-25T23:48:12Z", 
    "summary": "This work reports experiences on using radio ranging to position sensors in a\ngrid topology. The implementation is simple, efficient, and could be\npractically distributed. The paper describes an implementation and experimental\nresult based on RSSI distance estimation. Novel techniques such as fuzzy\nmembership functions and table lookup are used to obtain more accurate result\nand simplify the computation. An 86% accuracy is achieved in the experiment in\nspite of inaccurate RSSI distance estimates with errors up to 60%."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0602013v1", 
    "other_authors": "David Pritchard", 
    "title": "An Optimal Distributed Edge-Biconnectivity Algorithm", 
    "arxiv-id": "cs/0602013v1", 
    "author": "David Pritchard", 
    "publish": "2006-02-05T20:47:23Z", 
    "summary": "We describe a synchronous distributed algorithm which identifies the\nedge-biconnected components of a connected network. It requires a leader, and\nuses messages of size O(log |V|). The main idea is to preorder a BFS spanning\ntree, and then to efficiently compute least common ancestors so as to mark\ncycle edges. This algorithm takes O(Diam) time and uses O(|E|) messages.\nFurthermore, we show that no correct singly-initiated edge-biconnectivity\nalgorithm can beat either bound on any graph by more than a constant factor. We\nalso describe a near-optimal local algorithm for edge-biconnectivity."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0602026v1", 
    "other_authors": "Ashiq Anjum, Richard McClatchey, Arshad Ali, Ian Willers", 
    "title": "Bulk Scheduling with DIANA Scheduler", 
    "arxiv-id": "cs/0602026v1", 
    "author": "Ian Willers", 
    "publish": "2006-02-07T16:47:16Z", 
    "summary": "Results from and progress on the development of a Data Intensive and Network\nAware (DIANA) Scheduling engine, primarily for data intensive sciences such as\nphysics analysis, are described. Scientific analysis tasks can involve\nthousands of computing, data handling, and network resources and the size of\nthe input and output files and the amount of overall storage space allocated to\na user necessarily can have significant bearing on the scheduling of data\nintensive applications. If the input or output files must be retrieved from a\nremote location, then the time required transferring the files must also be\ntaken into consideration when scheduling compute resources for the given\napplication. The central problem in this study is the coordinated management of\ncomputation and data at multiple locations and not simply data movement.\nHowever, this can be a very costly operation and efficient scheduling can be a\nchallenge if compute and data resources are mapped without network cost. We\nhave implemented an adaptive algorithm within the DIANA Scheduler which takes\ninto account data location and size, network performance and computation\ncapability to make efficient global scheduling decisions. DIANA is a\nperformance-aware as well as an economy-guided Meta Scheduler. It iteratively\nallocates each job to the site that is likely to produce the best performance\nas well as optimizing the global queue for any remaining pending jobs.\nTherefore it is equally suitable whether a single job is being submitted or\nbulk scheduling is being performed. Results suggest that considerable\nperformance improvements are to be gained by adopting the DIANA scheduling\napproach."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0603035v2", 
    "other_authors": "Chiara Del Frate, Jose Galvez, Tamas Hauer, David Manset, Richard McClatchey, Mohammed Odeh, Dmitry Rogulin, Tony Solomonides, Ruth Warren", 
    "title": "Final Results from and Exploitation Plans for MammoGrid", 
    "arxiv-id": "cs/0603035v2", 
    "author": "Ruth Warren", 
    "publish": "2006-03-09T13:49:14Z", 
    "summary": "The MammoGrid project has delivered the first deployed instance of a\nhealthgrid for clinical mammography that spans national boundaries. During the\nlast year, the final MammoGrid prototype has undergone a series of rigorous\ntests undertaken by radiologists in the UK and Italy and this paper draws\nconclusions from those tests for the benefit of the Healthgrid community. In\naddition, lessons learned during the lifetime of the project are detailed and\nrecommendations drawn for future health applications using grids. Following the\ncompletion of the project, plans have been put in place for the\ncommercialisation of the MammoGrid system and this is also reported in this\narticle. Particular emphasis is placed on the issues surrounding the transition\nfrom collaborative research project to a marketable product. This paper\nconcludes by highlighting some of the potential areas of future development and\nresearch."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0603036v2", 
    "other_authors": "Joerg Freund, Dorin Comaniciu, Yannis Ioannis, Peiya Liu, Richard McClatchey, Edwin Morley-Fletcher, Xavier Pennec, Giacomo Pongiglione, Xiang, ZHOU", 
    "title": "Health-e-Child : An Integrated Biomedical Platform for Grid-Based   Paediatric Applications", 
    "arxiv-id": "cs/0603036v2", 
    "author": "ZHOU", 
    "publish": "2006-03-09T13:54:15Z", 
    "summary": "There is a compelling demand for the integration and exploitation of\nheterogeneous biomedical information for improved clinical practice, medical\nresearch, and personalised healthcare across the EU. The Health-e-Child project\naims at developing an integrated healthcare platform for European Paediatrics,\nproviding seamless integration of traditional and emerging sources of\nbiomedical information. The long-term goal of the project is to provide\nuninhibited access to universal biomedical knowledge repositories for\npersonalised and preventive healthcare, large-scale information-based\nbiomedical research and training, and informed policy making. The project focus\nwill be on individualised disease prevention, screening, early diagnosis,\ntherapy and follow-up of paediatric heart diseases, inflammatory diseases, and\nbrain tumours. The project will build a Grid-enabled European network of\nleading clinical centres that will share and annotate biomedical data, validate\nsystems clinically, and diffuse clinical excellence across Europe by setting up\nnew technologies, clinical workflows, and standards. This paper outlines the\ndesign approach being adopted in Health-e-Child to enable the delivery of an\nintegrated biomedical information platform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0603112v1", 
    "other_authors": "Joseph S. Kong, Jesse S. A. Bridgewater, Vwani P. Roychowdhury", 
    "title": "A General Framework for Scalability and Performance Analysis of DHT   Routing Systems", 
    "arxiv-id": "cs/0603112v1", 
    "author": "Vwani P. Roychowdhury", 
    "publish": "2006-03-28T22:54:37Z", 
    "summary": "In recent years, many DHT-based P2P systems have been proposed, analyzed, and\ncertain deployments have reached a global scale with nearly one million nodes.\nOne is thus faced with the question of which particular DHT system to choose,\nand whether some are inherently more robust and scalable.\n  Toward developing such a comparative framework, we present the reachable\ncomponent method (RCM) for analyzing the performance of different DHT routing\nsystems subject to random failures. We apply RCM to five DHT systems and obtain\nanalytical expressions that characterize their routability as a continuous\nfunction of system size and node failure probability. An important consequence\nis that in the large-network limit, the routability of certain DHT systems go\nto zero for any non-zero probability of node failure. These DHT routing\nalgorithms are therefore unscalable, while some others, including Kademlia,\nwhich powers the popular eDonkey P2P system, are found to be scalable."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0604109v1", 
    "other_authors": "K. Rabbertz, M. Thomas, S. Ashby, M. Corvo, S. Argir\u00f2, N. Darmenov, R. Darwish, D. Evans, B. Holzman, N. Ratnikova, S. Muzaffar, A. Nowack, T. Wildish, B. Kim, J. Weng, V. B\u00fcge, for the CMS Collaboration", 
    "title": "CMS Software Distribution on the LCG and OSG Grids", 
    "arxiv-id": "cs/0604109v1", 
    "author": "for the CMS Collaboration", 
    "publish": "2006-04-27T16:14:32Z", 
    "summary": "The efficient exploitation of worldwide distributed storage and computing\nresources available in the grids require a robust, transparent and fast\ndeployment of experiment specific software. The approach followed by the CMS\nexperiment at CERN in order to enable Monte-Carlo simulations, data analysis\nand software development in an international collaboration is presented. The\ncurrent status and future improvement plans are described."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605053v1", 
    "other_authors": "Hussein Gibbins, Rajkumar Buyya", 
    "title": "Gridscape II: A Customisable and Pluggable Grid Monitoring Portal and   its Integration with Google Maps", 
    "arxiv-id": "cs/0605053v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-12T09:41:28Z", 
    "summary": "Grid computing has emerged as an effective means of facilitating the sharing\nof distributed heterogeneous resources, enabling collaboration in large scale\nenvironments. However, the nature of Grid systems, coupled with the\noverabundance and fragmentation of information, makes it difficult to monitor\nresources, services, and computations in order to plan and make decisions. In\nthis paper we present Gridscape II, a customisable portal component that can be\nused on its own or plugged in to compliment existing Grid portals. Gridscape II\nmanages the gathering of information from arbitrary, heterogeneous and\ndistributed sources and presents them together seamlessly within a single\ninterface. It also leverages the Google Maps API in order to provide a highly\ninteractive user interface. Gridscape II is simple and easy to use, providing a\nsolution to those users who do not wish to invest heavily in developing their\nown monitoring portal from scratch, and also for those users who want something\nthat is easy to customise and extend for their specific needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605056v1", 
    "other_authors": "Chee Shin Yeo, Marcos Dias de Assuncao, Jia Yu, Anthony Sulistio, Srikumar Venugopal, Martin Placek, Rajkumar Buyya", 
    "title": "Utility Computing and Global Grids", 
    "arxiv-id": "cs/0605056v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-12T20:36:45Z", 
    "summary": "This chapter focuses on the use of Grid technologies to achieve utility\ncomputing. An overview of how Grids can support utility computing is first\npresented through the architecture of Utility Grids. Then, utility-based\nresource allocation is described in detail at each level of the architecture.\nFinally, some industrial solutions for utility computing are discussed."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605057v1", 
    "other_authors": "Rajiv Ranjan, Aaron Harwood, Rajkumar Buyya", 
    "title": "SLA-Based Coordinated Superscheduling Scheme and Performance for   Computational Grids", 
    "arxiv-id": "cs/0605057v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-15T13:41:27Z", 
    "summary": "The Service Level Agreement~(SLA) based grid superscheduling approach\npromotes coordinated resource sharing. Superscheduling is facilitated between\nadministratively and topologically distributed grid sites by grid schedulers\nsuch as Resource brokers. In this work, we present a market-based SLA\ncoordination mechanism. We based our SLA model on a well known \\emph{contract\nnet protocol}.\n  The key advantages of our approach are that it allows:~(i) resource owners to\nhave finer degree of control over the resource allocation that was previously\nnot possible through traditional mechanism; and (ii) superschedulers to bid for\nSLA contracts in the contract net with focus on completing the job within the\nuser specified deadline. In this work, we use simulation to show the\neffectiveness of our proposed approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0605060v1", 
    "other_authors": "Rajiv Ranjan, Aaron Harwood, Rajkumar Buyya", 
    "title": "A Case for Cooperative and Incentive-Based Coupling of Distributed   Clusters", 
    "arxiv-id": "cs/0605060v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-15T10:21:22Z", 
    "summary": "Research interest in Grid computing has grown significantly over the past\nfive years. Management of distributed resources is one of the key issues in\nGrid computing. Central to management of resources is the effectiveness of\nresource allocation as it determines the overall utility of the system. The\ncurrent approaches to superscheduling in a grid environment are non-coordinated\nsince application level schedulers or brokers make scheduling decisions\nindependently of the others in the system. Clearly, this can exacerbate the\nload sharing and utilization problems of distributed resources due to\nsuboptimal schedules that are likely to occur. To overcome these limitations,\nwe propose a mechanism for coordinated sharing of distributed clusters based on\ncomputational economy. The resulting environment, called\n\\emph{Grid-Federation}, allows the transparent use of resources from the\nfederation when local resources are insufficient to meet its users'\nrequirements. The use of computational economy methodology in coordinating\nresource allocation not only facilitates the QoS based scheduling, but also\nenhances utility delivered by resources."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0605141v1", 
    "other_authors": "Amos Korman", 
    "title": "General Compact Labeling Schemes for Dynamic Trees", 
    "arxiv-id": "cs/0605141v1", 
    "author": "Amos Korman", 
    "publish": "2006-05-30T13:54:26Z", 
    "summary": "Let $F$ be a function on pairs of vertices. An {\\em $F$- labeling scheme} is\ncomposed of a {\\em marker} algorithm for labeling the vertices of a graph with\nshort labels, coupled with a {\\em decoder} algorithm allowing one to compute\n$F(u,v)$ of any two vertices $u$ and $v$ directly from their labels. As\napplications for labeling schemes concern mainly large and dynamically changing\nnetworks, it is of interest to study {\\em distributed dynamic} labeling\nschemes. This paper investigates labeling schemes for dynamic trees.\n  This paper presents a general method for constructing labeling schemes for\ndynamic trees. Our method is based on extending an existing {\\em static} tree\nlabeling scheme to the dynamic setting. This approach fits many natural\nfunctions on trees, such as ancestry relation, routing (in both the adversary\nand the designer port models), nearest common ancestor etc.. Our resulting\ndynamic schemes incur overheads (over the static scheme) on the label size and\non the communication complexity. Informally, for any function $k(n)$ and any\nstatic $F$-labeling scheme on trees, we present an $F$-labeling scheme on\ndynamic trees incurring multiplicative overhead factors\n  (over the static scheme) of $O(\\log_{k(n)} n)$ on the label size and\n$O(k(n)\\log_{k(n)} n)$ on the amortized message complexity. In particular, by\nsetting $k(n)=n^{\\epsilon}$ for any $0<\\epsilon<1$, we obtain dynamic labeling\nschemes with asymptotically optimal label sizes and sublinear amortized message\ncomplexity for all the above mentioned functions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606028v1", 
    "other_authors": "E. V. Adutskevich, S. V. Bakhanovich, N. A. Likhoded", 
    "title": "Affine Transformations of Loop Nests for Parallel Execution and   Distribution of Data over Processors", 
    "arxiv-id": "cs/0606028v1", 
    "author": "N. A. Likhoded", 
    "publish": "2006-06-07T12:59:49Z", 
    "summary": "The paper is devoted to the problem of mapping affine loop nests onto\ndistributed memory parallel computers. A method to find affine transformations\nof loop nests for parallel execution and distribution of data over processors\nis presented. The method tends to minimize the number of communications between\nprocessors and to improve locality of data within one processor. A problem of\ndetermination of data exchange sequence is investigated. Conditions to\ndetermine the ability to arrange broadcast is presented."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606047v1", 
    "other_authors": "Giorgos Kollias, Efstratios Gallopoulos, Daniel B. Szyld", 
    "title": "Asynchronous iterative computations with Web information retrieval   structures: The PageRank case", 
    "arxiv-id": "cs/0606047v1", 
    "author": "Daniel B. Szyld", 
    "publish": "2006-06-11T09:36:26Z", 
    "summary": "There are several ideas being used today for Web information retrieval, and\nspecifically in Web search engines. The PageRank algorithm is one of those that\nintroduce a content-neutral ranking function over Web pages. This ranking is\napplied to the set of pages returned by the Google search engine in response to\nposting a search query. PageRank is based in part on two simple common sense\nconcepts: (i)A page is important if many important pages include links to it.\n(ii)A page containing many links has reduced impact on the importance of the\npages it links to. In this paper we focus on asynchronous iterative schemes to\ncompute PageRank over large sets of Web pages. The elimination of the\nsynchronizing phases is expected to be advantageous on heterogeneous platforms.\nThe motivation for a possible move to such large scale distributed platforms\nlies in the size of matrices representing Web structure. In orders of\nmagnitude: $10^{10}$ pages with $10^{11}$ nonzero elements and $10^{12}$ bytes\njust to store a small percentage of the Web (the already crawled); distributed\nmemory machines are necessary for such computations. The present research is\npart of our general objective, to explore the potential of asynchronous\ncomputational models as an underlying framework for very large scale\ncomputations over the Grid. The area of ``internet algorithmics'' appears to\noffer many occasions for computations of unprecedent dimensionality that would\nbe good candidates for this framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606095v1", 
    "other_authors": "Jean Krivine", 
    "title": "A verification algorithm for Declarative Concurrent Programming", 
    "arxiv-id": "cs/0606095v1", 
    "author": "Jean Krivine", 
    "publish": "2006-06-22T13:23:15Z", 
    "summary": "A verification method for distributed systems based on decoupling forward and\nbackward behaviour is proposed. This method uses an event structure based\nalgorithm that, given a CCS process, constructs its causal compression relative\nto a choice of observable actions. Verifying the original process equipped with\ndistributed backtracking on non-observable actions, is equivalent to verifying\nits relative compression which in general is much smaller. We call this method\nDeclarative Concurrent Programming (DCP). DCP technique compares well with\ndirect bisimulation based methods. Benchmarks for the classic dining\nphilosophers problem show that causal compression is rather efficient both\ntime- and space-wise. State of the art verification tools can successfully\nhandle more than 15 agents, whereas they can handle no more than 5 following\nthe traditional direct method; an altogether spectacular improvement, since in\nthis example the specification size is exponential in the number of agents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GCC.2006.49", 
    "link": "http://arxiv.org/pdf/cs/0608046v1", 
    "other_authors": "Arshad Ali, Richard McClatchey, Ashiq Anjum, Irfan Habib, Kamran Soomro, Mohammed Asif, Ali Adil, Athar Mohsin", 
    "title": "From Grid Middleware to a Grid Operating System", 
    "arxiv-id": "cs/0608046v1", 
    "author": "Athar Mohsin", 
    "publish": "2006-08-08T17:41:26Z", 
    "summary": "Grid computing has made substantial advances during the last decade. Grid\nmiddleware such as Globus has contributed greatly in making this possible.\nThere are, however, significant barriers to the adoption of Grid computing in\nother fields, most notably day-to-day user computing environments. We will\ndemonstrate in this paper that this is primarily due to the limitations of the\nexisting Grid middleware which does not take into account the needs of everyday\nscientific and business users. In this paper we will formally advocate a Grid\nOperating System and propose an architecture to migrate Grid computing into a\nGrid operating system which we believe would help remove most of the technical\nbarriers to the adoption of Grid computing and make it relevant to the\nday-to-day user. We believe this proposed transition to a Grid operating system\nwill drive more pervasive Grid computing research and application development\nand deployment in future."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CBMS.2006.109", 
    "link": "http://arxiv.org/pdf/cs/0608047v1", 
    "other_authors": "R. H. McClatchey, D. Manset, A. E. Solomonides", 
    "title": "Lessons Learned from MammoGrid for Integrated Biomedical Solutions", 
    "arxiv-id": "cs/0608047v1", 
    "author": "A. E. Solomonides", 
    "publish": "2006-08-08T17:49:28Z", 
    "summary": "This paper presents an overview of the MammoGrid project and some of its\nachievements. In terms of the global grid project, and European research in\nparticular, the project has successfully demonstrated the capacity of a\ngrid-based system to support effective collaboration between physicians,\nincluding handling and querying image databases, as well as using grid\nservices, such as image standardization and Computer-Aided Detection (CADe) of\nsuspect or indicative features. In terms of scientific results, in radiology,\nthere have been significant epidemiological findings in the assessment of\nbreast density as a risk factor, but the results for CADe are less clear-cut.\nFinally, the foundations of a technology transfer process to establish a\nworking MammoGrid plus system in Spain through the company Maat GKnowledge and\nthe collaboration of CIEMAT and hospitals in Extremadura."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608048v1", 
    "other_authors": "Ashiq Anjum, Richard McClatchey, Arshad Ali, Ian Willers", 
    "title": "Bulk Scheduling with the DIANA Scheduler", 
    "arxiv-id": "cs/0608048v1", 
    "author": "Ian Willers", 
    "publish": "2006-08-08T17:53:15Z", 
    "summary": "Results from the research and development of a Data Intensive and Network\nAware (DIANA) scheduling engine, to be used primarily for data intensive\nsciences such as physics analysis, are described. In Grid analyses, tasks can\ninvolve thousands of computing, data handling, and network resources. The\ncentral problem in the scheduling of these resources is the coordinated\nmanagement of computation and data at multiple locations and not just data\nreplication or movement. However, this can prove to be a rather costly\noperation and efficient sing can be a challenge if compute and data resources\nare mapped without considering network costs. We have implemented an adaptive\nalgorithm within the so-called DIANA Scheduler which takes into account data\nlocation and size, network performance and computation capability in order to\nenable efficient global scheduling. DIANA is a performance-aware and\neconomy-guided Meta Scheduler. It iteratively allocates each job to the site\nthat is most likely to produce the best performance as well as optimizing the\nglobal queue for any remaining jobs. Therefore it is equally suitable whether a\nsingle job is being submitted or bulk scheduling is being performed. Results\nindicate that considerable performance improvements can be gained by adopting\nthe DIANA scheduling approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608092v2", 
    "other_authors": "Ariel Daliot, Danny Dolev", 
    "title": "Self-Stabilizing Byzantine Pulse Synchronization", 
    "arxiv-id": "cs/0608092v2", 
    "author": "Danny Dolev", 
    "publish": "2006-08-24T16:07:07Z", 
    "summary": "The ``Pulse Synchronization'' problem can be loosely described as targeting\nto invoke a recurring distributed event as simultaneously as possible at the\ndifferent nodes and with a frequency that is as regular as possible. This\ntarget becomes surprisingly subtle and difficult to achieve when facing both\ntransient and permanent failures. In this paper we present an algorithm for\npulse synchronization that self-stabilizes while at the same time tolerating a\npermanent presence of Byzantine faults. The Byzantine nodes might incessantly\ntry to de-synchronize the correct nodes. Transient failures might throw the\nsystem into an arbitrary state in which correct nodes have no common notion\nwhat-so-ever, such as time or round numbers, and can thus not infer anything\nfrom their own local states upon the state of other correct nodes. The\npresented algorithm grants nodes the ability to infer that eventually all\ncorrect nodes will invoke their pulses within a very short time interval of\neach other and will do so regularly.\n  Pulse synchronization has previously been shown to be a powerful tool for\ndesigning general self-stabilizing Byzantine algorithms and is hitherto the\nonly method that provides for the general design of efficient practical\nprotocols in the confluence of these two fault models. The difficulty, in\ngeneral, to design any algorithm in this fault model may be indicated by the\nremarkably few algorithms resilient to both fault models. The few published\nself-stabilizing Byzantine algorithms are typically complicated and sometimes\nconverge from an arbitrary initial state only after exponential or super\nexponential time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608096v1", 
    "other_authors": "Ariel Daliot, Danny Dolev, Hanna Parnas", 
    "title": "Linear-time Self-stabilizing Byzantine Clock Synchronization", 
    "arxiv-id": "cs/0608096v1", 
    "author": "Hanna Parnas", 
    "publish": "2006-08-25T03:11:28Z", 
    "summary": "Clock synchronization is a very fundamental task in distributed system. It\nthus makes sense to require an underlying clock synchronization mechanism to be\nhighly fault-tolerant. A self-stabilizing algorithm seeks to attain\nsynchronization once lost; a Byzantine algorithm assumes synchronization is\nnever lost and focuses on containing the influence of the permanent presence of\nfaulty nodes. There are efficient self-stabilizing solutions for clock\nsynchronization as well as efficient solutions that are resilient to Byzantine\nfaults. In contrast, to the best of our knowledge there is no practical\nsolution that is self-stabilizing while tolerating the permanent presence of\nByzantine nodes. We present the first linear-time self-stabilizing Byzantine\nclock synchronization algorithm. Our deterministic clock synchronization\nalgorithm is based on the observation that all clock synchronization algorithms\nrequire events for exchanging clock values and re-synchronizing the clocks to\nwithin safe bounds. These events usually need to happen synchronously at the\ndifferent nodes. In classic Byzantine algorithms this is fulfilled or aided by\nhaving the clocks initially close to each other and thus the actual clock\nvalues can be used for synchronizing the events. This implies that clock values\ncannot differ arbitrarily, which necessarily renders these solutions to be\nnon-stabilizing. Our scheme suggests using an underlying distributed pulse\nsynchronization module that is uncorrelated to the clock values."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608112v1", 
    "other_authors": "B. Hudzia, L. McDermott, T. N. Illahi, M-T. Kechadi", 
    "title": "Entity Based Peer-to-Peer in a Data Grid Environment", 
    "arxiv-id": "cs/0608112v1", 
    "author": "M-T. Kechadi", 
    "publish": "2006-08-29T11:58:50Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, the grid characterisation,\nprogramming environments, etc. In this paper we present a new Grid system\ndedicated to deal with data issues, called DGET (Data Grid Environment and\nTools). DGET is characterized by its peer-to-peer communication system and\nentity-based architecture, therefore, taking advantage of the main\nfunctionality of both systems; P2P and Grid. DGET is currently under\ndevelopment and a prototype implementing the main components is in its first\nphase of testing. In this paper we limit our description to the system\narchitectural features and to the main differences with other systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608113v1", 
    "other_authors": "B. Hudzia, T. N. Ellahi, L. McDermott, T. Kechadi", 
    "title": "A Java Based Architecture of P2P-Grid Middleware", 
    "arxiv-id": "cs/0608113v1", 
    "author": "T. Kechadi", 
    "publish": "2006-08-29T13:11:05Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, characterization, programming\nenvironments, etc. We present a new Grid system dedicated to dealing with data\nissues, called DGET (Data Grid Environment and Tools). DGET is characterized by\nits peerto- peer communication system and entity-based architecture, therefore,\ntaking advantage of the main functionality of both systems; P2P and Grid. DGET\nis currently under development and a prototype implementing the main components\nis in its first phase of testing. In this paper we limit our description to the\nsystem architectural features and to the main differences with other systems.\nKeywords: Grid Computing, Peer to Peer, Peer to Peer Grid"
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608114v1", 
    "other_authors": "Benoit Hudzia, Serge Petiton", 
    "title": "Reliable multicast fault tolerant MPI in the Grid environment", 
    "arxiv-id": "cs/0608114v1", 
    "author": "Serge Petiton", 
    "publish": "2006-08-29T13:14:29Z", 
    "summary": "Grid environments have recently been developed with low stretch and overheads\nthat increase with the logarithm of the number of nodes in the system. Getting\nand sending data to/from a large numbers of nodes is gaining importance due to\nan increasing number of independent data providers and the heterogeneity of the\nnetwork/Grid. One of the key challenges is to achieve a balance between low\nbandwidth consumption and good reliability. In this paper we present an\nimplementation of a reliable multicast protocol over a fault tolerant MPI:\nMPICHV2. It can provide one way to solve the problem of transferring large\nchunks of data between applications running on a grid with limited network\nlinks. We first show that we can achieve similar performance as the MPICH-P4\nimplementation by using multicast with data compression in a cluster. Next, we\nprovide a theoretical cluster organization and GRID network architecture to\nharness the performance provided by using multicast. Finally, we present the\nconclusion and future work."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608116v1", 
    "other_authors": "T. N. Ellahi, B. Hudzia, L. McDermott, T. Kechadi", 
    "title": "Transparent Migration of Multi-Threaded Applications on a Java Based   Grid", 
    "arxiv-id": "cs/0608116v1", 
    "author": "T. Kechadi", 
    "publish": "2006-08-29T13:29:29Z", 
    "summary": "Grid computing has enabled pooling a very large number of heterogeneous\nresource administered by different security domains. Applications are\ndynamically deployed on the resources available at the time. Dynamic nature of\nthe resources and applications requirements makes needs the grid middleware to\nsupport the ability of migrating a running application to a different resource.\nEspecially, Grid applications are typically long running and thus stoping them\nand starting them from scratch is not a feasible option. This paper presents an\noverview of migration support in a java based grid middleware called DGET.\nMigration support in DGET includes multi-threaded migration and asynchronous\nmigration as well."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608118v1", 
    "other_authors": "B. Hudzia, M-T. Kechadi, A. Ottewill", 
    "title": "TreeP: A Tree-Based P2P Network Architecture", 
    "arxiv-id": "cs/0608118v1", 
    "author": "A. Ottewill", 
    "publish": "2006-08-29T15:08:42Z", 
    "summary": "In this paper we proposed a hierarchical P2P network based on a dynamic\npartitioning on a 1-D space. This hierarchy is created and maintained\ndynamically and provides a gridmiddleware (like DGET) a P2P basic functionality\nfor resource discovery and load-balancing.This network architecture is called\nTreeP (Tree based P2P network architecture) and is based on atessellation of a\n1-D space. We show that this topology exploits in an efficient way\ntheheterogeneity feature of the network while limiting the overhead introduced\nby the overlaymaintenance. Experimental results show that this topology is\nhighly resilient to a large number ofnetwork failures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0609027v1", 
    "other_authors": "Rajkumar Buyya, Al-Mukaddim Khan Pathan, James Broberg, Zahir Tari", 
    "title": "A Case for Peering of Content Delivery Networks", 
    "arxiv-id": "cs/0609027v1", 
    "author": "Zahir Tari", 
    "publish": "2006-09-06T23:41:32Z", 
    "summary": "The proliferation of Content Delivery Networks (CDN) reveals that existing\ncontent networks are owned and operated by individual companies. As a\nconsequence, closed delivery networks are evolved which do not cooperate with\nother CDNs and in practice, islands of CDNs are formed. Moreover, the logical\nseparation between contents and services in this context results in two content\nnetworking domains. But present trends in content networks and content\nnetworking capabilities give rise to the interest in interconnecting content\nnetworks. Finding ways for distinct content networks to coordinate and\ncooperate with other content networks is necessary for better overall service.\nIn addition to that, meeting the QoS requirements of users according to the\nnegotiated Service Level Agreements between the user and the content network is\na burning issue in this perspective. In this article, we present an open,\nscalable and Service-Oriented Architecture based system to assist the creation\nof open Content and Service Delivery Networks (CSDN) that scale and support\nsharing of resources with other CSDNs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0609163v1", 
    "other_authors": "Amos Korman, Shay Kutten", 
    "title": "Labeling Schemes with Queries", 
    "arxiv-id": "cs/0609163v1", 
    "author": "Shay Kutten", 
    "publish": "2006-09-29T12:31:35Z", 
    "summary": "We study the question of ``how robust are the known lower bounds of labeling\nschemes when one increases the number of consulted labels''. Let $f$ be a\nfunction on pairs of vertices. An $f$-labeling scheme for a family of graphs\n$\\cF$ labels the vertices of all graphs in $\\cF$ such that for every graph\n$G\\in\\cF$ and every two vertices $u,v\\in G$, the value $f(u,v)$ can be inferred\nby merely inspecting the labels of $u$ and $v$.\n  This paper introduces a natural generalization: the notion of $f$-labeling\nschemes with queries, in which the value $f(u,v)$ can be inferred by inspecting\nnot only the labels of $u$ and $v$ but possibly the labels of some additional\nvertices. We show that inspecting the label of a single additional vertex (one\n{\\em query}) enables us to reduce the label size of many labeling schemes\nsignificantly."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0610131v1", 
    "other_authors": "Loris Marchal, Veronika Rehn, Yves Robert, Fr\u00e9d\u00e9ric Vivien", 
    "title": "Scheduling and data redistribution strategies on star platforms", 
    "arxiv-id": "cs/0610131v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2006-10-23T07:45:01Z", 
    "summary": "In this work we are interested in the problem of scheduling and\nredistributing data on master-slave platforms. We consider the case were the\nworkers possess initial loads, some of which having to be redistributed in\norder to balance their completion times. We examine two different scenarios.\nThe first model assumes that the data consists of independent and identical\ntasks. We prove the NP-completeness in the strong sense for the general case,\nand we present two optimal algorithms for special platform types. Furthermore\nwe propose three heuristics for the general case. Simulations consolidate the\ntheoretical results. The second data model is based on Divisible Load Theory.\nThis problem can be solved in polynomial time by a combination of linear\nprogramming and simple analytical manipulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0611034v1", 
    "other_authors": "Yves Robert, Anne Benoit, Veronika Rehn", 
    "title": "Strategies for Replica Placement in Tree Networks", 
    "arxiv-id": "cs/0611034v1", 
    "author": "Veronika Rehn", 
    "publish": "2006-11-08T08:16:55Z", 
    "summary": "In this paper, we discuss and compare several policies to place replicas in\ntree networks, subject to server capacity and QoS constraints. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. The standard approach in the literature is to enforce that\nall requests of a client be served by the closest server in the tree. We\nintroduce and study two new policies. In the first policy, all requests from a\ngiven client are still processed by the same server, but this server can be\nlocated anywhere in the path from the client to the root. In the second policy,\nthe requests of a given client can be processed by multiple servers. One major\ncontribution of this paper is to assess the impact of these new policies on the\ntotal replication cost. Another important goal is to assess the impact of\nserver heterogeneity, both from a theoretical and a practical perspective. In\nthis paper, we establish several new complexity results, and provide several\nefficient polynomial heuristics for NP-complete instances of the problem. These\nheuristics are compared to an absolute lower bound provided by the formulation\nof the problem in terms of the solution of an integer linear program."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11768869_8", 
    "link": "http://arxiv.org/pdf/cs/0611139v1", 
    "other_authors": "Pierre-Lo\u00efc Garoche, Marc Pantel, Xavier Thirioux", 
    "title": "Static Safety for an Actor Dedicated Process Calculus by Abstract   Interpretation", 
    "arxiv-id": "cs/0611139v1", 
    "author": "Xavier Thirioux", 
    "publish": "2006-11-28T07:48:18Z", 
    "summary": "The actor model eases the definition of concurrent programs with non uniform\nbehaviors. Static analysis of such a model was previously done in a data-flow\noriented way, with type systems. This approach was based on constraint set\nresolution and was not able to deal with precise properties for communications\nof behaviors. We present here a new approach, control-flow oriented, based on\nthe abstract interpretation framework, able to deal with communication of\nbehaviors. Within our new analyses, we are able to verify most of the previous\nproperties we observed as well as new ones, principally based on occurrence\ncounting."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0611165v1", 
    "other_authors": "Ricardo C. Correa, Valmir C. Barbosa", 
    "title": "Partially ordered distributed computations on asynchronous   point-to-point networks", 
    "arxiv-id": "cs/0611165v1", 
    "author": "Valmir C. Barbosa", 
    "publish": "2006-11-30T13:01:36Z", 
    "summary": "Asynchronous executions of a distributed algorithm differ from each other due\nto the nondeterminism in the order in which the messages exchanged are handled.\nIn many situations of interest, the asynchronous executions induced by\nrestricting nondeterminism are more efficient, in an application-specific\nsense, than the others. In this work, we define partially ordered executions of\na distributed algorithm as the executions satisfying some restricted orders of\ntheir actions in two different frameworks, those of the so-called event- and\npulse-driven computations. The aim of these restrictions is to characterize\nasynchronous executions that are likely to be more efficient for some important\nclasses of applications. Also, an asynchronous algorithm that ensures the\noccurrence of partially ordered executions is given for each case. Two of the\napplications that we believe may benefit from the restricted nondeterminism are\nbacktrack search, in the event-driven case, and iterative algorithms for\nsystems of linear equations, in the pulse-driven case."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612013v2", 
    "other_authors": "Al-Mukaddim Khan Pathan, Rajkumar Buyya, James Broberg, Kris Bubendorfer", 
    "title": "Economy-based Content Replication for Peering Content Delivery Networks", 
    "arxiv-id": "cs/0612013v2", 
    "author": "Kris Bubendorfer", 
    "publish": "2006-12-04T06:29:16Z", 
    "summary": "Existing Content Delivery Networks (CDNs) exhibit the nature of closed\ndelivery networks which do not cooperate with other CDNs and in practice,\nislands of CDNs are formed. The logical separation between contents and\nservices in this context results in two content networking domains. In addition\nto that, meeting the Quality of Service requirements of users according to\nnegotiated Service Level Agreement is crucial for a CDN. Present trends in\ncontent networks and content networking capabilities give rise to the interest\nin interconnecting content networks. Hence, in this paper, we present an open,\nscalable, and Service-Oriented Architecture (SOA)-based system that assist the\ncreation of open Content and Service Delivery Networks (CSDNs), which scale and\nsupports sharing of resources through peering with other CSDNs. To encourage\nresource sharing and peering arrangements between different CDN providers at\nglobal level, we propose using market-based models by introducing an\neconomy-based strategy for content replication."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612025v1", 
    "other_authors": "Paul M. B. Vitanyi", 
    "title": "Registers", 
    "arxiv-id": "cs/0612025v1", 
    "author": "Paul M. B. Vitanyi", 
    "publish": "2006-12-05T16:51:25Z", 
    "summary": "Entry in: Encyclopedia of Algorithms, Ming-Yang Kao, Ed., Springer, To\nappear.\n  Synonyms: Wait-free registers, wait-free shared variables, asynchronous\ncommunication hardware. Problem Definition: Consider a system of asynchronous\nprocesses that communicate among themselves by only executing read and write\noperations on a set of shared variables (also known as shared registers). The\nsystem has no global clock or other synchronization primitives."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612035v1", 
    "other_authors": "Antonio Fernandez, Vincent Gramoli, Ernesto Jimenez, Anne-Marie Kermarrec, Michel Raynal", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "arxiv-id": "cs/0612035v1", 
    "author": "Michel Raynal", 
    "publish": "2006-12-06T13:57:54Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services that are\ncapable of dealing with resource assignment and management in a large-scale,\nheterogeneous and unreliable environment. One such service, the slicing\nservice, has been proposed to allow for an automatic partitioning of P2P\nnetworks into groups (slices) that represent a controllable amount of some\nresource and that are also relatively homogeneous with respect to that\nresource, in the face of churn and other failures. In this report we propose\ntwo algorithms to solve the distributed slicing problem. The first algorithm\nimproves upon an existing algorithm that is based on gossip-based sorting of a\nset of uniform random numbers. We speed up convergence via a heuristic for\ngossip peer selection. The second algorithm is based on a different approach:\nstatistical approximation of the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms relies on\ntheir gossip-based models. We present theoretical and experimental results to\nprove the viability of these algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0612125v2", 
    "other_authors": "Anolan Milan\u00e9s, Noemi Rodriguez, Bruno Schulze", 
    "title": "Heterogeneous Strong Computation Migration", 
    "arxiv-id": "cs/0612125v2", 
    "author": "Bruno Schulze", 
    "publish": "2006-12-22T18:27:25Z", 
    "summary": "The continuous increase in performance requirements, for both scientific\ncomputation and industry, motivates the need of a powerful computing\ninfrastructure. The Grid appeared as a solution for inexpensive execution of\nheavy applications in a parallel and distributed manner. It allows combining\nresources independently of their physical location and architecture to form a\nglobal resource pool available to all grid users. However, grid environments\nare highly unstable and unpredictable. Adaptability is a crucial issue in this\ncontext, in order to guarantee an appropriate quality of service to users.\nMigration is a technique frequently used for achieving adaptation. The\nobjective of this report is to survey the problem of strong migration in\nheterogeneous environments like the grids', the related implementation issues\nand the current solutions."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0701015v2", 
    "other_authors": "Pierre Sens, Luciana Arantes, Mathieu Bouillaguet, V\u00e9ronique Martin, Fabiola Greve", 
    "title": "Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants", 
    "arxiv-id": "cs/0701015v2", 
    "author": "Fabiola Greve", 
    "publish": "2007-01-03T12:52:58Z", 
    "summary": "We consider the problem of failure detection in dynamic networks such as\nMANETs. Unreliable failure detectors are classical mechanisms which provide\ninformation about process failures. However, most of current implementations\nconsider that the network is fully connected and that the initial number of\nnodes of the system is known. This assumption is not applicable to dynamic\nenvironments. Furthermore, such implementations are usually timer-based while\nin dynamic networks there is no upper bound for communication delays since\nnodes can move. This paper presents an asynchronous implementation of a failure\ndetector for unknown and mobile networks. Our approach does not rely on timers\nand neither the composition nor the number of nodes in the system are known. We\nprove that our algorithm can implement failure detectors of class <>S when\nbehavioral properties and connectivity conditions are satisfied by the\nunderlying system."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0701064v1", 
    "other_authors": "Kai Engelhardt, Yoram Moses", 
    "title": "Causing Communication Closure: Safe Program Composition with Reliable   Non-FIFO Channels", 
    "arxiv-id": "cs/0701064v1", 
    "author": "Yoram Moses", 
    "publish": "2007-01-09T12:18:14Z", 
    "summary": "A semantic framework for analyzing safe composition of distributed programs\nis presented. Its applicability is illustrated by a study of program\ncomposition when communication is reliable but not necessarily FIFO\\@. In this\nmodel, special care must be taken to ensure that messages do not accidentally\novertake one another in the composed program. We show that barriers do not\nexist in this model. Indeed, no program that sends or receives messages can\nautomatically be composed with arbitrary programs without jeopardizing their\nintended behavior. Safety of composition becomes context-sensitive and new\ntools are needed for ensuring it. A notion of \\emph{sealing} is defined, where\nif a program $P$ is immediately followed by a program $Q$ that seals $P$ then\n$P$ will be communication-closed--it will execute as if it runs in isolation.\nThe investigation of sealing in this model reveals a novel connection between\nLamport causality and safe composition. A characterization of sealable programs\nis given, as well as efficient algorithms for testing if $Q$ seals $P$ and for\nconstructing a seal for a significant class of programs. It is shown that every\nsealable program that is open to interference on $O(n^2)$ channels can be\nsealed using O(n) messages."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0701134v2", 
    "other_authors": "Wenbing Zhao", 
    "title": "Byzantine Fault Tolerance for Nondeterministic Applications", 
    "arxiv-id": "cs/0701134v2", 
    "author": "Wenbing Zhao", 
    "publish": "2007-01-21T20:44:52Z", 
    "summary": "All practical applications contain some degree of nondeterminism. When such\napplications are replicated to achieve Byzantine fault tolerance (BFT), their\nnondeterministic operations must be controlled to ensure replica consistency.\nTo the best of our knowledge, only the most simplistic types of replica\nnondeterminism have been dealt with. Furthermore, there lacks a systematic\napproach to handling common types of nondeterminism. In this paper, we propose\na classification of common types of replica nondeterminism with respect to the\nrequirement of achieving Byzantine fault tolerance, and describe the design and\nimplementation of the core mechanisms necessary to handle such nondeterminism\nwithin a Byzantine fault tolerance framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0701179v1", 
    "other_authors": "Yoann Dieudonn\u00e9, Franck Petit", 
    "title": "Scatter of Weak Robots", 
    "arxiv-id": "cs/0701179v1", 
    "author": "Franck Petit", 
    "publish": "2007-01-27T06:43:04Z", 
    "summary": "In this paper, we first formalize the problem to be solved, i.e., the Scatter\nProblem (SP). We then show that SP cannot be deterministically solved. Next, we\npropose a randomized algorithm for this problem. The proposed solution is\ntrivially self-stabilizing. We then show how to design a self-stabilizing\nversion of any deterministic solution for the Pattern Formation and the\nGathering problems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702066v1", 
    "other_authors": "Matthieu Gallet, Yves Robert, Fr\u00e9d\u00e9ric Vivien", 
    "title": "Comments on \"Design and performance evaluation of load distribution   strategies for multiple loads on heterogeneous linear daisy chain networks''", 
    "arxiv-id": "cs/0702066v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-02-10T17:43:35Z", 
    "summary": "Min, Veeravalli, and Barlas proposed strategies to minimize the overall\nexecution time of one or several divisible loads on a heterogeneous linear\nnetwork, using one or more installments. We show on a very simple example that\nthe proposed approach does not always produce a solution and that, when it\ndoes, the solution is often suboptimal. We also show how to find an optimal\nscheduling for any instance, once the number of installments per load is given.\nFinally, we formally prove that under a linear cost model, as in the original\npaper, an optimal schedule has an infinite number of installments. Such a cost\nmodel can therefore not be sed to design practical multi-installment\nstrategies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702076v2", 
    "other_authors": "Lionel Eyraud-Dubois, Arnaud Legrand, Martin Quinson, Fr\u00e9d\u00e9ric Vivien", 
    "title": "A First Step Towards Automatically Building Network Representations", 
    "arxiv-id": "cs/0702076v2", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-02-13T16:35:04Z", 
    "summary": "To fully harness Grids, users or middlewares must have some knowledge on the\ntopology of the platform interconnection network. As such knowledge is usually\nnot available, one must uses tools which automatically build a topological\nnetwork model through some measurements. In this article, we define a\nmethodology to assess the quality of these network model building tools, and we\napply this methodology to representatives of the main classes of model builders\nand to two new algorithms. We show that none of the main existing techniques\nbuild models that enable to accurately predict the running time of simple\napplication kernels for actual platforms. However some of the new algorithms we\npropose give excellent results in a wide range of situations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702114v1", 
    "other_authors": "David Pritchard", 
    "title": "Nearest Neighbor Network Traversal", 
    "arxiv-id": "cs/0702114v1", 
    "author": "David Pritchard", 
    "publish": "2007-02-20T03:54:12Z", 
    "summary": "A mobile agent in a network wants to visit every node of an n-node network,\nusing a small number of steps. We investigate the performance of the following\n``nearest neighbor'' heuristic: always go to the nearest unvisited node. If the\nnetwork graph never changes, then from (Rosenkrantz, Stearns and Lewis, 1977)\nand (Hurkens and Woeginger, 2004) it follows that Theta(n log n) steps are\nnecessary and sufficient in the worst case. We give a simpler proof of the\nupper bound and an example that improves the best known lower bound.\n  We investigate how the performance of this heuristic changes when it is\ndistributively implemented in a network. Even if network edges are allow to\nfail over time, we show that the nearest neighbor strategy never runs for more\nthan O(n^2) iterations. We also show that any strategy can be forced to take at\nleast n(n-1)/2 steps before all nodes are visited, if the edges of the network\nare deleted in an adversarial way."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703094v1", 
    "other_authors": "Olivier Powell, Sotiris Nikolesteas", 
    "title": "Geographic Routing Around Obstacles in Wireless Sensor Networks", 
    "arxiv-id": "cs/0703094v1", 
    "author": "Sotiris Nikolesteas", 
    "publish": "2007-03-20T12:17:57Z", 
    "summary": "Geographic routing is becoming the protocol of choice for many sensor network\napplications. The current state of the art is unsatisfactory: some algorithms\nare very efficient, however they require a preliminary planarization of the\ncommunication graph. Planarization induces overhead and is not realistic in\nmany scenarios. On the otherhand, georouting algorithms which do not rely on\nplanarization have fairly low success rates and either fail to route messages\naround all but the simplest obstacles or have a high topology control overhead\n(e.g. contour detection algorithms). To overcome these limitations, we propose\nGRIC, the first lightweight and efficient on demand (i.e. all-to-all)\ngeographic routing algorithm which does not require planarization and has\nalmost 100% delivery rates (when no obstacles are added). Furthermore, the\nexcellent behavior of our algorithm is maintained even in the presence of large\nconvex obstacles. The case of hard concave obstacles is also studied; such\nobstacles are hard instances for which performance diminishes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703112v1", 
    "other_authors": "Bharath Ramesh, Srinidhi Varadarajan", 
    "title": "User-level DSM System for Modern High-Performance Interconnection   Networks", 
    "arxiv-id": "cs/0703112v1", 
    "author": "Srinidhi Varadarajan", 
    "publish": "2007-03-22T19:15:00Z", 
    "summary": "In this paper, we introduce a new user-level DSM system which has the ability\nto directly interact with underlying interconnection networks. The DSM system\nprovides the application programmer a flexible API to program parallel\napplications either using shared memory semantics over physically distributed\nmemory or to use an efficient remote memory demand paging technique. We also\nintroduce a new time slice based memory consistency protocol which is used by\nthe DSM system. We present preliminary results from our implementation on a\nsmall Opteron Linux cluster interconnected over Myrinet."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703117v1", 
    "other_authors": "J. L. J. Laredo, E. A. Eiben, M. Schoenauer, P. A. Castillo, A. M. Mora, F. Fernandez, J. J. Merelo", 
    "title": "Self-adaptive Gossip Policies for Distributed Population-based   Algorithms", 
    "arxiv-id": "cs/0703117v1", 
    "author": "J. J. Merelo", 
    "publish": "2007-03-23T11:29:10Z", 
    "summary": "Gossipping has demonstrate to be an efficient mechanism for spreading\ninformation among P2P networks. Within the context of P2P computing, we propose\nthe so-called Evolvable Agent Model for distributed population-based algorithms\nwhich uses gossipping as communication policy, and represents every individual\nas a self-scheduled single thread. The model avoids obsolete nodes in the\npopulation by defining a self-adaptive refresh rate which depends on the\nlatency and bandwidth of the network. Such a mechanism balances the migration\nrate to the congestion of the links pursuing global population coherence. We\nperform an experimental evaluation of this model on a real parallel system and\nobserve how solution quality and algorithm speed scale with the number of\nprocessors with this seamless approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703122v1", 
    "other_authors": "Rastislav Kr\u00e1lovi\u010d, Richard Kr\u00e1lovi\u010d", 
    "title": "Rapid Almost-Complete Broadcasting in Faulty Networks", 
    "arxiv-id": "cs/0703122v1", 
    "author": "Richard Kr\u00e1lovi\u010d", 
    "publish": "2007-03-23T22:38:06Z", 
    "summary": "This paper studies the problem of broadcasting in synchronous point-to-point\nnetworks, where one initiator owns a piece of information that has to be\ntransmitted to all other vertices as fast as possible. The model of fractional\ndynamic faults with threshold is considered: in every step either a fixed\nnumber $T$, or a fraction $\\alpha$, of sent messages can be lost depending on\nwhich quantity is larger.\n  As the main result we show that in complete graphs and hypercubes it is\npossible to inform all but a constant number of vertices, exhibiting only a\nlogarithmic slowdown, i.e. in time $O(D\\log n)$ where $D$ is the diameter of\nthe network and $n$ is the number of vertices.\n  Moreover, for complete graphs under some additional conditions (sense of\ndirection, or $\\alpha<0.55$) the remaining constant number of vertices can be\ninformed in the same time, i.e. $O(\\log n)$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703137v1", 
    "other_authors": "Rajesh Sudarsan, Calvin J. Ribbens", 
    "title": "ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous   Applications in a Parallel Environment", 
    "arxiv-id": "cs/0703137v1", 
    "author": "Calvin J. Ribbens", 
    "publish": "2007-03-27T23:27:54Z", 
    "summary": "Applications in science and engineering often require huge computational\nresources for solving problems within a reasonable time frame. Parallel\nsupercomputers provide the computational infrastructure for solving such\nproblems. A traditional application scheduler running on a parallel cluster\nonly supports static scheduling where the number of processors allocated to an\napplication remains fixed throughout the lifetime of execution of the job. Due\nto the unpredictability in job arrival times and varying resource requirements,\nstatic scheduling can result in idle system resources thereby decreasing the\noverall system throughput. In this paper we present a prototype framework\ncalled ReSHAPE, which supports dynamic resizing of parallel MPI applications\nexecuted on distributed memory platforms. The framework includes a scheduler\nthat supports resizing of applications, an API to enable applications to\ninteract with the scheduler, and a library that makes resizing viable.\nApplications executed using the ReSHAPE scheduler framework can expand to take\nadvantage of additional free processors or can shrink to accommodate a high\npriority application, without getting suspended. In our research, we have\nmainly focused on structured applications that have two-dimensional data arrays\ndistributed across a two-dimensional processor grid. The resize library\nincludes algorithms for processor selection and processor mapping. Experimental\nresults show that the ReSHAPE framework can improve individual job turn-around\ntime and overall system throughput."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/0704.1827v1", 
    "other_authors": "Gerald Krafft", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids", 
    "arxiv-id": "0704.1827v1", 
    "author": "Gerald Krafft", 
    "publish": "2007-04-06T15:59:27Z", 
    "summary": "This paper analyses the possibilities of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1051/epjap:1998151", 
    "link": "http://arxiv.org/pdf/0704.2344v1", 
    "other_authors": "Christian Vollaire, Laurent Nicolas, Alain Nicolas", 
    "title": "Parallel computing for the finite element method", 
    "arxiv-id": "0704.2344v1", 
    "author": "Alain Nicolas", 
    "publish": "2007-04-18T13:20:25Z", 
    "summary": "A finite element method is presented to compute time harmonic microwave\nfields in three dimensional configurations. Nodal-based finite elements have\nbeen coupled with an absorbing boundary condition to solve open boundary\nproblems. This paper describes how the modeling of large devices has been made\npossible using parallel computation, New algorithms are then proposed to\nimplement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and\non a CRAY C98. Analysis of the computation efficiency is performed using simple\nproblems. The electromagnetic scattering of a plane wave by a perfect electric\nconducting airplane is finally given as example."
},{
    "category": "cs.DC", 
    "doi": "10.1051/epjap:1998151", 
    "link": "http://arxiv.org/pdf/0704.2355v1", 
    "other_authors": "Luigi Santocanale", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3", 
    "arxiv-id": "0704.2355v1", 
    "author": "Luigi Santocanale", 
    "publish": "2007-04-18T14:39:09Z", 
    "summary": "We address the problem of &#64257;nding nice labellings for event structures\nof degree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0704.3890v1", 
    "other_authors": "Rodolfo M. Pussente, Valmir C. Barbosa", 
    "title": "An algorithm for clock synchronization with the gradient property in   sensor networks", 
    "arxiv-id": "0704.3890v1", 
    "author": "Valmir C. Barbosa", 
    "publish": "2007-04-30T19:59:14Z", 
    "summary": "We introduce a distributed algorithm for clock synchronization in sensor\nnetworks. Our algorithm assumes that nodes in the network only know their\nimmediate neighborhoods and an upper bound on the network's diameter.\nClock-synchronization messages are only sent as part of the communication,\nassumed reasonably frequent, that already takes place among nodes. The\nalgorithm has the gradient property of [2], achieving an O(1) worst-case skew\nbetween the logical clocks of neighbors. As in the case of [3,8], the\nalgorithm's actions are such that no constant lower bound exists on the rate at\nwhich logical clocks progress in time, and for this reason the lower bound of\n[2,5] that forbids constant skew between neighbors does not apply."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0706.2069v1", 
    "other_authors": "Samuel Thibault, Raymond Namyst, Pierre-Andr\u00e9 Wacrenier", 
    "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors:   the BubbleSched Framework", 
    "arxiv-id": "0706.2069v1", 
    "author": "Pierre-Andr\u00e9 Wacrenier", 
    "publish": "2007-06-14T09:35:30Z", 
    "summary": "Exploiting full computational power of current more and more hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. Unfortunately, most\noperating systems only provide a poor scheduling API that does not allow\napplications to transmit valuable scheduling hints to the system. In a previous\npaper, we showed that using a bubble-based thread scheduler can significantly\nimprove applications' performance in a portable way. However, since\nmultithreaded applications have various scheduling requirements, there is no\nuniversal scheduler that could meet all these needs. In this paper, we present\na framework that allows scheduling experts to implement and experiment with\ncustomized thread schedulers. It provides a powerful API for dynamically\ndistributing bubbles among the machine in a high-level, portable, and efficient\nway. Several examples show how experts can then develop, debug and tune their\nown portable bubble schedulers."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0706.2146v1", 
    "other_authors": "Rajesh Sudarsan, Calvin J. Ribbens", 
    "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel   Computations", 
    "arxiv-id": "0706.2146v1", 
    "author": "Calvin J. Ribbens", 
    "publish": "2007-06-14T15:54:10Z", 
    "summary": "Traditional parallel schedulers running on cluster supercomputers support\nonly static scheduling, where the number of processors allocated to an\napplication remains fixed throughout the execution of the job. This results in\nunder-utilization of idle system resources thereby decreasing overall system\nthroughput. In our research, we have developed a prototype framework called\nReSHAPE, which supports dynamic resizing of parallel MPI applications executing\non distributed memory platforms. The resizing library in ReSHAPE includes\nsupport for releasing and acquiring processors and efficiently redistributing\napplication state to a new set of processors. In this paper, we derive an\nalgorithm for redistributing two-dimensional block-cyclic arrays from $P$ to\n$Q$ processors, organized as 2-D processor grids. The algorithm ensures a\ncontention-free communication schedule for data redistribution if $P_r \\leq\nQ_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row\nand column shifts on the communication schedule to minimize node contention."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3008v1", 
    "other_authors": "Areski Flissi, Philippe Merle", 
    "title": "A Generic Deployment Framework for Grid Computing and Distributed   Applications", 
    "arxiv-id": "0706.3008v1", 
    "author": "Philippe Merle", 
    "publish": "2007-06-20T15:17:47Z", 
    "summary": "Deployment of distributed applications on large systems, and especially on\ngrid infrastructures, becomes a more and more complex task. Grid users spend a\nlot of time to prepare, install and configure middleware and application\nbinaries on nodes, and eventually start their applications. The problem is that\nthe deployment process is composed of many heterogeneous tasks that have to be\norchestrated in a specific correct order. As a consequence, the automatization\nof the deployment process is currently very difficult to reach. To address this\nproblem, we propose in this paper a generic deployment framework allowing to\nautomatize the execution of heterogeneous tasks composing the whole deployment\nprocess. Our approach is based on a reification as software components of all\nrequired deployment mechanisms or existing tools. Grid users only have to\ndescribe the configuration to deploy in a simple natural language instead of\nprogramming or scripting how the deployment process is executed. As a toy\nexample, this framework is used to deploy CORBA component-based applications\nand OpenCCM middleware on one thousand nodes of the French Grid5000\ninfrastructure."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3350v3", 
    "other_authors": "Veronika Rehn-Sonigo", 
    "title": "Optimal Replica Placement in Tree Networks with QoS and Bandwidth   Constraints and the Closest Allocation Policy", 
    "arxiv-id": "0706.3350v3", 
    "author": "Veronika Rehn-Sonigo", 
    "publish": "2007-06-22T15:01:35Z", 
    "summary": "This paper deals with the replica placement problem on fully homogeneous tree\nnetworks known as the Replica Placement optimization problem. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. We investigate the latter problem using the Closest access\npolicy when adding QoS and bandwidth constraints. We propose an optimal\nalgorithm in two passes using dynamic programming."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3546v2", 
    "other_authors": "Samer Al Kiswany, Matei Ripeanu, Sudharshan S. Vazhkudai, Abdullah Gharaibeh", 
    "title": "stdchk: A Checkpoint Storage System for Desktop Grid Computing", 
    "arxiv-id": "0706.3546v2", 
    "author": "Abdullah Gharaibeh", 
    "publish": "2007-06-25T01:24:46Z", 
    "summary": "Checkpointing is an indispensable technique to provide fault tolerance for\nlong-running high-throughput applications like those running on desktop grids.\nThis paper argues that a dedicated checkpoint storage system, optimized to\noperate in these environments, can offer multiple benefits: reduce the load on\na traditional file system, offer high-performance through specialization, and,\nfinally, optimize data management by taking into account checkpoint application\nsemantics. Such a storage system can present a unifying abstraction to\ncheckpoint operations, while hiding the fact that there are no dedicated\nresources to store the checkpoint data. We prototype stdchk, a checkpoint\nstorage system that uses scavenged disk space from participating desktops to\nbuild a low-cost storage system, offering a traditional file system interface\nfor easy integration with applications. This paper presents the stdchk\narchitecture, key performance optimizations, support for incremental\ncheckpointing, and increased data availability. Our evaluation confirms that\nthe stdchk approach is viable in a desktop grid setting and offers a low cost\nstorage system with desirable performance characteristics: high write\nthroughput and reduced storage space and network effort to save checkpoint\nimages."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4009v2", 
    "other_authors": "Anne Benoit, Veronika Rehn-Sonigo, Yves Robert", 
    "title": "Multi-criteria scheduling of pipeline workflows", 
    "arxiv-id": "0706.4009v2", 
    "author": "Yves Robert", 
    "publish": "2007-06-27T13:43:16Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonist criteria should be optimized, such as throughput and latency (or a\ncombination). In this paper, we study the complexity of the bi-criteria mapping\nproblem for pipeline graphs on communication homogeneous platforms. In\nparticular, we assess the complexity of the well-known chains-to-chains problem\nfor different-speed processors, which turns out to be NP-hard. We provide\nseveral efficient polynomial bi-criteria heuristics, and their relative\nperformance is evaluated through extensive simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4015v1", 
    "other_authors": "Christian Boulinier, Franck Petit", 
    "title": "Self-Stabilizing Wavelets and r-Hops Coordination", 
    "arxiv-id": "0706.4015v1", 
    "author": "Franck Petit", 
    "publish": "2007-06-27T12:53:06Z", 
    "summary": "We introduce a simple tool called the wavelet (or, r-wavelet) scheme.\nWavelets deals with coordination among processes which are at most r hops away\nof each other. We present a selfstabilizing solution for this scheme. Our\nsolution requires no underlying structure and works in arbritrary anonymous\nnetworks, i.e., no process identifier is required. Moreover, our solution works\nunder any (even unfair) daemon. Next, we use the wavelet scheme to design\nself-stabilizing layer clocks. We show that they provide an efficient device in\nthe design of local coordination problems at distance r, i.e., r-barrier\nsynchronization and r-local resource allocation (LRA) such as r-local mutual\nexclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some\nsolutions to the r-LRA problem (e.g., r-LME) also provide transformers to\ntransform algorithms written assuming any r-central daemon into algorithms\nworking with any distributed daemon."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4038v2", 
    "other_authors": "Matthieu Gallet, Yves Robert, Fr\u00e9d\u00e9ric Vivien", 
    "title": "Scheduling multiple divisible loads on a linear processor network", 
    "arxiv-id": "0706.4038v2", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-06-27T14:43:13Z", 
    "summary": "Min, Veeravalli, and Barlas have recently proposed strategies to minimize the\noverall execution time of one or several divisible loads on a heterogeneous\nlinear network, using one or more installments. We show on a very simple\nexample that their approach does not always produce a solution and that, when\nit does, the solution is often suboptimal. We also show how to find an optimal\nschedule for any instance, once the number of installments per load is given.\nThen, we formally state that any optimal schedule has an infinite number of\ninstallments under a linear cost model as the one assumed in the original\npapers. Therefore, such a cost model cannot be used to design practical\nmulti-installment strategies. Finally, through extensive simulations we\nconfirmed that the best solution is always produced by the linear programming\napproach, while solutions of the original papers can be far away from the\noptimal."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4298v1", 
    "other_authors": "Christian Boulinier", 
    "title": "Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous   Anonymous Networks", 
    "arxiv-id": "0706.4298v1", 
    "author": "Christian Boulinier", 
    "publish": "2007-06-28T18:51:36Z", 
    "summary": "How to pass from local to global scales in anonymous networks? How to\norganize a selfstabilizing propagation of information with feedback. From the\nAngluin impossibility results, we cannot elect a leader in a general anonymous\nnetwork. Thus, it is impossible to build a rooted spanning tree. Many problems\ncan only be solved by probabilistic methods. In this paper we show how to use\nUnison to design a self-stabilizing barrier synchronization in an anonymous\nnetwork. We show that the commuication structure of this barrier\nsynchronization designs a self-stabilizing wave-stream, or pipelining wave, in\nanonymous networks. We introduce two variants of Wave: the strong waves and the\nwavelets. A strong wave can be used to solve the idempotent r-operator\nparametrized computation problem. A wavelet deals with k-distance computation.\nWe show how to use Unison to design a self-stabilizing wave stream, a\nself-stabilizing strong wave stream and a self-stabilizing wavelet stream."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0365v1", 
    "other_authors": "Heithem Abbes, Christophe C\u00e9rin, Jean-Christophe Dubacq, Mohamed Jemni", 
    "title": "Performance Analysis of Publish/Subscribe Systems", 
    "arxiv-id": "0707.0365v1", 
    "author": "Mohamed Jemni", 
    "publish": "2007-07-03T09:02:45Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. Its technology consists\nmainly in exploiting resources, geographically dispersed, to treat complex\napplications needing big power of calculation and/or important storage\ncapacity. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfigurations, decentralisation and performance\nbecomes more and more essential. Since such properties are exhibited by P2P\nsystems, the convergence of grid computing and P2P computing seems natural. In\nthis context, this paper evaluates the scalability and performance of P2P tools\nfor discovering and registering services. Three protocols are used for this\npurpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of\ntheses protocols related to two criteria: the elapsed time for registrations\nservices and the needed time to discover new services. Our aim is to analyse\nthese results in order to choose the best protocol we can use in order to\ncreate a decentralised middleware for desktop grid."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0740v1", 
    "other_authors": "A. Ali, A. Anjum, J. Bunn, F. Khan, R. McClatchey, H. Newman, C. Steenberg, M. Thomas, Ian Willers", 
    "title": "A Multi Interface Grid Discovery System", 
    "arxiv-id": "0707.0740v1", 
    "author": "Ian Willers", 
    "publish": "2007-07-05T09:22:45Z", 
    "summary": "Discovery Systems (DS) can be considered as entry points for global loosely\ncoupled distributed systems. An efficient Discovery System in essence increases\nthe performance, reliability and decision making capability of distributed\nsystems. With the rapid increase in scale of distributed applications, existing\nsolutions for discovery systems are fast becoming either obsolete or incapable\nof handling such complexity. They are particularly ineffective when handling\nservice lifetimes and providing up-to-date information, poor at enabling\ndynamic service access and they can also impose unwanted restrictions on\ninterfaces to widely available information repositories. In this paper we\npresent essential the design characteristics, an implementation and a\nperformance analysis for a discovery system capable of overcoming these\ndeficiencies in large, globally distributed environments."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0742v1", 
    "other_authors": "A. Ali, A. Anjum, T. Azim, J. Bunn, A. Ikram, R. McClatchey, H. Newman, C. Steenberg, M. Thomas, I. Willers", 
    "title": "Mobile Computing in Physics Analysis - An Indicator for eScience", 
    "arxiv-id": "0707.0742v1", 
    "author": "I. Willers", 
    "publish": "2007-07-05T09:32:29Z", 
    "summary": "This paper presents the design and implementation of a Grid-enabled physics\nanalysis environment for handheld and other resource-limited computing devices\nas one example of the use of mobile devices in eScience. Handheld devices offer\ngreat potential because they provide ubiquitous access to data and\nround-the-clock connectivity over wireless links. Our solution aims to provide\nusers of handheld devices the capability to launch heavy computational tasks on\ncomputational and data Grids, monitor the jobs status during execution, and\nretrieve results after job completion. Users carry their jobs on their handheld\ndevices in the form of executables (and associated libraries). Users can\ntransparently view the status of their jobs and get back their outputs without\nhaving to know where they are being executed. In this way, our system is able\nto act as a high-throughput computing environment where devices ranging from\npowerful desktop machines to small handhelds can employ the power of the Grid.\nThe results shown in this paper are readily applicable to the wider eScience\ncommunity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0743v1", 
    "other_authors": "A. Anjum, R. McClatchey, H. Stockinger, A. Ali, I. Willers, M. Thomas, M. Sagheer, K. Hasham, O. Alvi", 
    "title": "DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling", 
    "arxiv-id": "0707.0743v1", 
    "author": "O. Alvi", 
    "publish": "2007-07-05T09:36:18Z", 
    "summary": "The use of meta-schedulers for resource management in large-scale distributed\nsystems often leads to a hierarchy of schedulers. In this paper, we discuss why\nexisting meta-scheduling hierarchies are sometimes not sufficient for Grid\nsystems due to their inability to re-organise jobs already scheduled locally.\nSuch a job re-organisation is required to adapt to evolving loads which are\ncommon in heavily used Grid infrastructures. We propose a peer-to-peer\nscheduling model and evaluate it using case studies and mathematical modelling.\nWe detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and\nits queue management system for coping with the load distribution and for\nsupporting bulk job scheduling. We demonstrate that such a system is beneficial\nfor dynamic, distributed and self-organizing resource management and can assist\nin optimizing load or job distribution in complex Grid infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0748v1", 
    "other_authors": "F. Estrella, T. Hauer, R. McClatchey, M. Odeh, D Rogulin, T. Solomonides", 
    "title": "Experiences of Engineering Grid-Based Medical Software", 
    "arxiv-id": "0707.0748v1", 
    "author": "T. Solomonides", 
    "publish": "2007-07-05T10:06:41Z", 
    "summary": "Objectives: Grid-based technologies are emerging as potential solutions for\nmanaging and collaborating distributed resources in the biomedical domain. Few\nexamples exist, however, of successful implementations of Grid-enabled medical\nsystems and even fewer have been deployed for evaluation in practice. The\nobjective of this paper is to evaluate the use in clinical practice of a\nGrid-based imaging prototype and to establish directions for engineering future\nmedical Grid developments and their subsequent deployment. Method: The\nMammoGrid project has deployed a prototype system for clinicians using the Grid\nas its information infrastructure. To assist in the specification of the system\nrequirements (and for the first time in healthgrid applications), use-case\nmodelling has been carried out in close collaboration with clinicians and\nradiologists who had no prior experience of this modelling technique. A\ncritical qualitative and, where possible, quantitative analysis of the\nMammoGrid prototype is presented leading to a set of recommendations from the\ndelivery of the first deployed Grid-based medical imaging application. Results:\nWe report critically on the application of software engineering techniques in\nthe specification and implementation of the MammoGrid project and show that\nuse-case modelling is a suitable vehicle for representing medical requirements\nand for communicating effectively with the clinical community. This paper also\ndiscusses the practical advantages and limitations of applying the Grid to\nreal-life clinical applications and presents the consequent lessons learned."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0762v1", 
    "other_authors": "Irfan Habib, Kamran Soomro, Ashiq Anjum, Richard McClatchey, Arshad Ali, Peter Bloodsworth", 
    "title": "PhantomOS: A Next Generation Grid Operating System", 
    "arxiv-id": "0707.0762v1", 
    "author": "Peter Bloodsworth", 
    "publish": "2007-07-05T11:14:45Z", 
    "summary": "Grid Computing has made substantial advances in the past decade; these are\nprimarily due to the adoption of standardized Grid middleware. However Grid\ncomputing has not yet become pervasive because of some barriers that we believe\nhave been caused by the adoption of middleware centric approaches. These\nbarriers include: scant support for major types of applications such as\ninteractive applications; lack of flexible, autonomic and scalable Grid\narchitectures; lack of plug-and-play Grid computing and, most importantly, no\nstraightforward way to setup and administer Grids. PhantomOS is a project which\naims to address many of these barriers. Its goal is the creation of a user\nfriendly pervasive Grid computing platform that facilitates the rapid\ndeployment and easy maintenance of Grids whilst providing support for major\ntypes of applications on Grids of almost any topology. In this paper we present\nthe detailed system architecture and an overview of its implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0862v1", 
    "other_authors": "Richard McClatchey, Ashiq Anjum, Heinz Stockinger, Arshad Ali, Ian Willers, Michael Thomas", 
    "title": "Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments", 
    "arxiv-id": "0707.0862v1", 
    "author": "Michael Thomas", 
    "publish": "2007-07-05T19:46:51Z", 
    "summary": "In Grids scheduling decisions are often made on the basis of jobs being\neither data or computation intensive: in data intensive situations jobs may be\npushed to the data and in computation intensive situations data may be pulled\nto the jobs. This kind of scheduling, in which there is no consideration of\nnetwork characteristics, can lead to performance degradation in a Grid\nenvironment and may result in large processing queues and job execution delays\ndue to site overloads. In this paper we describe a Data Intensive and Network\nAware (DIANA) meta-scheduling approach, which takes into account data,\nprocessing power and network characteristics when making scheduling decisions\nacross multiple sites. Through a practical implementation on a Grid testbed, we\ndemonstrate that queue and execution times of data-intensive jobs can be\nsignificantly improved when we introduce our proposed DIANA scheduler. The\nbasic scheduling decisions are dictated by a weighting factor for each\npotential target location which is a calculated function of network\ncharacteristics, processing cycles and data location and size. The job\nscheduler provides a global ranking of the computing resources and then selects\nan optimal one on the basis of this overall access and execution cost. The\nDIANA approach considers the Grid as a combination of active network elements\nand takes network characteristics as a first class criterion in the scheduling\ndecision matrix along with computation and data. The scheduler can then make\ninformed decisions by taking into account the changing state of the network,\nlocality and size of the data and the pool of available processing cycles."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.1607v1", 
    "other_authors": "Erik Schnetter, Christian D. Ott, Gabrielle Allen, Peter Diener, Tom Goodale, Thomas Radke, Edward Seidel, John Shalf", 
    "title": "Cactus Framework: Black Holes to Gamma Ray Bursts", 
    "arxiv-id": "0707.1607v1", 
    "author": "John Shalf", 
    "publish": "2007-07-11T13:01:50Z", 
    "summary": "Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of\ncosmological origin. They are among the most scientifically interesting\nastrophysical systems, and the riddle concerning their central engines and\nemission mechanisms is one of the most complex and challenging problems of\nastrophysics today. In this article we outline our petascale approach to the\nGRB problem and discuss the computational toolkits and numerical codes that are\ncurrently in use and that will be scaled up to run on emerging petaflop scale\ncomputing platforms in the near future.\n  Petascale computing will require additional ingredients over conventional\nparallelism. We consider some of the challenges which will be caused by future\npetascale architectures, and discuss our plans for the future development of\nthe Cactus framework and its applications to meet these challenges in order to\nprofit from these new architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.0608v2", 
    "other_authors": "Z. Akbar, L. T. Handoko", 
    "title": "Resource Allocation in Public Cluster with Extended Optimization   Algorithm", 
    "arxiv-id": "0708.0608v2", 
    "author": "L. T. Handoko", 
    "publish": "2007-08-04T05:15:05Z", 
    "summary": "We introduce an optimization algorithm for resource allocation in the LIPI\nPublic Cluster to optimize its usage according to incoming requests from users.\nThe tool is an extended and modified genetic algorithm developed to match\nspecific natures of public cluster. We present a detail analysis of\noptimization, and compare the results with the exact calculation. We show that\nit would be very useful and could realize an automatic decision making system\nfor public clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.3446v2", 
    "other_authors": "Z. Akbar, L. T. Handoko", 
    "title": "Multi and Independent Block Approach in Public Cluster", 
    "arxiv-id": "0708.3446v2", 
    "author": "L. T. Handoko", 
    "publish": "2007-08-25T19:46:52Z", 
    "summary": "We present extended multi block approach in the LIPI Public Cluster. The\nmulti block approach enables a cluster to be divided into several independent\nblocks which run jobs owned by different users simultaneously. Previously, we\nhave maintained the blocks using single master node for all blocks due to\nefficiency and resource limitations. Following recent advancements and\nexpansion of node\\'s number, we have modified the multi block approach with\nmultiple master nodes, each of them is responsible for a single block. We argue\nthat this approach improves the overall performance significantly, for\nespecially data intensive computational works."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.3734v1", 
    "other_authors": "Igor Nitto, Rossano Venturini", 
    "title": "Searching for a dangerous host: randomized vs. deterministic", 
    "arxiv-id": "0708.3734v1", 
    "author": "Rossano Venturini", 
    "publish": "2007-08-28T09:24:13Z", 
    "summary": "A Black Hole is an harmful host in a network that destroys incoming agents\nwithout leaving any trace of such event. The problem of locating the black hole\nin a network through a team of agent coordinated by a common protocol is\nusually referred in literature as the Black Hole Search problem (or BHS for\nbrevity) and it is a consolidated research topic in the area of distributed\nalgorithms. The aim of this paper is to extend the results for BHS by\nconsidering more general (and hence harder) classes of dangerous host. In\nparticular we introduce rB-hole as a probabilistic generalization of the Black\nHole, in which the destruction of an incoming agent is a purely random event\nhappening with some fixed probability (like flipping a biased coin). The main\nresult we present is that if we tolerate an arbitrarily small error probability\nin the result then the rB-hole Search problem, or RBS, is not harder than the\nusual BHS. We establish this result in two different communication model,\nspecifically both in presence or absence of whiteboards non-located at the\nhomebase. The core of our methods is a general reduction tool for transforming\nalgorithms for the black hole into algorithms for the rB-hole."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.2635v1", 
    "other_authors": "G. A. Kohring, L. Lo Iacono", 
    "title": "Non-Blocking Signature of very large SOAP Messages", 
    "arxiv-id": "0709.2635v1", 
    "author": "L. Lo Iacono", 
    "publish": "2007-09-17T13:50:42Z", 
    "summary": "Data transfer and staging services are common components in Grid-based, or\nmore generally, in service-oriented applications. Security mechanisms play a\ncentral role in such services, especially when they are deployed in sensitive\napplication fields like e-health. The adoption of WS-Security and related\nstandards to SOAP-based transfer services is, however, problematic as a\nstraightforward adoption of SOAP with MTOM introduces considerable\ninefficiencies in the signature generation process when large data sets are\ninvolved. This paper proposes a non-blocking, signature generation approach\nenabling a stream-like processing with considerable performance enhancements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3689v1", 
    "other_authors": "Liao Ming-Xue, He Xiao-Xin, Fan Zhi-Hua", 
    "title": "Static Deadlock Detection in MPI Synchronization Communication", 
    "arxiv-id": "0709.3689v1", 
    "author": "Fan Zhi-Hua", 
    "publish": "2007-09-24T05:33:29Z", 
    "summary": "It is very common to use dynamic methods to detect deadlocks in MPI programs\nfor the reason that static methods have some restrictions. To guarantee high\nreliability of some important MPI-based application software, a model of MPI\nsynchronization communication is abstracted and a type of static method is\ndevised to examine deadlocks in such modes. The model has three forms with\ndifferent complexity: sequential model, single-loop model and nested-loop\nmodel. Sequential model is a base for all models. Single-loop model must be\ntreated with a special type of equation group and nested-loop model extends the\nmethods for the other two models. A standard Java-based software framework\noriginated from these methods is constructed for determining whether MPI\nprograms are free from synchronization communication deadlocks. Our practice\nshows the software framework is better than those tools using dynamic methods\nbecause it can dig out all synchronization communication deadlocks before an\nMPI-based program goes into running."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3692v2", 
    "other_authors": "Ming-xue Liao, Zhi-hua Fan", 
    "title": "Deadlock Detection in Basic Models of MPI Synchronization Communication   Programs", 
    "arxiv-id": "0709.3692v2", 
    "author": "Zhi-hua Fan", 
    "publish": "2007-09-24T06:02:29Z", 
    "summary": "A model of MPI synchronization communication programs is presented and its\nthree basic simplified models are also defined. A series of theorems and\nmethods for deciding whether deadlocks will occur among the three models are\ngiven and proved strictly. These theories and methods for simple models'\ndeadlock detection are the necessary base for real MPI program deadlock\ndetection. The methods are based on a static analysis through programs and with\nruntime detection in necessary cases and they are able to determine before\ncompiling whether it will be deadlocked for two of the three basic models. For\nanother model, some deadlock cases can be found before compiling and others at\nruntime. Our theorems can be used to prove the correctness of currently popular\nMPI program deadlock detection algorithms. Our methods may decrease codes that\nthose algorithms need to change to MPI source or profiling interface and may\ndetects deadlocks ahead of program execution, thus the overheads can be reduced\ngreatly."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3693v1", 
    "other_authors": "Liao Ming-Xue, He Xiao-Xin, Fan Zhi-Hua", 
    "title": "Algorithm of Static Deadlock Detection in MPI Synchronization   Communication Sequential Model", 
    "arxiv-id": "0709.3693v1", 
    "author": "Fan Zhi-Hua", 
    "publish": "2007-09-24T06:06:34Z", 
    "summary": "Detecting deadlocks in MPI synchronization communication programs is very\ndifficult and need building program models. All complex models are based on\nsequential models. The sequential model is mapped into a set of character\nstrings and its deadlock detection problem is translated into an equivalent\nmulti-queue string matching problem. An algorithm is devised and implemented to\nstatically detect deadlocks in sequential models of MPI synchronization\ncommunication programs. The time and space complexity of the algorithm is O(n)\nwhere n is the amount of message in model. The algorithm is better than usual\ncircle-detection methods and can adapt well to dynamic message stream."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3826v1", 
    "other_authors": "Gerald Krafft", 
    "title": "Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids", 
    "arxiv-id": "0709.3826v1", 
    "author": "Gerald Krafft", 
    "publish": "2007-09-24T19:25:27Z", 
    "summary": "This paper analyses the requirements of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the most promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.1499v1", 
    "other_authors": "Patrik Flor\u00e9en, Petteri Kaski, Topi Musto, Jukka Suomela", 
    "title": "Approximating max-min linear programs with local algorithms", 
    "arxiv-id": "0710.1499v1", 
    "author": "Jukka Suomela", 
    "publish": "2007-10-08T09:46:47Z", 
    "summary": "A local algorithm is a distributed algorithm where each node must operate\nsolely based on the information that was available at system startup within a\nconstant-size neighbourhood of the node. We study the applicability of local\nalgorithms to max-min LPs where the objective is to maximise $\\min_k \\sum_v\nc_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$\nfor each $v$. Here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $V_i =\n\\{v : a_{iv} > 0 \\}$, $V_k = \\{v : c_{kv}>0 \\}$, $I_v = \\{i : a_{iv} > 0 \\}$\nand $K_v = \\{k : c_{kv} > 0 \\}$ have bounded size. In the distributed setting,\neach agent $v$ is responsible for choosing the value of $x_v$, and the\ncommunication network is a hypergraph $\\mathcal{H}$ where the sets $V_k$ and\n$V_i$ constitute the hyperedges. We present inapproximability results for a\nwide range of structural assumptions; for example, even if $|V_i|$ and $|V_k|$\nare bounded by some constants larger than 2, there is no local approximation\nscheme. To contrast the negative results, we present a local approximation\nalgorithm which achieves good approximation ratios if we can bound the relative\ngrowth of the vertex neighbourhoods in $\\mathcal{H}$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.1784v1", 
    "other_authors": "Marc Shapiro, Nuno Pregui\u00e7a", 
    "title": "Designing a commutative replicated data type", 
    "arxiv-id": "0710.1784v1", 
    "author": "Nuno Pregui\u00e7a", 
    "publish": "2007-10-09T14:38:50Z", 
    "summary": "Commuting operations greatly simplify consistency in distributed systems.\nThis paper focuses on designing for commutativity, a topic neglected\npreviously. We show that the replicas of \\emph{any} data type for which\nconcurrent operations commute converges to a correct value, under some simple\nand standard assumptions. We also show that such a data type supports\ntransactions with very low cost. We identify a number of approaches and\ntechniques to ensure commutativity. We re-use some existing ideas\n(non-destructive updates coupled with invariant identification), but propose a\nmuch more efficient implementation. Furthermore, we propose a new technique,\nbackground consensus. We illustrate these ideas with a shared edit buffer data\ntype."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.5348v1", 
    "other_authors": "Cristian Ruz, Fran\u00e7oise Baude, Virginie Legrand Contes", 
    "title": "Towards Grid Monitoring and deployment in Jade, using ProActive", 
    "arxiv-id": "0710.5348v1", 
    "author": "Virginie Legrand Contes", 
    "publish": "2007-10-29T10:44:35Z", 
    "summary": "This document describes our current effort to gridify Jade, a java-based\nenvironment for the autonomic management of clustered J2EE application servers,\ndeveloped in the INRIA SARDES research team. Towards this objective, we use the\njava ProActive grid technology. We first present some of the challenges to turn\nsuch an autonomic management system initially dedicated to distributed\napplications running on clusters of machines, into one that can provide\nself-management capabilities to large-scale systems, i.e. deployed on grid\ninfrastructures. This leads us to a brief state of the art on grid monitoring\nsystems. Then, we recall the architecture of Jade, and consequently propose to\nreorganize it in a potentially more scalable way. Practical experiments pertain\nto the use of the grid deployment feature offered by ProActive to easily\nconduct the deployment of the Jade system or its revised version on any sort of\ngrid."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0314v1", 
    "other_authors": "Aleksandar Lazarevic, Lionel Sacks", 
    "title": "Resource and Application Models for Advanced Grid Schedulers", 
    "arxiv-id": "0711.0314v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:03:46Z", 
    "summary": "As Grid computing is becoming an inevitable future, managing, scheduling and\nmonitoring dynamic, heterogeneous resources will present new challenges.\nSolutions will have to be agile and adaptive, support self-organization and\nautonomous management, while maintaining optimal resource utilisation.\nPresented in this paper are basic principles and architectural concepts for\nefficient resource allocation in heterogeneous Grid environment."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0315v1", 
    "other_authors": "Aleksandar Lazarevic, Lionel Sacks", 
    "title": "Measuring and Monitoring Grid Resource Utilisation", 
    "arxiv-id": "0711.0315v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:12:27Z", 
    "summary": "Effective resource utilisation monitoring and highly granular yet adaptive\nmeasurements are prerequisites for a more efficient Grid scheduler. We present\na suite of measurement applications able to monitor per-process resource\nutilisation, and a customisable tool for emulating observed utilisation models."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0316v1", 
    "other_authors": "Aleksandar Lazarevic, Lionel Sacks", 
    "title": "A Study of Grid Applications: Scheduling Perspective", 
    "arxiv-id": "0711.0316v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:15:45Z", 
    "summary": "As the Grid evolves from a high performance cluster middleware to a\nmultipurpose utility computing framework, a good understanding of Grid\napplications, their statistics and utilisation patterns is required. This study\nlooks at job execution times and resource utilisations in a Grid environment,\nand their significance in cluster and network dimensioning, local level\nscheduling and resource management."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0325v1", 
    "other_authors": "Ioannis Liabotis, Ognjen Prnjat, Tope Olukemi, Adrian Li Mow Ching, Aleksandar Lazarevic, Lionel Sacks, Mike Fisher, Paul McKee", 
    "title": "Self-Organising management of Grid environments", 
    "arxiv-id": "0711.0325v1", 
    "author": "Paul McKee", 
    "publish": "2007-11-02T15:26:48Z", 
    "summary": "This paper presents basic concepts, architectural principles and algorithms\nfor efficient resource and security management in cluster computing\nenvironments and the Grid. The work presented in this paper is funded by\nBTExacT and the EPSRC project SO-GRM (GR/S21939)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0326v1", 
    "other_authors": "Aleksandar Lazarevic, Lionel Sacks, Ognjen Prnjat", 
    "title": "Enabling Adaptive Grid Scheduling and Resource Management", 
    "arxiv-id": "0711.0326v1", 
    "author": "Ognjen Prnjat", 
    "publish": "2007-11-02T15:30:53Z", 
    "summary": "Wider adoption of the Grid concept has led to an increasing amount of\nfederated computational, storage and visualisation resources being available to\nscientists and researchers. Distributed and heterogeneous nature of these\nresources renders most of the legacy cluster monitoring and management\napproaches inappropriate, and poses new challenges in workflow scheduling on\nsuch systems. Effective resource utilisation monitoring and highly granular yet\nadaptive measurements are prerequisites for a more efficient Grid scheduler. We\npresent a suite of measurement applications able to monitor per-process\nresource utilisation, and a customisable tool for emulating observed\nutilisation models. We also outline our future work on a predictive and\nprobabilistic Grid scheduler. The research is undertaken as part of UK\ne-Science EPSRC sponsored project SO-GRM (Self-Organising Grid Resource\nManagement) in cooperation with BT."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0327v1", 
    "other_authors": "Aleksandar Lazarevic, Lionel Sacks, Ognjen Prnjat", 
    "title": "Managing Uncertainty: A Case for Probabilistic Grid Scheduling", 
    "arxiv-id": "0711.0327v1", 
    "author": "Ognjen Prnjat", 
    "publish": "2007-11-02T15:36:36Z", 
    "summary": "The Grid technology is evolving into a global, service-orientated\narchitecture, a universal platform for delivering future high demand\ncomputational services. Strong adoption of the Grid and the utility computing\nconcept is leading to an increasing number of Grid installations running a wide\nrange of applications of different size and complexity. In this paper we\naddress the problem of elivering deadline/economy based scheduling in a\nheterogeneous application environment using statistical properties of job\nhistorical executions and its associated meta-data. This approach is motivated\nby a study of six-month computational load generated by Grid applications in a\nmulti-purpose Grid cluster serving a community of twenty e-Science projects.\nThe observed job statistics, resource utilisation and user behaviour is\ndiscussed in the context of management approaches and models most suitable for\nsupporting a probabilistic and autonomous scheduling architecture."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.1231v3", 
    "other_authors": "Anne Benoit, Veronika Rehn-Sonigo, Yves Robert", 
    "title": "Optimizing Latency and Reliability of Pipeline Workflow Applications", 
    "arxiv-id": "0711.1231v3", 
    "author": "Yves Robert", 
    "publish": "2007-11-08T14:45:12Z", 
    "summary": "Mapping applications onto heterogeneous platforms is a difficult challenge,\neven for simple application patterns such as pipeline graphs. The problem is\neven more complex when processors are subject to failure during the execution\nof the application. In this paper, we study the complexity of a bi-criteria\nmapping which aims at optimizing the latency (i.e., the response time) and the\nreliability (i.e., the probability that the computation will be successful) of\nthe application. Latency is minimized by using faster processors, while\nreliability is increased by replicating computations on a set of processors.\nHowever, replication increases latency (additional communications, slower\nprocessors). The application fails to be executed only if all the processors\nfail during execution. While simple polynomial algorithms can be found for\nfully homogeneous platforms, the problem becomes NP-hard when tackling\nheterogeneous platforms. This is yet another illustration of the additional\ncomplexity added by heterogeneity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.1786v1", 
    "other_authors": "Cyril Dumont, Fabrice Mourlin", 
    "title": "A Mobile Computing Architecture for Numerical Simulation", 
    "arxiv-id": "0711.1786v1", 
    "author": "Fabrice Mourlin", 
    "publish": "2007-11-12T14:39:34Z", 
    "summary": "The domain of numerical simulation is a place where the parallelization of\nnumerical code is common. The definition of a numerical context means the\nconfiguration of resources such as memory, processor load and communication\ngraph, with an evolving feature: the resources availability. A feature is often\nmissing: the adaptability. It is not predictable and the adaptable aspect is\nessential. Without calling into question these implementations of these codes,\nwe create an adaptive use of these implementations. Because the execution has\nto be driven by the availability of main resources, the components of a numeric\ncomputation have to react when their context changes. This paper offers a new\narchitecture, a mobile computing architecture, based on mobile agents and\nJavaSpace. At the end of this paper, we apply our architecture to several case\nstudies and obtain our first results."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.2062v1", 
    "other_authors": "Thomas Sandholm", 
    "title": "Autoregressive Time Series Forecasting of Computational Demand", 
    "arxiv-id": "0711.2062v1", 
    "author": "Thomas Sandholm", 
    "publish": "2007-11-14T00:57:13Z", 
    "summary": "We study the predictive power of autoregressive moving average models when\nforecasting demand in two shared computational networks, PlanetLab and Tycoon.\nDemand in these networks is very volatile, and predictive techniques to plan\nusage in advance can improve the performance obtained drastically.\n  Our key finding is that a random walk predictor performs best for\none-step-ahead forecasts, whereas ARIMA(1,1,0) and adaptive exponential\nsmoothing models perform better for two and three-step-ahead forecasts. A Monte\nCarlo bootstrap test is proposed to evaluate the continuous prediction\nperformance of different models with arbitrary confidence and statistical\nsignificance levels. Although the prediction results differ between the Tycoon\nand PlanetLab networks, we observe very similar overall statistical properties,\nsuch as volatility dynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.3949v1", 
    "other_authors": "Lei Ni, Aaron Harwood", 
    "title": "An Adaptive Checkpointing Scheme for Peer-to-Peer Based Volunteer   Computing Work Flows", 
    "arxiv-id": "0711.3949v1", 
    "author": "Aaron Harwood", 
    "publish": "2007-11-26T06:41:23Z", 
    "summary": "Volunteer Computing, sometimes called Public Resource Computing, is an\nemerging computational model that is very suitable for work-pooled parallel\nprocessing. As more complex grid applications make use of work flows in their\ndesign and deployment it is reasonable to consider the impact of work flow\ndeployment over a Volunteer Computing infrastructure. In this case, the inter\nwork flow I/O can lead to a significant increase in I/O demands at the work\npool server. A possible solution is the use of a Peer-to- Peer based parallel\ncomputing architecture to off-load this I/O demand to the workers; where the\nworkers can fulfill some aspects of work flow coordination and I/O checking,\netc. However, achieving robustness in such a large scale system is a\nchallenging hurdle towards the decentralized execution of work flows and\ngeneral parallel processes. To increase robustness, we propose and show the\nmerits of using an adaptive checkpoint scheme that efficiently checkpoints the\nstatus of the parallel processes according to the estimation of relevant\nnetwork and peer parameters. Our scheme uses statistical data observed during\nruntime to dynamically make checkpoint decisions in a completely de-\ncentralized manner. The results of simulation show support for our proposed\napproach in terms of reduced required runtime."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0712.3980v1", 
    "other_authors": "Antonio Fernandez, Vincent Gramoli, Ernesto Jimenez, Anne-Marie Kermarrec, Michel Raynal", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "arxiv-id": "0712.3980v1", 
    "author": "Michel Raynal", 
    "publish": "2007-12-26T13:55:47Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services capable of\ndealing with resource assignment and management in a large-scale, heterogeneous\nand unreliable environment. The slicing service, has been proposed to allow for\nan automatic partitioning of P2P networks into groups (slices) that represent a\ncontrollable amount of some resource and that are also relatively homogeneous\nwith respect to that resource. In this paper we propose two gossip-based\nalgorithms to solve the distributed slicing problem. The first algorithm speeds\nup an existing algorithm sorting a set of uniform random numbers. The second\nalgorithm statistically approximates the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms rely on\ntheir gossip-based models. These algorithms are proved viable theoretically and\nexperimentally."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1210v1", 
    "other_authors": "Daniel Lombrana Gonzalez, Francisco Fernandez de Vega, L. Trujillo, G. Olague, F. Chavez de la O, M. Cardenas, L. Araujo, P. Castillo, K. Sharman", 
    "title": "Increasing GP Computing Power via Volunteer Computing", 
    "arxiv-id": "0801.1210v1", 
    "author": "K. Sharman", 
    "publish": "2008-01-08T11:36:35Z", 
    "summary": "This paper describes how it is possible to increase GP Computing Power via\nVolunteer Computing (VC) using the BOINC framework. Two experiments using\nwell-known GP tools -Lil-gp & ECJ- are performed in order to demonstrate the\nbenefit of using VC in terms of computing power and speed up. Finally we\npresent an extension of the model where any GP tool or framework can be used\ninside BOINC regardless of its programming language, complexity or required\noperating system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1419v1", 
    "other_authors": "Vincent Gramoli, Anne-Marie Kermarrec, Achour Mostefaoui, Michel Raynal, Bruno Sericola", 
    "title": "Core Persistence in Peer-to-Peer Systems: Relating Size to Lifetime", 
    "arxiv-id": "0801.1419v1", 
    "author": "Bruno Sericola", 
    "publish": "2008-01-09T12:41:15Z", 
    "summary": "Distributed systems are now both very large and highly dynamic. Peer to peer\noverlay networks have been proved efficient to cope with this new deal that\ntraditional approaches can no longer accommodate. While the challenge of\norganizing peers in an overlay network has generated a lot of interest leading\nto a large number of solutions, maintaining critical data in such a network\nremains an open issue. In this paper, we are interested in defining the portion\nof nodes and frequency one has to probe, given the churn observed in the\nsystem, in order to achieve a given probability of maintaining the persistence\nof some critical data. More specifically, we provide a clear result relating\nthe size and the frequency of the probing set along with its proof as well as\nan analysis of the way of leveraging such an information in a large scale\ndynamic distributed system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1772v1", 
    "other_authors": "Anne Benoit, Harald Kosch, Veronika Rehn-Sonigo, Yves Robert", 
    "title": "Bi-criteria Pipeline Mappings for Parallel Image Processing", 
    "arxiv-id": "0801.1772v1", 
    "author": "Yves Robert", 
    "publish": "2008-01-11T14:48:43Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonistic criteria should be optimized, such as throughput and latency (or a\ncombination). Typical applications include digital image processing, where\nimages are processed in steady-state mode. In this paper, we study the mapping\nof a particular image processing application, the JPEG encoding. Mapping\npipelined JPEG encoding onto parallel platforms is useful for instance for\nencoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete,\nwe concentrate on the evaluation and performance of polynomial heuristics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.4150v1", 
    "other_authors": "G. Diaz, J. Florez-Lopez, V. Hamar, H. Hoeger, C. Mendoza, Z. Mendez, L. A. Nunez, N. Ruiz, R. Torrens, M. Uzcategui", 
    "title": "e-Science perspectives in Venezuela", 
    "arxiv-id": "0801.4150v1", 
    "author": "M. Uzcategui", 
    "publish": "2008-01-27T20:22:29Z", 
    "summary": "We describe the e-Science strategy in Venezuela, in particular initiatives by\nthe Centro Nacional de Calculo Cientifico Universidad de Los Andes (CECALCULA),\nMerida, the Universidad de Los Andes (ULA), Merida, and the Instituto\nVenezolano de Investigaciones Cientificas (IVIC), Caracas. We present the plans\nfor the Venezuelan Academic Grid and the current status of Grid ULA supported\nby Internet2. We show different web-based scientific applications that are\nbeing developed in quantum chemistry, atomic physics, structural damage\nanalysis, biomedicine and bioclimate within the framework of the\nE-Infrastructure shared between Europe and Latin America (EELA)"
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0802.0550v1", 
    "other_authors": "Erwan Le Merrer, Vincent Gramoli, Anne-Marie Kermarrec, Aline Viana, Marin Bertier", 
    "title": "Energy Aware Self-Organizing Density Management in Wireless Sensor   Networks", 
    "arxiv-id": "0802.0550v1", 
    "author": "Marin Bertier", 
    "publish": "2008-02-05T07:03:28Z", 
    "summary": "Energy consumption is the most important factor that determines sensor node\nlifetime. The optimization of wireless sensor network lifetime targets not only\nthe reduction of energy consumption of a single sensor node but also the\nextension of the entire network lifetime. We propose a simple and adaptive\nenergy-conserving topology management scheme, called SAND (Self-Organizing\nActive Node Density). SAND is fully decentralized and relies on a distributed\nprobing approach and on the redundancy resolution of sensors for energy\noptimizations, while preserving the data forwarding and sensing capabilities of\nthe network. We present the SAND's algorithm, its analysis of convergence, and\nsimulation results. Simulation results show that, though slightly increasing\npath lengths from sensor to sink nodes, the proposed scheme improves\nsignificantly the network lifetime for different neighborhood densities\ndegrees, while preserving both sensing and routing fidelity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0804.1607v2", 
    "other_authors": "S. Sundhar Ram, V. V. Veeravalli, A. Nedic", 
    "title": "Distributed and Recursive Parameter Estimation in Parametrized Linear   State-Space Models", 
    "arxiv-id": "0804.1607v2", 
    "author": "A. Nedic", 
    "publish": "2008-04-10T03:47:05Z", 
    "summary": "We consider a network of sensors deployed to sense a spatio-temporal field\nand estimate a parameter of interest. We are interested in the case where the\ntemporal process sensed by each sensor can be modeled as a state-space process\nthat is perturbed by random noise and parametrized by an unknown parameter. To\nestimate the unknown parameter from the measurements that the sensors\nsequentially collect, we propose a distributed and recursive estimation\nalgorithm, which we refer to as the incremental recursive prediction error\nalgorithm. This algorithm has the distributed property of incremental gradient\nalgorithms and the on-line property of recursive prediction error algorithms.\nWe study the convergence behavior of the algorithm and provide sufficient\nconditions for its convergence. Our convergence result is rather general and\ncontains as special cases the known convergence results for the incremental\nversions of the least-mean square algorithm. Finally, we use the algorithm\ndeveloped in this paper to identify the source of a gas-leak (diffusing source)\nin a closed warehouse and also report numerical simulations to verify\nconvergence."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0804.4590v1", 
    "other_authors": "Heithem Abbes, Christophe C\u00e9rin, Jean-Christophe Dubacq, Mohamed Jemni", 
    "title": "\u00c9tude de performance des syst\u00e8mes de d\u00e9couverte de ressources", 
    "arxiv-id": "0804.4590v1", 
    "author": "Mohamed Jemni", 
    "publish": "2008-04-29T11:51:19Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. This technology consists\nmainly in exploiting PC resources, geographically dispersed, to treat time\nconsuming applications and/or important storage capacity requiring\napplications. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfiguration, decentralization and performance\nbecomes more and more essential. In this context, this paper evaluates the\nscalability and performance of P2P tools for registering and discovering\nservices (Publish/Subscribe systems). Three protocols are used in this purpose:\nBonjour, Avahi and Pastry. We have studied the behaviour of these protocols\nrelated to two criteria: the elapsed time for registrations services and the\nneeded time to discover new services."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0804.4815v1", 
    "other_authors": "Patrik Flor\u00e9en, Marja Hassinen, Petteri Kaski, Jukka Suomela", 
    "title": "Tight local approximation results for max-min linear programs", 
    "arxiv-id": "0804.4815v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-04-30T12:54:34Z", 
    "summary": "In a bipartite max-min LP, we are given a bipartite graph $\\myG = (V \\cup I\n\\cup K, E)$, where each agent $v \\in V$ is adjacent to exactly one constraint\n$i \\in I$ and exactly one objective $k \\in K$. Each agent $v$ controls a\nvariable $x_v$. For each $i \\in I$ we have a nonnegative linear constraint on\nthe variables of adjacent agents. For each $k \\in K$ we have a nonnegative\nlinear objective function of the variables of adjacent agents. The task is to\nmaximise the minimum of the objective functions. We study local algorithms\nwhere each agent $v$ must choose $x_v$ based on input within its\nconstant-radius neighbourhood in $\\myG$. We show that for every $\\epsilon>0$\nthere exists a local algorithm achieving the approximation ratio ${\\Delta_I (1\n- 1/\\Delta_K)} + \\epsilon$. We also show that this result is the best possible\n-- no local algorithm can achieve the approximation ratio ${\\Delta_I (1 -\n1/\\Delta_K)}$. Here $\\Delta_I$ is the maximum degree of a vertex $i \\in I$, and\n$\\Delta_K$ is the maximum degree of a vertex $k \\in K$. As a methodological\ncontribution, we introduce the technique of graph unfolding for the design of\nlocal approximation algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.0444v2", 
    "other_authors": "David Eisenstat", 
    "title": "Two-enqueuer queue in Common2", 
    "arxiv-id": "0805.0444v2", 
    "author": "David Eisenstat", 
    "publish": "2008-05-04T21:08:50Z", 
    "summary": "The question of whether all shared objects with consensus number 2 belong to\nCommon2, the set of objects that can be implemented in a wait-free manner by\nany type of consensus number 2, was first posed by Herlihy. In the absence of\ngeneral results, several researchers have obtained implementations for\nrestricted-concurrency versions of FIFO queues. We present the first Common2\nalgorithm for a queue with two enqueuers and any number of dequeuers."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.2068v1", 
    "other_authors": "Christian Cachin, Idit Keidar, Alexander Shraer", 
    "title": "Fork Sequential Consistency is Blocking", 
    "arxiv-id": "0805.2068v1", 
    "author": "Alexander Shraer", 
    "publish": "2008-05-14T14:23:53Z", 
    "summary": "We consider an untrusted server storing shared data on behalf of clients. We\nshow that no storage access protocol can on the one hand preserve sequential\nconsistency and wait-freedom when the server is correct, and on the other hand\nalways preserve fork sequential consistency."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.3196v1", 
    "other_authors": "Fr\u00e9d\u00e9ric Autran, Jean-Philippe Auzelle, Denise Cattan, Jean-Luc Garnier, Dominique Luzeaux, Fr\u00e9d\u00e9rique Mayer, Marc Peyrichon, Jean-Ren\u00e9 Ruault", 
    "title": "Coupling Component Systems towards Systems of Systems", 
    "arxiv-id": "0805.3196v1", 
    "author": "Jean-Ren\u00e9 Ruault", 
    "publish": "2008-05-21T05:02:22Z", 
    "summary": "Systems of systems (SoS) are a hot topic in our \"fully connected global\nworld\". Our aim is not to provide another definition of what SoS are, but\nrather to focus on the adequacy of reusing standard system architecting\ntechniques within this approach in order to improve performance, fault\ndetection and safety issues in large-scale coupled systems that definitely\nqualify as SoS, whatever the definition is. A key issue will be to secure the\navailability of the services provided by the SoS despite the evolution of the\nvarious systems composing the SoS. We will also tackle contracting issues and\nresponsibility transfers, as they should be addressed to ensure the expected\nbehavior of the SoS whilst the various independently contracted systems evolve\nasynchronously."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0808.1505v1", 
    "other_authors": "Ittai Abraham, Danny Dolev, Joseph Y. Halpern", 
    "title": "An Almost-Surely Terminating Polynomial Protocol for Asynchronous   Byzantine Agreement with Optimal Resilience", 
    "arxiv-id": "0808.1505v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2008-08-11T12:22:12Z", 
    "summary": "Consider an asynchronous system with private channels and $n$ processes, up\nto $t$ of which may be faulty. We settle a longstanding open question by\nproviding a Byzantine agreement protocol that simultaneously achieves three\nproperties:\n  1. (optimal) resilience: it works as long as $n>3t$\n  2. (almost-sure) termination: with probability one, all nonfaulty processes\nterminate\n  3. (polynomial) efficiency: the expected computation time, memory\nconsumption, message size, and number of messages sent are all polynomial in\n$n$.\n  Earlier protocols have achieved only two of these three properties. In\nparticular, the protocol of Bracha is not polynomially efficient, the protocol\nof Feldman and Micali is not optimally resilient, and the protocol of Canetti\nand Rabin does not have almost-sure termination. Our protocol utilizes a new\nprimitive called shunning (asynchronous) verifiable secret sharing (SVSS),\nwhich ensures, roughly speaking, that either a secret is successfully shared or\na new faulty process is ignored from this point onwards by some nonfaulty\nprocess."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0808.1802v1", 
    "other_authors": "Robert L. Grossman, Yunhong Gu, Michael Sabala, Wanzhi Zhang", 
    "title": "Compute and Storage Clouds Using Wide Area High Performance Networks", 
    "arxiv-id": "0808.1802v1", 
    "author": "Wanzhi Zhang", 
    "publish": "2008-08-13T09:48:37Z", 
    "summary": "We describe a cloud based infrastructure that we have developed that is\noptimized for wide area, high performance networks and designed to support data\nmining applications. The infrastructure consists of a storage cloud called\nSector and a compute cloud called Sphere. We describe two applications that we\nhave built using the cloud and some experimental studies."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0808.3019v1", 
    "other_authors": "Robert L Grossman, Yunhong Gu", 
    "title": "Data Mining Using High Performance Data Clouds: Experimental Studies   Using Sector and Sphere", 
    "arxiv-id": "0808.3019v1", 
    "author": "Yunhong Gu", 
    "publish": "2008-08-22T01:24:06Z", 
    "summary": "We describe the design and implementation of a high performance cloud that we\nhave used to archive, analyze and mine large distributed data sets. By a cloud,\nwe mean an infrastructure that provides resources and/or services over the\nInternet. A storage cloud provides storage services, while a compute cloud\nprovides compute services. We describe the design of the Sector storage cloud\nand how it provides the storage services required by the Sphere compute cloud.\nWe also describe the programming paradigm supported by the Sphere compute\ncloud. Sector and Sphere are designed for analyzing large data sets using\ncomputer clusters connected with wide area high performance networks (for\nexample, 10+ Gb/s). We describe a distributed data mining application that we\nhave developed using Sector and Sphere. Finally, we describe some experimental\nstudies comparing Sector/Sphere to Hadoop."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0808.3535v1", 
    "other_authors": "Ioan Raicu, Yong Zhao, Ian Foster, Alex Szalay", 
    "title": "Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for   Data Intensive Applications", 
    "arxiv-id": "0808.3535v1", 
    "author": "Alex Szalay", 
    "publish": "2008-08-26T15:19:44Z", 
    "summary": "Data intensive applications often involve the analysis of large datasets that\nrequire large amounts of compute and storage resources. While dedicated compute\nand/or storage farms offer good task/data throughput, they suffer low resource\nutilization problem under varying workloads conditions. If we instead move such\ndata to distributed computing resources, then we incur expensive data transfer\ncost. In this paper, we propose a data diffusion approach that combines dynamic\nresource provisioning, on-demand data replication and caching, and data\nlocality-aware scheduling to achieve improved resource efficiency under varying\nworkloads. We define an abstract \"data diffusion model\" that takes into\nconsideration the workload characteristics, data accessing cost, application\nthroughput and resource utilization; we validate the model using a real-world\nlarge-scale astronomy application. Our results show that data diffusion can\nincrease the performance index by as much as 34X, and improve application\nresponse time by over 506X, while achieving near-optimal throughputs and\nexecution times."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0808.3536v1", 
    "other_authors": "Ioan Raicu, Zhao Zhang, Mike Wilde, Ian Foster", 
    "title": "Enabling Loosely-Coupled Serial Job Execution on the IBM BlueGene/P   Supercomputer and the SiCortex SC5832", 
    "arxiv-id": "0808.3536v1", 
    "author": "Ian Foster", 
    "publish": "2008-08-26T16:59:41Z", 
    "summary": "Our work addresses the enabling of the execution of highly parallel\ncomputations composed of loosely coupled serial jobs with no modifications to\nthe respective applications, on large-scale systems. This approach allows\nnew-and potentially far larger-classes of application to leverage systems such\nas the IBM Blue Gene/P supercomputer and similar emerging petascale\narchitectures. We present here the challenges of I/O performance encountered in\nmaking this model practical, and show results using both micro-benchmarks and\nreal applications on two large-scale systems, the BG/P and the SiCortex SC5832.\nOur preliminary benchmarks show that we can scale to 4096 processors on the\nBlue Gene/P and 5832 processors on the SiCortex with high efficiency, and can\nachieve thousands of tasks/sec sustained execution rates for parallel workloads\nof ordinary serial applications. We measured applications from two domains,\neconomic energy modeling and molecular dynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SC.2008.5219768", 
    "link": "http://arxiv.org/pdf/0808.3540v2", 
    "other_authors": "Ioan Raicu, Zhao Zhang, Mike Wilde, Ian Foster, Pete Beckman, Kamil Iskra, Ben Clifford", 
    "title": "Towards Loosely-Coupled Programming on Petascale Systems", 
    "arxiv-id": "0808.3540v2", 
    "author": "Ben Clifford", 
    "publish": "2008-08-26T16:48:14Z", 
    "summary": "We have extended the Falkon lightweight task execution framework to make\nloosely coupled programming on petascale systems a practical and useful\nprogramming model. This work studies and measures the performance factors\ninvolved in applying this approach to enable the use of petascale systems by a\nbroader user community, and with greater ease. Our work enables the execution\nof highly parallel computations composed of loosely coupled serial jobs with no\nmodifications to the respective applications. This approach allows a new-and\npotentially far larger-class of applications to leverage petascale systems,\nsuch as the IBM Blue Gene/P supercomputer. We present the challenges of I/O\nperformance encountered in making this model practical, and show results using\nboth microbenchmarks and real applications from two domains: economic energy\nmodeling and molecular dynamics. Our benchmarks show that we can scale up to\n160K processor-cores with high efficiency, and can achieve sustained execution\nrates of thousands of tasks per second."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1383519.1383521", 
    "link": "http://arxiv.org/pdf/0808.3546v1", 
    "other_authors": "Ioan Raicu, Yong Zhao, Ian Foster, Alex Szalay", 
    "title": "Accelerating Large-scale Data Exploration through Data Diffusion", 
    "arxiv-id": "0808.3546v1", 
    "author": "Alex Szalay", 
    "publish": "2008-08-26T16:02:50Z", 
    "summary": "Data-intensive applications often require exploratory analysis of large\ndatasets. If analysis is performed on distributed resources, data locality can\nbe crucial to high throughput and performance. We propose a \"data diffusion\"\napproach that acquires compute and storage resources dynamically, replicates\ndata in response to demand, and schedules computations close to data. As demand\nincreases, more resources are acquired, thus allowing faster response to\nsubsequent requests that refer to the same data; when demand drops, resources\nare released. This approach can provide the benefits of dedicated hardware\nwithout the associated high costs, depending on workload and resource\ncharacteristics. The approach is reminiscent of cooperative caching,\nweb-caching, and peer-to-peer storage systems, but addresses different\napplication demands. Other data-aware scheduling approaches assume dedicated\nresources, which can be expensive and/or inefficient if load varies\nsignificantly. To explore the feasibility of the data diffusion approach, we\nhave extended the Falkon resource provisioning and task scheduling system to\nsupport data caching and data-aware scheduling. Performance results from both\nmicro-benchmarks and a large scale astronomy application demonstrate that our\napproach improves performance relative to alternative approaches, as well as\nprovides improved scalability as aggregated I/O bandwidth scales linearly with\nthe number of data cache nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0808.3558v1", 
    "other_authors": "Rajkumar Buyya, Chee Shin Yeo, Srikumar Venugopal", 
    "title": "Market-Oriented Cloud Computing: Vision, Hype, and Reality for   Delivering IT Services as Computing Utilities", 
    "arxiv-id": "0808.3558v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2008-08-26T17:16:11Z", 
    "summary": "This keynote paper: presents a 21st century vision of computing; identifies\nvarious computing paradigms promising to deliver the vision of computing\nutilities; defines Cloud computing and provides the architecture for creating\nmarket-oriented Clouds by leveraging technologies such as VMs; provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; presents some representative Cloud platforms\nespecially those developed in industries along with our current work towards\nrealising market-oriented resource allocation of Clouds by leveraging the 3rd\ngeneration Aneka enterprise Grid technology; reveals our early thoughts on\ninterconnecting Clouds for dynamically creating an atmospheric computing\nenvironment along with pointers to future community research; and concludes\nwith the need for convergence of competing IT paradigms for delivering our 21st\ncentury vision."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0808.3693v1", 
    "other_authors": "Xavier Grehant, J. M. Dana", 
    "title": "Providing Virtual Execution Environments: A Twofold Illustration", 
    "arxiv-id": "0808.3693v1", 
    "author": "J. M. Dana", 
    "publish": "2008-08-27T12:48:39Z", 
    "summary": "Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0809.1181v2", 
    "other_authors": "Yunhong Gu, Robert L Grossman", 
    "title": "Sector and Sphere: Towards Simplified Storage and Processing of Large   Scale Distributed Data", 
    "arxiv-id": "0809.1181v2", 
    "author": "Robert L Grossman", 
    "publish": "2008-09-06T18:37:51Z", 
    "summary": "Cloud computing has demonstrated that processing very large datasets over\ncommodity clusters can be done simply given the right programming model and\ninfrastructure. In this paper, we describe the design and implementation of the\nSector storage cloud and the Sphere compute cloud. In contrast to existing\nstorage and compute clouds, Sector can manage data not only within a data\ncenter, but also across geographically distributed data centers. Similarly, the\nSphere compute cloud supports User Defined Functions (UDF) over data both\nwithin a data center and across data centers. As a special case, MapReduce\nstyle programming can be implemented in Sphere by using a Map UDF followed by a\nReduce UDF. We describe some experimental studies comparing Sector/Sphere and\nHadoop using the Terasort Benchmark. In these studies, Sector is about twice as\nfast as Hadoop. Sector/Sphere is open source."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1583991.1584058", 
    "link": "http://arxiv.org/pdf/0809.1489v1", 
    "other_authors": "Patrik Flor\u00e9en, Joel Kaasinen, Petteri Kaski, Jukka Suomela", 
    "title": "An optimal local approximation algorithm for max-min linear programs", 
    "arxiv-id": "0809.1489v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-09-09T06:11:31Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\napproximating max-min LPs. The objective is to maximise $\\omega$ subject to $Ax\n\\le 1$, $Cx \\ge \\omega 1$, and $x \\ge 0$ for nonnegative matrices $A$ and $C$.\nThe approximation ratio of our algorithm is the best possible for any local\nalgorithm; there is a matching unconditional lower bound."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1583991.1584058", 
    "link": "http://arxiv.org/pdf/0809.4107v1", 
    "other_authors": "Jean-Claude Laprie, Karama Kanoun, Mohamed Kaaniche", 
    "title": "Modelling interdependencies between the electricity and information   infrastructures", 
    "arxiv-id": "0809.4107v1", 
    "author": "Mohamed Kaaniche", 
    "publish": "2008-09-24T07:26:09Z", 
    "summary": "The aim of this paper is to provide qualitative models characterizing\ninterdependencies related failures of two critical infrastructures: the\nelectricity infrastructure and the associated information infrastructure. The\ninterdependencies of these two infrastructures are increasing due to a growing\nconnection of the power grid networks to the global information infrastructure,\nas a consequence of market deregulation and opening. These interdependencies\nincrease the risk of failures. We focus on cascading, escalating and\ncommon-cause failures, which correspond to the main causes of failures due to\ninterdependencies. We address failures in the electricity infrastructure, in\ncombination with accidental failures in the information infrastructure, then we\nshow briefly how malicious attacks in the information infrastructure can be\naddressed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2175v1", 
    "other_authors": "Valentin Polishchuk, Jukka Suomela", 
    "title": "A simple local 3-approximation algorithm for vertex cover", 
    "arxiv-id": "0810.2175v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-10-13T12:45:15Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\nfinding a 3-approximate vertex cover in bounded-degree graphs. The algorithm is\ndeterministic, and no auxiliary information besides port numbering is required."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2226v1", 
    "other_authors": "Bogdan Nicolae, Gabriel Antoniu, Luc Boug\u00e9", 
    "title": "Enabling Lock-Free Concurrent Fine-Grain Access to Massive Distributed   Data: Application to Supernovae Detection", 
    "arxiv-id": "0810.2226v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2008-10-13T13:07:18Z", 
    "summary": "We consider the problem of efficiently managing massive data in a large-scale\ndistributed environment. We consider data strings of size in the order of\nTerabytes, shared and accessed by concurrent clients. On each individual\naccess, a segment of a string, of the order of Megabytes, is read or modified.\nOur goal is to provide the clients with efficient fine-grain access the data\nstring as concurrently as possible, without locking the string itself. This\nissue is crucial in the context of applications in the field of astronomy,\ndatabases, data mining and multimedia. We illustrate these requiremens with the\ncase of an application for searching supernovae. Our solution relies on\ndistributed, RAM-based data storage, while leveraging a DHT-based, parallel\nmetadata management scheme. The proposed architecture and algorithms have been\nvalidated through a software prototype and evaluated in a cluster environment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2227v1", 
    "other_authors": "Bogdan Nicolae, Gabriel Antoniu, Luc Boug\u00e9", 
    "title": "Distributed Management of Massive Data: an Efficient Fine-Grain Data   Access Scheme", 
    "arxiv-id": "0810.2227v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2008-10-13T13:08:14Z", 
    "summary": "This paper addresses the problem of efficiently storing and accessing massive\ndata blocks in a large-scale distributed environment, while providing efficient\nfine-grain access to data subsets. This issue is crucial in the context of\napplications in the field of databases, data mining and multimedia. We propose\na data sharing service based on distributed, RAM-based storage of data, while\nleveraging a DHT-based, natively parallel metadata management scheme. As\nopposed to the most commonly used grid storage infrastructures that provide\nmechanisms for explicit data localization and transfer, we provide a\ntransparent access model, where data are accessed through global identifiers.\nOur proposal has been validated through a prototype implementation whose\npreliminary evaluation provides promising results."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.3715v1", 
    "other_authors": "Carlo Fischione, Alberto Speranzon, Karl H. Johansson, Alberto Sangiovanni-Vincentelli", 
    "title": "Distributed Estimation over Wireless Sensor Networks with Packet Losses", 
    "arxiv-id": "0810.3715v1", 
    "author": "Alberto Sangiovanni-Vincentelli", 
    "publish": "2008-10-21T01:00:26Z", 
    "summary": "A distributed adaptive algorithm to estimate a time-varying signal, measured\nby a wireless sensor network, is designed and analyzed. One of the major\nfeatures of the algorithm is that no central coordination among the nodes needs\nto be assumed. The measurements taken by the nodes of the network are affected\nby noise, and the communication among the nodes is subject to packet losses.\nNodes exchange local estimates and measurements with neighboring nodes. Each\nnode of the network locally computes adaptive weights that minimize the\nestimation error variance. Decentralized conditions on the weights, needed for\nthe convergence of the estimation error throughout the overall network, are\npresented. A Lipschitz optimization problem is posed to guarantee stability and\nthe minimization of the variance. An efficient strategy to distribute the\ncomputation of the optimal solution is investigated. A theoretical performance\nanalysis of the distributed algorithm is carried out both in the presence of\nperfect and lossy links. Numerical simulations illustrate performance for\nvarious network topologies and packet loss probabilities."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.3836v3", 
    "other_authors": "Bertrand Ducourthial, Sofiane Khalfallah, Franck Petit", 
    "title": "Best-effort Group Service in Dynamic Networks", 
    "arxiv-id": "0810.3836v3", 
    "author": "Franck Petit", 
    "publish": "2008-10-21T13:58:50Z", 
    "summary": "We propose a group membership service for dynamic ad hoc networks. It\nmaintains as long as possible the existing groups and ensures that each group\ndiameter is always smaller than a constant, fixed according to the application\nusing the groups. The proposed protocol is self-stabilizing and works in\ndynamic distributed systems. Moreover, it ensures a kind of continuity in the\nservice offer to the application while the system is converging, except if too\nstrong topology changes happen. Such a best effort behavior allows applications\nto rely on the groups while the stabilization has not been reached, which is\nvery useful in dynamic ad hoc networks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.4440v1", 
    "other_authors": "Shlomi Dolev, Nir Tzachar", 
    "title": "Randomization Adaptive Self-Stabilization", 
    "arxiv-id": "0810.4440v1", 
    "author": "Nir Tzachar", 
    "publish": "2008-10-24T12:15:40Z", 
    "summary": "We present a scheme to convert self-stabilizing algorithms that use\nrandomization during and following convergence to self-stabilizing algorithms\nthat use randomization only during convergence. We thus reduce the number of\nrandom bits from an infinite number to a bounded number. The scheme is\napplicable to the cases in which there exits a local predicate for each node,\nsuch that global consistency is implied by the union of the local predicates.\nWe demonstrate our scheme over the token circulation algorithm of Herman and\nthe recent constant time Byzantine self-stabilizing clock synchronization\nalgorithm by Ben-Or, Dolev and Hoch. The application of our scheme results in\nthe first constant time Byzantine self-stabilizing clock synchronization\nalgorithm that uses a bounded number of random bits."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5439v1", 
    "other_authors": "Peter Tr\u00f6ger", 
    "title": "The Multi-Core Era - Trends and Challenges", 
    "arxiv-id": "0810.5439v1", 
    "author": "Peter Tr\u00f6ger", 
    "publish": "2008-10-30T08:34:48Z", 
    "summary": "Since the very beginning of hardware development, computer processors were\ninvented with ever-increasing clock frequencies and sophisticated in-build\noptimization strategies. Due to physical limitations, this 'free lunch' of\nspeedup has come to an end.\n  The following article gives a summary and bibliography for recent trends and\nchallenges in CMP architectures. It discusses how 40 years of parallel\ncomputing research need to be considered in the upcoming multi-core era. We\nargue that future research must be driven from two sides - a better expression\nof hardware structures, and a domain-specific understanding of software\nparallelism."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5596v1", 
    "other_authors": "R. Nuriyev", 
    "title": "Programming languages with algorithmically parallelizing problem", 
    "arxiv-id": "0810.5596v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-10-31T00:25:15Z", 
    "summary": "The study consists of two parts. Objective of the first part is modern\nlanguage constructions responsible for algorithmically insolvability of\nparallelizing problem. Second part contains several ways to modify the\nconstructions to make the problem algorithmically solvable"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5732v1", 
    "other_authors": "R. Nuriyev", 
    "title": "Practical language based on systems of definitions", 
    "arxiv-id": "0810.5732v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-10-31T16:42:45Z", 
    "summary": "The article suggests a description of a system of tables with a set of\nspecial lists absorbing a semantics of data and reflects a fullness of data. It\nshows how their parallel processing can be constructed based on the\ndescriptions. The approach also might be used for definition intermediate\ntargets for data mining and unstructured data processing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5758v1", 
    "other_authors": "Renat Nuriyev", 
    "title": "Non procedural language for parallel programs", 
    "arxiv-id": "0810.5758v1", 
    "author": "Renat Nuriyev", 
    "publish": "2008-10-31T18:44:38Z", 
    "summary": "Probably building non procedural languages is the most prospective way for\nparallel programming just because non procedural means no fixed way for\nexecution. The article consists of 3 parts. In first part we consider formal\nsystems for definition a named datasets and studying an expression power of\ndifferent subclasses. In the second part we consider a complexity of algorithms\nof building sets by the definitions. In third part we consider a fullness and\nflexibility of the class of program based data set definitions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0811.1504v1", 
    "other_authors": "R. Nuriyev", 
    "title": "Parallel execution of portfolio optimization", 
    "arxiv-id": "0811.1504v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-11-10T15:52:25Z", 
    "summary": "Analysis of asset liability management (ALM) strategies especially for long\nterm horizon is a crucial issue for banks, funds and insurance companies.\n  Modern economic models, investment strategies and optimization criteria make\nALM studies computationally very intensive task. It attracts attention to\nmultiprocessor system and especially to the cheapest one: multi core PCs and PC\nclusters.\n  In this article we are analyzing problem of parallel organization of\nportfolio optimization, results of using clusters for optimization and the most\nefficient cluster architecture for these kinds of tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-89335-6_9", 
    "link": "http://arxiv.org/pdf/0811.3176v1", 
    "other_authors": "Ezra N. Hoch, Danny Bickson, Danny Dolev", 
    "title": "Self-stabilizing Numerical Iterative Computation", 
    "arxiv-id": "0811.3176v1", 
    "author": "Danny Dolev", 
    "publish": "2008-11-19T19:11:46Z", 
    "summary": "Many challenging tasks in sensor networks, including sensor calibration,\nranking of nodes, monitoring, event region detection, collaborative filtering,\ncollaborative signal processing, {\\em etc.}, can be formulated as a problem of\nsolving a linear system of equations. Several recent works propose different\ndistributed algorithms for solving these problems, usually by using linear\niterative numerical methods.\n  In this work, we extend the settings of the above approaches, by adding\nanother dimension to the problem. Specifically, we are interested in {\\em\nself-stabilizing} algorithms, that continuously run and converge to a solution\nfrom any initial state. This aspect of the problem is highly important due to\nthe dynamic nature of the network and the frequent changes in the measured\nenvironment.\n  In this paper, we link together algorithms from two different domains. On the\none hand, we use the rich linear algebra literature of linear iterative methods\nfor solving systems of linear equations, which are naturally distributed with\nrapid convergence properties. On the other hand, we are interested in\nself-stabilizing algorithms, where the input to the computation is constantly\nchanging, and we would like the algorithms to converge from any initial state.\nWe propose a simple novel method called \\syncAlg as a self-stabilizing variant\nof the linear iterative methods. We prove that under mild conditions the\nself-stabilizing algorithm converges to a desired result. We further extend\nthese results to handle the asynchronous case.\n  As a case study, we discuss the sensor calibration problem and provide\nsimulation results to support the applicability of our approach."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-89335-6_9", 
    "link": "http://arxiv.org/pdf/0812.0736v1", 
    "other_authors": "Alain Bui, Olivier Flauzac, Cyril Rabat", 
    "title": "Fully distributed and fault tolerant task management based on diffusions", 
    "arxiv-id": "0812.0736v1", 
    "author": "Cyril Rabat", 
    "publish": "2008-12-03T14:58:19Z", 
    "summary": "The task management is a critical component for the computational grids. The\naim is to assign tasks on nodes according to a global scheduling policy and a\nview of local resources of nodes. A peer-to-peer approach for the task\nmanagement involves a better scalability for the grid and a higher fault\ntolerance. But some mechanisms have to be proposed to avoid the computation of\nreplicated tasks that can reduce the efficiency and increase the load of nodes.\nIn the same way, these mechanisms have to limit the number of exchanged\nmessages to avoid the overload of the network.\n  In a previous paper, we have proposed two methods for the task management\ncalled active and passive. These methods are based on a random walk: they are\nfully distributed and fault tolerant. Each node owns a local tasks states set\nupdated thanks to a random walk and each node is in charge of the local\nassignment. Here, we propose three methods to improve the efficiency of the\nactive method. These new methods are based on a circulating word. The nodes\nlocal tasks states sets are updated thanks to periodical diffusions along trees\nbuilt from the circulating word. Particularly, we show that these methods\nincrease the efficiency of the active method: they produce less replicated\ntasks. These three methods are also fully distributed and fault tolerant. On\nthe other way, the circulating word can be exploited for other applications\nlike the resources management or the nodes synchronization."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GCE.2008.4738445", 
    "link": "http://arxiv.org/pdf/0901.0131v1", 
    "other_authors": "Ian Foster, Yong Zhao, Ioan Raicu, Shiyong Lu", 
    "title": "Cloud Computing and Grid Computing 360-Degree Compared", 
    "arxiv-id": "0901.0131v1", 
    "author": "Shiyong Lu", 
    "publish": "2008-12-31T19:13:05Z", 
    "summary": "Cloud Computing has become another buzzword after Web 2.0. However, there are\ndozens of different definitions for Cloud Computing and there seems to be no\nconsensus on what a Cloud is. On the other hand, Cloud Computing is not a\ncompletely new concept; it has intricate connection to the relatively new but\nthirteen-year established Grid Computing paradigm, and other relevant\ntechnologies such as utility computing, cluster computing, and distributed\nsystems in general. This paper strives to compare and contrast Cloud Computing\nwith Grid Computing from various angles and give insights into the essential\ncharacteristics of both."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.0134v1", 
    "other_authors": "Zhao Zhang, Allan Espinosa, Kamil Iskra, Ioan Raicu, Ian Foster, Michael Wilde", 
    "title": "Design and Evaluation of a Collective IO Model for Loosely Coupled   Petascale Programming", 
    "arxiv-id": "0901.0134v1", 
    "author": "Michael Wilde", 
    "publish": "2008-12-31T19:35:07Z", 
    "summary": "Loosely coupled programming is a powerful paradigm for rapidly creating\nhigher-level applications from scientific programs on petascale systems,\ntypically using scripting languages. This paradigm is a form of many-task\ncomputing (MTC) which focuses on the passing of data between programs as\nordinary files rather than messages. While it has the significant benefits of\ndecoupling producer and consumer and allowing existing application programs to\nbe executed in parallel with no recoding, its typical implementation using\nshared file systems places a high performance burden on the overall system and\non the user who will analyze and consume the downstream data. Previous efforts\nhave achieved great speedups with loosely coupled programs, but have done so\nwith careful manual tuning of all shared file system access. In this work, we\nevaluate a prototype collective IO model for file-based MTC. The model enables\nefficient and easy distribution of input data files to computing nodes and\ngathering of output results from them. It eliminates the need for such manual\ntuning and makes the programming of large-scale clusters using a loosely\ncoupled model easier. Our approach, inspired by in-memory approaches to\ncollective operations for parallel programming, builds on fast local file\nsystems to provide high-speed local file caches for parallel scripts, uses a\nbroadcast approach to handle distribution of common input data, and uses\nefficient scatter/gather and caching techniques for input and output. We\ndescribe the design of the prototype model, its implementation on the Blue\nGene/P supercomputer, and present preliminary measurements of its performance\non synthetic benchmarks and on a large-scale molecular dynamics application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.1307v1", 
    "other_authors": "Sylvain Collange, Yoginder Dandass, Marc Daumas, David Defour", 
    "title": "Using Graphics Processors for Parallelizing Hash-based Data Carving", 
    "arxiv-id": "0901.1307v1", 
    "author": "David Defour", 
    "publish": "2009-01-09T20:15:26Z", 
    "summary": "The ability to detect fragments of deleted image files and to reconstruct\nthese image files from all available fragments on disk is a key activity in the\nfield of digital forensics. Although reconstruction of image files from the\nfile fragments on disk can be accomplished by simply comparing the content of\nsectors on disk with the content of known files, this brute-force approach can\nbe time consuming. This paper presents results from research into the use of\nGraphics Processing Units (GPUs) in detecting specific image file byte patterns\nin disk clusters. Unique identifying pattern for each disk sector is compared\nagainst patterns in known images. A pattern match indicates the potential\npresence of an image and flags the disk sector for further in-depth examination\nto confirm the match. The GPU-based implementation outperforms the software\nimplementation by a significant margin."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.3384v1", 
    "other_authors": "Michael I. Ham, Marko A. Rodriguez", 
    "title": "A Boundary Approximation Algorithm for Distributed Sensor Networks", 
    "arxiv-id": "0901.3384v1", 
    "author": "Marko A. Rodriguez", 
    "publish": "2009-01-22T00:59:01Z", 
    "summary": "We present an algorithm for boundary approximation in locally-linked sensor\nnetworks that communicate with a remote monitoring station. Delaunay\ntriangulations and Voronoi diagrams are used to generate a sensor communication\nnetwork and define boundary segments between sensors, respectively. The\nproposed algorithm reduces remote station communication by approximating\nboundaries via a decentralized computation executed within the sensor network.\nMoreover, the algorithm identifies boundaries based on differences between\nneighboring sensor readings, and not absolute sensor values. An analysis of the\nbandwidth consumption of the algorithm is presented and compared to two naive\napproaches. The proposed algorithm reduces the amount of remote communication\n(compared to the naive approaches) and becomes increasingly useful in networks\nwith more nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0902.2685v2", 
    "other_authors": "J. T. Mo\u015bcicki, F. Brochu, J. Ebke, U. Egede, J. Elmsheuser, K. Harrison, R. W. L. Jones, H. C. Lee, D. Liko, A. Maier, A. Muraru, G. N. Patrick, K. Pajchel, W. Reece, B. H. Samset, M. W. Slater, A. Soroko, C. L. Tan, D. C. Vanderster, M. Williams", 
    "title": "Ganga: a tool for computational-task management and easy access to Grid   resources", 
    "arxiv-id": "0902.2685v2", 
    "author": "M. Williams", 
    "publish": "2009-02-16T13:31:44Z", 
    "summary": "In this paper, we present the computational task-management tool Ganga, which\nallows for the specification, submission, bookkeeping and post-processing of\ncomputational tasks on a wide set of distributed resources. Ganga has been\ndeveloped to solve a problem increasingly common in scientific projects, which\nis that researchers must regularly switch between different processing systems,\neach with its own command set, to complete their computational tasks. Ganga\nprovides a homogeneous environment for processing data on heterogeneous\nresources. We give examples from High Energy Physics, demonstrating how an\nanalysis can be developed on a local system and then transparently moved to a\nGrid system for processing of all available data. Ganga has an API that can be\nused via an interactive interface, in scripts, or through a GUI. Specific\nknowledge about types of tasks or computational resources is provided at\nrun-time through a plugin system, making new developments easy to integrate. We\ngive an overview of the Ganga architecture, give examples of current use, and\ndemonstrate how Ganga can be used in many different areas of science."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0902.4730v1", 
    "other_authors": "Saul Youssef, John Brunelle, John Huth, David C. Parkes, Margo Seltzer, Jim Shank", 
    "title": "Minimal Economic Distributed Computing", 
    "arxiv-id": "0902.4730v1", 
    "author": "Jim Shank", 
    "publish": "2009-02-27T00:00:55Z", 
    "summary": "In an ideal distributed computing infrastructure, users would be able to use\ndiverse distributed computing resources in a simple coherent way, with\nguaranteed security and efficient use of shared resources in accordance with\nthe wishes of the owners of the resources. Our strategy for approaching this\nideal is to first find the simplest structure within which these goals can\nplausibly be achieved. This structure, we find, is given by a particular\nrecursive distributive lattice freely constructed from a presumed partially\nordered set of all data in the infrastructure. Minor syntactic adjustments to\nthe resulting algebra yields a simple language resembling a UNIX shell, a\nconcept of execution and an interprocess protocol. Persons, organizations and\nservers within the system express their interests explicitly via a hierarchical\ncurrency. The currency provides a common framework for treating authentication,\naccess control and resource sharing as economic problems while also introducing\na new dimension for improving the infrastructure over time by designing system\ncomponents which compete with each other to earn the currency. We explain these\nresults, discuss experience with an implementation called egg and point out\nareas where more research is needed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0904.4181v1", 
    "other_authors": "Husam Alustwani, Jacques M. Bahi, Ahmed Mostefaoui, Michel Salomon", 
    "title": "Java Technology : a Strategic Solution for Interactive Distributed   Applications", 
    "arxiv-id": "0904.4181v1", 
    "author": "Michel Salomon", 
    "publish": "2009-04-27T15:16:32Z", 
    "summary": "In a world demanding the best performance from financial investments,\ndistributed applications occupy the first place among the proposed solutions.\nThis particularity is due to their distributed architecture which is able to\nacheives high performance. Currently, many research works aim to develop tools\nthat facilitate the implementation of such applications. The urgent need for\nsuch applications in all areas pushes researchers to accelerate this process.\nHowever, the lack of standardization results in the absence of strategic\ndecisions taken by computer science community. In this article, we argue that\nJava technology represents an elegant compromise ahead of the list of the\ncurrently available solutions. In fact, by promoting the independence of\nhardware and software, Java technology makes it possible to overcome pitfalls\nthat are inherent to the creation of distributed applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0905.1113v1", 
    "other_authors": "Bogdan Nicolae, Gabriel Antoniu, Luc Boug\u00e9", 
    "title": "BlobSeer: How to Enable Efficient Versioning for Large Object Storage   under Heavy Access Concurrency", 
    "arxiv-id": "0905.1113v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2009-05-07T19:37:48Z", 
    "summary": "To accommodate the needs of large-scale distributed P2P systems, scalable\ndata management strategies are required, allowing applications to efficiently\ncope with continuously growing, highly dis tributed data. This paper addresses\nthe problem of efficiently stor ing and accessing very large binary data\nobjects (blobs). It proposesan efficient versioning scheme allowing a large\nnumber of clients to concurrently read, write and append data to huge blobs\nthat are fragmented and distributed at a very large scale. Scalability under\nheavy concurrency is achieved thanks to an original metadata scheme, based on a\ndistributed segment tree built on top of a Distributed Hash Table (DHT). Our\napproach has been implemented and experimented within our BlobSeer prototype on\nthe Grid'5000 testbed, using up to 175 nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0905.1786v1", 
    "other_authors": "Alain Cournier, Swan Dubois, Vincent Villain", 
    "title": "Une CNS pour l'acheminement de messages instantan\u00e9ment stabilisant", 
    "arxiv-id": "0905.1786v1", 
    "author": "Vincent Villain", 
    "publish": "2009-05-12T08:28:47Z", 
    "summary": "A snap-stabilizing algorithm ensures that it always behaves according to its\nspecifications whenever it starts from an arbitrary configuration. In this\npaper, we interest in the message forwarding problem in a message-switched\nnetwork. We must manage network ressources in order to deliver messages to any\nprocessor of the network. In this goal, we need information given by a routing\nalgorithm. But, due to the context of stabilization, this information can be\ninitially corrupted. It is why the existence of snap-stabilizing algorithms for\nthis task (proved in [CDV09]) implies that we can ask the system to begin\nforwarding messages even if routing tables are initially corrupted. In this\npaper, we generalize the previous result given a necessary and sufficient\ncondition to solve the forwarding problem in a snap-stabilizing way."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0905.4147v1", 
    "other_authors": "Zvika Brakerski, Boaz Patt-Shamir", 
    "title": "Distributed Discovery of Large Near-Cliques", 
    "arxiv-id": "0905.4147v1", 
    "author": "Boaz Patt-Shamir", 
    "publish": "2009-05-26T09:51:35Z", 
    "summary": "Given an undirected graph and $0\\le\\epsilon\\le1$, a set of nodes is called\n$\\epsilon$-near clique if all but an $\\epsilon$ fraction of the pairs of nodes\nin the set have a link between them. In this paper we present a fast\nsynchronous network algorithm that uses small messages and finds a near-clique.\nSpecifically, we present a constant-time algorithm that finds, with constant\nprobability of success, a linear size $\\epsilon$-near clique if there exists an\n$\\epsilon^3$-near clique of linear size in the graph. The algorithm uses\nmessages of $O(\\log n)$ bits. The failure probability can be reduced to\n$n^{-\\Omega(1)}$ in $O(\\log n)$ time, and the algorithm also works if the graph\ncontains a clique of size $\\Omega(n/\\log^{\\alpha}\\log n)$ for some $\\alpha \\in\n(0,1)$."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0906.0281v1", 
    "other_authors": "I. Firmansyah, Z. Akbar, L. T. Handoko", 
    "title": "Microcontroller based distributed and networked control system for   public cluster", 
    "arxiv-id": "0906.0281v1", 
    "author": "L. T. Handoko", 
    "publish": "2009-06-01T13:25:21Z", 
    "summary": "We present the architecture and application of the distributed control in\npublic cluster, a parallel machine which is open for public access. Following\nthe nature of public cluster, the integrated distributed control system is\nfully accessible through network using a user-friendly web interface. The\nsystem is intended mainly to control the power of each node in a block of\nparallel computers provided to certain users. This is especially important to\nextend the life-time of related hardwares, and to reduce the whole running and\nmaintainance costs. The system consists of two parts : the master- and\nnode-controllers, and both are connected each other through RS-485 interface.\nEach node-controller is assigned with a unique address to distinguish each of\nthem. We also discuss briefly the implementation of the system at the LIPI\nPublic Cluster."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0906.1328v1", 
    "other_authors": "Wei Zhou, Jianfeng Zhan, Dan Meng", 
    "title": "Multidimensional Analysis of System Logs in Large-scale Cluster Systems", 
    "arxiv-id": "0906.1328v1", 
    "author": "Dan Meng", 
    "publish": "2009-06-07T06:03:14Z", 
    "summary": "It is effective to improve the reliability and availability of large-scale\ncluster systems through the analysis of failures. Existed failure analysis\nmethods understand and analyze failures from one or few dimension. The analysis\nresults are partial and with less precision because of the limitation of data\nsource. This paper presents multidimensional analysis based on graph mining to\nanalyze multi-source system logs, which is a promising failure analysis method\nto get more complete and precise failure knowledge."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0906.1346v6", 
    "other_authors": "Jianfeng Zhan, Lei Wang, Bibo Tu, Yong Li, Peng Wang, Wei Zhou, Dan Meng", 
    "title": "Phoenix Cloud: Consolidating Different Computing Loads on Shared Cluster   System for Large Organization", 
    "arxiv-id": "0906.1346v6", 
    "author": "Dan Meng", 
    "publish": "2009-06-07T10:15:28Z", 
    "summary": "Different departments of a large organization often run dedicated cluster\nsystems for different computing loads, like HPC (high performance computing)\njobs or Web service applications. In this paper, we have designed and\nimplemented a cloud management system software Phoenix Cloud to consolidate\nheterogeneous workloads from different departments affiliated to the same\norganization on the shared cluster system. We have also proposed cooperative\nresource provisioning and management policies for a large organization and its\naffiliated departments, running HPC jobs and Web service applications, to share\nthe consolidated cluster system. The experiments show that in comparison with\nthe case that each department operates its dedicated cluster system, Phoenix\nCloud significantly decreases the scale of the required cluster system for a\nlarge organization, improves the benefit of the scientific computing\ndepartment, and at the same time provisions enough resources to the other\ndepartment running Web services with varying loads."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0906.1947v1", 
    "other_authors": "Mikhail Nesterenko, S\u00e9bastien Tixeuil", 
    "title": "Ideal Stabilization", 
    "arxiv-id": "0906.1947v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2009-06-10T14:31:18Z", 
    "summary": "We define and explore the concept of ideal stabilization. The program is\nideally stabilizing if its every state is legitimate. Ideal stabilization\nallows the specification designer to prescribe with arbitrary degree of\nprecision not only the fault-free program behavior but also its recovery\noperation. Specifications may or may not mention all possible states. We\nidentify approaches to designing ideal stabilization to both kinds of\nspecifications. For the first kind, we state the necessary condition for an\nideally stabilizing solution. On the basis of this condition we prove that\nthere is no ideally stabilizing solution to the leader election problem. We\nillustrate the utility of the concept by providing examples of well-known\nprograms and proving them ideally stabilizing. Specifically, we prove ideal\nstabilization of the conflict manager, the alternator, the propagation of\ninformation with feedback and the alternating bit protocol."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0906.2143v1", 
    "other_authors": "J. T. Moscicki, A. Manara, M. Lamanna, P. Mendez, A. Muraru", 
    "title": "Dependable Distributed Computing for the International Telecommunication   Union Regional Radio Conference RRC06", 
    "arxiv-id": "0906.2143v1", 
    "author": "A. Muraru", 
    "publish": "2009-06-11T16:04:12Z", 
    "summary": "The International Telecommunication Union (ITU) Regional Radio Conference\n(RRC06) established in 2006 a new frequency plan for the introduction of\ndigital broadcasting in European, African, Arab, CIS countries and Iran. The\npreparation of the plan involved complex calculations under short deadline and\nrequired dependable and efficient computing capability. The ITU designed and\ndeployed in-situ a dedicated PC farm, in parallel to the European Organization\nfor Nuclear Research (CERN) which provided and supported a system based on the\nEGEE Grid. The planning cycle at the RRC06 required a periodic execution in the\norder of 200,000 short jobs, using several hundreds of CPU hours, in a period\nof less than 12 hours. The nature of the problem required dynamic\nworkload-balancing and low-latency access to the computing resources. We\npresent the strategy and key technical choices that delivered a reliable\nservice to the RRC06."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062069", 
    "link": "http://arxiv.org/pdf/0906.2914v2", 
    "other_authors": "Michal Zerola, J\u00e9r\u00f4me Lauret, Roman Bart\u00e1k, Michal \u0160umbera", 
    "title": "Efficient Multi-site Data Movement Using Constraint Programming for Data   Hungry Science", 
    "arxiv-id": "0906.2914v2", 
    "author": "Michal \u0160umbera", 
    "publish": "2009-06-16T12:33:25Z", 
    "summary": "For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data\nsets that have been increasing in size as a function of time. In order to\noptimize all available resources (geographically spread) and minimize the\nprocessing time, it is necessary to face also the question of efficient data\ntransfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to\nthe truly distributed task scheduling we present the technique using a\nConstraint Programming (CP) approach. The CP technique schedules data transfers\nfrom multiple resources considering all available paths of diverse\ncharacteristic (capacity, sharing and storage) having minimum user's waiting\ntime as an objective. We introduce a model for planning data transfers to a\nsingle destination (data transfer) as well as its extension for an optimal data\nset spreading strategy (data placement). Several enhancements for a solver of\nthe CP model will be shown, leading to a faster schedule computation time using\nsymmetry breaking, branch cutting, well studied principles from job-shop\nscheduling field and several heuristics. Finally, we will present the design\nand implementation of a corner-stone application aimed at moving datasets\naccording to the schedule. Results will include comparison of performance and\ntrade-off between CP techniques and a Peer-2-Peer model from simulation\nframework as well as the real case scenario taken from a practical usage of a\nCP scheduler."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062069", 
    "link": "http://arxiv.org/pdf/0906.3424v1", 
    "other_authors": "Ziyuan Wang, Lars Kulik, Kotagiri Ramamohanarao", 
    "title": "Decentralized Traffic Management Strategies for Sensor-Enabled Cars", 
    "arxiv-id": "0906.3424v1", 
    "author": "Kotagiri Ramamohanarao", 
    "publish": "2009-06-18T12:16:06Z", 
    "summary": "Traffic Congestions and accidents are major concerns in today's\ntransportation systems. This thesis investigates how to optimize traffic flow\non highways, in particular for merging situations such as intersections where a\nramp leads onto the highway. In our work, cars are equipped with sensors that\ncan detect distance to neighboring cars, and communicate their velocity and\nacceleration readings with one another. Sensor-enabled cars can locally\nexchange sensed information about the traffic and adapt their behavior much\nearlier than regular cars.\n  We propose proactive algorithms for merging different streams of\nsensor-enabled cars into a single stream. A proactive merging algorithm\ndecouples the decision point from the actual merging point. Sensor-enabled cars\nallow us to decide where and when a car merges before it arrives at the actual\nmerging point. This leads to a significant improvement in traffic flow as\nvelocities can be adjusted appropriately. We compare proactive merging\nalgorithms against the conventional priority-based merging algorithm in a\ncontrolled simulation environment. Experiment results show that proactive\nmerging algorithms outperform the priority-based merging algorithm in terms of\nflow and delay."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0906.4302v1", 
    "other_authors": "Ahmed Mihoob, Carlos Molina-Jimenez", 
    "title": "A Peer to Peer Protocol for Online Dispute Resolution over Storage   Consumption", 
    "arxiv-id": "0906.4302v1", 
    "author": "Carlos Molina-Jimenez", 
    "publish": "2009-06-23T16:30:51Z", 
    "summary": "In bilateral accounting of resource consumption both the consumer and\nprovider independently measure the amount of resources consumed by the\nconsumer. The problem here is that potential disparities between the provider's\nand consumer's accountings, might lead to conflicts between the two parties\nthat need to be resolved. We argue that with the proper mechanisms available,\nmost of these conflicts can be solved online, as opposite to in court\nresolution; the design of such mechanisms is still a research topic; to help\ncover the gap, in this paper we propose a peer--to--peer protocol for online\ndispute resolution over storage consumption. The protocol is peer--to--peer and\ntakes into consideration the possible causes (e.g, transmission delays,\nunsynchronized metric collectors, etc.) of the disparity between the provider's\nand consumer's accountings to make, if possible, the two results converge."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0906.4680v1", 
    "other_authors": "Eric Benoit, Marc-Philippe Huget, Patrice Moreaux, Olivier Passalacqua", 
    "title": "Reconfiguration of Distributed Information Fusion System ? A case study", 
    "arxiv-id": "0906.4680v1", 
    "author": "Olivier Passalacqua", 
    "publish": "2009-06-25T12:35:57Z", 
    "summary": "Information Fusion Systems are now widely used in different fusion contexts,\nlike scientific processing, sensor networks, video and image processing. One of\nthe current trends in this area is to cope with distributed systems. In this\ncontext, we have defined and implemented a Dynamic Distributed Information\nFusion System runtime model. It allows us to cope with dynamic execution\nsupports while trying to maintain the functionalities of a given Dynamic\nDistributed Information Fusion System. The paper presents our system, the\nreconfiguration problems we are faced with and our solutions."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0907.0929v1", 
    "other_authors": "Mihai Letia, Nuno Pregui\u00e7a, Marc Shapiro", 
    "title": "CRDTs: Consistency without concurrency control", 
    "arxiv-id": "0907.0929v1", 
    "author": "Marc Shapiro", 
    "publish": "2009-07-06T08:01:05Z", 
    "summary": "A CRDT is a data type whose operations commute when they are concurrent.\nReplicas of a CRDT eventually converge without any complex concurrency control.\nAs an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer\ncalled Treedoc. We outline the design, implementation and performance of\nTreedoc. We discuss how the CRDT concept can be generalised, and its\nlimitations."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.1375v1", 
    "other_authors": "C\u00e9dric Chevalier, Fran\u00e7ois Pellegrini", 
    "title": "PT-Scotch: A tool for efficient parallel graph ordering", 
    "arxiv-id": "0907.1375v1", 
    "author": "Fran\u00e7ois Pellegrini", 
    "publish": "2009-07-08T15:11:00Z", 
    "summary": "The parallel ordering of large graphs is a difficult problem, because on the\none hand minimum degree algorithms do not parallelize well, and on the other\nhand the obtainment of high quality orderings with the nested dissection\nalgorithm requires efficient graph bipartitioning heuristics, the best\nsequential implementations of which are also hard to parallelize. This paper\npresents a set of algorithms, implemented in the PT-Scotch software package,\nwhich allows one to order large graphs in parallel, yielding orderings the\nquality of which is only slightly worse than the one of state-of-the-art\nsequential algorithms. Our implementation uses the classical nested dissection\napproach but relies on several novel features to solve the parallel graph\nbipartitioning problem. Thanks to these improvements, PT-Scotch produces\nconsistently better orderings than ParMeTiS on large numbers of processors."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.2949v2", 
    "other_authors": "Julien M. Hendrickx, Alex Olshevsky, John N. Tsitsiklis", 
    "title": "Distributed anonymous function computation in information fusion and   multiagent systems", 
    "arxiv-id": "0907.2949v2", 
    "author": "John N. Tsitsiklis", 
    "publish": "2009-07-16T22:42:40Z", 
    "summary": "We propose a model for deterministic distributed function computation by a\nnetwork of identical and anonymous nodes, with bounded computation and storage\ncapabilities that do not scale with the network size. Our goal is to\ncharacterize the class of functions that can be computed within this model. In\nour main result, we exhibit a class of non-computable functions, and prove that\nevery function outside this class can at least be approximated. The problem of\ncomputing averages in a distributed manner plays a central role in our\ndevelopment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.3118v1", 
    "other_authors": "Olivier Bournez, Philippe Chassaing, Johanne Cohen, Lucas Gerin, Xavier Koegler", 
    "title": "On the Convergence of Population Protocols When Population Goes to   Infinity", 
    "arxiv-id": "0907.3118v1", 
    "author": "Xavier Koegler", 
    "publish": "2009-07-17T17:22:12Z", 
    "summary": "Population protocols have been introduced as a model of sensor networks\nconsisting of very limited mobile agents with no control over their own\nmovement. A population protocol corresponds to a collection of anonymous\nagents, modeled by finite automata, that interact with one another to carry out\ncomputations, by updating their states, using some rules. Their computational\npower has been investigated under several hypotheses but always when restricted\nto finite size populations. In particular, predicates stably computable in the\noriginal model have been characterized as those definable in Presburger\narithmetic. We study mathematically the convergence of population protocols\nwhen the size of the population goes to infinity. We do so by giving general\nresults, that we illustrate through the example of a particular population\nprotocol for which we even obtain an asymptotic development. This example shows\nin particular that these protocols seem to have a rather different\ncomputational power when a huge population hypothesis is considered."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.4810v1", 
    "other_authors": "Robert Grossman, Yunhong Gu, Michal Sabala, Collin Bennet, Jonathan Seidman, Joe Mambratti", 
    "title": "The Open Cloud Testbed: A Wide Area Testbed for Cloud Computing   Utilizing High Performance Network Services", 
    "arxiv-id": "0907.4810v1", 
    "author": "Joe Mambratti", 
    "publish": "2009-07-28T00:54:23Z", 
    "summary": "Recently, a number of cloud platforms and services have been developed for\ndata intensive computing, including Hadoop, Sector, CloudStore (formerly KFS),\nHBase, and Thrift. In order to benchmark the performance of these systems, to\ninvestigate their interoperability, and to experiment with new services based\non flexible compute node and network provisioning capabilities, we have\ndesigned and implemented a large scale testbed called the Open Cloud Testbed\n(OCT). Currently the OCT has 120 nodes in four data centers: Baltimore, Chicago\n(two locations), and San Diego. In contrast to other cloud testbeds, which are\nin small geographic areas and which are based on commodity Internet services,\nthe OCT is a wide area testbed and the four data centers are connected with a\nhigh performance 10Gb/s network, based on a foundation of dedicated lightpaths.\nThis testbed can address the requirements of extremely large data streams that\nchallenge other types of distributed infrastructure. We have also developed\nseveral utilities to support the development of cloud computing systems and\nservices, including novel node and network provisioning services, a monitoring\nsystem, and a RPC system. In this paper, we describe the OCT architecture and\nmonitoring system. We also describe some benchmarks that we developed and some\ninteroperability studies we performed using these benchmarks."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0908.0160v1", 
    "other_authors": "Ariel Daliot, Danny Dolev", 
    "title": "Self-stabilizing Byzantine Agreement", 
    "arxiv-id": "0908.0160v1", 
    "author": "Danny Dolev", 
    "publish": "2009-08-02T21:09:20Z", 
    "summary": "Byzantine agreement algorithms typically assume implicit initial state\nconsistency and synchronization among the correct nodes and then operate in\ncoordinated rounds of information exchange to reach agreement based on the\ninput values. The implicit initial assumptions enable correct nodes to infer\nabout the progression of the algorithm at other nodes from their local state.\nThis paper considers a more severe fault model than permanent Byzantine\nfailures, one in which the system can in addition be subject to severe\ntransient failures that can temporarily throw the system out of its assumption\nboundaries. When the system eventually returns to behave according to the\npresumed assumptions it may be in an arbitrary state in which any\nsynchronization among the nodes might be lost, and each node may be at an\narbitrary state. We present a self-stabilizing Byzantine agreement algorithm\nthat reaches agreement among the correct nodes in an optimal ration of faulty\nto correct, by using only the assumption of eventually bounded message\ntransmission delay. In the process of solving the problem, two additional\nimportant and challenging building blocks were developed: a unique\nself-stabilizing protocol for assigning consistent relative times to protocol\ninitialization and a Reliable Broadcast primitive that progresses at the speed\nof actual message delivery time."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_25", 
    "link": "http://arxiv.org/pdf/0908.1797v2", 
    "other_authors": "Kajari Ghosh Dastidar, Ted Herman", 
    "title": "Separation of Circulating Tokens", 
    "arxiv-id": "0908.1797v2", 
    "author": "Ted Herman", 
    "publish": "2009-08-12T20:52:52Z", 
    "summary": "Self-stabilizing distributed control is often modeled by token abstractions.\nA system with a single token may implement mutual exclusion; a system with\nmultiple tokens may ensure that immediate neighbors do not simultaneously enjoy\na privilege. For a cyber-physical system, tokens may represent physical objects\nwhose movement is controlled. The problem studied in this paper is to ensure\nthat a synchronous system with m circulating tokens has at least d distance\nbetween tokens. This problem is first considered in a ring where d is given\nwhilst m and the ring size n are unknown. The protocol solving this problem can\nbe uniform, with all processes running the same program, or it can be\nnon-uniform, with some processes acting only as token relays. The protocol for\nthis first problem is simple, and can be expressed with Petri net formalism. A\nsecond problem is to maximize d when m is given, and n is unknown. For the\nsecond problem, the paper presents a non-uniform protocol with a single\ncorrective process."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_25", 
    "link": "http://arxiv.org/pdf/0908.2222v1", 
    "other_authors": "Gerald Krafft, Vladimir Getov", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids: Design and Experience", 
    "arxiv-id": "0908.2222v1", 
    "author": "Vladimir Getov", 
    "publish": "2009-08-16T08:30:48Z", 
    "summary": "In this paper we analyse the requirements of performing parallel\ntransaction-oriented simulations within loosely coupled systems like ad hoc\ngrids. We focus especially on the space-parallel approach to parallel\nsimulation and on discrete event synchronisation algorithms that are suitable\nfor transaction-oriented simulation and the target environment of ad hoc grids.\nTo demonstrate our findings, a Java-based parallel simulator for the\ntransaction-oriented language GPSS/H is implemented on the basis of the most\npromising shock-resistant Time Warp (SRTW) synchronisation algorithm and using\nthe grid framework ProActive. The analysis of our parallel simulator, based on\nexperiments using the Grid5000 platform, shows that the SRTW algorithm can\nsuccessfully reduce the number of rolled back transaction moves but it also\nreveals circumstances in which the SRTW algorithm can be outperformed by the\nnormal Time Warp algorithm. Finally, possible improvements to the SRTW\nalgorithm are proposed in order to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0908.2295v1", 
    "other_authors": "Danny Dolev, Ezra N. Hoch, Yoram Moses", 
    "title": "An Optimal Self-Stabilizing Firing Squad", 
    "arxiv-id": "0908.2295v1", 
    "author": "Yoram Moses", 
    "publish": "2009-08-17T07:39:06Z", 
    "summary": "Consider a fully connected network where up to $t$ processes may crash, and\nall processes start in an arbitrary memory state. The self-stabilizing firing\nsquad problem consists of eventually guaranteeing simultaneous response to an\nexternal input. This is modeled by requiring that the non-crashed processes\n\"fire\" simultaneously if some correct process received an external \"GO\" input,\nand that they only fire as a response to some process receiving such an input.\nThis paper presents FireAlg, the first self-stabilizing firing squad algorithm.\n  The FireAlg algorithm is optimal in two respects: (a) Once the algorithm is\nin a safe state, it fires in response to a GO input as fast as any other\nalgorithm does, and (b) Starting from an arbitrary state, it converges to a\nsafe state as fast as any other algorithm does."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0908.2958v1", 
    "other_authors": "Que Thu Dung Nguyen", 
    "title": "Design and Implementation of a Distributed Middleware for Parallel   Execution of Legacy Enterprise Applications", 
    "arxiv-id": "0908.2958v1", 
    "author": "Que Thu Dung Nguyen", 
    "publish": "2009-08-20T16:26:21Z", 
    "summary": "A typical enterprise uses a local area network of computers to perform its\nbusiness. During the off-working hours, the computational capacities of these\nnetworked computers are underused or unused. In order to utilize this\ncomputational capacity an application has to be recoded to exploit concurrency\ninherent in a computation which is clearly not possible for legacy applications\nwithout any source code. This thesis presents the design an implementation of a\ndistributed middleware which can automatically execute a legacy application on\nmultiple networked computers by parallelizing it. This middleware runs multiple\ncopies of the binary executable code in parallel on different hosts in the\nnetwork. It wraps up the binary executable code of the legacy application in\norder to capture the kernel level data access system calls and perform them\ndistributively over multiple computers in a safe and conflict free manner. The\nmiddleware also incorporates a dynamic scheduling technique to execute the\ntarget application in minimum time by scavenging the available CPU cycles of\nthe hosts in the network. This dynamic scheduling also supports the CPU\navailability of the hosts to change over time and properly reschedule the\nreplicas performing the computation to minimize the execution time. A prototype\nimplementation of this middleware has been developed as a proof of concept of\nthe design. This implementation has been evaluated with a few typical case\nstudies and the test results confirm that the middleware works as expected."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0910.0317v1", 
    "other_authors": "Abbas Karimi, Faraneh Zarafshan, Adznan. b. Jantan, A. R Ramli, M. Iqbal b. Saripan", 
    "title": "A New Fuzzy Approach for Dynamic Load Balancing Algorithm", 
    "arxiv-id": "0910.0317v1", 
    "author": "M. Iqbal b. Saripan", 
    "publish": "2009-10-02T03:32:09Z", 
    "summary": "Load balancing is the process of improving the Performance of a parallel and\ndistributed system through is distribution of load among the processors [1-2].\nMost of the previous work in load balancing and distributed decision making in\ngeneral, do not effectively take into account the uncertainty and inconsistency\nin state information but in fuzzy logic, we have advantage of using crisps\ninputs. In this paper, we present a new approach for implementing dynamic load\nbalancing algorithm with fuzzy logic, which can face to uncertainty and\ninconsistency of previous algorithms, further more our algorithm shows better\nresponse time than round robin and randomize algorithm respectively 30.84\npercent and 45.45 percent."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0910.1852v1", 
    "other_authors": "Mohammad Ali Jabraeil Jamali, Ahmad Khademzadeh", 
    "title": "DAMQ-Based Schemes for Efficiently Using the Buffer Spaces of a NoC   Router", 
    "arxiv-id": "0910.1852v1", 
    "author": "Ahmad Khademzadeh", 
    "publish": "2009-10-09T20:24:57Z", 
    "summary": "In this paper we present high performance dynamically allocated multi-queue\n(DAMQ) buffer schemes for fault tolerance systems on chip applications that\nrequire an interconnection network. Two or four virtual channels shared the\nsame buffer space. On the message switching layer, we make improvement to boost\nsystem performance when there are faults involved in the components\ncommunication. The proposed schemes are when a node or a physical channel is\ndeemed as faulty, the previous hop node will terminate the buffer occupancy of\nmessages destined to the failed link. The buffer usage decisions are made at\nswitching layer without interactions with higher abstract layer, thus buffer\nspace will be released to messages destined to other healthy nodes quickly.\nTherefore, the buffer space will be efficiently used in case fault occurs at\nsome nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_4", 
    "link": "http://arxiv.org/pdf/0910.1974v1", 
    "other_authors": "Rajkumar Buyya, Suraj Pandey, Christian Vecchiola", 
    "title": "Cloudbus Toolkit for Market-Oriented Cloud Computing", 
    "arxiv-id": "0910.1974v1", 
    "author": "Christian Vecchiola", 
    "publish": "2009-10-11T06:26:29Z", 
    "summary": "This keynote paper: (1) presents the 21st century vision of computing and\nidentifies various IT paradigms promising to deliver computing as a utility;\n(2) defines the architecture for creating market-oriented Clouds and computing\natmosphere by leveraging technologies such as virtual machines; (3) provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; (4) presents the work carried out as part of\nour new Cloud Computing initiative, called Cloudbus: (i) Aneka, a Platform as a\nService software system containing SDK (Software Development Kit) for\nconstruction of Cloud applications and deployment on private or public Clouds,\nin addition to supporting market-oriented resource management; (ii)\ninternetworking of Clouds for dynamic creation of federated computing\nenvironments for scaling of elastic applications; (iii) creation of 3rd party\nCloud brokering services for building content delivery networks and e-Science\napplications and their deployment on capabilities of IaaS providers such as\nAmazon along with Grid mashups; (iv) CloudSim supporting modelling and\nsimulation of Clouds for performance studies; (v) Energy Efficient Resource\nAllocation Mechanisms and Techniques for creation and management of Green\nClouds; and (vi) pathways for future research."
},{
    "category": "cs.DC", 
    "doi": "10.1109/I-SPAN.2009.150", 
    "link": "http://arxiv.org/pdf/0910.1979v1", 
    "other_authors": "Christian Vecchiola, Suraj Pandey, Rajkumar Buyya", 
    "title": "High-Performance Cloud Computing: A View of Scientific Applications", 
    "arxiv-id": "0910.1979v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-10-11T07:42:44Z", 
    "summary": "Scientific computing often requires the availability of a massive number of\ncomputers for performing large scale experiments. Traditionally, these needs\nhave been addressed by using high-performance computing solutions and installed\nfacilities such as clusters and super computers, which are difficult to setup,\nmaintain, and operate. Cloud computing provides scientists with a completely\nnew model of utilizing the computing infrastructure. Compute resources, storage\nresources, as well as applications, can be dynamically provisioned (and\nintegrated within the existing infrastructure) on a pay per use basis. These\nresources can be released when they are no more needed. Such services are often\noffered within the context of a Service Level Agreement (SLA), which ensure the\ndesired Quality of Service (QoS). Aneka, an enterprise Cloud computing\nsolution, harnesses the power of compute resources by relying on private and\npublic Clouds and delivers to users the desired QoS. Its flexible and service\nbased infrastructure supports multiple programming paradigms that make Aneka\naddress a variety of different scenarios: from finance applications to\ncomputational science. As examples of scientific computing in the Cloud, we\npresent a preliminary case study on using Aneka for the classification of gene\nexpression data and the execution of fMRI brain imaging workflow."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/5/052014", 
    "link": "http://arxiv.org/pdf/0910.4507v1", 
    "other_authors": "Sam Skipsey, David Ambrose-Griffith, Greig Cowan, Mike Kenyon, Orlando Richards, Phil Roffe, Graeme Stewart", 
    "title": "ScotGrid: Providing an Effective Distributed Tier-2 in the LHC Era", 
    "arxiv-id": "0910.4507v1", 
    "author": "Graeme Stewart", 
    "publish": "2009-10-23T13:02:19Z", 
    "summary": "ScotGrid is a distributed Tier-2 centre in the UK with sites in Durham,\nEdinburgh and Glasgow. ScotGrid has undergone a huge expansion in hardware in\nanticipation of the LHC and now provides more than 4MSI2K and 500TB to the LHC\nVOs. Scaling up to this level of provision has brought many challenges to the\nTier-2 and we show in this paper how we have adopted new methods of organising\nthe centres, from fabric management and monitoring to remote management of\nsites to management and operational procedures, to meet these challenges. We\ndescribe how we have coped with different operational models at the sites,\nwhere Glagsow and Durham sites are managed \"in house\" but resources at\nEdinburgh are managed as a central university resource. This required the\nadoption of a different fabric management model at Edinburgh and a special\nengagement with the cluster managers. Challenges arose from the different job\nmodels of local and grid submission that required special attention to resolve.\nWe show how ScotGrid has successfully provided an infrastructure for ATLAS and\nLHCb Monte Carlo production. Special attention has been paid to ensuring that\nuser analysis functions efficiently, which has required optimisation of local\nstorage and networking to cope with the demands of user analysis. Finally,\nalthough these Tier-2 resources are pledged to the whole VO, we have\nestablished close links with our local physics user communities as being the\nbest way to ensure that the Tier-2 functions effectively as a part of the LHC\ngrid computing framework.."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062066", 
    "link": "http://arxiv.org/pdf/0910.4510v1", 
    "other_authors": "Sam Skipsey, Greig Cowan, Mike Kenyon, Stuart Purdie, Graeme Stewart", 
    "title": "Optimised access to user analysis data using the gLite DPM", 
    "arxiv-id": "0910.4510v1", 
    "author": "Graeme Stewart", 
    "publish": "2009-10-23T13:29:44Z", 
    "summary": "The ScotGrid distributed Tier-2 now provides more that 4MSI2K and 500TB for\nLHC computing, which is spread across three sites at Durham, Edinburgh and\nGlasgow. Tier-2 sites have a dual role to play in the computing models of the\nLHC VOs. Firstly, their CPU resources are used for the generation of Monte\nCarlo event data. Secondly, the end user analysis data is distributed across\nthe grid to the site's storage system and held on disk ready for processing by\nphysicists' analysis jobs. In this paper we show how we have designed the\nScotGrid storage and data management resources in order to optimise access by\nphysicists to LHC data. Within ScotGrid, all sites use the gLite DPM storage\nmanager middleware. Using the EGEE grid to submit real ATLAS analysis code to\nprocess VO data stored on the ScotGrid sites, we present an analysis of the\nperformance of the architecture at one site, and procedures that may be\nundertaken to improve such. The results will be presented from the point of\nview of the end user (in terms of number of events processed/second) and from\nthe point of view of the site, which wishes to minimise load and the impact\nthat analysis activity has on other users of the system."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_35", 
    "link": "http://arxiv.org/pdf/0910.4568v1", 
    "other_authors": "Ilango Sriram", 
    "title": "SPECI, a simulation tool exploring cloud-scale data centres", 
    "arxiv-id": "0910.4568v1", 
    "author": "Ilango Sriram", 
    "publish": "2009-10-23T19:05:29Z", 
    "summary": "There is a rapid increase in the size of data centres (DCs) used to provide\ncloud computing services. It is commonly agreed that not all properties in the\nmiddleware that manages DCs will scale linearly with the number of components.\nFurther, \"normal failure\" complicates the assessment of the per-formance of a\nDC. However, unlike in other engineering domains, there are no well established\ntools that allow the prediction of the performance and behav-iour of future\ngenerations of DCs. SPECI, Simulation Program for Elastic Cloud\nInfrastructures, is a simulation tool which allows exploration of aspects of\nscaling as well as performance properties of future DCs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_35", 
    "link": "http://arxiv.org/pdf/0911.0910v1", 
    "other_authors": "Mandhapati P. Raju, Siddhartha Khaitan", 
    "title": "Domain Decomposition Based High Performance Parallel Computing", 
    "arxiv-id": "0911.0910v1", 
    "author": "Siddhartha Khaitan", 
    "publish": "2009-11-04T18:56:03Z", 
    "summary": "The study deals with the parallelization of finite element based\nNavier-Stokes codes using domain decomposition and state-ofart sparse direct\nsolvers. There has been significant improvement in the performance of sparse\ndirect solvers. Parallel sparse direct solvers are not found to exhibit good\nscalability. Hence, the parallelization of sparse direct solvers is done using\ndomain decomposition techniques. A highly efficient sparse direct solver\nPARDISO is used in this study. The scalability of both Newton and modified\nNewton algorithms are tested."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.3945v1", 
    "other_authors": "Xiao Ming Zhang", 
    "title": "A Semantic Grid Oriented to E-Tourism", 
    "arxiv-id": "0911.3945v1", 
    "author": "Xiao Ming Zhang", 
    "publish": "2009-11-20T01:51:30Z", 
    "summary": "With increasing complexity of tourism business models and tasks, there is a\nclear need of the next generation e-Tourism infrastructure to support flexible\nautomation, integration, computation, storage, and collaboration. Currently\nseveral enabling technologies such as semantic Web, Web service, agent and grid\ncomputing have been applied in the different e-Tourism applications, however\nthere is no a unified framework to be able to integrate all of them. So this\npaper presents a promising e-Tourism framework based on emerging semantic grid,\nin which a number of key design issues are discussed including architecture,\nontologies structure, semantic reconciliation, service and resource discovery,\nrole based authorization and intelligent agent. The paper finally provides the\nimplementation of the framework."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.5438v1", 
    "other_authors": "Jochen L. Leidner, Gary Berosik", 
    "title": "Building and Installing a Hadoop/MapReduce Cluster from Commodity   Components", 
    "arxiv-id": "0911.5438v1", 
    "author": "Gary Berosik", 
    "publish": "2009-11-28T23:50:28Z", 
    "summary": "This tutorial presents a recipe for the construction of a compute cluster for\nprocessing large volumes of data, using cheap, easily available personal\ncomputer hardware (Intel/AMD based PCs) and freely available open source\nsoftware (Ubuntu Linux, Apache Hadoop)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.5593v1", 
    "other_authors": "Franck Cappello, Henri Casanova, Yves Robert", 
    "title": "Checkpointing vs. Migration for Post-Petascale Machines", 
    "arxiv-id": "0911.5593v1", 
    "author": "Yves Robert", 
    "publish": "2009-11-30T09:39:15Z", 
    "summary": "We craft a few scenarios for the execution of sequential and parallel jobs on\nfuture generation machines. Checkpointing or migration, which technique to\nchoose?"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0912.1835v1", 
    "other_authors": "T. T. Lwin, T. Thein", 
    "title": "High Availability Cluster System for Local Disaster Recovery with Markov   Modeling Approach", 
    "arxiv-id": "0912.1835v1", 
    "author": "T. Thein", 
    "publish": "2009-12-09T18:50:52Z", 
    "summary": "The need for high availability (HA) and disaster recovery (DR) in IT\nenvironment is more stringent than most of the other sectors of enterprises.\nMany businesses require the availability of business-critical applications 24\nhours a day, seven days a week, and can afford no data loss in the event of a\ndisaster. It is vital that the IT infrastructure is resilient with regard to\ndisruption, even site failures, and that business operations can continue\nwithout significant impact. As a result, DR has gained great importance in IT.\nClustering of multiple industries standard servers together to allow workload\nsharing and fail-over capabilities is a low cost approach. In this paper, we\npresent the availability model through Semi-Markov Process (SMP) and also\nanalyze the difference in downtime of the SMP model and the approximate\nContinuous Time Markov Chain (CTMC) model. To acquire system availability, we\nperform numerical analysis and SHARPE tool evaluation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.0421v1", 
    "other_authors": "Javier Tordable", 
    "title": "MapReduce for Integer Factorization", 
    "arxiv-id": "1001.0421v1", 
    "author": "Javier Tordable", 
    "publish": "2010-01-04T00:15:58Z", 
    "summary": "Integer factorization is a very hard computational problem. Currently no\nefficient algorithm for integer factorization is publicly known. However, this\nis an important problem on which it relies the security of many real world\ncryptographic systems.\n  I present an implementation of a fast factorization algorithm on MapReduce.\nMapReduce is a programming model for high performance applications developed\noriginally at Google. The quadratic sieve algorithm is split into the different\nMapReduce phases and compared against a standard implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.2785v2", 
    "other_authors": "Emmanuel Godard, Yves M\u00e9tivier, Gerard Tel", 
    "title": "Termination Detection of Local Computations", 
    "arxiv-id": "1001.2785v2", 
    "author": "Gerard Tel", 
    "publish": "2010-01-18T20:12:54Z", 
    "summary": "Contrary to the sequential world, the processes involved in a distributed\nsystem do not necessarily know when a computation is globally finished. This\npaper investigates the problem of the detection of the termination of local\ncomputations. We define four types of termination detection: no detection,\ndetection of the local termination, detection by a distributed observer,\ndetection of the global termination. We give a complete characterisation\n(except in the local termination detection case where a partial one is given)\nfor each of this termination detection and show that they define a strict\nhierarchy. These results emphasise the difference between computability of a\ndistributed task and termination detection. Furthermore, these\ncharacterisations encompass all standard criteria that are usually formulated :\ntopological restriction (tree, rings, or triangu- lated networks ...),\ntopological knowledge (size, diameter ...), and local knowledge to distinguish\nnodes (identities, sense of direction). These results are now presented as\ncorollaries of generalising theorems. As a very special and important case, the\ntechniques are also applied to the election problem. Though given in the model\nof local computations, these results can give qualitative insight for similar\nresults in other standard models. The necessary conditions involve graphs\ncovering and quasi-covering; the sufficient conditions (constructive local\ncomputations) are based upon an enumeration algorithm of Mazurkiewicz and a\nstable properties detection algorithm of Szymanski, Shi and Prywes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3259v1", 
    "other_authors": "Ilango Sriram, Ali Khajeh-Hosseini", 
    "title": "Research Agenda in Cloud Technologies", 
    "arxiv-id": "1001.3259v1", 
    "author": "Ali Khajeh-Hosseini", 
    "publish": "2010-01-19T15:53:41Z", 
    "summary": "Cloud computing is the latest effort in delivering computing resources as a\nservice. It represents a shift away from computing as a product that is\npurchased, to computing as a service that is delivered to consumers over the\ninternet from large-scale data centres - or \"clouds\". Whilst cloud computing is\ngaining growing popularity in the IT industry, academia appeared to be lagging\nbehind the rapid developments in this field. This paper is the first systematic\nreview of peer-reviewed academic research published in this field, and aims to\nprovide an overview of the swiftly developing advances in the technical\nfoundations of cloud computing and their research efforts. Structured along the\ntechnical aspects on the cloud agenda, we discuss lessons from related\ntechnologies; advances in the introduction of protocols, interfaces, and\nstandards; techniques for modelling and building clouds; and new use-cases\narising through cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3718v1", 
    "other_authors": "Satish. G. Dappin, M. Vaidehi, G. Nithya Nair, T. R. Gopalakrsihnan Nair", 
    "title": "Severity Prediction of Drought in A Large Geographical Area Using   Distributed Wireless Sensor Networks", 
    "arxiv-id": "1001.3718v1", 
    "author": "T. R. Gopalakrsihnan Nair", 
    "publish": "2010-01-21T04:47:58Z", 
    "summary": "In this paper, the severity prediction of drought through the implementation\nof modern sensor networks is discussed. We describe how to design a drought\nprediction system using wireless sensor networks. This paper will describe a\nterrestrial interconnected wireless sensor network paradigm for the prediction\nof severity of drought over a vast area of 10,000 sq km. The communication\narchitecture for sensor network is outlined and the protocols developed for\neach layer is explored. The data integration model and sensor data analysis at\nthe central computer is explained. The advantages and limitations are discussed\nalong with the use of wireless standards. They are analyzed for its relevance.\nFinally a conclusion is presented along with open research issues."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3824v1", 
    "other_authors": "Federico D. Sacerdoti", 
    "title": "Performance and Fault Tolerance in the StoreTorrent Parallel Filesystem", 
    "arxiv-id": "1001.3824v1", 
    "author": "Federico D. Sacerdoti", 
    "publish": "2010-01-21T15:17:30Z", 
    "summary": "With a goal of supporting the timely and cost-effective analysis of Terabyte\ndatasets on commodity components, we present and evaluate StoreTorrent, a\nsimple distributed filesystem with integrated fault tolerance for efficient\nhandling of small data records. Our contributions include an application-OS\npipelining technique and metadata structure to increase small write and read\nperformance by a factor of 1-10, and the use of peer-to-peer communication of\nreplica-location indexes to avoid transferring data during parallel analysis\neven in a degraded state. We evaluated StoreTorrent, PVFS, and Gluster\nfilesystems using 70 storage nodes and 560 parallel clients on an 8-core/node\nEthernet cluster with directly attached SATA disks. StoreTorrent performed\nparallel small writes at an aggregate rate of 1.69 GB/s, and supported reads\nover the network at 8.47 GB/s. We ported a parallel analysis task and\ndemonstrate it achieved parallel reads at the full aggregate speed of the\nstorage node local filesystems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.4197v1", 
    "other_authors": "R. Nallusamy, K. Duraiswamy, R. Dhanalaksmi, P. Parthiban", 
    "title": "Optimization of Multiple Vehicle Routing Problems using Approximation   Algorithms", 
    "arxiv-id": "1001.4197v1", 
    "author": "P. Parthiban", 
    "publish": "2010-01-23T19:28:13Z", 
    "summary": "This paper deals with generating of an optimized route for multiple Vehicle\nrouting Problems (mVRP). We used a methodology of clustering the given cities\ndepending upon the number of vehicles and each cluster is allotted to a\nvehicle. k- Means clustering algorithm has been used for easy clustering of the\ncities. In this way the mVRP has been converted into VRP which is simple in\ncomputation compared to mVRP. After clustering, an optimized route is generated\nfor each vehicle in its allotted cluster. Once the clustering had been done and\nafter the cities were allocated to the various vehicles, each cluster/tour was\ntaken as an individual Vehicle Routing problem and the steps of Genetic\nAlgorithm were applied to the cluster and iterated to obtain the most optimal\nvalue of the distance after convergence takes place. After the application of\nthe various heuristic techniques, it was found that the Genetic algorithm gave\na better result and a more optimal tour for mVRPs in short computational time\nthan other Algorithms due to the extensive search and constructive nature of\nthe algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1002.0125v1", 
    "other_authors": "Matti \u00c5strand, Valentin Polishchuk, Joel Rybicki, Jukka Suomela, Jara Uitto", 
    "title": "Local algorithms in (weakly) coloured graphs", 
    "arxiv-id": "1002.0125v1", 
    "author": "Jara Uitto", 
    "publish": "2010-01-31T13:46:27Z", 
    "summary": "A local algorithm is a distributed algorithm that completes after a constant\nnumber of synchronous communication rounds. We present local approximation\nalgorithms for the minimum dominating set problem and the maximum matching\nproblem in 2-coloured and weakly 2-coloured graphs. In a weakly 2-coloured\ngraph, both problems admit a local algorithm with the approximation factor\n$(\\Delta+1)/2$, where $\\Delta$ is the maximum degree of the graph. We also give\na matching lower bound proving that there is no local algorithm with a better\napproximation factor for either of these problems. Furthermore, we show that\nthe stronger assumption of a 2-colouring does not help in the case of the\ndominating set problem, but there is a local approximation scheme for the\nmaximum matching problem in 2-coloured graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3492v1", 
    "other_authors": "Ali Khajeh-Hosseini, David Greenwood, Ian Sommerville", 
    "title": "Cloud Migration: A Case Study of Migrating an Enterprise IT System to   IaaS", 
    "arxiv-id": "1002.3492v1", 
    "author": "Ian Sommerville", 
    "publish": "2010-02-18T11:25:49Z", 
    "summary": "This case study illustrates the potential benefits and risks associated with\nthe migration of an IT system in the oil & gas industry from an in-house data\ncenter to Amazon EC2 from a broad variety of stakeholder perspectives across\nthe enterprise, thus transcending the typical, yet narrow, financial and\ntechnical analysis offered by providers. Our results show that the system\ninfrastructure in the case study would have cost 37% less over 5 years on EC2,\nand using cloud computing could have potentially eliminated 21% of the support\ncalls for this system. These findings seem significant enough to call for a\nmigration of the system to the cloud but our stakeholder impact analysis\nrevealed that there are significant risks associated with this. Whilst the\nbenefits of using the cloud are attractive, we argue that it is important that\nenterprise decision-makers consider the overall organizational implications of\nthe changes brought about with cloud computing to avoid implementing local\noptimizations at the cost of organization-wide performance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3602v2", 
    "other_authors": "Xingkai Bao, Jing Li", 
    "title": "Mobile Wireless Localization through Cooperation", 
    "arxiv-id": "1002.3602v2", 
    "author": "Jing Li", 
    "publish": "2010-02-18T20:38:03Z", 
    "summary": "This paper considers N mobile nodes that move together in the vicinity of\neach other, whose initial poses as well as subsequent movements must be\naccurately tracked in real time with the assist of M(>=3) reference nodes. By\nengaging the neighboring mobile nodes in a simple but effective cooperation,\nand by exploiting both the time-of-arrival (TOA) information (between mobile\nnodes and reference nodes) and the received-signal-strength (RSS) information\n(between mobile nodes), an effective new localization strategy, termed\ncooperative TOA and RSS (COTAR), is developed. An optimal maximum likelihood\ndetector is first formulated, followed by the derivation of a low-complexity\niterative approach that can practically achieve the Cramer-Rao lower bound.\nInstead of using simplified channel models as in many previous studies, a\nsophisticated and realistic channel model is used, which can effectively\naccount for the critical fact that the direct path is not necessarily the\nstrongest path. Extensive simulations are conducted in static and mobile\nsettings, and various practical issues and system parameters are evaluated. It\nis shown that COTAR significantly outperforms the existing strategies,\nachieving a localization accuracy of only a few tenths of a meter in clear\nenvironments and a couple of meters in heavily obstructed environments."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3629v1", 
    "other_authors": "Xingkai Bao, Jing Li", 
    "title": "Generalized Adaptive Network Coded Cooperation (GANCC): A Unified   Framework for Network Coding and Channel Coding", 
    "arxiv-id": "1002.3629v1", 
    "author": "Jing Li", 
    "publish": "2010-02-18T22:28:03Z", 
    "summary": "This paper considers distributed coding for multi-source single-sink data\ncollection wireless networks. A unified framework for network coding and\nchannel coding, termed \"generalized adaptive network coded cooperation\"\n(GANCC), is proposed. Key ingredients of GANCC include: matching code graphs\nwith the dynamic network graphs on-the-fly, and integrating channel coding with\nnetwork coding through circulant low-density parity-check codes. Several code\nconstructing methods and several families of sparse-graph codes are proposed,\nand information theoretical analysis is performed. It is shown that GANCC is\nsimple to operate, adaptive in real time, distributed in nature, and capable of\nproviding remarkable coding gains even with a very limited number of\ncooperating users."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3757v1", 
    "other_authors": "Andrea Clementi, Angelo Monti, Riccardo Silvestri", 
    "title": "Fast Flooding over Manhattan", 
    "arxiv-id": "1002.3757v1", 
    "author": "Riccardo Silvestri", 
    "publish": "2010-02-19T15:02:50Z", 
    "summary": "We consider a Mobile Ad-hoc NETwork (MANET) formed by n agents that move at\nspeed V according to the Manhattan Random-Way Point model over a square region\nof side length L. The resulting stationary (agent) spatial probability\ndistribution is far to be uniform: the average density over the \"central zone\"\nis asymptotically higher than that over the \"suburb\". Agents exchange data iff\nthey are at distance at most R within each other.\n  We study the flooding time of this MANET: the number of time steps required\nto broadcast a message from one source agent to all agents of the network in\nthe stationary phase. We prove the first asymptotical upper bound on the\nflooding time. This bound holds with high probability, it is a decreasing\nfunction of R and V, and it is tight for a wide and relevant range of the\nnetwork parameters (i.e. L, R and V).\n  A consequence of our result is that flooding over the sparse and\nhighly-disconnected suburb can be as fast as flooding over the dense and\nconnected central zone. Rather surprisingly, this property holds even when R is\nexponentially below the connectivity threshold of the MANET and the speed V is\nvery low."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4003v1", 
    "other_authors": "Parneeta Dhaliwal, M. P. S. Bhatia, Priti Bansal", 
    "title": "A Cluster-based Approach for Outlier Detection in Dynamic Data Streams   (KORM: k-median OutlieR Miner)", 
    "arxiv-id": "1002.4003v1", 
    "author": "Priti Bansal", 
    "publish": "2010-02-21T19:39:35Z", 
    "summary": "Outlier detection in data streams has gained wide importance presently due to\nthe increasing cases of fraud in various applications of data streams. The\ntechniques for outlier detection have been divided into either statistics\nbased, distance based, density based or deviation based. Till now, most of the\nwork in the field of fraud detection was distance based but it is incompetent\nfrom computational point of view. In this paper we introduced a new clustering\nbased approach, which divides the stream in chunks and clusters each chunk\nusing kmedian into variable number of clusters. Instead of storing complete\ndata stream chunk in memory, we replace it with the weighted medians found\nafter mining a data stream chunk and pass that information along with the newly\narrived data chunk to the next phase. The weighted medians found in each phase\nare tested for outlierness and after a given number of phases, it is either\ndeclared as a real outlier or an inlier. Our technique is theoretically better\nthan the k-means as it does not fix the number of clusters to k rather gives a\nrange to it and provides a more stable and better solution which runs in\npoly-logarithmic space."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4561v1", 
    "other_authors": "Valerie King, Jared Saia", 
    "title": "Breaking the O(n^2) Bit Barrier: Scalable Byzantine agreement with an   Adaptive Adversary", 
    "arxiv-id": "1002.4561v1", 
    "author": "Jared Saia", 
    "publish": "2010-02-24T15:19:55Z", 
    "summary": "We describe an algorithm for Byzantine agreement that is scalable in the\nsense that each processor sends only $\\tilde{O}(\\sqrt{n})$ bits, where $n$ is\nthe total number of processors. Our algorithm succeeds with high probability\nagainst an \\emph{adaptive adversary}, which can take over processors at any\ntime during the protocol, up to the point of taking over arbitrarily close to a\n1/3 fraction. We assume synchronous communication but a \\emph{rushing}\nadversary. Moreover, our algorithm works in the presence of flooding:\nprocessors controlled by the adversary can send out any number of messages. We\nassume the existence of private channels between all pairs of processors but\nmake no other cryptographic assumptions. Finally, our algorithm has latency\nthat is polylogarithmic in $n$. To the best of our knowledge, ours is the first\nalgorithm to solve Byzantine agreement against an adaptive adversary, while\nrequiring $o(n^{2})$ total bits of communication."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4738v1", 
    "other_authors": "Graham Kirby, Alan Dearle, Angus Macdonald, Alvaro Fernandes", 
    "title": "An Approach to Ad hoc Cloud Computing", 
    "arxiv-id": "1002.4738v1", 
    "author": "Alvaro Fernandes", 
    "publish": "2010-02-25T10:19:37Z", 
    "summary": "We consider how underused computing resources within an enterprise may be\nharnessed to improve utilization and create an elastic computing\ninfrastructure. Most current cloud provision involves a data center model, in\nwhich clusters of machines are dedicated to running cloud infrastructure\nsoftware. We propose an additional model, the ad hoc cloud, in which\ninfrastructure software is distributed over resources harvested from machines\nalready in existence within an enterprise. In contrast to the data center cloud\nmodel, resource levels are not established a priori, nor are resources\ndedicated exclusively to the cloud while in use. A participating machine is not\ndedicated to the cloud, but has some other primary purpose such as running\ninteractive processes for a particular user. We outline the major\nimplementation challenges and one approach to tackling them."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1003.0951v2", 
    "other_authors": "Rui Ren, Xiaoyu Fu, Jianfeng Zhan, Wei Zhou", 
    "title": "LogMaster: Mining Event Correlations in Logs of Large scale Cluster   Systems", 
    "arxiv-id": "1003.0951v2", 
    "author": "Wei Zhou", 
    "publish": "2010-03-04T02:47:07Z", 
    "summary": "This paper presents a methodology and a system, named LogMaster, for mining\ncorrelations of events that have multiple attributions, i.e., node ID,\napplication ID, event type, and event severity, in logs of large-scale cluster\nsystems. Different from traditional transactional data, e.g., supermarket\npurchases, system logs have their unique characteristic, and hence we propose\nseveral innovative approaches to mine their correlations. We present a simple\nmetrics to measure correlations of events that may happen interleavedly. On the\nbasis of the measurement of correlations, we propose two approaches to mine\nevent correlations; meanwhile, we propose an innovative abstraction: event\ncorrelation graphs (ECGs) to represent event correlations, and present an ECGs\nbased algorithm for predicting events. For two system logs of a production\nHadoop-based cloud computing system at Research Institution of China Mobile and\na production HPC cluster system at Los Alamos National Lab (LANL), we evaluate\nour approaches in three scenarios: (a) predicting all events on the basis of\nboth failure and non-failure events; (b) predicting only failure events on the\nbasis of both failure and non-failure events; (c) predicting failure events\nafter removing non-failure events."
},{
    "category": "cs.DC", 
    "doi": "10.4203/ccp.101.22", 
    "link": "http://arxiv.org/pdf/1003.0952v3", 
    "other_authors": "Vicente H. F. Batista, George O. Ainsworth Jr., Fernando L. B. Ribeiro", 
    "title": "Parallel structurally-symmetric sparse matrix-vector products on   multi-core processors", 
    "arxiv-id": "1003.0952v3", 
    "author": "Fernando L. B. Ribeiro", 
    "publish": "2010-03-04T03:25:41Z", 
    "summary": "We consider the problem of developing an efficient multi-threaded\nimplementation of the matrix-vector multiplication algorithm for sparse\nmatrices with structural symmetry. Matrices are stored using the compressed\nsparse row-column format (CSRC), designed for profiting from the symmetric\nnon-zero pattern observed in global finite element matrices. Unlike classical\ncompressed storage formats, performing the sparse matrix-vector product using\nthe CSRC requires thread-safe access to the destination vector. To avoid race\nconditions, we have implemented two partitioning strategies. In the first one,\neach thread allocates an array for storing its contributions, which are later\ncombined in an accumulation step. We analyze how to perform this accumulation\nin four different ways. The second strategy employs a coloring algorithm for\ngrouping rows that can be concurrently processed by threads. Our results\nindicate that, although incurring an increase in the working set size, the\nformer approach leads to the best performance improvements for most matrices."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-13284-1_11", 
    "link": "http://arxiv.org/pdf/1003.1058v2", 
    "other_authors": "Carole Delporte-Gallet, St\u00e9phane Devismes, Hugues Fauconnier, Mikel Larrea", 
    "title": "Algorithms For Extracting Timeliness Graphs", 
    "arxiv-id": "1003.1058v2", 
    "author": "Mikel Larrea", 
    "publish": "2010-03-04T14:47:17Z", 
    "summary": "We consider asynchronous message-passing systems in which some links are\ntimely and processes may crash. Each run defines a timeliness graph among\ncorrect processes: (p; q) is an edge of the timeliness graph if the link from p\nto q is timely (that is, there is bound on communication delays from p to q).\nThe main goal of this paper is to approximate this timeliness graph by graphs\nhaving some properties (such as being trees, rings, ...). Given a family S of\ngraphs, for runs such that the timeliness graph contains at least one graph in\nS then using an extraction algorithm, each correct process has to converge to\nthe same graph in S that is, in a precise sense, an approximation of the\ntimeliness graph of the run. For example, if the timeliness graph contains a\nring, then using an extraction algorithm, all correct processes eventually\nconverge to the same ring and in this ring all nodes will be correct processes\nand all links will be timely. We first present a general extraction algorithm\nand then a more specific extraction algorithm that is communication efficient\n(i.e., eventually all the messages of the extraction algorithm use only links\nof the extracted graph)."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1291v1", 
    "other_authors": "Alejandro Lorca, Eduardo Huedo, Ignacio M. Llorente", 
    "title": "The Grid[Way] Job Template Manager, a tool for parameter sweeping", 
    "arxiv-id": "1003.1291v1", 
    "author": "Ignacio M. Llorente", 
    "publish": "2010-03-05T15:34:09Z", 
    "summary": "Parameter sweeping is a widely used algorithmic technique in computational\nscience. It is specially suited for high-throughput computing since the jobs\nevaluating the parameter space are loosely coupled or independent.\n  A tool that integrates the modeling of a parameter study with the control of\njobs in a distributed architecture is presented. The main task is to facilitate\nthe creation and deletion of job templates, which are the elements describing\nthe jobs to be run. Extra functionality relies upon the GridWay Metascheduler,\nacting as the middleware layer for job submission and control. It supports\ninteresting features like multi-dimensional sweeping space, wildcarding of\nparameters, functional evaluation of ranges, value-skipping and job template\nautomatic indexation.\n  The use of this tool increases the reliability of the parameter sweep study\nthanks to the systematic bookkeping of job templates and respective job\nstatuses. Furthermore, it simplifies the porting of the target application to\nthe grid reducing the required amount of time and effort."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1395v2", 
    "other_authors": "Peter Burcsi, Attila Kov\u00e1cs, Antal T\u00e1trai", 
    "title": "Start-phase control of distributed systems written in Erlang/OTP", 
    "arxiv-id": "1003.1395v2", 
    "author": "Antal T\u00e1trai", 
    "publish": "2010-03-06T15:47:21Z", 
    "summary": "This paper presents a realization for the reliable and fast startup of\ndistributed systems written in Erlang. The traditional startup provided by the\nErlang/OTP library is sequential, parallelization usually requires unsafe and\nad-hoc solutions. The proposed method calls only for slight modifications in\nthe Erlang/OTP stdlib by applying a system dependency graph. It makes the\nstartup safe, quick, and it is equally easy to use in newly developed and\nlegacy systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1397v1", 
    "other_authors": "Stefan Korecko, Branislav Sobota", 
    "title": "Using Coloured Petri Nets for design of parallel raytracing environment", 
    "arxiv-id": "1003.1397v1", 
    "author": "Branislav Sobota", 
    "publish": "2010-03-06T16:24:29Z", 
    "summary": "This paper deals with the parallel raytracing part of virtual-reality system\nPROLAND, developed at the home institution of authors. It describes an actual\nimplementation of the raytracing part and introduces a Coloured Petri Nets\nmodel of the implementation. The model is used for an evaluation of the\nimplementation by means of simulation-based performance analysis and also forms\nthe basis for future improvements of its parallelization strategy."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1608v1", 
    "other_authors": "Leonid Barenboim, Michael Elkin", 
    "title": "Deterministic Distributed Vertex Coloring in Polylogarithmic Time", 
    "arxiv-id": "1003.1608v1", 
    "author": "Michael Elkin", 
    "publish": "2010-03-08T12:00:09Z", 
    "summary": "Consider an n-vertex graph G = (V,E) of maximum degree Delta, and suppose\nthat each vertex v \\in V hosts a processor. The processors are allowed to\ncommunicate only with their neighbors in G. The communication is synchronous,\ni.e., it proceeds in discrete rounds. In the distributed vertex coloring\nproblem the objective is to color G with Delta + 1, or slightly more than Delta\n+ 1, colors using as few rounds of communication as possible. (The number of\nrounds of communication will be henceforth referred to as running time.)\nEfficient randomized algorithms for this problem are known for more than twenty\nyears \\cite{L86, ABI86}. Specifically, these algorithms produce a (Delta +\n1)-coloring within O(log n) time, with high probability. On the other hand, the\nbest known deterministic algorithm that requires polylogarithmic time employs\nO(Delta^2) colors. This algorithm was devised in a seminal FOCS'87 paper by\nLinial \\cite{L87}. Its running time is O(log^* n). In the same paper Linial\nasked whether one can color with significantly less than Delta^2 colors in\ndeterministic polylogarithmic time. By now this question of Linial became one\nof the most central long-standing open questions in this area. In this paper we\nanswer this question in the affirmative, and devise a deterministic algorithm\nthat employs \\Delta^{1 +o(1)} colors, and runs in polylogarithmic time.\nSpecifically, the running time of our algorithm is O(f(Delta) log Delta log n),\nfor an arbitrarily slow-growing function f(Delta) = \\omega(1). We can also\nproduce O(Delta^{1 + \\eta})-coloring in O(log Delta log n)-time, for an\narbitrarily small constant \\eta > 0, and O(Delta)-coloring in\nO(Delta^{\\epsilon} log n) time, for an arbitrarily small constant \\epsilon > 0."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3305v1", 
    "other_authors": "Maher Khemakhem, Abdelfettah Belghith, Sousse University, Tunisia, Manouba University, Tunisia)", 
    "title": "Towards trusted volunteer grid environments", 
    "arxiv-id": "1003.3305v1", 
    "author": "Tunisia)", 
    "publish": "2010-03-17T06:28:40Z", 
    "summary": "Intensive experiences show and confirm that grid environments can be\nconsidered as the most promising way to solve several kinds of problems\nrelating either to cooperative work especially where involved collaborators are\ndispersed geographically or to some very greedy applications which require\nenough power of computing or/and storage. Such environments can be classified\ninto two categories; first, dedicated grids where the federated computers are\nsolely devoted to a specific work through its end. Second, Volunteer grids\nwhere federated computers are not completely devoted to a specific work but\ninstead they can be randomly and intermittently used, at the same time, for any\nother purpose or they can be connected or disconnected at will by their owners\nwithout any prior notification. Each category of grids includes surely several\nadvantages and disadvantages; nevertheless, we think that volunteer grids are\nvery promising and more convenient especially to build a general multipurpose\ndistributed scalable environment. Unfortunately, the big challenge of such\nenvironments is, however, security and trust. Indeed, owing to the fact that\nevery federated computer in such an environment can randomly be used at the\nsame time by several users or can be disconnected suddenly, several security\nproblems will automatically arise. In this paper, we propose a novel solution\nbased on identity federation, agent technology and the dynamic enforcement of\naccess control policies that lead to the design and implementation of trusted\nvolunteer grid environments."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3684v1", 
    "other_authors": "Andy Yoo, Keith Henderson", 
    "title": "Parallel Generation of Massive Scale-Free Graphs", 
    "arxiv-id": "1003.3684v1", 
    "author": "Keith Henderson", 
    "publish": "2010-03-18T22:11:14Z", 
    "summary": "One of the biggest huddles faced by researchers studying algorithms for\nmassive graphs is the lack of large input graphs that are essential for the\ndevelopment and test of the graph algorithms. This paper proposes two efficient\nand highly scalable parallel graph generation algorithms that can produce\nmassive realistic graphs to address this issue. The algorithms, designed to\nachieve high degree of parallelism by minimizing inter-processor\ncommunications, are two of the fastest graph generators which are capable of\ngenerating scale-free graphs with billions of vertices and edges. The synthetic\ngraphs generated by the proposed methods possess the most common properties of\nreal complex networks such as power-law degree distribution, small-worldness,\nand communities-within-communities. Scalability was tested on a large cluster\nat Lawrence Livermore National Laboratory. In the experiment, we were able to\ngenerate a graph with 1 billion vertices and 5 billion edges in less than 13\nseconds. To the best of our knowledge, this is the largest synthetic scale-free\ngraph reported in the literature."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3866v2", 
    "other_authors": "Ali Khajeh-Hosseini, David Greenwood, James W. Smith, Ian Sommerville", 
    "title": "The Cloud Adoption Toolkit: Addressing the Challenges of Cloud Adoption   in Enterprise", 
    "arxiv-id": "1003.3866v2", 
    "author": "Ian Sommerville", 
    "publish": "2010-03-19T19:40:41Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper: i) describes the challenges that\ndecision makers face when attempting to determine the feasibility of the\nadoption of cloud computing in their organisations; ii) illustrates a lack of\nexisting work to address the feasibility challenges of cloud adoption in the\nenterprise; iii) introduces the Cloud Adoption Toolkit that provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. The paper adopts a position paper methodology such that case\nstudy evidence is provided, where available, to support claims. We conclude\nthat the Cloud Adoption Toolkit, whilst still under development, shows signs\nthat it is a useful tool for decision makers as it helps address the\nfeasibility challenges of cloud adoption in the enterprise."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3920v1", 
    "other_authors": "Rajkumar Buyya, Rajiv Ranjan, Rodrigo N. Calheiros", 
    "title": "InterCloud: Utility-Oriented Federation of Cloud Computing Environments   for Scaling of Application Services", 
    "arxiv-id": "1003.3920v1", 
    "author": "Rodrigo N. Calheiros", 
    "publish": "2010-03-20T10:54:43Z", 
    "summary": "Cloud computing providers have setup several data centers at different\ngeographical locations over the Internet in order to optimally serve needs of\ntheir customers around the world. However, existing systems do not support\nmechanisms and policies for dynamically coordinating load distribution among\ndifferent Cloud-based data centers in order to determine optimal location for\nhosting application services to achieve reasonable QoS levels. Further, the\nCloud computing providers are unable to predict geographic distribution of\nusers consuming their services, hence the load coordination must happen\nautomatically, and distribution of services must change in response to changes\nin the load. To counter this problem, we advocate creation of federated Cloud\ncomputing environment (InterCloud) that facilitates just-in-time,\nopportunistic, and scalable provisioning of application services, consistently\nachieving QoS targets under variable workload, resource and network conditions.\nThe overall goal is to create a computing environment that supports dynamic\nexpansion or contraction of capabilities (VMs, services, storage, and database)\nfor handling sudden variations in service demands.\n  This paper presents vision, challenges, and architectural elements of\nInterCloud for utility-oriented federation of Cloud computing environments. The\nproposed InterCloud environment supports scaling of applications across\nmultiple vendor clouds. We have validated our approach by conducting a set of\nrigorous performance evaluation study using the CloudSim toolkit. The results\ndemonstrate that federated Cloud computing model has immense potential as it\noffers significant performance gains as regards to response time and cost\nsaving under dynamic workload scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.5342v1", 
    "other_authors": "Samih Mohemmed Mostafa", 
    "title": "Improving Waiting Time of Tasks Scheduled Under Preemptive Round Robin   Using Changeable Time Quantum", 
    "arxiv-id": "1003.5342v1", 
    "author": "Samih Mohemmed Mostafa", 
    "publish": "2010-03-28T06:34:20Z", 
    "summary": "Minimizing waiting time for tasks waiting in the queue for execution is one\nof the important scheduling cri-teria which took a wide area in scheduling\npreemptive tasks. In this paper we present Changeable Time Quan-tum (CTQ)\napproach combined with the round-robin algorithm, we try to adjust the time\nquantum according to the burst times of the tasks in the ready queue. There are\ntwo important benefits of using (CTQ) approach: minimizing the average waiting\ntime of the tasks, consequently minimizing the average turnaround time, and\nkeeping the number of context switches as low as possible, consequently\nminimizing the scheduling overhead. In this paper, we consider the scheduling\nproblem for preemptive tasks, where the time costs of these tasks are known a\npriori. Our experimental results demonstrate that CTQ can provide much lower\nscheduling overhead and better scheduling criteria."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.5794v3", 
    "other_authors": "Wei Zhou, Lei Wang, Dan Meng, Lin Yuan, Jianfeng Zhan", 
    "title": "Scalable Group Management in Large-Scale Virtualized Clusters", 
    "arxiv-id": "1003.5794v3", 
    "author": "Jianfeng Zhan", 
    "publish": "2010-03-30T11:22:55Z", 
    "summary": "To save cost, recently more and more users choose to provision virtual\nmachine resources in cluster systems, especially in data centres. Maintaining a\nconsistent member view is the foundation of reliable cluster managements, and\nit also raises several challenge issues for large scale cluster systems\ndeployed with virtual machines (which we call virtualized clusters). In this\npaper, we introduce our experiences in design and implementation of scalable\nmember view management on large-scale virtual clusters. Our research\ncontributions are three-fold: 1) we propose a scalable and reliable management\ninfrastructure that combines a peer-to-peer structure and a hierarchy structure\nto maintain a consistent member view in virtual clusters; 2) we present a\nlight-weighted group membership algorithm that can reach the consistent member\nview within a single round of message exchange; and 3) we design and implement\na scalable membership service that can provision virtual machines and maintain\na consistent member view in virtual clusters. Our work is verified on Dawning\n5000A, which ranked No.10 of Top 500 super computers in November, 2008."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICECCS.2010.21", 
    "link": "http://arxiv.org/pdf/1004.0728v1", 
    "other_authors": "Ilango Sriram, Dave Cliff", 
    "title": "Effects of component-subscription network topology on large-scale data   centre performance scaling", 
    "arxiv-id": "1004.0728v1", 
    "author": "Dave Cliff", 
    "publish": "2010-04-05T22:08:46Z", 
    "summary": "Modern large-scale date centres, such as those used for cloud computing\nservice provision, are becoming ever-larger as the operators of those data\ncentres seek to maximise the benefits from economies of scale. With these\nincreases in size comes a growth in system complexity, which is usually\nproblematic. There is an increased desire for automated \"self-star\"\nconfiguration, management, and failure-recovery of the data-centre\ninfrastructure, but many traditional techniques scale much worse than linearly\nas the number of nodes to be managed increases. As the number of nodes in a\nmedian-sized data-centre looks set to increase by two or three orders of\nmagnitude in coming decades, it seems reasonable to attempt to explore and\nunderstand the scaling properties of the data-centre middleware before such\ndata-centres are constructed. In [1] we presented SPECI, a simulator that\npredicts aspects of large-scale data-centre middleware performance,\nconcentrating on the influence of status changes such as policy updates or\nroutine node failures. [...]. In [1] we used a first-approximation assumption\nthat such subscriptions are distributed wholly at random across the data\ncentre. In this present paper, we explore the effects of introducing more\nrealistic constraints to the structure of the internal network of\nsubscriptions. We contrast the original results [...] exploring the effects of\nmaking the data-centre's subscription network have a regular lattice-like\nstructure, and also semi-random network structures resulting from parameterised\nnetwork generation functions that create \"small-world\" and \"scale-free\"\nnetworks. We show that for distributed middleware topologies, the structure and\ndistribution of tasks carried out in the data centre can significantly\ninfluence the performance overhead imposed by the middleware."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICECCS.2010.21", 
    "link": "http://arxiv.org/pdf/1004.1276v1", 
    "other_authors": "Lei Wang, Jianfeng Zhan, Weisong Shi, Yi Liang", 
    "title": "In Cloud, Can Scientific Communities Benefit from the Economies of   Scale?", 
    "arxiv-id": "1004.1276v1", 
    "author": "Yi Liang", 
    "publish": "2010-04-08T08:07:08Z", 
    "summary": "The basic idea behind Cloud computing is that resource providers offer\nelastic resources to end users. In this paper, we intend to answer one key\nquestion to the success of Cloud computing: in Cloud, can small or medium-scale\nscientific computing communities benefit from the economies of scale? Our\nresearch contributions are three-fold: first, we propose an enhanced scientific\npublic cloud model (ESP) that encourages small- or medium-scale organizations\nto rent elastic resources from a public cloud provider; second, on a basis of\nthe ESP model, we design and implement the DawningCloud system that can\nconsolidate heterogeneous scientific workloads on a Cloud site; third, we\npropose an innovative emulation methodology and perform a comprehensive\nevaluation. We found that for two typical workloads: high throughput computing\n(HTC) and many task computing (MTC), DawningCloud saves the resource\nconsumption maximally by 44.5% (HTC) and 72.6% (MTC) for service providers, and\nsaves the total resource consumption maximally by 47.3% for a resource provider\nwith respect to the previous two public Cloud solutions. To this end, we\nconclude that for typical workloads: HTC and MTC, DawningCloud can enable\nscientific communities to benefit from the economies of scale of public Clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1324v1", 
    "other_authors": "Michael Burke, Neil Audsley", 
    "title": "Distributed Fault-Tolerant Avionic Systems - A Real-Time Perspective", 
    "arxiv-id": "1004.1324v1", 
    "author": "Neil Audsley", 
    "publish": "2010-04-08T13:03:11Z", 
    "summary": "This paper examines the problem of introducing advanced forms of\nfault-tolerance via reconfiguration into safety-critical avionic systems. This\nis required to enable increased availability after fault occurrence in\ndistributed integrated avionic systems(compared to static federated systems).\nThe approach taken is to identify a migration path from current architectures\nto those that incorporate re-configuration to a lesser or greater degree. Other\nchallenges identified include change of the development process; incremental\nand flexible timing and safety analyses; configurable kernels applicable for\nsafety-critical systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1746v1", 
    "other_authors": "S Qamar, Niranjan Lal, Mrityunjay Singh", 
    "title": "Internet ware cloud computing :Challenges", 
    "arxiv-id": "1004.1746v1", 
    "author": "Mrityunjay Singh", 
    "publish": "2010-04-10T22:17:18Z", 
    "summary": "After decades of engineering development and infrastructural investment,\nInternet connections have become commodity product in many countries, and\nInternet scale \"cloud computing\" has started to compete with traditional\nsoftware business through its technological advantages and economy of scale.\nCloud computing is a promising enabling technology of Internet ware Cloud\nComputing is termed as the next big thing in the modern corporate world. Apart\nfrom the present day software and technologies, cloud computing will have a\ngrowing impact on enterprise IT and business activities in many large\norganizations. This paper provides an insight to cloud computing, its impacts\nand discusses various issues that business organizations face while\nimplementing cloud computing. Further, it recommends various strategies that\norganizations need to adopt while migrating to cloud computing. The purpose of\nthis paper is to develop an understanding of cloud computing in the modern\nworld and its impact on organizations and businesses. Initially the paper\nprovides a brief description of the cloud computing model introduction and its\npurposes. Further it discusses various technical and non-technical issues that\nneed to be overcome in order for the benefits of cloud computing to be realized\nin corporate businesses and organizations. It then provides various\nrecommendations and strategies that businesses need to work on before stepping\ninto new technologies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1864v2", 
    "other_authors": "Wenbing Zhao, P. M. Melliar-Smith, L. E. Moser", 
    "title": "The Low Latency Fault Tolerance System", 
    "arxiv-id": "1004.1864v2", 
    "author": "L. E. Moser", 
    "publish": "2010-04-12T01:25:49Z", 
    "summary": "The Low Latency Fault Tolerance (LLFT) system provides fault tolerance for\ndistributed applications, using the leader-follower replication technique. The\nLLFT system provides application-transparent replication, with strong replica\nconsistency, for applications that involve multiple interacting processes or\nthreads. The LLFT system comprises a Low Latency Messaging Protocol, a\nLeader-Determined Membership Protocol, and a Virtual Determinizer Framework.\nThe Low Latency Messaging Protocol provides reliable, totally ordered message\ndelivery by employing a direct group-to-group multicast, where the message\nordering is determined by the primary replica in the group. The\nLeader-Determined Membership Protocol provides reconfiguration and recovery\nwhen a replica becomes faulty and when a replica joins or leaves a group, where\nthe membership of the group is determined by the primary replica. The Virtual\nDeterminizer Framework captures the ordering information at the primary replica\nand enforces the same ordering at the backup replicas for major sources of\nnon-determinism, including multi-threading, time-related operations and socket\ncommunication. The LLFT system achieves low latency message delivery during\nnormal operation and low latency reconfiguration and recovery when a fault\noccurs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.2308v1", 
    "other_authors": "David Isaac Wolinsky, Pierre St. Juste, P. Oscar Boykin, Renato Figueiredo", 
    "title": "Addressing the P2P Bootstrap Problem for Small Networks", 
    "arxiv-id": "1004.2308v1", 
    "author": "Renato Figueiredo", 
    "publish": "2010-04-14T03:14:49Z", 
    "summary": "P2P overlays provide a framework for building distributed applications\nconsisting of few to many resources with features including self-configuration,\nscalability, and resilience to node failures. Such systems have been\nsuccessfully adopted in large-scale services for content delivery networks,\nfile sharing, and data storage. In small-scale systems, they can be useful to\naddress privacy concerns and for network applications that lack dedicated\nservers. The bootstrap problem, finding an existing peer in the overlay,\nremains a challenge to enabling these services for small-scale P2P systems. In\nlarge networks, the solution to the bootstrap problem has been the use of\ndedicated services, though creating and maintaining these systems requires\nexpertise and resources, which constrain their usefulness and make them\nunappealing for small-scale systems. This paper surveys and summarizes\nrequirements that allow peers potentially constrained by network connectivity\nto bootstrap small-scale overlays through the use of existing public overlays.\nIn order to support bootstrapping, a public overlay must support the following\nrequirements: a method for reflection in order to obtain publicly reachable\naddresses, so peers behind network address translators and firewalls can\nreceive incoming connection requests; communication relaying to share public\naddresses and communicate when direct communication is not feasible; and\nrendezvous for discovering remote peers, when the overlay lacks stable\nmembership. After presenting a survey of various public overlays, we identify\ntwo overlays that match the requirements: XMPP overlays, such as Google Talk\nand Live Journal Talk, and Brunet, a structured overlay based upon Symphony. We\npresent qualitative experiences with prototypes that demonstrate the ability to\nbootstrap small-scale private structured overlays from public Brunet or XMPP\ninfrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.2772v2", 
    "other_authors": "Alfons Laarman, Jaco van de Pol, Michael Weber", 
    "title": "Boosting Multi-Core Reachability Performance with Shared Hash Tables", 
    "arxiv-id": "1004.2772v2", 
    "author": "Michael Weber", 
    "publish": "2010-04-16T07:53:38Z", 
    "summary": "This paper focuses on data structures for multi-core reachability, which is a\nkey component in model checking algorithms and other verification methods. A\ncornerstone of an efficient solution is the storage of visited states. In\nrelated work, static partitioning of the state space was combined with\nthread-local storage and resulted in reasonable speedups, but left open whether\nimprovements are possible. In this paper, we present a scaling solution for\nshared state storage which is based on a lockless hash table implementation.\nThe solution is specifically designed for the cache architecture of modern\nCPUs. Because model checking algorithms impose loose requirements on the hash\ntable operations, their design can be streamlined substantially compared to\nrelated work on lockless hash tables. Still, an implementation of the hash\ntable presented here has dozens of sensitive performance parameters (bucket\nsize, cache line size, data layout, probing sequence, etc.). We analyzed their\nimpact and compared the resulting speedups with related tools. Our\nimplementation outperforms two state-of-the-art multi-core model checkers (SPIN\nand DiVinE) by a substantial margin, while placing fewer constraints on the\nload balancing and search algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.3395v1", 
    "other_authors": "Ioannis Chatzigiannakis, Othon Michail, Stavros Nikolaou, Andreas Pavlogiannis, Paul G. Spirakis", 
    "title": "Passively Mobile Communicating Logarithmic Space Machines", 
    "arxiv-id": "1004.3395v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2010-04-20T10:03:01Z", 
    "summary": "We propose a new theoretical model for passively mobile Wireless Sensor\nNetworks. We call it the PALOMA model, standing for PAssively mobile\nLOgarithmic space MAchines. The main modification w.r.t. the Population\nProtocol model is that agents now, instead of being automata, are Turing\nMachines whose memory is logarithmic in the population size n. Note that the\nnew model is still easily implementable with current technology. We focus on\ncomplete communication graphs. We define the complexity class PLM, consisting\nof all symmetric predicates on input assignments that are stably computable by\nthe PALOMA model. We assume that the agents are initially identical.\nSurprisingly, it turns out that the PALOMA model can assign unique consecutive\nids to the agents and inform them of the population size! This allows us to\ngive a direct simulation of a Deterministic Turing Machine of O(nlogn) space,\nthus, establishing that any symmetric predicate in SPACE(nlogn) also belongs to\nPLM. We next prove that the PALOMA model can simulate the Community Protocol\nmodel, thus, improving the previous lower bound to all symmetric predicates in\nNSPACE(nlogn). Going one step further, we generalize the simulation of the\ndeterministic TM to prove that the PALOMA model can simulate a Nondeterministic\nTM of O(nlogn) space. Although providing the same lower bound, the important\nremark here is that the bound is now obtained in a direct manner, in the sense\nthat it does not depend on the simulation of a TM by a Pointer Machine.\nFinally, by showing that a Nondeterministic TM of O(nlogn) space decides any\nlanguage stably computable by the PALOMA model, we end up with an exact\ncharacterization for PLM: it is precisely the class of all symmetric predicates\nin NSPACE(nlogn)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.3566v1", 
    "other_authors": "G. Murugesan, C. Chellappan", 
    "title": "An Economic-based Resource Management and Scheduling for Grid Computing   Applications", 
    "arxiv-id": "1004.3566v1", 
    "author": "C. Chellappan", 
    "publish": "2010-04-20T20:32:31Z", 
    "summary": "Resource management and scheduling plays a crucial role in achieving high\nutilization of resources in grid computing environments. Due to heterogeneity\nof resources, scheduling an application is significantly complicated and\nchallenging task in grid system. Most of the researches in this area are mainly\nfocused on to improve the performance of the grid system. There were some\nallocation model has been proposed based on divisible load theory with\ndifferent type of workloads and a single originating processor. In this paper\nwe introduce a new resource allocation model with multiple load originating\nprocessors as an economic model. Solutions for an optimal allocation of\nfraction of loads to nodes obtained to minimize the cost of the grid users via\nlinear programming approach. It is found that the resource allocation model can\nefficiently and effectively allocate workloads to proper resources.\nExperimental results showed that the proposed model obtained the better\nsolution in terms of cost and time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.4559v1", 
    "other_authors": "Supriya Krishnamurthy, John Ardelius, Erik Aurell, Mads Dam, Rolf Stadler, Fetahi Wuhib", 
    "title": "The Accuracy of Tree-based Counting in Dynamic Networks", 
    "arxiv-id": "1004.4559v1", 
    "author": "Fetahi Wuhib", 
    "publish": "2010-04-26T15:51:23Z", 
    "summary": "Tree-based protocols are ubiquitous in distributed systems. They are\nflexible, they perform generally well, and, in static conditions, their\nanalysis is mostly simple. Under churn, however, node joins and failures can\nhave complex global effects on the tree overlays, making analysis surprisingly\nsubtle. To our knowledge, few prior analytic results for performance estimation\nof tree based protocols under churn are currently known. We study a simple\nBellman-Ford-like protocol which performs network size estimation over a\ntree-shaped overlay. A continuous time Markov model is constructed which allows\nkey protocol characteristics to be estimated, including the expected number of\nnodes at a given (perceived) distance to the root and, for each such node, the\nexpected (perceived) size of the subnetwork rooted at that node. We validate\nthe model by simulation, using a range of network sizes, node degrees, and\nchurn-to-protocol rates, with convincing results."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1004.4701v3", 
    "other_authors": "Eli Gafni, Petr Kuznetsov", 
    "title": "Relating L-Resilience and Wait-Freedom via Hitting Sets", 
    "arxiv-id": "1004.4701v3", 
    "author": "Petr Kuznetsov", 
    "publish": "2010-04-27T03:29:43Z", 
    "summary": "The condition of t-resilience stipulates that an n-process program is only\nobliged to make progress when at least n-t processes are correct. Put another\nway, the live sets, the collection of process sets such that progress is\nrequired if all the processes in one of these sets are correct, are all sets\nwith at least n-t processes.\n  We show that the ability of arbitrary collection of live sets L to solve\ndistributed tasks is tightly related to the minimum hitting set of L, a minimum\ncardinality subset of processes that has a non-empty intersection with every\nlive set. Thus, finding the computing power of L is NP-complete.\n  For the special case of colorless tasks that allow participating processes to\nadopt input or output values of each other, we use a simple simulation to show\nthat a task can be solved L-resiliently if and only if it can be solved\n(h-1)-resiliently, where h is the size of the minimum hitting set of L.\n  For general tasks, we characterize L-resilient solvability of tasks with\nrespect to a limited notion of weak solvability: in every execution where all\nprocesses in some set in L are correct, outputs must be produced for every\nprocess in some (possibly different) participating set in L. Given a task T, we\nconstruct another task T_L such that T is solvable weakly L-resiliently if and\nonly if T_L is solvable weakly wait-free."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1004.5256v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "Construction auto-stabilisante d'arbre couvrant en d\u00e9pit d'actions   malicieuses", 
    "arxiv-id": "1004.5256v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-04-29T12:02:13Z", 
    "summary": "A self-stabilizing protocol provides by definition a tolerance to transient\nfailures. Recently, a new class of self-stabilizing protocols appears. These\nprotocols provides also a tolerance to a given number of permanent failures. In\nthis article, we are interested in self-stabilizing protocols that deal with\nByzantines failures. We prove that, for some problems which not allow strict\nstabilization (see [Nesterenko,Arora,2002]), there exist solutions that\ntolerates Byzantine faults if we define a new criteria of tolerance."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1006.0308v1", 
    "other_authors": "Rajkumar Buyya, Anton Beloglazov, Jemal Abawajy", 
    "title": "Energy-Efficient Management of Data Center Resources for Cloud   Computing: A Vision, Architectural Elements, and Open Challenges", 
    "arxiv-id": "1006.0308v1", 
    "author": "Jemal Abawajy", 
    "publish": "2010-06-02T06:45:07Z", 
    "summary": "Cloud computing is offering utility-oriented IT services to users worldwide.\nBased on a pay-as-you-go model, it enables hosting of pervasive applications\nfrom consumer, scientific, and business domains. However, data centers hosting\nCloud applications consume huge amounts of energy, contributing to high\noperational costs and carbon footprints to the environment. Therefore, we need\nGreen Cloud computing solutions that can not only save energy for the\nenvironment but also reduce operational costs. This paper presents vision,\nchallenges, and architectural elements for energy-efficient management of Cloud\ncomputing environments. We focus on the development of dynamic resource\nprovisioning and allocation algorithms that consider the synergy between\nvarious data center infrastructures (i.e., the hardware, power units, cooling\nand software), and holistically work to boost data center energy efficiency and\nperformance. In particular, this paper proposes (a) architectural principles\nfor energy-efficient management of Clouds; (b) energy-efficient resource\nallocation policies and scheduling algorithms considering quality-of-service\nexpectations, and devices power usage characteristics; and (c) a novel software\ntechnology for energy-efficient management of Clouds. We have validated our\napproach by conducting a set of rigorous performance evaluation study using the\nCloudSim toolkit. The results demonstrate that Cloud computing model has\nimmense potential as it offers significant performance gains as regards to\nresponse time and cost saving under dynamic workload scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1006.1074v1", 
    "other_authors": "M. Monnerville, G. S\u00e9mah", 
    "title": "Youpi, a Web-based Astronomical Image Processing Pipeline", 
    "arxiv-id": "1006.1074v1", 
    "author": "G. S\u00e9mah", 
    "publish": "2010-06-05T22:27:14Z", 
    "summary": "Youpi stands for \"YOUpi is your processing PIpeline\". It is a portable, easy\nto use web application providing high level functionalities to perform data\nreduction on scientific FITS images. It is built on top of open source\nprocessing tools that are released to the community by Terapix, in order to\norganize your data on a computer cluster, to manage your processing jobs in\nreal time and to facilitate teamwork by allowing fine-grain sharing of results\nand data. On the server side, Youpi is written in the Python programming\nlanguage and uses the Django web framework. On the client side, Ajax techniques\nare used along with the Prototype and script.aculo.us Javascript librairies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2301", 
    "link": "http://arxiv.org/pdf/1006.1177v1", 
    "other_authors": "Srirangam V Addepallil, Per Andersen, George L Barnes", 
    "title": "Efficient Resource Matching in Heterogeneous Grid Using Resource Vector", 
    "arxiv-id": "1006.1177v1", 
    "author": "George L Barnes", 
    "publish": "2010-06-07T06:22:05Z", 
    "summary": "In this paper, a method for efficient scheduling to obtain optimum job\nthroughput in a distributed campus grid environment is presented; Traditional\njob schedulers determine job scheduling using user and job resource attributes.\nUser attributes are related to current usage, historical usage, user priority\nand project access. Job resource attributes mainly comprise of soft\nrequirements (compilers, libraries) and hard requirements like memory, storage\nand interconnect. A job scheduler dispatches jobs to a resource if a job's hard\nand soft requirements are met by a resource. In current scenario during\nexecution of a job, if a resource becomes unavailable, schedulers are presented\nwith limited options, namely re-queuing job or migrating job to a different\nresource. Both options are expensive in terms of data and compute time. These\nsituations can be avoided, if the often ignored factor, availability time of a\nresource in a grid environment is considered. We propose resource rank\napproach, in which jobs are dispatched to a resource which has the highest rank\namong all resources that match the job's requirement. The results show that our\napproach can increase throughput of many serial / monolithic jobs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1191v1", 
    "other_authors": "Yaser Miaji Osman Gazali, Suhaidi Hassan", 
    "title": "Survey on the Event Orderings Semantics Used for Distributed System", 
    "arxiv-id": "1006.1191v1", 
    "author": "Suhaidi Hassan", 
    "publish": "2010-06-07T07:26:00Z", 
    "summary": "Event ordering in distributed system (DS) is disputable and proactive subject\nin DS particularly with the emergence of multimedia synchronization. According\nto the literature, different type of event ordering is used for different DS\nmode such as asynchronous or synchronous. Recently, there are several novel\nimplementation of these types introduced to fulfill the demand for establishing\na certain order according to a specific criterion in DS with lighter\ncomplexity."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1401v2", 
    "other_authors": "Jianfeng Zhan, Lei Wang, Weisong Shi, Shimin Gong, Xiutao Zang", 
    "title": "PhoenixCloud: Provisioning Resources for Heterogeneous Workloads in   Cloud Computing", 
    "arxiv-id": "1006.1401v2", 
    "author": "Xiutao Zang", 
    "publish": "2010-06-08T00:42:20Z", 
    "summary": "As more and more service providers choose Cloud platforms, which is provided\nby third party resource providers, resource providers needs to provision\nresources for heterogeneous workloads in different Cloud scenarios. Taking into\naccount the dramatic differences of heterogeneous workloads, can we\ncoordinately provision resources for heterogeneous workloads in Cloud\ncomputing? In this paper we focus on this important issue, which is\ninvestigated by few previous work. Our contributions are threefold: (1) we\nrespectively propose a coordinated resource provisioning solution for\nheterogeneous workloads in two typical Cloud scenarios: first, a large\norganization operates a private Cloud for two heterogeneous workloads; second,\na large organization or two service providers running heterogeneous workloads\nrevert to a public Cloud; (2) we build an agile system PhoenixCloud that\nenables a resource provider to create coordinated runtime environments on\ndemand for heterogeneous workloads when they are consolidated on a Cloud site;\nand (3) A comprehensive evaluation has been performed in experiments. For two\ntypical heterogeneous workload traces: parallel batch jobs and Web services,\nour experiments show that: a) in a private Cloud scenario, when the throughput\nis almost same like that of a dedicated cluster system, our solution decreases\nthe configuration size of a cluster by about 40%; b) in a public Cloud\nscenario, our solution decreases not only the total resource consumption, but\nalso the peak resource consumption maximally to 31% with respect to that of EC2\n+RightScale solution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1926v1", 
    "other_authors": "Daniel Kunkle", 
    "title": "Roomy: A System for Space Limited Computations", 
    "arxiv-id": "1006.1926v1", 
    "author": "Daniel Kunkle", 
    "publish": "2010-06-09T23:12:33Z", 
    "summary": "There are numerous examples of problems in symbolic algebra in which the\nrequired storage grows far beyond the limitations even of the distributed RAM\nof a cluster. Often this limitation determines how large a problem one can\nsolve in practice. Roomy provides a minimally invasive system to modify the\ncode for such a computation, in order to use the local disks of a cluster or a\nSAN as a transparent extension of RAM.\n  Roomy is implemented as a C/C++ library. It provides some simple data\nstructures (arrays, unordered lists, and hash tables). Some typical programming\nconstructs that one might employ in Roomy are: map, reduce, duplicate\nelimination, chain reduction, pair reduction, and breadth-first search. All\naspects of parallelism and remote I/O are hidden within the Roomy library."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.2650v1", 
    "other_authors": "Ajanta De Sarkar, Nandini Mukherjee", 
    "title": "A Study on Performance Analysis Tools for Applications Running on Large   Distributed Systems", 
    "arxiv-id": "1006.2650v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-06-14T09:39:45Z", 
    "summary": "The evolution of distributed architectures and programming paradigms for\nperformance-oriented program development, challenge the state-of-the-art\ntechnology for performance tools. The area of high performance computing is\nrapidly expanding from single parallel systems to clusters and grids of\nheterogeneous sequential and parallel systems. Performance analysis and tuning\napplications is becoming crucial because it is hardly possible to otherwise\nachieve the optimum performance of any application. The objective of this paper\nis to study the state-of-the-art technology of the existing performance tools\nfor distributed systems. The paper surveys some representative tools from\ndifferent aspects in order to highlight the approaches and technologies used by\nthem."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3432v1", 
    "other_authors": "Anissa Lamani, Alain Cournier, Swan Dubois, Franck Petit, Vincent Villain", 
    "title": "Snap-Stabilizing Linear Message Forwarding", 
    "arxiv-id": "1006.3432v1", 
    "author": "Vincent Villain", 
    "publish": "2010-06-17T11:22:27Z", 
    "summary": "In this paper, we present the first snap-stabilizing message forwarding\nprotocol that uses a number of buffers per node being inde- pendent of any\nglobal parameter, that is 4 buffers per link. The protocol works on a linear\nchain of nodes, that is possibly an overlay on a large- scale and dynamic\nsystem, e.g., Peer-to-Peer systems, Grids. . . Provided that the topology\nremains a linear chain and that nodes join and leave \"neatly\", the protocol\ntolerates topology changes. We expect that this protocol will be the base to\nget similar results on more general topologies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3452v1", 
    "other_authors": "Graham Kirby, Alan Dearle, Stuart Norcross", 
    "title": "Generating a Family of Byzantine Tolerant Protocol Implementations Using   a Meta-Model Architecture", 
    "arxiv-id": "1006.3452v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-17T12:46:19Z", 
    "summary": "We describe an approach to modelling a Byzantine tolerant distributed\nalgorithm as a family of related finite state machines, generated from a single\nmeta-model. Various artefacts are generated from each state machine, including\ndiagrams and source-level protocol implementations. The approach allows a state\nmachine formulation to be applied to problems for which it would not otherwise\nbe suitable, increasing confidence in correctness."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3465v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Stuart Norcross", 
    "title": "Hosting Byzantine Fault Tolerant Services on a Chord Ring", 
    "arxiv-id": "1006.3465v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-17T13:40:59Z", 
    "summary": "In this paper we demonstrate how stateful Byzantine Fault Tolerant services\nmay be hosted on a Chord ring. The strategy presented is fourfold: firstly a\nreplication scheme that dissociates the maintenance of replicated service state\nfrom ring recovery is developed. Secondly, clients of the ring based services\nare made replication aware. Thirdly, a consensus protocol is introduced that\nsupports the serialization of updates. Finally Byzantine fault tolerant\nreplication protocols are developed that ensure the integrity of service data\nhosted on the ring."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3724v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Stuart Norcross, Andrew McCarthy", 
    "title": "A Peer-to-Peer Middleware Framework for Resilient Persistent Programming", 
    "arxiv-id": "1006.3724v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T15:35:50Z", 
    "summary": "The persistent programming systems of the 1980s offered a programming model\nthat integrated computation and long-term storage. In these systems, reliable\napplications could be engineered without requiring the programmer to write\ntranslation code to manage the transfer of data to and from non-volatile\nstorage. More importantly, it simplified the programmer's conceptual model of\nan application, and avoided the many coherency problems that result from\nmultiple cached copies of the same information. Although technically\ninnovative, persistent languages were not widely adopted, perhaps due in part\nto their closed-world model. Each persistent store was located on a single\nhost, and there were no flexible mechanisms for communication or transfer of\ndata between separate stores. Here we re-open the work on persistence and\ncombine it with modern peer-to-peer techniques in order to provide support for\northogonal persistence in resilient and potentially long-running distributed\napplications. Our vision is of an infrastructure within which an application\ncan be developed and distributed with minimal modification, whereupon the\napplication becomes resilient to certain failure modes. If a node, or the\nconnection to it, fails during execution of the application, the objects are\nre-instantiated from distributed replicas, without their reference holders\nbeing aware of the failure. Furthermore, we believe that this can be achieved\nwithin a spectrum of application programmer intervention, ranging from minimal\nto totally prescriptive, as desired. The same mechanisms encompass an\northogonally persistent programming model. We outline our approach to\nimplementing this vision, and describe current progress."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3728v1", 
    "other_authors": "Scott Walker, Alan Dearle, Stuart Norcross, Graham Kirby, Andrew McCarthy", 
    "title": "RAFDA: A Policy-Aware Middleware Supporting the Flexible Separation of   Application Logic from Distribution", 
    "arxiv-id": "1006.3728v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T15:39:22Z", 
    "summary": "Middleware technologies often limit the way in which object classes may be\nused in distributed applications due to the fixed distribution policies that\nthey impose. These policies permeate applications developed using existing\nmiddleware systems and force an unnatural encoding of application level\nsemantics. For example, the application programmer has no direct control over\ninter-address-space parameter passing semantics. Semantics are fixed by the\ndistribution topology of the application, which is dictated early in the design\ncycle. This creates applications that are brittle with respect to changes in\ndistribution. This paper explores technology that provides control over the\nextent to which inter-address-space communication is exposed to programmers, in\norder to aid the creation, maintenance and evolution of distributed\napplications. The described system permits arbitrary objects in an application\nto be dynamically exposed for remote access, allowing applications to be\nwritten without concern for distribution. Programmers can conceal or expose the\ndistributed nature of applications as required, permitting object placement and\ndistribution boundaries to be decided late in the design cycle and even\ndynamically. Inter-address-space parameter passing semantics may also be\ndecided independently of object implementation and at varying times in the\ndesign cycle, again possibly as late as run-time. Furthermore, transmission\npolicy may be defined on a per-class, per-method or per-parameter basis,\nmaximizing plasticity. This flexibility is of utility in the development of new\ndistributed applications, and the creation of management and monitoring\ninfrastructures for existing applications."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3732v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Stuart Norcross, Angus Macdonald, Greg Bigwood", 
    "title": "Towards Adaptable and Adaptive Policy-Free Middleware", 
    "arxiv-id": "1006.3732v1", 
    "author": "Greg Bigwood", 
    "publish": "2010-06-18T15:47:35Z", 
    "summary": "We believe that to fully support adaptive distributed applications,\nmiddleware must itself be adaptable, adaptive and policy-free. In this paper we\npresent a new language-independent adaptable and adaptive policy framework\nsuitable for integration in a wide variety of middleware systems. This\nframework facilitates the construction of adaptive distributed applications.\nThe framework addresses adaptability through its ability to represent a wide\nrange of specific middleware policies. Adaptiveness is supported by a rich\ncontextual model, through which an application programmer may control precisely\nhow policies should be selected for any particular interaction with the\nmiddleware. A contextual pattern mechanism facilitates the succinct expression\nof both coarse- and fine-grain policy contexts. Policies may be specified and\naltered dynamically, and may themselves take account of dynamic conditions. The\nframework contains no hard-wired policies; instead, all policies can be\nconfigured."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3739v1", 
    "other_authors": "Scott Walker, Alan Dearle, Graham Kirby, Stuart Norcross", 
    "title": "Promoting Component Reuse by Separating Transmission Policy from   Implementation", 
    "arxiv-id": "1006.3739v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-18T16:12:58Z", 
    "summary": "In this paper we present a methodology and set of tools which assist the\nconstruction of applications from components, by separating the issues of\ntransmission policy from component definition and implementation. This promotes\na greater degree of software reuse than is possible using traditional\nmiddleware environments. Whilst component technologies are usually presented as\na mechanism for promoting reuse, reuse is often limited due to design choices\nthat permeate component implementation. The programmer has no direct control\nover inter-address-space parameter passing semantics: it is fixed by the\ndistributed application's structure, based on the remote accessibility of the\ncomponents. Using traditional middleware tools and environments, the\napplication designer may be forced to use an unnatural encoding of application\nlevel semantics since application parameter passing semantics are tightly\ncoupled with the component deployment topology. This paper describes how\ninter-address-space parameter passing semantics may be decided independently of\ncomponent implementation. Transmission policy may be dynamically defined on a\nper-class, per-method or per-parameter basis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3742v1", 
    "other_authors": "Alan Dearle, Scott Walker, Stuart Norcross, Graham Kirby, Andrew McCarthy", 
    "title": "RAFDA: Middleware Supporting the Separation of Application Logic from   Distribution Policy", 
    "arxiv-id": "1006.3742v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T16:22:10Z", 
    "summary": "Middleware technologies often limit the way in which object classes may be\nused in distributed applications due to the fixed distribution policies imposed\nby the Middleware system. These policies permeate the applications developed\nusing them and force an unnatural encoding of application level semantics. For\nexample, the application programmer has no direct control over\ninter-address-space parameter passing semantics since it is fixed by the\napplication's distribution topology which is dictated early in the design cycle\nby the Middleware. This creates applications that are brittle with respect to\nchanges in the way in which the applications are distributed. This paper\nexplores technology permitting arbitrary objects in an application to be\ndynamically exposed for remote access. Using this, the application can be\nwritten without concern for its distribution with object placement and\ndistribution boundaries decided late in the design cycle and even dynamically.\nInter-address-space parameter passing semantics may also be decided\nindependently of object implementation and at varying times in the design\ncycle, again, possibly as late as run-time. Furthermore, transmission policy\nmay be defined on a per-class, per-method or per-parameter basis maximizing\nplasticity. This flexibility is of utility in the development of new\ndistributed applications and the creation of management and monitoring\ninfrastructures for existing applications."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3909v2", 
    "other_authors": "Ruo-Wei Hung", 
    "title": "Constructing Two Edge-Disjoint Hamiltonian Cycles and Two Equal   Node-Disjoint Cycles in Twisted Cubes", 
    "arxiv-id": "1006.3909v2", 
    "author": "Ruo-Wei Hung", 
    "publish": "2010-06-20T05:37:07Z", 
    "summary": "The hypercube is one of the most popular interconnection networks since it\nhas simple structure and is easy to implement. The $n$-dimensional twisted\ncube, denoted by $TQ_n$, an important variation of the hypercube, possesses\nsome properties superior to the hypercube. Recently, some interesting\nproperties of $TQ_n$ were investigated. In this paper, we construct two\nedge-disjoint Hamiltonian cycles in $TQ_n$ for any odd integer $n\\geqslant 5$.\nThe presence of two edge-disjoint Hamiltonian cycles provides an advantage when\nimplementing two algorithms that require a ring structure by allowing message\ntraffic to be spread evenly across the twisted cube. Furthermore, we construct\ntwo equal node-disjoint cycles in $TQ_n$ for any odd integer $n\\geqslant 3$, in\nwhich these two cycles contain the same number of nodes and every node appears\nin one cycle exactly once. In other words, we decompose a twisted cube into two\ncomponents with the same size such that each component contains a Hamiltonian\ncycle."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.3919v1", 
    "other_authors": "Ying Cui, Vincent K. N. Lau", 
    "title": "Convergence-Optimal Quantizer Design of Distributed Contraction-based   Iterative Algorithms with Quantized Message Passing", 
    "arxiv-id": "1006.3919v1", 
    "author": "Vincent K. N. Lau", 
    "publish": "2010-06-20T08:32:16Z", 
    "summary": "In this paper, we study the convergence behavior of distributed iterative\nalgorithms with quantized message passing. We first introduce general iterative\nfunction evaluation algorithms for solving fixed point problems distributively.\nWe then analyze the convergence of the distributed algorithms, e.g. Jacobi\nscheme and Gauss-Seidel scheme, under the quantized message passing. Based on\nthe closed-form convergence performance derived, we propose two quantizer\ndesigns, namely the time invariant convergence-optimal quantizer (TICOQ) and\nthe time varying convergence-optimal quantizer (TVCOQ), to minimize the effect\nof the quantization error on the convergence. We also study the tradeoff\nbetween the convergence error and message passing overhead for both TICOQ and\nTVCOQ. As an example, we apply the TICOQ and TVCOQ designs to the iterative\nwaterfilling algorithm of MIMO interference game."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4504v1", 
    "other_authors": "Scott Walker, Alan Dearle, Graham Kirby, Stuart Norcross", 
    "title": "Exposing Application Components as Web Services", 
    "arxiv-id": "1006.4504v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-23T12:59:19Z", 
    "summary": "This paper explores technology permitting arbitrary application components to\nbe exposed for remote access from other software. Using this, the application\nand its constituent components can be written without concern for its\ndistribution. Software running in different address spaces, on different\nmachines, can perform operations on the remotely accessible components. This is\nof utility in the creation of distributed applications and in permitting tools\nsuch as debuggers, component browsers, observers or remote probes access to\napplication components. Current middleware systems do not allow arbitrary\nexposure of application components: instead, the programmer is forced to decide\nstatically which classes of component will support remote accessibility. In the\nwork described here, arbitrary components of any class can be dynamically\nexposed via Web Services. Traditional Web Services are extended with a remote\nreference scheme. This extension permits application components to be invoked\nusing either the traditional pass-by-value semantics supported by Web Services\nor pass-by-reference semantics. The latter permits the preservation of local\ncall semantics across address space boundaries."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4549v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Andrew McCarthy, Juan-Carlos Diaz y Carballo", 
    "title": "A Flexible and Secure Deployment Framework for Distributed Applications", 
    "arxiv-id": "1006.4549v1", 
    "author": "Juan-Carlos Diaz y Carballo", 
    "publish": "2010-06-23T15:07:26Z", 
    "summary": "This paper describes an implemented system which is designed to support the\ndeployment of applications offering distributed services, comprising a number\nof distributed components. This is achieved by creating high level placement\nand topology descriptions which drive tools that deploy applications consisting\nof components running on multiple hosts. The system addresses issues of\nheterogeneity by providing abstractions over host-specific attributes yielding\na homogeneous run-time environment into which components may be deployed. The\nrun-time environments provide secure binding mechanisms that permit deployed\ncomponents to bind to stored data and services on the hosts on which they are\nrunning."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4572v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Andrew McCarthy", 
    "title": "A Framework for Constraint-Based Deployment and Autonomic Management of   Distributed Applications", 
    "arxiv-id": "1006.4572v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-23T15:28:42Z", 
    "summary": "We propose a framework for deployment and subsequent autonomic management of\ncomponent-based distributed applications. An initial deployment goal is\nspecified using a declarative constraint language, expressing constraints over\naspects such as component-host mappings and component interconnection topology.\nA constraint solver is used to find a configuration that satisfies the goal,\nand the configuration is deployed automatically. The deployed application is\ninstrumented to allow subsequent autonomic management. If, during execution,\nthe manager detects that the original goal is no longer being met, the\nsatisfy/deploy process can be repeated automatically in order to generate a\nrevised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4708v1", 
    "other_authors": "Siamak Sarmady", 
    "title": "A Survey on Peer-to-Peer and DHT", 
    "arxiv-id": "1006.4708v1", 
    "author": "Siamak Sarmady", 
    "publish": "2010-06-24T07:56:26Z", 
    "summary": "Distributed systems with different levels of dependence to central services\nhave been designed and used during recent years. Pure peer-to-peer systems\namong distributed systems have no dependence on a central resource. DHT is one\nof the main techniques behind these systems resulting into failure tolerant\nsystems which are also able to isolate continuous changes to the network to a\nsmall section of it and therefore making it possible to scale up such networks\nto millions of nodes. This survey takes a look at P2P in general and DHT\nalgorithms and implementations in more detail."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4730v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Andrew McCarthy", 
    "title": "A Framework for Constraint-Based Deployment and Autonomic Management of   Distributed Applications (Extended Abstract)", 
    "arxiv-id": "1006.4730v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-24T09:44:38Z", 
    "summary": "We propose a framework for the deployment and subsequent autonomic management\nof component-based distributed applications. An initial deployment goal is\nspecified using a declarative constraint language, expressing constraints over\naspects such as component-host mappings and component interconnection topology.\nA constraint solver is used to find a configuration that satisfies the goal,\nand the configuration is deployed automatically. The deployed application is\ninstrumented to allow subsequent autonomic management. If, during execution,\nthe manager detects that the original goal is no longer being met, the\nsatisfy/deploy process can be repeated automatically in order to generate a\nrevised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4733v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Andrew McCarthy", 
    "title": "A Middleware Framework for Constraint-Based Deployment and Autonomic   Management of Distributed Applications", 
    "arxiv-id": "1006.4733v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-24T09:53:35Z", 
    "summary": "We propose a middleware framework for deployment and subsequent autonomic\nmanagement of component-based distributed applications. An initial deployment\ngoal is specified using a declarative constraint language, expressing\nconstraints over aspects such as component-host mappings and component\ninterconnection topology. A constraint solver is used to find a configuration\nthat satisfies the goal, and the configuration is deployed automatically. The\ndeployed application is instrumented to allow subsequent autonomic management.\nIf, during execution, the manager detects that the original goal is no longer\nbeing met, the satisfy/deploy process can be repeated automatically in order to\ngenerate a revised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4746v1", 
    "other_authors": "Graham Kirby, Alan Dearle, Ron Morrison, Mark Dunlop, Richard Connor, Paddy Nixon", 
    "title": "Active Architecture for Pervasive Contextual Services", 
    "arxiv-id": "1006.4746v1", 
    "author": "Paddy Nixon", 
    "publish": "2010-06-24T11:03:00Z", 
    "summary": "Pervasive services may be defined as services that are available \"to any\nclient (anytime, anywhere)\". Here we focus on the software and network\ninfrastructure required to support pervasive contextual services operating over\na wide area. One of the key requirements is a matching service capable of\nas-similating and filtering information from various sources and determining\nmatches relevant to those services. We consider some of the challenges in\nengineering a globally distributed matching service that is scalable,\nmanageable, and able to evolve incrementally as usage patterns, data formats,\nservices, network topologies and deployment technologies change. We outline an\napproach based on the use of a peer-to-peer architecture to distribute user\nevents and data, and to support the deployment and evolution of the\ninfrastructure itself."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.4827v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Ron Morrison, Andrew McCarthy, Kevin Mullen, Yanyan Yang, Richard Connor, Paula Welen, Andy Wilson", 
    "title": "Architectural Support for Global Smart Spaces", 
    "arxiv-id": "1006.4827v1", 
    "author": "Andy Wilson", 
    "publish": "2010-06-24T16:27:03Z", 
    "summary": "A GLObal Smart Space (GLOSS) provides support for interaction amongst people,\nartefacts and places while taking account of both context and movement on a\nglobal scale. Crucial to the definition of a GLOSS is the provision of a set of\nlocation-aware services that detect, convey, store and exploit location\ninformation. We use one of these services, hearsay, to illustrate the\nimplementation dimensions of a GLOSS. The focus of the paper is on both local\nand global software architecture to support the implementation of such\nservices. The local architecture is based on XML pipe-lines and is used to\nconstruct location-aware components. The global architecture is based on a\nhybrid peer-to-peer routing scheme and provides the local architectures with\nthe means to communicate in the global context."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.4900v2", 
    "other_authors": "R. Seth Terashima, James D. Fix", 
    "title": "Optimal Degree Distributions for Uniform Small World Rings", 
    "arxiv-id": "1006.4900v2", 
    "author": "James D. Fix", 
    "publish": "2010-06-25T02:14:57Z", 
    "summary": "Motivated by Kleinberg's (2000) and subsequent work, we consider the\nperformance of greedy routing on a directed ring of $n$ nodes augmented with\nlong-range contacts. In this model, each node $u$ is given an additional $D_u$\nedges, a degree chosen from a specified probability distribution. Each such\nedge from $u$ is linked to a random node at distance $r$ ahead in the ring with\nprobability proportional to $1/r$, a \"harmonic\" distance distribution of\ncontacts. Aspnes et al. (2002) have shown an $O(\\log^2 n / \\ell)$ bound on the\nexpected length of greedy routes in the case when each node is assigned exactly\n$\\ell$ contacts and, as a consequence of recent work by Dietzfelbinger and\nWoelfel (2009), this bound is known to be tight. In this paper, we generalize\nAspnes' upper bound to show that any degree distribution with mean $\\ell$ and\nmaximum value $O(\\log n)$ has greedy routes of expected length $O(\\log^2n /\n\\ell)$, implying that any harmonic ring in this family is asymptotically\noptimal. Furthermore, for a more general family of rings, we show that a fixed\ndegree distribution is optimal. More precisely, if each random contact is\nchosen at distance $r$ with a probability that decreases with $r$, then among\ndegree distributions with mean $\\ell$, greedy routing time is smallest when\nevery node is assigned $\\floor{\\ell}$ or $\\ceiling{\\ell}$ contacts."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5309v1", 
    "other_authors": "Toralf Kirsten, Lars Kolb, Michael Hartung, Anika Gro\u00df, Hanna K\u00f6pcke, Erhard Rahm", 
    "title": "Data Partitioning for Parallel Entity Matching", 
    "arxiv-id": "1006.5309v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-06-28T10:25:53Z", 
    "summary": "Entity matching is an important and difficult step for integrating web data.\nTo reduce the typically high execution time for matching we investigate how we\ncan perform entity matching in parallel on a distributed infrastructure. We\npropose different strategies to partition the input data and generate multiple\nmatch tasks that can be independently executed. One of our strategies supports\nboth, blocking to reduce the search space for matching and parallel matching to\nimprove efficiency. Special attention is given to the number and size of data\npartitions as they impact the overall communication overhead and memory\nrequirements of individual match tasks. We have developed a service-based\ndistributed infrastructure for the parallel execution of match workflows. We\nevaluate our approach in detail for different match strategies for matching\nreal-world product data of different web shops. We also consider caching of\nin-put entities and affinity-based scheduling of match tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5376v1", 
    "other_authors": "Mark Stillwell, David Schanzenbach, Fr\u00e9d\u00e9ric Vivien, Henri Casanova", 
    "title": "Resource Allocation using Virtual Clusters", 
    "arxiv-id": "1006.5376v1", 
    "author": "Henri Casanova", 
    "publish": "2010-06-28T15:28:43Z", 
    "summary": "In this report we demonstrate the potential utility of resource allocation\nmanagement systems that use virtual machine technology for sharing parallel\ncomputing resources among competing jobs. We formalize the resource allocation\nproblem with a number of underlying assumptions, determine its complexity,\npropose several heuristic algorithms to find near-optimal solutions, and\nevaluate these algorithms in simulation. We find that among our algorithms one\nis very efficient and also leads to the best resource allocations. We then\ndescribe how our approach can be made more general by removing several of the\nunderlying assumptions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5572v1", 
    "other_authors": "Hiroaki Inoue", 
    "title": "A Multi-Core Processor Platform for Open Embedded Systems", 
    "arxiv-id": "1006.5572v1", 
    "author": "Hiroaki Inoue", 
    "publish": "2010-06-29T11:35:55Z", 
    "summary": "Recent proliferation of embedded systems has generated a bold new paradigm,\nknown as open embedded systems. While traditional embedded systems provide only\nclosed base applications (natively-installed software) to users, open embedded\nsystems allow the users to freely execute open applications\n(additionally-installed software) in order to meet various user requirements,\nsuch as user personalization and device coordination. Key to the success of\nplatforms required for open embedded systems is the achievement of both the\nscalable extension of base applications and the secure execution of open\napplications. Most existing platforms, however, have focused on either scalable\nor secure execution, limiting their applicability. This dissertation presents a\nnew secure platform using multi-core processors, which achieves both\nscalability and security. Four techniques feature the new platform: (1)\nseamless communication, by which legacy applications designed for a single\nprocessor make it possible to be executed on multiple processors without any\nsoftware modifications; (2) secure processor partitioning with hardware\nsupport, by which Operating Systems (OSs) required for base and open\napplications are securely executed on separate processors; (3) asymmetric\nvirtualization, by which many OSs over the number of processors are securely\nexecuted under secure processor partitioning; and (4) secure dynamic\npartitioning, by which the number of processors allocated to individual OSs\nmakes it possible to be dynamically changed under secure processor\npartitioning."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5643v1", 
    "other_authors": "\u00c1lvaro Reb\u00f3n Portillo, Scott Walker, Graham Kirby, Alan Dearle", 
    "title": "A Reflective Approach to Providing Flexibility in Application   Distribution", 
    "arxiv-id": "1006.5643v1", 
    "author": "Alan Dearle", 
    "publish": "2010-06-29T14:57:44Z", 
    "summary": "Current middleware systems suffer from drawbacks. Often one is forced to make\ndecisions early in the design process about which classes may participate in\ninter-machine communication. Further, application level and middleware specific\nsemantics cannot be separated forcing an unnatural design. The RAFDA project\nproposes to adress these deficiencies by creating an adaptive, reflective\nframework that enables the transformation of non-distributed applications into\nisomorphic applications whose distribution architecture is flexible. This paper\ndescribes the code transformation techniques that have been developed as part\nof the project. The system enables the distribution of a program according to a\nflexible configuration without user intervention. Proxy objects can then be\nsubstituted, permitting cross-address space communication. The distributed\nprogram can adapt to its environment by dynamically altering its distribution\nboundaries."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5661v1", 
    "other_authors": "Joelle Coutaz, Alan Dearle, Sophie Dupuy-Chessa, Graham Kirby, Christophe Lachenal, Ron Morrison, Gaetan Rey, Evangelos Zirintsis", 
    "title": "Working Document on Gloss Ontology", 
    "arxiv-id": "1006.5661v1", 
    "author": "Evangelos Zirintsis", 
    "publish": "2010-06-29T15:59:22Z", 
    "summary": "This document describes the Gloss Ontology. The ontology and associated class\nmodel are organised into several packages. Section 2 describes each package in\ndetail, while Section 3 contains a summary of the whole ontology."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5940v1", 
    "other_authors": "Alan Dearle, Graham Kirby, Andrew McCarthy, Juan-Carlos Diaz y Carballo", 
    "title": "An Information Flow Architecture for Global Smart Spaces", 
    "arxiv-id": "1006.5940v1", 
    "author": "Juan-Carlos Diaz y Carballo", 
    "publish": "2010-06-29T14:32:01Z", 
    "summary": "In this paper we describe an architecture which: Permits the deployment and\nexecution of components in appropriate geographical locations. Provides\nsecurity mechanisms that prevent misuse of the architecture. Supports a\nprogramming model that is familiar to application programmers. Permits\ninstalled components to share data. Permits the deployed components to\ncommunicate via communication channels. Provides evolution mechanisms\npermitting the dynamic rearrangement of inter-connection topologies the\ncomponents that they connect. Supports the specification and deployment of\ndistributed component deployments."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5941v1", 
    "other_authors": "Evangelos Zirintsis, Graham Kirby, Alan Dearle, Ben Allen, Rob MacInnis, Andrew McCarthy, Ron Morrison, Paddy Nixon, Andrew Jamieson, Chris Nicholson, Steven Harris", 
    "title": "Second Set of Spaces", 
    "arxiv-id": "1006.5941v1", 
    "author": "Steven Harris", 
    "publish": "2010-06-29T15:53:53Z", 
    "summary": "This document describes the Gloss infrastructure supporting implementation of\nlocation-aware services. The document is in two parts. The first part describes\nsoftware architecture for the smart space. As described in D8, a local\narchitecture provides a framework for constructing Gloss applications, termed\nassemblies, that run on individual physical nodes, whereas a global\narchitecture defines an overlay network for linking individual assemblies. The\nsecond part outlines the hardware installation for local sensing. This\ndescribes the first phase of the installation in Strathclyde University."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1009.0056v1", 
    "other_authors": "Gokarna Sharma, Costas Busch", 
    "title": "A Competitive Analysis for Balanced Transactional Memory Workloads", 
    "arxiv-id": "1009.0056v1", 
    "author": "Costas Busch", 
    "publish": "2010-08-31T23:54:57Z", 
    "summary": "We consider transactional memory contention management in the context of\nbalanced workloads, where if a transaction is writing, the number of write\noperations it performs is a constant fraction of its total reads and writes. We\nexplore the theoretical performance boundaries of contention management in\nbalanced workloads from the worst-case perspective by presenting and analyzing\ntwo new contention management algorithms. The first algorithm Clairvoyant is\nO(\\surd s)-competitive, where s is the number of shared resources. This\nalgorithm depends on explicitly knowing the conflict graph. The second\nalgorithm Non-Clairvoyant is O(\\surd s \\cdot log n)-competitive, with high\nprobability, which is only a O(log n) factor worse, but does not require\nknowledge of the conflict graph, where n is the number of transactions. Both of\nthese algorithms are greedy. We also prove that the performance of Clairvoyant\nis tight since there is no contention management algorithm that is better than\nO((\\surd s)^(1-\\epsilon))-competitive for any constant \\epsilon > 0, unless\nNP\\subseteq ZPP. To our knowledge, these results are significant improvements\nover the best previously known O(s) competitive ratio bound."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1009.2314v1", 
    "other_authors": "Ajay Prasad, Sandeep Chaurasia, Arjun Singh, Deepak Gour", 
    "title": "Mapping Cloud Computing onto Useful e-Governance", 
    "arxiv-id": "1009.2314v1", 
    "author": "Deepak Gour", 
    "publish": "2010-09-13T07:36:47Z", 
    "summary": "Most of the services viewed in context to grid and cloud computing are mostly\nconfined to services that are available for intellectual purposes. The grid or\ncloud computing are large scale distributed systems. The essence of large scale\ndistribution can only be realized if the services are rendered to common man.\nThe only organization which has exposure to almost every single resident is the\nrespective governments in every country. As the size of population increases so\nthe need for a larger purview arises. The problem of having a large purview can\nbe solved by means of large scale grid for online services. The government\nservices can be rendered through fully customized Service-oriented Clouds. In\nthis paper we are presenting tight similarities between generic government\nfunctioning and the service oriented grid/cloud approach. Also, we will discuss\nthe major issues in establishing services oriented grids for governmental\norganization."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1009.4048v1", 
    "other_authors": "Zeeshan Ahmed", 
    "title": "A Middleware road towards Web (Grid) Services", 
    "arxiv-id": "1009.4048v1", 
    "author": "Zeeshan Ahmed", 
    "publish": "2010-09-21T10:29:50Z", 
    "summary": "Middleware technologies is a very big field, containing a strong already done\nresearch as well as the currently running research to confirm already done\nresearch's results and the to have some new solution by theoretical as well as\nthe experimental (practical) way. This document has been produced by Zeeshan\nAhmed (Student: Connectivity Software Technologies Blekinge Institute of\nTechnologies). This describes the research already done in the field of\nmiddleware technologies including Web Services, Grid Computing, Grid Services\nand Open Grid Service Infrastructure & Architecture. This document concludes\nwith the overview of Web (Grid) Service, Chain of Web (Grid) Services and the\nnecessary security issue."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1009.4870v1", 
    "other_authors": "Tobias Baumgartner, Sandor P. Fekete, Tom Kamphans, Alexander Kroeller, Max Pagel", 
    "title": "Hallway Monitoring: Distributed Data Processing with Wireless Sensor   Networks", 
    "arxiv-id": "1009.4870v1", 
    "author": "Max Pagel", 
    "publish": "2010-09-24T15:42:30Z", 
    "summary": "We present a sensor network testbed that monitors a hallway. It consists of\n120 load sensors and 29 passive infrared sensors (PIRs), connected to 30\nwireless sensor nodes. There are also 29 LEDs and speakers installed, operating\nas actuators, and enabling a direct interaction between the testbed and\npassers-by. Beyond that, the network is heterogeneous, consisting of three\ndifferent circuit boards---each with its specific responsibility. The design of\nthe load sensors is of extremely low cost compared to industrial solutions and\neasily transferred to other settings. The network is used for in-network data\nprocessing algorithms, offering possibilities to develop, for instance,\ndistributed target-tracking algorithms. Special features of our installation\nare highly correlated sensor data and the availability of miscellaneous sensor\ntypes."
},{
    "category": "cs.DC", 
    "doi": "10.5121/jgraphoc.2010.2302", 
    "link": "http://arxiv.org/pdf/1010.0562v1", 
    "other_authors": "Somayeh Abdi, Hossein Pedram, Somayeh Mohamadi", 
    "title": "The Impact of Data Replicatino on Job Scheduling Performance in   Hierarchical data Grid", 
    "arxiv-id": "1010.0562v1", 
    "author": "Somayeh Mohamadi", 
    "publish": "2010-10-04T12:25:04Z", 
    "summary": "In data-intensive applications data transfer is a primary cause of job\nexecution delay. Data access time depends on bandwidth. The major bottleneck to\nsupporting fast data access in Grids is the high latencies of Wide Area\nNetworks and Internet. Effective scheduling can reduce the amount of data\ntransferred across the internet by dispatching a job to where the needed data\nare present. Another solution is to use a data replication mechanism. Objective\nof dynamic replica strategies is reducing file access time which leads to\nreducing job runtime. In this paper we develop a job scheduling policy and a\ndynamic data replication strategy, called HRS (Hierarchical Replication\nStrategy), to improve the data access efficiencies. We study our approach and\nevaluate it through simulation. The results show that our algorithm has\nimproved 12% over the current strategies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/jgraphoc.2010.2302", 
    "link": "http://arxiv.org/pdf/1010.0958v1", 
    "other_authors": "Punit Sharma, Partha Sarathi Mandal", 
    "title": "Reconstruction of Aggregation Tree in spite of Faulty Nodes in Wireless   Sensor Networks", 
    "arxiv-id": "1010.0958v1", 
    "author": "Partha Sarathi Mandal", 
    "publish": "2010-10-05T17:54:58Z", 
    "summary": "Recent advances in wireless sensor networks (WSNs) have led to many new\npromissing applications. However data communication between nodes consumes a\nlarge portion of the total energy of WSNs. Consequently efficient data\naggregation technique can help greatly to reduce power consumption. Data\naggregation has emerged as a basic approach in WSNs in order to reduce the\nnumber of transmissions of sensor nodes over {\\it aggregation tree} and hence\nminimizing the overall power consumption in the network. If a sensor node fails\nduring data aggregation then the aggregation tree is disconnected. Hence the\nWSNs rely on in-network aggregation for efficiency but a single faulty node can\nseverely influence the outcome by contributing an arbitrary partial aggregate\nvalue.\n  In this paper we have presented a distributed algorithm that reconstruct the\naggregation tree from the initial aggregation tree excluding the faulty sensor\nnode. This is a synchronous model that is completed in several rounds. Our\nproposed scheme can handle multiple number of faulty nodes as well."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1015v1", 
    "other_authors": "Keith Wiley, Andrew Connolly, Jeff Gardner, Simon Krughof, Magdalena Balazinska, Bill Howe, YongChul Kwon, YingYi Bu", 
    "title": "Astronomy in the Cloud: Using MapReduce for Image Coaddition", 
    "arxiv-id": "1010.1015v1", 
    "author": "YingYi Bu", 
    "publish": "2010-10-05T20:35:53Z", 
    "summary": "In the coming decade, astronomical surveys of the sky will generate tens of\nterabytes of images and detect hundreds of millions of sources every night. The\nstudy of these sources will involve computation challenges such as anomaly\ndetection and classification, and moving object tracking. Since such studies\nbenefit from the highest quality data, methods such as image coaddition\n(stacking) will be a critical preprocessing step prior to scientific\ninvestigation. With a requirement that these images be analyzed on a nightly\nbasis to identify moving sources or transient objects, these data streams\npresent many computational challenges. Given the quantity of data involved, the\ncomputational load of these problems can only be addressed by distributing the\nworkload over a large number of nodes. However, the high data throughput\ndemanded by these applications may present scalability challenges for certain\nstorage architectures. One scalable data-processing method that has emerged in\nrecent years is MapReduce, and in this paper we focus on its popular\nopen-source implementation called Hadoop. In the Hadoop framework, the data is\npartitioned among storage attached directly to worker nodes, and the processing\nworkload is scheduled in parallel on the nodes that contain the required input\ndata. A further motivation for using Hadoop is that it allows us to exploit\ncloud computing resources, e.g., Amazon's EC2. We report on our experience\nimplementing a scalable image-processing pipeline for the SDSS imaging database\nusing Hadoop. This multi-terabyte imaging dataset provides a good testbed for\nalgorithm development since its scope and structure approximate future surveys.\nFirst, we describe MapReduce and how we adapted image coaddition to the\nMapReduce framework. Then we describe a number of optimizations to our basic\napproach and report experimental results comparing their performance."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1112v1", 
    "other_authors": "Leonid Barenboim, Shlomi Dolev, Rafail Ostrovsky", 
    "title": "Deterministic and Energy-Optimal Wireless Synchronization", 
    "arxiv-id": "1010.1112v1", 
    "author": "Rafail Ostrovsky", 
    "publish": "2010-10-06T10:18:02Z", 
    "summary": "We consider the problem of clock synchronization in a wireless setting where\nprocessors must power-down their radios in order to save energy. Energy\nefficiency is a central goal in wireless networks, especially if energy\nresources are severely limited. In the current setting, the problem is to\nsynchronize clocks of $m$ processors that wake up in arbitrary time points,\nsuch that the maximum difference between wake up times is bounded by a positive\ninteger $n$, where time intervals are appropriately discretized. Currently, the\nbest-known results for synchronization for single-hop networks of $m$\nprocessors is a randomized algorithm due to \\cite{BKO09} of O(\\sqrt {n /m}\n\\cdot poly-log(n)) awake times per processor and a lower bound of\nOmega(\\sqrt{n/m}) of the number of awake times needed per processor\n\\cite{BKO09}. The main open question left in their work is to close the\npoly-log gap between the upper and the lower bound and to de-randomize their\nprobabilistic construction and eliminate error probability. This is exactly\nwhat we do in this paper.\n  That is, we show a {deterministic} algorithm with radio use of Theta(\\sqrt {n\n/m}) that never fails. We stress that our upper bound exactly matches the lower\nbound proven in \\cite{BKO09}, up to a small multiplicative constant. Therefore,\nour algorithm is {optimal} in terms of energy efficiency and completely\nresolves a long sequence of works in this area. In order to achieve these\nresults we devise a novel {adaptive} technique that determines the times when\ndevices power their radios on and off. In addition, we prove several lower\nbounds on the energy efficiency of algorithms for {multi-hop networks}.\nSpecifically, we show that any algorithm for multi-hop networks must have radio\nuse of Omega(\\sqrt n) per processor."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1812v1", 
    "other_authors": "Muhammad Mahbubur Rahman, Afroza Nahar", 
    "title": "Modified Bully Algorithm using Election Commission", 
    "arxiv-id": "1010.1812v1", 
    "author": "Afroza Nahar", 
    "publish": "2010-10-09T06:00:33Z", 
    "summary": "Electing leader is a vital issue not only in distributed computing but also\nin communication network [1, 2, 3, 4, 5], centralized mutual exclusion\nalgorithm [6, 7], centralized control IPC, etc. A leader is required to make\nsynchronization between different processes. And different election algorithms\nare used to elect a coordinator among the available processes in the system\nsuch a way that there will be only one coordinator at any time. Bully election\nalgorithm is one of the classical and well-known approaches in coordinator\nelection process. This paper will present a modified version of bully election\nalgorithm using a new concept called election commission. This approach will\nnot only reduce redundant elections but also minimize total number of elections\nand hence it will minimize message passing, network traffic, and complexity of\nthe existing system."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.2000v1", 
    "other_authors": "Henricus Bouwmeester, Julien Langou", 
    "title": "A Critical Path Approach to Analyzing Parallelism of Algorithmic   Variants. Application to Cholesky Inversion", 
    "arxiv-id": "1010.2000v1", 
    "author": "Julien Langou", 
    "publish": "2010-10-11T05:34:40Z", 
    "summary": "Algorithms come with multiple variants which are obtained by changing the\nmathematical approach from which the algorithm is derived. These variants offer\na wide spectrum of performance when implemented on a multicore platform and we\nseek to understand these differences in performances from a theoretical point\nof view. To that aim, we derive and present the critical path lengths of each\nalgorithmic variant for our application problem which enables us to determine a\nlower bound on the time to solution. This metric provides an intuitive grasp of\nthe performance of a variant and we present numerical experiments to validate\nthe tightness of our lower bounds on practical applications. Our case study is\nthe Cholesky inversion and its use in computing the inverse of a symmetric\npositive definite matrix."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.2454v1", 
    "other_authors": "Leonid Barenboim, Michael Elkin", 
    "title": "Distributed Deterministic Edge Coloring using Bounded Neighborhood   Independence", 
    "arxiv-id": "1010.2454v1", 
    "author": "Michael Elkin", 
    "publish": "2010-10-12T17:51:01Z", 
    "summary": "We study the {edge-coloring} problem in the message-passing model of\ndistributed computing. This is one of the most fundamental and well-studied\nproblems in this area. Currently, the best-known deterministic algorithms for\n(2Delta -1)-edge-coloring requires O(Delta) + log-star n time \\cite{PR01},\nwhere Delta is the maximum degree of the input graph. Also, recent results of\n\\cite{BE10} for vertex-coloring imply that one can get an\nO(Delta)-edge-coloring in O(Delta^{epsilon} \\cdot \\log n) time, and an\nO(Delta^{1 + epsilon})-edge-coloring in O(log Delta log n) time, for an\narbitrarily small constant epsilon > 0.\n  In this paper we devise a drastically faster deterministic edge-coloring\nalgorithm. Specifically, our algorithm computes an O(Delta)-edge-coloring in\nO(Delta^{epsilon}) + log-star n time, and an O(Delta^{1 +\nepsilon})-edge-coloring in O(log Delta) + log-star n time. This result improves\nthe previous state-of-the-art {exponentially} in a wide range of Delta,\nspecifically, for 2^{Omega(\\log-star n)} \\leq Delta \\leq polylog(n). In\naddition, for small values of Delta our deterministic algorithm outperforms all\nthe existing {randomized} algorithms for this problem.\n  On our way to these results we study the {vertex-coloring} problem on the\nfamily of graphs with bounded {neighborhood independence}. This is a large\nfamily, which strictly includes line graphs of r-hypergraphs for any r = O(1),\nand graphs of bounded growth. We devise a very fast deterministic algorithm for\nvertex-coloring graphs with bounded neighborhood independence. This algorithm\ndirectly gives rise to our edge-coloring algorithms, which apply to {general}\ngraphs.\n  Our main technical contribution is a subroutine that computes an\nO(Delta/p)-defective p-vertex coloring of graphs with bounded neighborhood\nindependence in O(p^2) + \\log-star n time, for a parameter p, 1 \\leq p \\leq\nDelta."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.4", 
    "link": "http://arxiv.org/pdf/1010.2824v1", 
    "other_authors": "Rab\u00e9a Ameur-Boulifa, Ludovic Henrio, Eric Madelaine", 
    "title": "Behavioural Models for Group Communications", 
    "arxiv-id": "1010.2824v1", 
    "author": "Eric Madelaine", 
    "publish": "2010-10-14T05:16:17Z", 
    "summary": "Group communication is becoming a more and more popular infrastructure for\nefficient distributed applications. It consists in representing locally a group\nof remote objects as a single object accessed in a single step; communications\nare then broadcasted to all members. This paper provides models for automatic\nverification of group-based applications, typically for detecting deadlocks or\nchecking message ordering. We show how to encode group communication, together\nwith different forms of synchronisation for group results. The proposed models\nare parametric such that, for example, different group sizes or group members\ncould be experimented with the minimum modification of the original model."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.2828v1", 
    "other_authors": "Abdul Malik Khan, Sophie Chabridon, Antoine Beugnard", 
    "title": "A Reusable Component for Communication and Data Synchronization in   Mobile Distributed Interactive Applications", 
    "arxiv-id": "1010.2828v1", 
    "author": "Antoine Beugnard", 
    "publish": "2010-10-14T05:16:43Z", 
    "summary": "In Distributed Interactive Applications (DIA) such as multiplayer games,\nwhere many participants are involved in a same game session and communicate\nthrough a network, they may have an inconsistent view of the virtual world\nbecause of the communication delays across the network. This issue becomes even\nmore challenging when communicating through a cellular network while executing\nthe DIA client on a mobile terminal. Consistency maintenance algorithms may be\nused to obtain a uniform view of the virtual world. These algorithms are very\ncomplex and hard to program and therefore, the implementation and the future\nevolution of the application logic code become difficult. To solve this\nproblem, we propose an approach where the consistency concerns are handled\nseparately by a distributed component called a Synchronization Medium, which is\nresponsible for the communication management as well as the consistency\nmaintenance. We present the detailed architecture of the Synchronization Medium\nand the generic interfaces it offers to DIAs. We evaluate our approach both\nqualitatively and quantitatively. We first demonstrate that the Synchronization\nMedium is a reusable component through the development of two game\napplications, a car racing game and a space war game. A performance evaluation\nthen shows that the overhead introduced by the Synchronization Medium remains\nacceptable."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.2881v1", 
    "other_authors": "Linlin Wu, Rajkumar Buyya", 
    "title": "Service Level Agreement (SLA) in Utility Computing Systems", 
    "arxiv-id": "1010.2881v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2010-10-14T11:33:28Z", 
    "summary": "In recent years, extensive research has been conducted in the area of Service\nLevel Agreement (SLA) for utility computing systems. An SLA is a formal\ncontract used to guarantee that consumers' service quality expectation can be\nachieved. In utility computing systems, the level of customer satisfaction is\ncrucial, making SLAs significantly important in these environments. Fundamental\nissue is the management of SLAs, including SLA autonomy management or trade off\namong multiple Quality of Service (QoS) parameters. Many SLA languages and\nframeworks have been developed as solutions; however, there is no overall\nclassification for these extensive works. Therefore, the aim of this chapter is\nto present a comprehensive survey of how SLAs are created, managed and used in\nutility computing environment. We discuss existing use cases from Grid and\nCloud computing systems to identify the level of SLA realization in\nstate-of-art systems and emerging challenges for future research."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.3053v1", 
    "other_authors": "Lars Kolb, Andreas Thor, Erhard Rahm", 
    "title": "Parallel Sorted Neighborhood Blocking with MapReduce", 
    "arxiv-id": "1010.3053v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-10-15T00:28:44Z", 
    "summary": "Cloud infrastructures enable the efficient parallel execution of\ndata-intensive tasks such as entity resolution on large datasets. We\ninvestigate challenges and possible solutions of using the MapReduce\nprogramming model for parallel entity resolution. In particular, we propose and\nevaluate two MapReduce-based implementations for Sorted Neighborhood blocking\nthat either use multiple MapReduce jobs or apply a tailored data replication."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.3233v1", 
    "other_authors": "Joshua White, Adam Pilbeam", 
    "title": "A Survey of Virtualization Technologies With Performance Testing", 
    "arxiv-id": "1010.3233v1", 
    "author": "Adam Pilbeam", 
    "publish": "2010-10-15T17:51:50Z", 
    "summary": "Virtualization has rapidly become a go-to technology for increasing\nefficiency in the data center. With virtualization technologies providing\ntremendous flexibility, even disparate architectures may be deployed on a\nsingle machine without interference. Awareness of limitations and requirements\nof physical hosts to be used for virtualization is important. This paper\nreviews the present virtualization methods, virtual computing software, and\nprovides a brief analysis of the performance issues inherent to each. In the\nend we present testing results of KVM-QEMU on two current Multi-Core CPU\nArchitectures and System Configurations."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4018v1", 
    "other_authors": "Chadi Kari", 
    "title": "A Paradigm for Channel Assignment and Data Migration in Distributed   Systems", 
    "arxiv-id": "1010.4018v1", 
    "author": "Chadi Kari", 
    "publish": "2010-10-19T19:37:09Z", 
    "summary": "In this manuscript, we consider the problems of channel assignment in\nwireless networks and data migration in heterogeneous storage systems. We show\nthat a soft edge coloring approach to both problems gives rigorous\napproximation guarantees. In the channel assignment problem arising in wireless\nnetworks a pair of edges incident to a vertex are said to be conflicting if the\nchannels assigned to them are the same. Our goal is to assign channels (color\nedges) so that the number of conflicts is minimized. The problem is NP-hard by\na reduction from Edge coloring and we present two combinatorial algorithms for\nthis case. The first algorithm is based on a distributed greedy method and\ngives a solution with at most $2(1-\\frac{1}{k})|E|$ more conflicts than the\noptimal solution.The approximation ratio if the second algorithm is $1 +\n\\frac{|V|}{|E|}$, which gives a ($1 + o(1)$)-factor for dense graphs and is the\nbest possible unless P = NP. We also consider the data migration problem in\nheterogeneous storage systems. In such systems, data layouts may need to be\nreconfigured over time for load balancing or in the event of system\nfailure/upgrades. It is critical to migrate data to their target locations as\nquickly as possible to obtain the best performance of the system. Most of the\nprevious results on data migration assume that each storage node can perform\nonly one data transfer at a time. However, storage devices tend to have\nheterogeneous capabilities as devices may be added over time due to storage\ndemand increase. We develop algorithms to minimize the data migration time. We\nshow that it is possible to find an optimal migration schedule when all $c_v$'s\nare even. Furthermore, though the problem is NP-hard in general, we give an\nefficient soft edge coloring algorithm that offers a rigorous $(1 +\no(1))$-approximation guarantee."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4639v1", 
    "other_authors": "Antonio Wendell De Oliveira Rodrigues, Fr\u00e9d\u00e9ric Guyomarch, Yvonnick Le Menach, Jean-Luc Dekeyser", 
    "title": "Parallel Sparse Matrix Solver on the GPU Applied to Simulation of   Electrical Machines", 
    "arxiv-id": "1010.4639v1", 
    "author": "Jean-Luc Dekeyser", 
    "publish": "2010-10-22T08:46:04Z", 
    "summary": "Nowadays, several industrial applications are being ported to parallel\narchitectures. In fact, these platforms allow acquire more performance for\nsystem modelling and simulation. In the electric machines area, there are many\nproblems which need speed-up on their solution. This paper examines the\nparallelism of sparse matrix solver on the graphics processors. More\nspecifically, we implement the conjugate gradient technique with input matrix\nstored in CSR, and Symmetric CSR and CSC formats. This method is one of the\nmost efficient iterative methods available for solving the finite-element basis\nfunctions of Maxwell's equations. The GPU (Graphics Processing Unit), which is\nused for its implementation, provides mechanisms to parallel the algorithm.\nThus, it increases significantly the computation speed in relation to serial\ncode on CPU based systems."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4952v1", 
    "other_authors": "Mikael Fernandus Simalango, Mun-Young Kang, Sangyoon Oh", 
    "title": "Towards Constraint-based High Performance Cloud System in the Process of   Cloud Computing Adoption in an Organization", 
    "arxiv-id": "1010.4952v1", 
    "author": "Sangyoon Oh", 
    "publish": "2010-10-24T12:08:12Z", 
    "summary": "Cloud computing is penetrating into various domains and environments, from\ntheoretical computer science to economy, from marketing hype to educational\ncurriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently\ndeveloping state of cloud computing leaves several issues to address and also\naffects cloud computing adoption by organizations. In this paper, we explain\nhow the transition into the cloud can occur in an organization and describe the\nmechanism for transforming legacy infrastructure into a virtual\ninfrastructure-based cloud. We describe the state of the art of infrastructural\ncloud, which is essential in the decision making on cloud adoption, and\nhighlight the challenges that can limit the scale and speed of the adoption. We\nthen suggest a strategic framework for designing a high performance cloud\nsystem. This framework is applicable when transformation cloudbased deployment\nmodel collides with some constraints. We give an example of the implementation\nof the framework in a design of a budget-constrained high availability cloud\nsystem."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.5421v1", 
    "other_authors": "Subhash Kak", 
    "title": "On the Mesh Array for Matrix Multiplication", 
    "arxiv-id": "1010.5421v1", 
    "author": "Subhash Kak", 
    "publish": "2010-10-26T15:10:37Z", 
    "summary": "This article presents new properties of the mesh array for matrix\nmultiplication. In contrast to the standard array that requires 3n-2 steps to\ncomplete its computation, the mesh array requires only 2n-1 steps. Symmetries\nof the mesh array computed values are presented which enhance the efficiency of\nthe array for specific applications. In multiplying symmetric matrices, the\nresults are obtained in 3n/2+1 steps. The mesh array is examined for its\napplication as a scrambling system."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.0715v1", 
    "other_authors": "Zach Miller, Dan Bradley, Todd Tannenbaum, Igor Sfiligoi", 
    "title": "Flexible Session Management in a Distributed Environment", 
    "arxiv-id": "1011.0715v1", 
    "author": "Igor Sfiligoi", 
    "publish": "2010-11-02T19:42:04Z", 
    "summary": "Many secure communication libraries used by distributed systems, such as SSL,\nTLS, and Kerberos, fail to make a clear distinction between the authentication,\nsession, and communication layers. In this paper we introduce CEDAR, the secure\ncommunication library used by the Condor High Throughput Computing software,\nand present the advantages to a distributed computing system resulting from\nCEDAR's separation of these layers. Regardless of the authentication method\nused, CEDAR establishes a secure session key, which has the flexibility to be\nused for multiple capabilities. We demonstrate how a layered approach to\nsecurity sessions can avoid round-trips and latency inherent in network\nauthentication. The creation of a distinct session management layer allows for\noptimizations to improve scalability by way of delegating sessions to other\ncomponents in the system. This session delegation creates a chain of trust that\nreduces the overhead of establishing secure connections and enables centralized\nenforcement of system-wide security policies. Additionally, secure channels\nbased upon UDP datagrams are often overlooked by existing libraries; we show\nhow CEDAR's structure accommodates this as well. As an example of the utility\nof this work, we show how the use of delegated security sessions and other\ntechniques inherent in CEDAR's architecture enables US CMS to meet their\nscalability requirements in deploying Condor over large-scale, wide-area grid\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.1173v1", 
    "other_authors": "Christian Walder", 
    "title": "Rank k Cholesky Up/Down-dating on the GPU: gpucholmodV0.2", 
    "arxiv-id": "1011.1173v1", 
    "author": "Christian Walder", 
    "publish": "2010-11-04T14:39:26Z", 
    "summary": "In this note we briefly describe our Cholesky modification algorithm for\nstreaming multiprocessor architectures. Our implementation is available in C++\nwith Matlab binding, using CUDA to utilise the graphics processing unit (GPU).\nLimited speed ups are possible due to the bandwidth bound nature of the\nproblem. Furthermore, a complex dependency pattern must be obeyed, requiring\nmultiple kernels to be launched. Nonetheless, this makes for an interesting\nproblem, and our approach can reduce the computation time by a factor of around\n7 for matrices of size 5000 by 5000 and k=16, in comparison with the LINPACK\nsuite running on a CPU of comparable vintage. Much larger problems can be\nhandled however due to the O(n) scaling in required GPU memory of our method."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2235v3", 
    "other_authors": "Konstantinos I. Tsianos, Michael G. Rabbat", 
    "title": "Multiscale Gossip for Efficient Decentralized Averaging in Wireless   Packet Networks", 
    "arxiv-id": "1011.2235v3", 
    "author": "Michael G. Rabbat", 
    "publish": "2010-11-09T23:50:10Z", 
    "summary": "This paper describes and analyzes a hierarchical gossip algorithm for solving\nthe distributed average consensus problem in wireless sensor networks. The\nnetwork is recursively partitioned into subnetworks. Initially, nodes at the\nfinest scale gossip to compute local averages. Then, using geographic routing\nto enable gossip between nodes that are not directly connected, these local\naverages are progressively fused up the hierarchy until the global average is\ncomputed. We show that the proposed hierarchical scheme with $k$ levels of\nhierarchy is competitive with state-of-the-art randomized gossip algorithms, in\nterms of message complexity, achieving $\\epsilon$-accuracy with high\nprobability after $O\\big(n \\log \\log n \\log \\frac{kn}{\\epsilon} \\big)$\nmessages. Key to our analysis is the way in which the network is recursively\npartitioned. We find that the optimal scaling law is achieved when subnetworks\nat scale $j$ contain $O(n^{(2/3)^j})$ nodes; then the message complexity at any\nindividual scale is $O(n \\log \\frac{kn}{\\epsilon})$, and the total number of\nscales in the hierarchy grows slowly, as $\\Theta(\\log \\log n)$. Another\nimportant consequence of hierarchical construction is that the longest distance\nover which messages are exchanged is $O(n^{1/3})$ hops (at the highest scale),\nand most messages (at lower scales) travel shorter distances. In networks that\nuse link-level acknowledgements, this results in less congestion and resource\nusage by reducing message retransmissions. Simulations illustrate that the\nproposed scheme is more message-efficient than existing state-of-the-art\nrandomized gossip algorithms based on averaging along paths."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2312v1", 
    "other_authors": "Emmanuelle Anceaume, Xavier D\u00e9fago, Maria Potop-Butucaru, Matthieu Roy", 
    "title": "A framework for proving the self-organization of dynamic systems", 
    "arxiv-id": "1011.2312v1", 
    "author": "Matthieu Roy", 
    "publish": "2010-11-10T08:37:53Z", 
    "summary": "This paper aims at providing a rigorous definition of self- organization, one\nof the most desired properties for dynamic systems (e.g., peer-to-peer systems,\nsensor networks, cooperative robotics, or ad-hoc networks). We characterize\ndifferent classes of self-organization through liveness and safety properties\nthat both capture information re- garding the system entropy. We illustrate\nthese classes through study cases. The first ones are two representative P2P\noverlays (CAN and Pas- try) and the others are specific implementations of\n\\Omega (the leader oracle) and one-shot query abstractions for dynamic\nsettings. Our study aims at understanding the limits and respective power of\nexisting self-organized protocols and lays the basis of designing robust\nalgorithm for dynamic systems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2953v1", 
    "other_authors": "Thibault Bernard, Alain Bui, Laurence Pilard, Devan Sohier", 
    "title": "A Distributed Clustering Algorithm for Dynamic Networks", 
    "arxiv-id": "1011.2953v1", 
    "author": "Devan Sohier", 
    "publish": "2010-11-12T15:35:10Z", 
    "summary": "We propose an algorithm that builds and maintains clusters over a network\nsubject to mobility. This algorithm is fully decentralized and makes all the\ndifferent clusters grow concurrently. The algorithm uses circulating tokens\nthat collect data and move according to a random walk traversal scheme. Their\ntask consists in (i) creating a cluster with the nodes it discovers and (ii)\nmanaging the cluster expansion; all decisions affecting the cluster are taken\nonly by a node that owns the token. The size of each cluster is maintained\nhigher than $m$ nodes ($m$ is a parameter of the algorithm). The obtained\nclustering is locally optimal in the sense that, with only a local view of each\nclusters, it computes the largest possible number of clusters (\\emph{ie} the\nsizes of the clusters are as close to $m$ as possible). This algorithm is\ndesigned as a decentralized control algorithm for large scale networks and is\nmobility-adaptive: after a series of topological changes, the algorithm\nconverges to a clustering. This recomputation only affects nodes in clusters in\nwhich topological changes happened, and in adjacent clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3094v1", 
    "other_authors": "Longhua Ma, Tengkai Yuan, Feng Xia, Ming Xu, Jun Yao, Meng Shao", 
    "title": "A High-confidence Cyber-Physical Alarm System: Design and Implementation", 
    "arxiv-id": "1011.3094v1", 
    "author": "Meng Shao", 
    "publish": "2010-11-13T03:23:15Z", 
    "summary": "Most traditional alarm systems cannot address security threats in a\nsatisfactory manner. To alleviate this problem, we developed a high-confidence\ncyber-physical alarm system (CPAS), a new kind of alarm systems. This system\nestablishes the connection of the Internet (i.e. TCP/IP) through GPRS/CDMA/3G.\nIt achieves mutual communication control among terminal equipments, human\nmachine interfaces and users by using the existing mobile communication\nnetwork. The CPAS will enable the transformation in alarm mode from traditional\none-way alarm to two-way alarm. The system has been successfully applied in\npractice. The results show that the CPAS could avoid false alarms and satisfy\nresidents' security needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3170v1", 
    "other_authors": "James Aspnes", 
    "title": "Slightly smaller splitter networks", 
    "arxiv-id": "1011.3170v1", 
    "author": "James Aspnes", 
    "publish": "2010-11-14T00:52:14Z", 
    "summary": "The classic renaming protocol of Moir and Anderson (1995) uses a network of\nTheta(n^2) splitters to assign unique names to n processes with unbounded\ninitial names. We show how to reduce this bound to Theta(n^{3/2}) splitters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3632v2", 
    "other_authors": "Shlomi Dolev, Swan Dubois, Maria Potop-Butucaru, S\u00e9bastien Tixeuil", 
    "title": "Stabilizing data-link over non-FIFO channels with optimal   fault-resilience", 
    "arxiv-id": "1011.3632v2", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-11-16T10:04:34Z", 
    "summary": "Self-stabilizing systems have the ability to converge to a correct behavior\nwhen started in any configuration. Most of the work done so far in the\nself-stabilization area assumed either communication via shared memory or via\nFIFO channels. This paper is the first to lay the bases for the design of\nself-stabilizing message passing algorithms over unreliable non-FIFO channels.\nWe propose a fault-send-deliver optimal stabilizing data-link layer that\nemulates a reliable FIFO communication channel over unreliable capacity bounded\nnon-FIFO channels."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.4135v1", 
    "other_authors": "Yunghsiang Han, Soji Omiwade, Rong Zheng", 
    "title": "Progressive Decoding for Data Availability and Reliability in   Distributed Networked Storage", 
    "arxiv-id": "1011.4135v1", 
    "author": "Rong Zheng", 
    "publish": "2010-11-18T04:47:28Z", 
    "summary": "To harness the ever growing capacity and decreasing cost of storage,\nproviding an abstraction of dependable storage in the presence of crash-stop\nand Byzantine failures is compulsory. We propose a decentralized Reed Solomon\ncoding mechanism with minimum communication overhead. Using a progressive data\nretrieval scheme, a data collector contacts only the necessary number of\nstorage nodes needed to guarantee data integrity. The scheme gracefully adapts\nthe cost of successful data retrieval to the number of storage node failures.\nMoreover, by leveraging the Welch-Berlekamp algorithm, it avoids unnecessary\ncomputations. Compared to the state-of-the-art decoding scheme, the\nimplementation and evaluation results show that our progressive data retrieval\nscheme has up to 35 times better computation performance for low Byzantine node\nrates. Additionally, the communication cost in data retrieval is derived\nanalytically and corroborated by Monte-Carlo simulation results. Our\nimplementation is flexible in that the level of redundancy it provides is\nindependent of the number of data generating nodes, a requirement for\ndistributed storage systems"
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5064v1", 
    "other_authors": "Umesh Bellur, Chetan S Rao, Madhu Kumar SD", 
    "title": "Optimal Placement Algorithms for Virtual Machines", 
    "arxiv-id": "1011.5064v1", 
    "author": "Madhu Kumar SD", 
    "publish": "2010-11-23T11:26:48Z", 
    "summary": "Cloud computing provides a computing platform for the users to meet their\ndemands in an efficient, cost-effective way. Virtualization technologies are\nused in the clouds to aid the efficient usage of hardware. Virtual machines\n(VMs) are utilized to satisfy the user needs and are placed on physical\nmachines (PMs) of the cloud for effective usage of hardware resources and\nelectricity in the cloud. Optimizing the number of PMs used helps in cutting\ndown the power consumption by a substantial amount.\n  In this paper, we present an optimal technique to map virtual machines to\nphysical machines (nodes) such that the number of required nodes is minimized.\nWe provide two approaches based on linear programming and quadratic programming\ntechniques that significantly improve over the existing theoretical bounds and\nefficiently solve the problem of virtual machine (VM) placement in data\ncenters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5470v2", 
    "other_authors": "Fabian Kuhn, Thomas Moscibroda, Roger Wattenhofer", 
    "title": "Local Computation: Lower and Upper Bounds", 
    "arxiv-id": "1011.5470v2", 
    "author": "Roger Wattenhofer", 
    "publish": "2010-11-24T19:56:31Z", 
    "summary": "The question of what can be computed, and how efficiently, are at the core of\ncomputer science. Not surprisingly, in distributed systems and networking\nresearch, an equally fundamental question is what can be computed in a\n\\emph{distributed} fashion. More precisely, if nodes of a network must base\ntheir decision on information in their local neighborhood only, how well can\nthey compute or approximate a global (optimization) problem? In this paper we\ngive the first poly-logarithmic lower bound on such local computation for\n(optimization) problems including minimum vertex cover, minimum (connected)\ndominating set, maximum matching, maximal independent set, and maximal\nmatching. In addition we present a new distributed algorithm for solving\ngeneral covering and packing linear programs. For some problems this algorithm\nis tight with the lower bounds, for others it is a distributed approximation\nscheme. Together, our lower and upper bounds establish the local computability\nand approximability of a large class of problems, characterizing how much local\ninformation is required to solve these tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5568v1", 
    "other_authors": "Eric M. Heien, Derrick Kondo, Anderson David", 
    "title": "Correlated Resource Models of Internet End Hosts", 
    "arxiv-id": "1011.5568v1", 
    "author": "Anderson David", 
    "publish": "2010-11-25T08:41:38Z", 
    "summary": "Understanding and modelling resources of Internet end hosts is essential for\nthe design of desktop software and Internet-distributed applications. In this\npaper we develop a correlated resource model of Internet end hosts based on\nreal trace data taken from the SETI@home project. This data covers a 5-year\nperiod with statistics for 2.7 million hosts. The resource model is based on\nstatistical analysis of host computational power, memory, and storage as well\nas how these resources change over time and the correlations between them. We\nfind that resources with few discrete values (core count, memory) are well\nmodeled by exponential laws governing the change of relative resource\nquantities over time. Resources with a continuous range of values are well\nmodeled with either correlated normal distributions (processor speed for\ninteger operations and floating point operations) or log-normal distributions\n(available disk space). We validate and show the utility of the models by\napplying them to a resource allocation problem for Internet-distributed\napplications, and demonstrate their value over other models. We also make our\ntrace data and tool for automatically generating realistic Internet end hosts\npublicly available."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5808v1", 
    "other_authors": "Nuno Pregui\u00e7a, Carlos Baquero, Paulo S\u00e9rgio Almeida, Victor Fonte, Ricardo Gon\u00e7alves", 
    "title": "Dotted Version Vectors: Logical Clocks for Optimistic Replication", 
    "arxiv-id": "1011.5808v1", 
    "author": "Ricardo Gon\u00e7alves", 
    "publish": "2010-11-26T14:45:53Z", 
    "summary": "In cloud computing environments, a large number of users access data stored\nin highly available storage systems. To provide good performance to\ngeographically disperse users and allow operation even in the presence of\nfailures or network partitions, these systems often rely on optimistic\nreplication solutions that guarantee only eventual consistency. In this\nscenario, it is important to be able to accurately and efficiently identify\nupdates executed concurrently. In this paper, first we review, and expose\nproblems with current approaches to causality tracking in optimistic\nreplication: these either lose information about causality or do not scale, as\nthey require replicas to maintain information that grows linearly with the\nnumber of clients or updates. Then, we propose a novel solution that fully\ncaptures causality while being very concise in that it maintains information\nthat grows linearly only with the number of servers that register updates for a\ngiven data element, bounded by the degree of replication."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.6596v1", 
    "other_authors": "Paulo Jesus, Carlos Baquero, Paulo S\u00e9rgio Almeida", 
    "title": "Dependability in Aggregation by Averaging", 
    "arxiv-id": "1011.6596v1", 
    "author": "Paulo S\u00e9rgio Almeida", 
    "publish": "2010-11-30T16:10:49Z", 
    "summary": "Aggregation is an important building block of modern distributed\napplications, allowing the determination of meaningful properties (e.g. network\nsize, total storage capacity, average load, majorities, etc.) that are used to\ndirect the execution of the system. However, the majority of the existing\naggregation algorithms exhibit relevant dependability issues, when prospecting\ntheir use in real application environments. In this paper, we reveal some\ndependability issues of aggregation algorithms based on iterative averaging\ntechniques, giving some directions to solve them. This class of algorithms is\nconsidered robust (when compared to common tree-based approaches), being\nindependent from the used routing topology and providing an aggregation result\nat all nodes. However, their robustness is strongly challenged and their\ncorrectness often compromised, when changing the assumptions of their working\nenvironment to more realistic ones. The correctness of this class of algorithms\nrelies on the maintenance of a fundamental invariant, commonly designated as\n\"mass conservation\". We will argue that this main invariant is often broken in\npractical settings, and that additional mechanisms and modifications are\nrequired to maintain it, incurring in some degradation of the algorithms\nperformance. In particular, we discuss the behavior of three representative\nalgorithms Push-Sum Protocol, Push-Pull Gossip protocol and Distributed Random\nGrouping under asynchronous and faulty (with message loss and node crashes)\nenvironments. More specifically, we propose and evaluate two new versions of\nthe Push-Pull Gossip protocol, which solve its message interleaving problem\n(evidenced even in a synchronous operation mode)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.0093v1", 
    "other_authors": "Markus Wittmann, Georg Hager", 
    "title": "Optimizing ccNUMA locality for task-parallel execution under OpenMP and   TBB on multicore-based systems", 
    "arxiv-id": "1101.0093v1", 
    "author": "Georg Hager", 
    "publish": "2010-12-30T14:55:02Z", 
    "summary": "Task parallelism as employed by the OpenMP task construct or some Intel\nThreading Building Blocks (TBB) components, although ideal for tackling\nirregular problems or typical producer/consumer schemes, bears some potential\nfor performance bottlenecks if locality of data access is important, which is\ntypically the case for memory-bound code on ccNUMA systems. We present a thin\nsoftware layer ameliorates adverse effects of dynamic task distribution by\nsorting tasks into locality queues, each of which is preferably processed by\nthreads that belong to the same locality domain. Dynamic scheduling is fully\npreserved inside each domain, and is preferred over possible load imbalance\neven if nonlocal access is required, making this strategy well-suited for\ntypical multicore-mutisocket systems. The effectiveness of the approach is\ndemonstrated by using a blocked six-point stencil solver as a toy model."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.0357v1", 
    "other_authors": "R. J. Sobie, A. Agarwal, M. Anderson, P. Armstrong, K. Fransham, I. Gable, D. Harris, C. Leavett-Brown, M. Paterson, D. Penfold-Brown, M. Vliet, A. Charbonneau, R. Impey, W. Podaima", 
    "title": "Data Intensive High Energy Physics Analysis in a Distributed Cloud", 
    "arxiv-id": "1101.0357v1", 
    "author": "W. Podaima", 
    "publish": "2011-01-01T17:44:04Z", 
    "summary": "We show that distributed Infrastructure-as-a-Service (IaaS) compute clouds\ncan be effectively used for the analysis of high energy physics data. We have\ndesigned a distributed cloud system that works with any application using large\ninput data sets requiring a high throughput computing environment. The system\nuses IaaS-enabled science and commercial clusters in Canada and the United\nStates. We describe the process in which a user prepares an analysis virtual\nmachine (VM) and submits batch jobs to a central scheduler. The system boots\nthe user-specific VM on one of the IaaS clouds, runs the jobs and returns the\noutput to the user. The user application accesses a central database for\ncalibration data during the execution of the application. Similarly, the data\nis located in a central location and streamed by the running application. The\nsystem can easily run one hundred simultaneous jobs in an efficient manner and\nshould scale to many hundreds and possibly thousands of user jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.0664v1", 
    "other_authors": "G. A. Tarnavsky, E. V. Vorozhtsov", 
    "title": "Computer Simulation Center in Internet", 
    "arxiv-id": "1101.0664v1", 
    "author": "E. V. Vorozhtsov", 
    "publish": "2011-01-04T06:58:07Z", 
    "summary": "The general description of infrastructure and content of SciShop.ru computer\nsimulation center is given. This resource is a new form of knowledge generation\nand remote education using modern Cloud Computing technologies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.1680v1", 
    "other_authors": "Ted Herman", 
    "title": "Safe Register Token Transfer in a Ring", 
    "arxiv-id": "1101.1680v1", 
    "author": "Ted Herman", 
    "publish": "2011-01-09T22:24:44Z", 
    "summary": "A token ring is an arrangement of N processors that take turns engaging in an\nactivity which must be controlled. A token confers the right to engage in the\ncontrolled activity. Processors communicate with neighbors in the ring to\nobtain and release a token. The communication mechanism investigated in this\npaper is the safe register abstraction, which may arbitrarily corrupt a value\nthat a processor reads when the operation reading a register is concurrent with\nan write operation on that register by a neighboring processor. The main\nresults are simple protocols for quasi-atomic communication, constructed from\nsafe registers. A quasi-atomic register behaves atomically except that a\nspecial undefined value may be returned in the case of concurrent read and\nwrite operations. Under certain conditions that constrain the number of writes\nand registers, quasi-atomic protocols are adequate substitutes for atomic\nprotocols. The paper demonstrates how quasi-atomic protocols can be used to\nimplement a self-stabilizing token ring, either by using two safe registers\nbetween neighboring processors or by using O(lg N) safe registers between\nneighbors, which lowers read complexity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.1846v1", 
    "other_authors": "S. Hissoiny, P. Despr\u00e9s, B. Ozell", 
    "title": "Using graphics processing units to generate random numbers", 
    "arxiv-id": "1101.1846v1", 
    "author": "B. Ozell", 
    "publish": "2011-01-10T15:27:48Z", 
    "summary": "The future of high-performance computing is aligning itself towards the\nefficient use of highly parallel computing environments. One application where\nthe use of massive parallelism comes instinctively is Monte Carlo simulations,\nwhere a large number of independent events have to be simulated. At the core of\nthe Monte Carlo simulation lies the Random Number Generator (RNG). In this\npaper, the massively parallel implementation of a collection of pseudo-random\nnumber generators on a graphics processing unit (GPU) is presented. The results\nof the GPU implementation, in terms of samples/s, effective bandwidth and\noperations per second, are presented. A comparison with other implementations\non different hardware platforms, in terms of samples/s, power efficiency and\ncost-benefit, is also presented. Random numbers generation throughput of up to\n~18MSamples/s have been achieved on the graphics hardware used. Efficient\nhardware utilization, in terms of operations per second, has reached ~98% of\nthe possible integer operation throughput."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.1902v1", 
    "other_authors": "Michael T. Goodrich, Nodari Sitchinava, Qin Zhang", 
    "title": "Sorting, Searching, and Simulation in the MapReduce Framework", 
    "arxiv-id": "1101.1902v1", 
    "author": "Qin Zhang", 
    "publish": "2011-01-10T17:46:40Z", 
    "summary": "In this paper, we study the MapReduce framework from an algorithmic\nstandpoint and demonstrate the usefulness of our approach by designing and\nanalyzing efficient MapReduce algorithms for fundamental sorting, searching,\nand simulation problems. This study is motivated by a goal of ultimately\nputting the MapReduce framework on an equal theoretical footing with the\nwell-known PRAM and BSP parallel models, which would benefit both the theory\nand practice of MapReduce algorithms. We describe efficient MapReduce\nalgorithms for sorting, multi-searching, and simulations of parallel algorithms\nspecified in the BSP and CRCW PRAM models. We also provide some applications of\nthese results to problems in parallel computational geometry for the MapReduce\nframework, which result in efficient MapReduce algorithms for sorting, 2- and\n3-dimensional convex hulls, and fixed-dimensional linear programming. For the\ncase when mappers and reducers have a memory/message-I/O size of\n$M=\\Theta(N^\\epsilon)$, for a small constant $\\epsilon>0$, all of our MapReduce\nalgorithms for these applications run in a constant number of rounds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.1932v1", 
    "other_authors": "Washington Taylor, Jud Leonard, Lawrence C. Stewart", 
    "title": "Efficient tilings of de Bruijn and Kautz graphs", 
    "arxiv-id": "1101.1932v1", 
    "author": "Lawrence C. Stewart", 
    "publish": "2011-01-10T19:34:56Z", 
    "summary": "Kautz and de Bruijn graphs have a high degree of connectivity which makes\nthem ideal candidates for massively parallel computer network topologies. In\norder to realize a practical computer architecture based on these graphs, it is\nuseful to have a means of constructing a large-scale system from smaller,\nsimpler modules. In this paper we consider the mathematical problem of\nuniformly tiling a de Bruijn or Kautz graph. This can be viewed as a\ngeneralization of the graph bisection problem. We focus on the problem of graph\ntilings by a set of identical subgraphs. Tiles should contain a maximal number\nof internal edges so as to minimize the number of edges connecting distinct\ntiles. We find necessary and sufficient conditions for the construction of\ntilings. We derive a simple lower bound on the number of edges which must leave\neach tile, and construct a class of tilings whose number of edges leaving each\ntile agrees asymptotically in form with the lower bound to within a constant\nfactor. These tilings make possible the construction of large-scale computing\nsystems based on de Bruijn and Kautz graph topologies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1101.2573v1", 
    "other_authors": "Sanjay Bansal, Nirved Pandey", 
    "title": "An Overview of Portable Distributed Techniques", 
    "arxiv-id": "1101.2573v1", 
    "author": "Nirved Pandey", 
    "publish": "2011-01-13T14:50:33Z", 
    "summary": "In this paper, we reviewed of several portable parallel programming paradigms\nfor use in a distributed programming environment. The Techniques reviewed here\nare portable. These are mainly distributing computing using MPI pure java\nbased, MPI native java based (JNI) and PVM. We will discuss architecture and\nutilities of each technique based on our literature review. We explored these\nportable distributed techniques in four important characteristics scalability,\nfault tolerance, load balancing and performance. We have identified the various\nfactors and issues for improving these four important characteristics."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4116v3", 
    "other_authors": "Riccardo Murri, Peter Z. Kunszt, Sergio Maffioletti, Valery Tschopp", 
    "title": "GridCertLib: a Single Sign-on Solution for Grid Web Applications and   Portals", 
    "arxiv-id": "1101.4116v3", 
    "author": "Valery Tschopp", 
    "publish": "2011-01-21T11:56:07Z", 
    "summary": "This paper describes the design and implementation of GridCertLib, a Java\nlibrary leveraging a Shibboleth-based authentication infrastructure and the\nSLCS online certificate signing service, to provide short-lived X.509\ncertificates and Grid proxies. The main use case envisioned for GridCertLib, is\nto provide seamless and secure access to Grid/X.509 certificates and proxies in\nweb applications and portals: when a user logs in to the portal using\nShibboleth authentication, GridCertLib can automatically obtain a Grid/X.509\ncertificate from the SLCS service and generate a VOMS proxy from it. We give an\noverview of the architecture of GridCertLib and briefly describe its\nprogramming model. Its application to some deployment scenarios is outlined, as\nwell as a report on practical experience integrating GridCertLib into portals\nfor Bioinformatics and Computational Chemistry applications, based on the\npopular P-GRADE and Django softwares."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4193v2", 
    "other_authors": "Franck Butelle, Camille Coti", 
    "title": "A Model for Coherent Distributed Memory For Race Condition Detection", 
    "arxiv-id": "1101.4193v2", 
    "author": "Camille Coti", 
    "publish": "2011-01-21T17:47:24Z", 
    "summary": "We present a new model for distributed shared memory systems, based on remote\ndata accesses. Such features are offered by network interface cards that allow\none-sided operations, remote direct memory access and OS bypass. This model\nleads to new interpretations of distributed algorithms allowing us to propose\nan innovative detection technique of race conditions only based on logical\nclocks. Indeed, the presence of (data) races in a parallel program makes it\nhard to reason about and is usually considered as a bug."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4474v1", 
    "other_authors": "Cristina Serban, Carmen Maftei", 
    "title": "Thermal Analysis of Climate Regions using Remote Sensing and Grid   Computing", 
    "arxiv-id": "1101.4474v1", 
    "author": "Carmen Maftei", 
    "publish": "2011-01-24T09:32:03Z", 
    "summary": "The analysis of climate regions is very important for designers and\narchitects, because the increase in density and built up spaces and reduction\nin open spaces and green lands induce the increase of heat, especially in an\nurban area, deteriorating the environment and causing health problems. This\nstudy analyzes the Land Surface Temperature (LST) differences in the region of\nDobrogea, Romania, and compares with the land use and land cover types using TM\nand ETM+ data of 1989 and 2000. As the analysis is performed on large data\nsets, we used Grid Computing to implement a service for using on Computational\nGrids with a Web-based client interface, which will be greatly useful and\nconvenient for those who are studying the ground thermal environment and heat\nisland effects by using Landsat TM/ETM+ bands, and have typical workstations,\nwith no special computing and storing resources for computationally intensive\nsatellite image processing and no license for a commercial image processing\ntool. Based on the satellite imagery, the paper also addresses a Supervised\nClassification algorithm and the computation of two indices of great value in\nwater resources management, Normalized Difference Vegetation Index (NDVI),\nrespectively Land Surface Emissivity (LSE)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.0516v1", 
    "other_authors": "Sanjay Patel, Madhuri Bhavsar", 
    "title": "QOS based user driven scheduler for grid environment", 
    "arxiv-id": "1102.0516v1", 
    "author": "Madhuri Bhavsar", 
    "publish": "2011-02-02T17:36:58Z", 
    "summary": "As grids are in essence heterogeneous, dynamic, shared and distributed\nenvironments, managing these kinds of platforms efficiently is extremely\ncomplex. A promising scalable approach to deal with these intricacies is the\ndesign of self-managing of autonomic applications. Autonomic applications adapt\ntheir execution accordingly by considering knowledge about their own behaviour\nand environmental conditions.QoS based User Driven scheduling for grid that\nprovides the self-optimizing ability in autonomic applications. Computational\ngrids to provide a user to solve large scale problem by spreading a single\nlarge computation across multiple machines of physical location. QoS based User\nDriven scheduler for grid also provides reliability of the grid systems and\nincrease the performance of the grid to reducing the execution time of job by\napplying scheduling policies defined by the user. The main aim of this paper is\nto distribute the computational load among the available grid nodes and to\ndeveloped a QoS based scheduling algorithm for grid and making grid more\nreliable.Grid computing system is different from conventional distributed\ncomputing systems by its focus on large scale resource sharing, where\nprocessors and communication have significant inuence on Grid computing\nreliability. Reliability capabilities initiated by end users from within\napplications they submit to the grid for execution. Reliability of\ninfrastructure and management services that perform essential functions\nnecessary for grid systems to operate, such as resource allocation and\nscheduling."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.1754v1", 
    "other_authors": "S. Rohini, K. Indumathi", 
    "title": "Probability Based Adaptive Invoked Clustering Algorithm in MANETs", 
    "arxiv-id": "1102.1754v1", 
    "author": "K. Indumathi", 
    "publish": "2011-02-09T00:25:31Z", 
    "summary": "A mobile ad hoc network (MANET), is a self-configuring network of mobile\ndevices connected by wireless links. In order to achieve stable clusters, the\ncluster-heads maintaining the cluster should be stable with minimum overhead of\ncluster re-elections. In this paper we propose a Probability Based Adaptive\nInvoked Weighted Clustering Algorithm (PAIWCA) which can enhance the stability\nof the clusters by taking battery power of the nodes into considerations for\nthe clustering formation and electing stable cluster-heads using cluster head\nprobability of a node. In this simulation study a comparison was conducted to\nmeasure the performance of our algorithm with maximal weighted independent set\n(MWIS) in terms of the number of clusters formed, the connectivity of the\nnetwork, dominant set updates,throughput of the overall network and packet\ndelivery ratio. The result shows that our algorithm performs better than\nexisting one and is also tunable to different kinds of network conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2131v2", 
    "other_authors": "Usha Batra, Deepak Dahiya, Sachin Bhardwaj", 
    "title": "Analytical Study of Object Components for Distributed and Ubiquitous   Computing Environment", 
    "arxiv-id": "1102.2131v2", 
    "author": "Sachin Bhardwaj", 
    "publish": "2011-02-10T14:39:49Z", 
    "summary": "The Distributed object computing is a paradigm that allows objects to be\ndistributed across a heterogeneous network, and allows each of the components\nto interoperate as a unified whole. A new generation of distributed\napplications, such as telemedicine and e-commerce applications, are being\ndeployed in heterogeneous and ubiquitous computing environments. The objective\nof this paper is to explore an applicability of a component based services in\nubiquitous computational environment. While the fundamental structure of\nvarious distributed object components is similar, there are differences that\ncan profoundly impact an application developer or the administrator of a\ndistributed simulation exercise and to implement in Ubiquitous Computing\nEnvironment."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2608v1", 
    "other_authors": "Lskrao Chimakurthi, Madhu Kumar S D", 
    "title": "Power Efficient Resource Allocation for Clouds Using Ant Colony   Framework", 
    "arxiv-id": "1102.2608v1", 
    "author": "Madhu Kumar S D", 
    "publish": "2011-02-13T15:56:29Z", 
    "summary": "Cloud computing is one of the rapidly improving technologies. It provides\nscalable resources needed for the ap- plications hosted on it. As cloud-based\nservices become more dynamic, resource provisioning becomes more challenging.\nThe QoS constrained resource allocation problem is considered in this paper, in\nwhich customers are willing to host their applications on the provider's cloud\nwith a given SLA requirements for performance such as throughput and response\ntime. Since, the data centers hosting the applications consume huge amounts of\nenergy and cause huge operational costs, solutions that reduce energy\nconsumption as well as operational costs are gaining importance. In this work,\nwe propose an energy efficient mechanism that allocates the cloud resources to\nthe applications without violating the given service level agreements(SLA)\nusing Ant colony framework."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2616v1", 
    "other_authors": "Sanjay Bansal, Sanjeev Sharma", 
    "title": "An Improved Multiple Faults Reassignment based Recovery in Cluster   Computing", 
    "arxiv-id": "1102.2616v1", 
    "author": "Sanjeev Sharma", 
    "publish": "2011-02-13T16:50:30Z", 
    "summary": "In case of multiple node failures performance becomes very low as compare to\nsingle node failure. Failures of nodes in cluster computing can be tolerated by\nmultiple fault tolerant computing. Existing recovery schemes are efficient for\nsingle fault but not with multiple faults. Recovery scheme proposed in this\npaper having two phases; sequentially phase, concurrent phase. In sequentially\nphase, loads of all working nodes are uniformly and evenly distributed by\nproposed dynamic rank based and load distribution algorithm. In concurrent\nphase, loads of all failure nodes as well as new job arrival are assigned\nequally to all available nodes by just finding the least loaded node among the\nseveral nodes by failure nodes job allocation algorithm. Sequential and\nconcurrent executions of algorithms improve the performance as well better\nresource utilization. Dynamic rank based algorithm for load redistribution\nworks as a sequential restoration algorithm and reassignment algorithm for\ndistribution of failure nodes to least loaded computing nodes works as a\nconcurrent recovery reassignment algorithm. Since load is evenly and uniformly\ndistributed among all available working nodes with less number of iterations,\nlow iterative time and communication overheads hence performance is improved.\nDynamic ranking algorithm is low overhead, high convergence algorithm for\nreassignment of tasks uniformly among all available nodes. Reassignments of\nfailure nodes are done by a low overhead efficient failure job allocation\nalgorithm. Test results to show effectiveness of the proposed scheme are\npresented."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/5/052019", 
    "link": "http://arxiv.org/pdf/1102.3114v1", 
    "other_authors": "Samuel C Skipsey, Wahid Bhimji, Mike Kenyon", 
    "title": "Establishing Applicability of SSDs to LHC Tier-2 Hardware Configuration", 
    "arxiv-id": "1102.3114v1", 
    "author": "Mike Kenyon", 
    "publish": "2011-02-15T16:05:04Z", 
    "summary": "Solid State Disk technologies are increasingly replacing high-speed hard\ndisks as the storage technology in high-random-I/O environments. There are\nseveral potentially I/O bound services within the typical LHC Tier-2 - in the\nback-end, with the trend towards many-core architectures continuing, worker\nnodes running many single-threaded jobs and storage nodes delivering many\nsimultaneous files can both exhibit I/O limited efficiency. We estimate the\neffectiveness of affordable SSDs in the context of worker nodes, on a large\nTier-2 production setup using both low level tools and real LHC I/O intensive\ndata analysis jobs comparing and contrasting with high performance spinning\ndisk based solutions. We consider the applicability of each solution in the\ncontext of its price/performance metrics, with an eye on the pragmatic issues\nfacing Tier-2 provision and upgrades"
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/5/052019", 
    "link": "http://arxiv.org/pdf/1102.3563v1", 
    "other_authors": "Alexander Semenov, Oleg Zaikin, Dmitry Bespalov, Mikhail Posypkin", 
    "title": "Parallel algorithms for SAT in application to inversion problems of some   discrete functions", 
    "arxiv-id": "1102.3563v1", 
    "author": "Mikhail Posypkin", 
    "publish": "2011-02-17T11:28:21Z", 
    "summary": "In this article we consider the inversion problem for polynomially computable\ndiscrete functions. These functions describe behavior of many discrete systems\nand are used in model checking, hardware verification, cryptanalysis, computer\nbiology and other domains. Quite often it is necessary to invert these\nfunctions, i.e. to find an unknown preimage if an image and algorithm of\nfunction computation are given. In general case this problem is computationally\nintractable. However, many of it's special cases are very important in\npractical applications. Thus development of algorithms that are applicable to\nthese special cases is of importance. The practical applicability of such\nalgorithms can be validated by their ability to solve the problems that are\nconsidered to be computationally hard (for example cryptanalysis problems). In\nthis article we propose the technology of solving the inversion problem for\npolynomially computable discrete functions. This technology was implemented in\ndistributed computing environments (parallel clusters and Grid-systems). It is\nbased on reducing the inversion problem for the considered function to some SAT\nproblem. We describe a general approach to coarse-grained parallelization for\nobtained SAT problems. Efficiency of each parallelization scheme is determined\nby the means of a special predictive function. The proposed technology was\nvalidated by successful solving of cryptanalysis problems for some keystream\ngenerators. The main practical result of this work is a complete cryptanalysis\nof keystream generator A5/1 which was performed in a Grid system specially\nbuilt for this task."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.4423v1", 
    "other_authors": "Martin Biely, Peter Robinson, Ulrich Schmid", 
    "title": "Solving k-Set Agreement with Stable Skeleton Graphs", 
    "arxiv-id": "1102.4423v1", 
    "author": "Ulrich Schmid", 
    "publish": "2011-02-22T07:41:38Z", 
    "summary": "In this paper we consider the k-set agreement problem in distributed\nmessage-passing systems using a round-based approach: Both synchrony of\ncommunication and failures are captured just by means of the messages that\narrive within a round, resulting in round-by-round communication graphs that\ncan be characterized by simple communication predicates. We introduce the weak\ncommunication predicate PSources(k) and show that it is tight for k-set\nagreement, in the following sense: We (i) prove that there is no algorithm for\nsolving (k-1)-set agreement in systems characterized by PSources(k), and (ii)\npresent a novel distributed algorithm that achieves k-set agreement in runs\nwhere PSources(k) holds. Our algorithm uses local approximations of the stable\nskeleton graph, which reflects the underlying perpetual synchrony of a run. We\nprove that this approximation is correct in all runs, regardless of the\ncommunication predicate, and show that graph-theoretic properties of the stable\nskeleton graph can be used to solve k-set agreement if PSources(k) holds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.4946v1", 
    "other_authors": "Armando Casta\u00f1eda, Maurice Herlihy, Sergio Rajsbaum", 
    "title": "An Equivariance Theorem with Applications to Renaming (Preliminary   Version)", 
    "arxiv-id": "1102.4946v1", 
    "author": "Sergio Rajsbaum", 
    "publish": "2011-02-24T10:24:43Z", 
    "summary": "In the renaming problem, each process in a distributed system is issued a\nunique name from a large name space, and the processes must coordinate with one\nanother to choose unique names from a much smaller name space. We show that\nlower bounds on the solvability of renaming in an asynchronous distributed\nsystem can be formulated as a purely topological question about the existence\nof an equivariant chain map from a topological disk to a topological annulus.\nProving the non-existence of such a map implies the non-existence of a\ndistributed renaming algorithm in several related models of computation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.5328v1", 
    "other_authors": "Emmanuel Agullo, Jack Dongarra, Rajib Nath, Stanimire Tomov", 
    "title": "Fully Empirical Autotuned QR Factorization For Multicore Architectures", 
    "arxiv-id": "1102.5328v1", 
    "author": "Stanimire Tomov", 
    "publish": "2011-02-25T20:21:32Z", 
    "summary": "Tuning numerical libraries has become more difficult over time, as systems\nget more sophisticated. In particular, modern multicore machines make the\nbehaviour of algorithms hard to forecast and model. In this paper, we tackle\nthe issue of tuning a dense QR factorization on multicore architectures. We\nshow that it is hard to rely on a model, which motivates us to design a fully\nempirical approach. We exhibit few strong empirical properties that enable us\nto efficiently prune the search space. Our method is automatic, fast and\nreliable. The tuning process is indeed fully performed at install time in less\nthan one and ten minutes on five out of seven platforms. We achieve an average\nperformance varying from 97% to 100% of the optimum performance depending on\nthe platform. This work is a basis for autotuning the PLASMA library and\nenabling easy performance portability across hardware systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.0759v1", 
    "other_authors": "Fangfei Zhou, Manish Goel, Peter Desnoyers, Ravi Sundaram", 
    "title": "Scheduler Vulnerabilities and Attacks in Cloud Computing", 
    "arxiv-id": "1103.0759v1", 
    "author": "Ravi Sundaram", 
    "publish": "2011-03-03T19:09:47Z", 
    "summary": "In hardware virtualization a hypervisor provides multiple Virtual Machines\n(VMs) on a single physical system, each executing a separate operating system\ninstance. The hypervisor schedules execution of these VMs much as the scheduler\nin an operating system does, balancing factors such as fairness and I/O\nperformance. As in an operating system, the scheduler may be vulnerable to\nmalicious behavior on the part of users seeking to deny service to others or\nmaximize their own resource usage.\n  Recently, publically available cloud computing services such as Amazon EC2\nhave used virtualization to provide customers with virtual machines running on\nthe provider's hardware, typically charging by wall clock time rather than\nresources consumed. Under this business model, manipulation of the scheduler\nmay allow theft of service at the expense of other customers, rather than\nmerely reallocating resources within the same administrative domain.\n  We describe a flaw in the Xen scheduler allowing virtual machines to consume\nalmost all CPU time, in preference to other users, and demonstrate kernel-based\nand user-space versions of the attack. We show results demonstrating the\nvulnerability in the lab, consuming as much as 98% of CPU time regardless of\nfair share, as well as on Amazon EC2, where Xen modifications protect other\nusers but still allow theft of service. In case of EC2, following the\nresponsible disclosure model, we have reported this vulnerability to Amazon;\nthey have since implemented a fix that we have tested and verified (See\nAppendix B). We provide a novel analysis of the necessary conditions for such\nattacks, and describe scheduler modifications to eliminate the vulnerability.\n  We present experimental results demonstrating the effectiveness of these\ndefenses while imposing negligible overhead."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.1207v1", 
    "other_authors": "Ms. Deepti Sharma, Ms. Archana B. Saxena", 
    "title": "Framework to Solve Load Balancing Problem in Heterogeneous Web Servers", 
    "arxiv-id": "1103.1207v1", 
    "author": "Ms. Archana B. Saxena", 
    "publish": "2011-03-07T07:40:22Z", 
    "summary": "For popular websites most important concern is to handle incoming load\ndynamically among web servers, so that they can respond to their client without\nany wait or failure. Different websites use different strategies to distribute\nload among web servers but most of the schemes concentrate on only one factor\nthat is number of requests, but none of the schemes consider the point that\ndifferent type of requests will require different level of processing efforts\nto answer, status record of all the web servers that are associated with one\ndomain name and mechanism to handle a situation when one of the servers is not\nworking. Therefore, there is a fundamental need to develop strategy for dynamic\nload allocation on web side. In this paper, an effort has been made to\nintroduce a cluster based frame work to solve load distribution problem. This\nframework aims to distribute load among clusters on the basis of their\noperational capabilities. Moreover, the experimental results are shown with the\nhelp of example, algorithm and analysis of the algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.1302v9", 
    "other_authors": "Petr Kuznetsov, Srivatsan Ravi", 
    "title": "On the Cost of Concurrency in Transactional Memory", 
    "arxiv-id": "1103.1302v9", 
    "author": "Srivatsan Ravi", 
    "publish": "2011-03-07T15:37:44Z", 
    "summary": "The crux of software transactional memory (STM) is to combine an easy-to-use\nprogramming interface with an efficient utilization of the concurrent-computing\nabilities provided by modern machines. But does this combination come with an\ninherent cost? We evaluate the cost of concurrency by measuring the amount of\nexpensive synchronization that must be employed in an STM implementation that\nensures positive concurrency, i.e., allows for concurrent transaction\nprocessing in some executions. We focus on two popular progress conditions that\nprovide positive concurrency: progressiveness and permissiveness. We show that\nin permissive STMs, providing a very high degree of concurrency, a transaction\nperforms a linear number of expensive synchronization patterns with respect to\nits read-set size. In contrast, progressive STMs provide a very small degree of\nconcurrency but, as we demonstrate, can be implemented using at most one\nexpensive synchronization pattern per transaction. However, we show that even\nin progressive STMs, a transaction has to \"protect\" (e.g., by using locks or\nstrong synchronization primitives) a linear amount of data with respect to its\nwrite-set size. Our results suggest that looking for high degrees of\nconcurrency in STM implementations may bring a considerable synchronization\ncost."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.2590v1", 
    "other_authors": "Yi Wei, Karthik Sukumar, Christian Vecchiola, Dileban Karunamoorthy, Rajkumar Buyya", 
    "title": "Aneka Cloud Application Platform and Its Integration with Windows Azure", 
    "arxiv-id": "1103.2590v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-03-14T06:38:11Z", 
    "summary": "Aneka is an Application Platform-as-a-Service (Aneka PaaS) for Cloud\nComputing. It acts as a framework for building customized applications and\ndeploying them on either public or private Clouds. One of the key features of\nAneka is its support for provisioning resources on different public Cloud\nproviders such as Amazon EC2, Windows Azure and GoGrid. In this chapter, we\nwill present Aneka platform and its integration with one of the public Cloud\ninfrastructures, Windows Azure, which enables the usage of Windows Azure\nCompute Service as a resource provider of Aneka PaaS. The integration of the\ntwo platforms will allow users to leverage the power of Windows Azure Platform\nfor Aneka Cloud Computing, employing a large number of compute instances to run\ntheir applications in parallel. Furthermore, customers of the Windows Azure\nplatform can benefit from the integration with Aneka PaaS by embracing the\nadvanced features of Aneka in terms of multiple programming models, scheduling\nand management services, application execution services, accounting and pricing\nservices and dynamic provisioning services. Finally, in addition to the Windows\nAzure Platform we will illustrate in this chapter the integration of Aneka PaaS\nwith other public Cloud platforms such as Amazon EC2 and GoGrid, and virtual\nmachine management platforms such as Xen Server. The new support of\nprovisioning resources on Windows Azure once again proves the adaptability,\nextensibility and flexibility of Aneka."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.3515v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "Self-Stabilization, Byzantine Containment, and Maximizable Metrics:   Necessary Conditions", 
    "arxiv-id": "1103.3515v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-03-17T20:37:17Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties leads to some impossibility results. In this paper, we provide two\nnecessary conditions to construct maximum metric tree in presence of transients\nand (permanent) Byzantine faults."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.3671v2", 
    "other_authors": "Martin Biely, Peter Robinson, Ulrich Schmid", 
    "title": "Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems", 
    "arxiv-id": "1103.3671v2", 
    "author": "Ulrich Schmid", 
    "publish": "2011-03-18T17:31:41Z", 
    "summary": "Despite of being quite similar agreement problems, consensus and general\nk-set agreement require surprisingly different techniques for proving the\nimpossibility in asynchronous systems with crash failures: Rather than\nrelatively simple bivalence arguments as in the impossibility proof for\nconsensus (= 1-set agreement) in the presence of a single crash failure, known\nproofs for the impossibility of k-set agreement in systems with at least k>1\ncrash failures use algebraic topology or a variant of Sperner's Lemma. In this\npaper, we present a generic theorem for proving the impossibility of k-set\nagreement in various message passing settings, which is based on a simple\nreduction to the consensus impossibility in a certain subsystem. We demonstrate\nthe broad applicability of our result by exploring the\npossibility/impossibility border of k-set agreement in several message-passing\nsystem models: (i) asynchronous systems with crash failures, (ii) partially\nsynchronous processes with (initial) crash failures, and (iii) asynchronous\nsystems augmented with failure detectors. In (i) and (ii), the impossibility\npart is just an instantiation of our main theorem, whereas the possibility of\nachieving k-set agreement in (ii) follows by generalizing the consensus\nalgorithm for initial crashes by Fisher, Lynch and Patterson. In (iii),\napplying our technique yields the exact border for the parameter k where k-set\nagreement is solvable with the failure detector class (Sigma_k,Omega_k), for\n(1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be\nnecessary for solving k-set agreement, this result yields new insights on the\nquest for the weakest failure detector."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.4690v2", 
    "other_authors": "Wojciech Golab, Lisa Higham, Philipp Woelfel", 
    "title": "Linearizable Implementations Do Not Suffice for Randomized Distributed   Computation", 
    "arxiv-id": "1103.4690v2", 
    "author": "Philipp Woelfel", 
    "publish": "2011-03-24T07:54:19Z", 
    "summary": "Linearizability is the gold standard among algorithm designers for deducing\nthe correctness of a distributed algorithm using implemented shared objects\nfrom the correctness of the corresponding algorithm using atomic versions of\nthe same objects. We show that linearizability does not suffice for this\npurpose when processes can exploit randomization, and we discuss the existence\nof alternative correctness conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5743v1", 
    "other_authors": "M. Shahriar Hossain, M. Muztaba Fuad, Debzani Deb, Kazi Muhammad Najmul Hasan Khan, Md. Mahbubul Alam Joarder", 
    "title": "Load Balancing in a Networked Environment through Homogenization", 
    "arxiv-id": "1103.5743v1", 
    "author": "Md. Mahbubul Alam Joarder", 
    "publish": "2011-03-29T19:56:06Z", 
    "summary": "Distributed processing across a networked environment suffers from\nunpredictable behavior of speedup due to heterogeneous nature of the hardware\nand software in the remote machines. It is challenging to get a better\nperformance from a distributed system by distributing task in an intelligent\nmanner such that the heterogeneous nature of the system do not have any effect\non the speedup ratio. This paper introduces homogenization, a technique that\ndistributes and balances the workload in such a manner that the user gets the\nhighest speedup possible from a distributed environment. Along with providing\nbetter performance, homogenization is totally transparent to the user and\nrequires no interaction with the system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5760v1", 
    "other_authors": "M. Shahriar Hossain, Kazi Muhammad Najmul Hasan Khan, M. Muztaba Fuad, Debzani Deb", 
    "title": "Triangular Dynamic Architecture for Distributed Computing in a LAN   Environment", 
    "arxiv-id": "1103.5760v1", 
    "author": "Debzani Deb", 
    "publish": "2011-03-29T20:05:50Z", 
    "summary": "A computationally intensive large job, granulized to concurrent pieces and\noperating in a dynamic environment should reduce the total processing time.\nHowever, distributing jobs across a networked environment is a tedious and\ndifficult task. Job distribution in a Local Area Network based on Triangular\nDynamic Architecture (TDA) is a mechanism that establishes a dynamic\nenvironment for job distribution, load balancing and distributed processing\nwith minimum interaction from the user. This paper introduces TDA and discusses\nits architecture and shows the benefits gained by utilizing such architecture\nin a distributed computing environment."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5764v1", 
    "other_authors": "M. Shahriar Hossain, M. Muztaba Fuad, Md. Mahbubul Alam Joarder", 
    "title": "Agent Based Processing of Global Evaluation Function", 
    "arxiv-id": "1103.5764v1", 
    "author": "Md. Mahbubul Alam Joarder", 
    "publish": "2011-03-29T20:15:48Z", 
    "summary": "Load balancing across a networked environment is a monotonous job. Moreover,\nif the job to be distributed is a constraint satisfying one, the distribution\nof load demands core intelligence. This paper proposes parallel processing\nthrough Global Evaluation Function by means of randomly initialized agents for\nsolving Constraint Satisfaction Problems. A potential issue about the number of\nagents in a machine under the invocation of distribution is discussed here for\nsecuring the maximum benefit from Global Evaluation and parallel processing.\nThe proposed system is compared with typical solution that shows an exclusive\noutcome supporting the nobility of parallel implementation of Global Evaluation\nFunction with certain number of agents in each invoked machine."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5794v2", 
    "other_authors": "Maryam Helmi, Lisa Higham, Eduardo Pacheco, Philipp Woelfel", 
    "title": "The Space Complexity of Long-lived and One-Shot Timestamp   Implementations", 
    "arxiv-id": "1103.5794v2", 
    "author": "Philipp Woelfel", 
    "publish": "2011-03-29T23:43:21Z", 
    "summary": "This paper is concerned with the problem of implementing an unbounded\ntimestamp object from multi-writer atomic registers, in an asynchronous\ndistributed system of n processors with distinct identifiers where timestamps\nare taken from an arbitrary universe. Ellen, Fatourou and Ruppert (2008) showed\nthat sqrt{n}/2-O(1) registers are required for any obstruction-free\nimplementation of long-lived timestamp systems from atomic registers (meaning\nprocessors can repeatedly get timestamps). We improve this existing lower bound\nin two ways. First we establish a lower bound of n/6 - O(1) registers for the\nobstruction-free long-lived timestamp problem. Previous such linear lower\nbounds were only known for constrained versions of the timestamp problem. This\nbound is asymptotically tight; Ellen, Fatourou and Ruppert (2008) constructed a\nwait-free algorithm that uses n-1 registers. Second we show that sqrt{n} - O(1)\nregisters are required for any obstruction-free implementation of one-shot\ntimestamp systems(meaning each processor can get a timestamp at most once). We\nshow that this bound is also asymptotically tight by providing a wait-free\none-shot timestamp system that uses fewer than 2 sqrt{n} registers, thus\nestablishing a space complexity gap between one-shot and long-lived timestamp\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.6087v1", 
    "other_authors": "Xu Liu, Jianfeng Zhan, Kunlin Zhan, Weisong Shi, Lin Yuan, Dan Meng, Lei Wang", 
    "title": "Automatic Performance Debugging of SPMD-style Parallel Programs", 
    "arxiv-id": "1103.6087v1", 
    "author": "Lei Wang", 
    "publish": "2011-03-31T05:38:39Z", 
    "summary": "The simple program and multiple data (SPMD) programming model is widely used\nfor both high performance computing and Cloud computing. In this paper, we\ndesign and implement an innovative system, AutoAnalyzer, that automates the\nprocess of debugging performance problems of SPMD-style parallel programs,\nincluding data collection, performance behavior analysis, locating bottlenecks,\nand uncovering their root causes. AutoAnalyzer is unique in terms of two\nfeatures: first, without any apriori knowledge, it automatically locates\nbottlenecks and uncovers their root causes for performance optimization;\nsecond, it is lightweight in terms of the size of performance data to be\ncollected and analyzed. Our contributions are three-fold: first, we propose two\neffective clustering algorithms to investigate the existence of performance\nbottlenecks that cause process behavior dissimilarity or code region behavior\ndisparity, respectively; meanwhile, we present two searching algorithms to\nlocate bottlenecks; second, on a basis of the rough set theory, we propose an\ninnovative approach to automatically uncovering root causes of bottlenecks;\nthird, on the cluster systems with two different configurations, we use two\nproduction applications, written in Fortran 77, and one open source\ncode-MPIBZIP2 (http://compression.ca/mpibzip2/), written in C++, to verify the\neffectiveness and correctness of our methods. For three applications, we also\npropose an experimental approach to investigating the effects of different\nmetrics on locating bottlenecks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.6114v2", 
    "other_authors": "Alexander Jaffe, Thomas Moscibroda, Laura Effinger-Dean, Luis Ceze, Karin Strauss", 
    "title": "The Impact of Memory Models on Software Reliability in Multiprocessors", 
    "arxiv-id": "1103.6114v2", 
    "author": "Karin Strauss", 
    "publish": "2011-03-31T07:43:58Z", 
    "summary": "The memory consistency model is a fundamental system property characterizing\na multiprocessor. The relative merits of strict versus relaxed memory models\nhave been widely debated in terms of their impact on performance, hardware\ncomplexity and programmability. This paper adds a new dimension to this\ndiscussion: the impact of memory models on software reliability. By allowing\nsome instructions to reorder, weak memory models may expand the window between\ncritical memory operations. This can increase the chance of an undesirable\nthread-interleaving, thus allowing an otherwise-unlikely concurrency bug to\nmanifest. To explore this phenomenon, we define and study a probabilistic model\nof shared-memory parallel programs that takes into account such reordering. We\nuse this model to formally derive bounds on the \\emph{vulnerability} to\nconcurrency bugs of different memory models. Our results show that for 2 (or a\nsmall constant number of) concurrent threads, weaker memory models do indeed\nhave a higher likelihood of allowing bugs. On the other hand, we show that as\nthe number of parallel threads increases, the gap between the different memory\nmodels becomes proportionally insignificant. This suggests the\ncounter-intuitive rule that \\emph{as the number of parallel threads in the\nsystem increases, the importance of using a strict memory model diminishes};\nwhich potentially has major implications on the choice of memory consistency\nmodels in future multi-core systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0149v1", 
    "other_authors": "Ali Khajeh-Hosseini, Ian Sommerville, Jurgen Bogaerts, Pradeep Teregowda", 
    "title": "Decision Support Tools for Cloud Migration in the Enterprise", 
    "arxiv-id": "1105.0149v1", 
    "author": "Pradeep Teregowda", 
    "publish": "2011-05-01T07:48:42Z", 
    "summary": "This paper describes two tools that aim to support decision making during the\nmigration of IT systems to the cloud. The first is a modeling tool that\nproduces cost estimates of using public IaaS clouds. The tool enables IT\narchitects to model their applications, data and infrastructure requirements in\naddition to their computational resource usage patterns. The tool can be used\nto compare the cost of different cloud providers, deployment options and usage\nscenarios. The second tool is a spreadsheet that outlines the benefits and\nrisks of using IaaS clouds from an enterprise perspective; this tool provides a\nstarting point for risk assessment. Two case studies were used to evaluate the\ntools. The tools were useful as they informed decision makers about the costs,\nbenefits and risks of using the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0296v1", 
    "other_authors": "Yang D. Li", 
    "title": "A Formal Model of Anonymous Systems", 
    "arxiv-id": "1105.0296v1", 
    "author": "Yang D. Li", 
    "publish": "2011-05-02T10:44:12Z", 
    "summary": "We put forward a formal model of anonymous systems. And we concentrate on the\nanonymous failure detectors in our model. In particular, we give three examples\nof anonymous failure detectors and show that they can be used to solve the\nconsensus problem and that they are equivalent to their classic counterparts.\nMoreover, we show some relationship among them and provide a simple\nclassification of anonymous failure detectors."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0545v4", 
    "other_authors": "Stefano Ferretti", 
    "title": "On the Degree Distribution of Faulty Peer-to-Peer Overlays", 
    "arxiv-id": "1105.0545v4", 
    "author": "Stefano Ferretti", 
    "publish": "2011-05-03T11:02:37Z", 
    "summary": "This paper presents an analytical framework to model fault-tolerance in\nunstructured peer-to-peer overlays, represented as complex networks. We define\na distributed protocol peers execute for managing the overlay and reacting to\nnode faults. Based on the protocol, evolution equations are defined and\nmanipulated by resorting to generating functions. Obtained outcomes provide\ninsights on the nodes' degree probability distribution. From the study of the\ndegree distribution, it is possible to estimate other important metrics of the\npeer-to-peer overlay, such as the diameter of the network. We study different\nnetworks, characterized by three specific desired degree distributions, i.e.\nnets with nodes having a fixed desired degree, random graphs and scale-free\nnetworks. All these networks are assessed via the analytical tool and\nsimulation as well. Results show that the approach can be factually employed to\ndynamically tune the average attachment rate at peers so that they maintain\ntheir own desired degree and, in general, the desired network topology."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0668v2", 
    "other_authors": "Partha Sarathi Mandal, Anil K. Ghosh", 
    "title": "Secure Position Verification for Wireless Sensor Networks in Noisy   Channels", 
    "arxiv-id": "1105.0668v2", 
    "author": "Anil K. Ghosh", 
    "publish": "2011-05-03T19:59:13Z", 
    "summary": "Position verification in wireless sensor networks (WSNs) is quite tricky in\npresence of attackers (malicious sensor nodes), who try to break the\nverification protocol by reporting their incorrect positions (locations) during\nthe verification stage. In the literature of WSNs, most of the existing methods\nof position verification have used trusted verifiers, which are in fact\nvulnerable to attacks by malicious nodes. They also depend on some distance\nestimation techniques, which are not accurate in noisy channels (mediums). In\nthis article, we propose a secure position verification scheme for WSNs in\nnoisy channels without relying on any trusted entities. Our verification scheme\ndetects and filters out all malicious nodes from the network with very high\nprobability."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1074v2", 
    "other_authors": "Dorina Thanou, Effrosyni Kokiopoulou, Pascal Frossard", 
    "title": "Progressive quantization in distributed average consensus", 
    "arxiv-id": "1105.1074v2", 
    "author": "Pascal Frossard", 
    "publish": "2011-05-05T13:56:48Z", 
    "summary": "We consider the problem of distributed average consensus in a sensor network\nwhere sensors exchange quantized information with their neighbors. We propose a\nnovel quantization scheme that exploits the increasing correlation between the\nvalues exchanged by the sensors throughout the iterations of the consensus\nalgorithm. A low complexity, uniform quantizer is implemented in each sensor,\nand refined quantization is achieved by progressively reducing the quantization\nintervals during the convergence of the consensus algorithm. We propose a\nrecurrence relation for computing the quantization parameters that depend on\nthe network topology and the communication rate. We further show that the\nrecurrence relation can lead to a simple exponential model for the size of the\nquantization step size over the iterations, whose parameters can be computed a\npriori. Finally, simulation results demonstrate the effectiveness of the\nprogressive quantization scheme that leads to the consensus solution even at\nlow communication rate."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1248v1", 
    "other_authors": "Patrick Briest, Bastian Degener, Barbara Kempkes, Peter Kling, Peter Pietrzyk", 
    "title": "A Distributed Approximation Algorithm for the Metric Uncapacitated   Facility Location Problem in the Congest Model", 
    "arxiv-id": "1105.1248v1", 
    "author": "Peter Pietrzyk", 
    "publish": "2011-05-06T09:36:03Z", 
    "summary": "We present a randomized distributed approximation algorithm for the metric\nuncapacitated facility location problem. The algorithm is executed on a\nbipartite graph in the Congest model yielding a (1.861 + epsilon) approximation\nfactor, where epsilon is an arbitrary small positive constant. It needs\nO(n^{3/4}log_{1+epsilon}^2(n) communication rounds with high probability (n\ndenoting the number of facilities and clients). To the best of our knowledge,\nour algorithm currently has the best approximation factor for the facility\nlocation problem in a distributed setting. It is based on a greedy sequential\napproximation algorithm by Jain et al. (J. ACM 50(6), pages: 795-824, 2003).\nThe main difficulty in executing this sequential algorithm lies in dealing with\nsituations, where multiple facilities are eligible for opening, but (in order\nto preserve the approximation factor of the sequential algorithm) only a subset\nof them can actually be opened. Note that while the presented runtime bound of\nour algorithm is \"with high probability\", the approximation factor is not \"in\nexpectation\" but always guaranteed to be (1.861 + epsilon). Thus, our main\ncontribution is a sublinear time selection mechanism that, while increasing the\napproximation factor by an arbitrary small additive term, allows us to decide\nwhich of the eligible facilities to open."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1982v1", 
    "other_authors": "Vaibhav Khadilkar, Murat Kantarcioglu, Bhavani Thuraisingham, Sharad Mehrotra", 
    "title": "Secure Data Processing in a Hybrid Cloud", 
    "arxiv-id": "1105.1982v1", 
    "author": "Sharad Mehrotra", 
    "publish": "2011-05-10T15:49:38Z", 
    "summary": "Cloud computing has made it possible for a user to be able to select a\ncomputing service precisely when needed. However, certain factors such as\nsecurity of data and regulatory issues will impact a user's choice of using\nsuch a service. A solution to these problems is the use of a hybrid cloud that\ncombines a user's local computing capabilities (for mission- or\norganization-critical tasks) with a public cloud (for less influential tasks).\nWe foresee three challenges that must be overcome before the adoption of a\nhybrid cloud approach: 1) data design: How to partition relations in a hybrid\ncloud? The solution to this problem must account for the sensitivity of\nattributes in a relation as well as the workload of a user; 2) data security:\nHow to protect a user's data in a public cloud with encryption while enabling\nquery processing over this encrypted data? and 3) query processing: How to\nexecute queries efficiently over both, encrypted and unencrypted data? This\npaper addresses these challenges and incorporates their solutions into an\nadd-on tool for a Hadoop and Hive based cloud computing infrastructure."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.2213v1", 
    "other_authors": "Elarbi Badidi, Larbi Esmahi", 
    "title": "A Cloud-based Approach for Context Information Provisioning", 
    "arxiv-id": "1105.2213v1", 
    "author": "Larbi Esmahi", 
    "publish": "2011-05-10T18:02:56Z", 
    "summary": "As a result of the phenomenal proliferation of modern mobile Internet-enabled\ndevices and the widespread utilization of wireless and cellular data networks,\nmobile users are increasingly requiring services tailored to their current\ncontext. High-level context information is typically obtained from context\nservices that aggregate raw context information sensed by various sensors and\nmobile devices. Given the massive amount of sensed data, traditional context\nservices are lacking the necessary resources to store and process these data,\nas well as to disseminate high-level context information to a variety of\npotential context consumers. In this paper, we propose a novel framework for\ncontext information provisioning, which relies on deploying context services on\nthe cloud and using context brokers to mediate between context consumers and\ncontext services using a publish/subscribe model. Moreover, we describe a\nmulti-attributes decision algorithm for the selection of potential context\nservices that can fulfill context consumers' requests for context information.\nThe algorithm calculates the score of each context service, per context\ninformation type, based on the quality-of-service (QoS) and quality-of-context\ninformation (QoC) requirements expressed by the context consumer. One of the\nbenefits of the approach is that context providers can scale up and down, in\nterms of cloud resources they use, depending on current demand for context\ninformation. Besides, the selection algorithm allows ranking context services\nby matching their QoS and QoC offers against the QoS and QoC requirements of\nthe context consumer."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.2301v4", 
    "other_authors": "Gabriele D'Angelo", 
    "title": "Parallel and Distributed Simulation from Many Cores to the Public Cloud   (Extended Version)", 
    "arxiv-id": "1105.2301v4", 
    "author": "Gabriele D'Angelo", 
    "publish": "2011-05-11T20:01:11Z", 
    "summary": "In this tutorial paper, we will firstly review some basic simulation concepts\nand then introduce the parallel and distributed simulation techniques in view\nof some new challenges of today and tomorrow. More in particular, in the last\nyears there has been a wide diffusion of many cores architectures and we can\nexpect this trend to continue. On the other hand, the success of cloud\ncomputing is strongly promoting the everything as a service paradigm. Is\nparallel and distributed simulation ready for these new challenges? The current\napproaches present many limitations in terms of usability and adaptivity: there\nis a strong need for new evaluation metrics and for revising the currently\nimplemented mechanisms. In the last part of the paper, we propose a new\napproach based on multi-agent systems for the simulation of complex systems. It\nis possible to implement advanced techniques such as the migration of simulated\nentities in order to build mechanisms that are both adaptive and very easy to\nuse. Adaptive mechanisms are able to significantly reduce the communication\ncost in the parallel/distributed architectures, to implement load-balance\ntechniques and to cope with execution environments that are both variable and\ndynamic. Finally, such mechanisms will be used to build simulations on top of\nunreliable cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.2584v1", 
    "other_authors": "James W. Smith, Ian Sommerville", 
    "title": "Workload Classification & Software Energy Measurement for Efficient   Scheduling on Private Cloud Platforms", 
    "arxiv-id": "1105.2584v1", 
    "author": "Ian Sommerville", 
    "publish": "2011-05-12T22:00:36Z", 
    "summary": "At present there are a number of barriers to creating an energy efficient\nworkload scheduler for a Private Cloud based data center. Firstly, the\nrelationship between different workloads and power consumption must be\ninvestigated. Secondly, current hardware-based solutions to providing energy\nusage statistics are unsuitable in warehouse scale data centers where low cost\nand scalability are desirable properties. In this paper we discuss the effect\nof different workloads on server power consumption in a Private Cloud platform.\nWe display a noticeable difference in energy consumption when servers are given\ntasks that dominate various resources (CPU, Memory, Hard Disk and Network). We\nthen use this insight to develop CloudMonitor, a software utility that is\ncapable of >95% accurate power predictions from monitoring resource consumption\nof workloads, after a \"training phase\" in which a dynamic power model is\ndeveloped."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.4424v1", 
    "other_authors": "Antonio Wendell De Oliveira Rodrigues, Fr\u00e9d\u00e9ric Guyomarc'H, Jean-Luc Dekeyser", 
    "title": "A Modeling Approach based on UML/MARTE for GPU Architecture", 
    "arxiv-id": "1105.4424v1", 
    "author": "Jean-Luc Dekeyser", 
    "publish": "2011-05-23T07:54:02Z", 
    "summary": "Nowadays, the High Performance Computing is part of the context of embedded\nsystems. Graphics Processing Units (GPUs) are more and more used in\nacceleration of the most part of algorithms and applications. Over the past\nyears, not many efforts have been done to describe abstractions of applications\nin relation to their target architectures. Thus, when developers need to\nassociate applications and GPUs, for example, they find difficulty and prefer\nusing API for these architectures. This paper presents a metamodel extension\nfor MARTE profile and a model for GPU architectures. The main goal is to\nspecify the task and data allocation in the memory hierarchy of these\narchitectures. The results show that this approach will help to generate code\nfor GPUs based on model transformations using Model Driven Engineering (MDE)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.5817v1", 
    "other_authors": "Zohir Bouzid, Anissa Lamani", 
    "title": "Robot Networks with Homonyms: The Case of Patterns Formation", 
    "arxiv-id": "1105.5817v1", 
    "author": "Anissa Lamani", 
    "publish": "2011-05-29T19:23:54Z", 
    "summary": "In this paper, we consider the problem of formation of a series of geometric\npatterns [4] by a network of oblivious mobile robots that communicate only\nthrough vision. So far, the problem has been studied in models where robots are\neither assumed to have distinct identifiers or to be completely anonymous. To\ngeneralize these results and to better understand how anonymity affects the\ncomputational power of robots, we study the problem in a new model, introduced\nrecently in [5], in which n robots may share up to 1 <= h <= n different\nidentifiers. We present necessary and sufficient conditions, relating\nsymmetricity and homonymy, that makes the problem solvable. We also show that\nin the case where h = n, making the identifiers of robots invisible does not\nlimit their computational power. This contradicts a result of [4]. To present\nour algorithms, we use a function that computes the Weber point for many\nregular and symmetric configurations. This function is interesting in its own\nright, since the problem of finding Weber points has been solved up to now for\nonly few other patterns."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2301", 
    "link": "http://arxiv.org/pdf/1105.6040v1", 
    "other_authors": "Alaa Ismail Elnashar", 
    "title": "Parallel Performance of MPI Sorting Algorithms on Dual-Core Processor   Windows-Based Systems", 
    "arxiv-id": "1105.6040v1", 
    "author": "Alaa Ismail Elnashar", 
    "publish": "2011-05-30T16:53:36Z", 
    "summary": "Message Passing Interface (MPI) is widely used to implement parallel\nprograms. Although Windowsbased architectures provide the facilities of\nparallel execution and multi-threading, little attention has been focused on\nusing MPI on these platforms. In this paper we use the dual core Window-based\nplatform to study the effect of parallel processes number and also the number\nof cores on the performance of three MPI parallel implementations for some\nsorting algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0231v1", 
    "other_authors": "Chiara Bodei, Viet Dung Dinh, Gian Luigi Ferrari", 
    "title": "Predicting global usages of resources endowed with local policies", 
    "arxiv-id": "1108.0231v1", 
    "author": "Gian Luigi Ferrari", 
    "publish": "2011-08-01T03:58:16Z", 
    "summary": "The effective usages of computational resources are a primary concern of\nup-to-date distributed applications. In this paper, we present a methodology to\nreason about resource usages (acquisition, release, revision, ...), and\ntherefore the proposed approach enables to predict bad usages of resources.\nKeeping in mind the interplay between local and global information occurring in\nthe application-resource interactions, we model resources as entities with\nlocal policies and global properties governing the overall interactions.\nFormally, our model takes the shape of an extension of pi-calculus with\nprimitives to manage resources. We develop a Control Flow Analysis computing a\nstatic approximation of process behaviour and therefore of the resource usages."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0599v1", 
    "other_authors": "Tomas Ramirez Picarzo, Francisco Fernandez de Vega, Daniel Lombrana Gonzalez", 
    "title": "Proposal for improvement in the transfer and execution of multiple   instances of a virtual image", 
    "arxiv-id": "1108.0599v1", 
    "author": "Daniel Lombrana Gonzalez", 
    "publish": "2011-08-02T15:45:12Z", 
    "summary": "Virtualization technology allows currently any application run any\napplication complex and expensive computational (the scientific applications\nare a good example) on heterogeneous distributed systems, which make regular\nuse of Grid and Cloud technologies, enabling significant savings in computing\ntime. This model is particularly interesting for the mass execution of\nscientific simulations and calculations, allowing parallel execution of\napplications using the same execution environment (unchanged) used by the\nscientist as usual. However, the use and distribution of large virtual images\ncan be a problem (up to tens of GBytes), which is aggravated when attempting a\nmass mailing on a large number of distributed computers. This work has as main\nobjective to present an analysis of how implementation and a proposal for the\nimprovement (reduction in size) of the virtual images pretending reduce\ndistribution time in distributed systems. This analysis is done very specific\nrequirements that need an operating system (guest OS) on some aspects of its\nexecution."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0972v1", 
    "other_authors": "Phisan Kaewprapha, Jing Li, Nattakan Puttarak", 
    "title": "Network Localization on Unit Disk Graphs", 
    "arxiv-id": "1108.0972v1", 
    "author": "Nattakan Puttarak", 
    "publish": "2011-08-04T01:22:53Z", 
    "summary": "We study the problem of cooperative localization of a large network of nodes\nin integer-coordinated unit disk graphs, a simplified but useful version of\ngeneral random graph. Exploiting the property that the radius $r$ sets clear\ncut on the connectivity of two nodes, we propose an essential philosophy that\n\"no connectivity is also useful information just like the information being\nconnected\" in unit disk graphs. Exercising this philosophy, we show that the\nconventional network localization problem can be re-formulated to significantly\nreduce the search space, and that global rigidity, a necessary and sufficient\ncondition for the existence of unique solution in general graphs, is no longer\nnecessary. While the problem is still NP-hard, we show that a (depth-first)\ntree-search algorithm with memory O(N) ($N$ is the network size) can be\ndeveloped, and for practical setups, the search complexity and speed is very\nmanageable, and is magnitudes less than the conventional problem, especially\nwhen the graph is sparse or when only very limited anchor nodes are available."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.1321v1", 
    "other_authors": "Vijay Bhaskar Semwal, K Susheel Kumar, Vinay S Bhaskar, Meenakshi Sati", 
    "title": "Accurate location estimation of moving object with energy constraint &   adaptive update algorithms to save data", 
    "arxiv-id": "1108.1321v1", 
    "author": "Meenakshi Sati", 
    "publish": "2011-08-05T12:56:43Z", 
    "summary": "In research paper \"Accurate estimation of the target location of object with\nenergy constraint & Adaptive Update Algorithms to Save Data\" one of the central\nissues in sensor networks is track the location, of moving object which have\noverhead of saving data, an accurate estimation of the target location of\nobject with energy constraint .We do not have any mechanism which control and\nmaintain data .The wireless communication bandwidth is also very limited. Some\nfield which is using this technique are flood and typhoon detection, forest\nfire detection, temperature and humidity and ones we have these information use\nthese information back to a central air conditioning and ventilation system. In\nthis research paper, we propose protocol based on the prediction and adaptive\nbased algorithm which is using less sensor node reduced by an accurate\nestimation of the target location. we are using minimum three sensor node to\nget the accurate position .We can extend it upto four or five to find more\naccurate location but we have energy constraint so we are using three with\naccurate estimation of location help us to reduce sensor node..We show that our\ntracking method performs well in terms of energy saving regardless of mobility\npattern of the mobile target .We extends the life time of network with less\nsensor node. Once a new object is detected, a mobile agent will be initiated to\ntrack the roaming path of the object. The agent is mobile since it will choose\nthe sensor closest to the object to stay. The agent may invite some nearby\nslave sensors to cooperatively position the object and inhibit other irrelevant\n(i.e., farther) sensors from tracking the object. As a result, the\ncommunication and sensing overheads are greatly reduced."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1462v1", 
    "other_authors": "Chitta Ranjan Tripathy, Nibedita Adhikari", 
    "title": "On a New Multicomputer Interconnection Topology for Massively Parallel   Systems", 
    "arxiv-id": "1108.1462v1", 
    "author": "Nibedita Adhikari", 
    "publish": "2011-08-06T08:03:43Z", 
    "summary": "This paper introduces a new interconnection network topology called Balanced\nVarietal Hypercube (BVH), suitable for massively parallel systems. The proposed\ntopology being a hybrid structure retains almost all the attractive properties\nof Balanced Hypercube and Varietal Hypercube. The topology, various parameters,\nrouting and broadcasting of Balanced Varietal Hypercube are presented. The\nperformance of the Balanced Varietal Hypercube is compared with other networks.\nIn terms of diameter, cost and average distance and reliability the proposed\nnetwork is found to be better than the Hypercube, Balanced Hypercube and\nVarietal Hypercube. Also it is more reliable and cost-effective than Hypercube\nand Balanced Hypercube."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1631v1", 
    "other_authors": "Lars Kolb, Andreas Thor, Erhard Rahm", 
    "title": "Load Balancing for MapReduce-based Entity Resolution", 
    "arxiv-id": "1108.1631v1", 
    "author": "Erhard Rahm", 
    "publish": "2011-08-08T08:48:57Z", 
    "summary": "The effectiveness and scalability of MapReduce-based implementations of\ncomplex data-intensive tasks depend on an even redistribution of data between\nmap and reduce tasks. In the presence of skewed data, sophisticated\nredistribution approaches thus become necessary to achieve load balancing among\nall reduce tasks to be executed in parallel. For the complex problem of entity\nresolution, we propose and evaluate two approaches for such skew handling and\nload balancing. The approaches support blocking techniques to reduce the search\nspace of entity resolution, utilize a preprocessing MapReduce job to analyze\nthe data distribution, and distribute the entities of large blocks among\nmultiple reduce tasks. The evaluation on a real cloud infrastructure shows the\nvalue and effectiveness of the proposed load balancing approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1928v1", 
    "other_authors": "Yongquan Fu, Yijie Wang, Ernst Biersack", 
    "title": "HybridNN: Supporting Network Location Service on Generalized Delay   Metrics", 
    "arxiv-id": "1108.1928v1", 
    "author": "Ernst Biersack", 
    "publish": "2011-08-09T13:57:18Z", 
    "summary": "Distributed Nearest Neighbor Search (DNNS) locates service nodes that have\nshortest interactive delay towards requesting hosts. DNNS provides an important\nservice for large-scale latency sensitive networked applications, such as VoIP,\nonline network games, or interactive network services on the cloud. Existing\nwork assumes the delay to be symmetric, which does not generalize to\napplications that are sensitive to one-way delays, such as the multimedia video\ndelivery from the servers to the hosts. We propose a relaxed inframetric model\nfor the network delay space that does not assume the triangle inequality and\ndelay symmetry to hold. We prove that the DNNS requests can be completed\nefficiently if the delay space exhibits modest inframetric dimensions, which we\ncan observe empirically. Finally, we propose a DNNS method named HybridNN\n(\\textit{Hybrid} \\textit{N}earest \\textit{N}eighbor search) based on the\ninframetric model for fast and accurate DNNS. For DNNS requests, HybridNN\nchooses closest neighbors accurately via the inframetric modelling, and\nscalably by combining delay predictions with direct probes to a pruned set of\nneighbors. Simulation results show that HybridNN locates nearly optimally the\nnearest neighbor. Experiments on PlanetLab show that HybridNN can provide\naccurate nearest neighbors that are close to optimal with modest query overhead\nand maintenance traffic."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.3268v1", 
    "other_authors": "Abdelgadir Tageldin Abdelgadir, Al-Sakib Khan Pathan, Mohiuddin Ahmed", 
    "title": "On the Performance of MPI-OpenMP on a 12 nodes Multi-core Cluster", 
    "arxiv-id": "1108.3268v1", 
    "author": "Mohiuddin Ahmed", 
    "publish": "2011-08-16T15:26:44Z", 
    "summary": "With the increasing number of Quad-Core-based clusters and the introduction\nof compute nodes designed with large memory capacity shared by multiple cores,\nnew problems related to scalability arise. In this paper, we analyze the\noverall performance of a cluster built with nodes having a dual Quad-Core\nProcessor on each node. Some benchmark results are presented and some\nobservations are mentioned when handling such processors on a benchmark test. A\nQuad-Core-based cluster's complexity arises from the fact that both local\ncommunication and network communications between the running processes need to\nbe addressed. The potentials of an MPI-OpenMP approach are pinpointed because\nof its reduced communication overhead. At the end, we come to a conclusion that\nan MPI-OpenMP solution should be considered in such clusters since optimizing\nnetwork communications between nodes is as important as optimizing local\ncommunications between processors in a multi-core cluster."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1108.4471v1", 
    "other_authors": "Jens-Wolfhard Schicke, Kirstin Peters, Ursula Goltz", 
    "title": "Synchrony vs. Causality in Asynchronous Petri Nets", 
    "arxiv-id": "1108.4471v1", 
    "author": "Ursula Goltz", 
    "publish": "2011-08-23T01:24:08Z", 
    "summary": "Given a synchronous system, we study the question whether the behaviour of\nthat system can be exhibited by a (non-trivially) distributed and hence\nasynchronous implementation. In this paper we show, by counterexample, that\nsynchronous systems cannot in general be implemented in an asynchronous fashion\nwithout either introducing an infinite implementation or changing the causal\nstructure of the system behaviour."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1131v1", 
    "other_authors": "Matthew Anderson, Maciej Brodowicz, Hartmut Kaiser, Bryce Adelstein-Lelbach, Thomas Sterling", 
    "title": "Adaptive Mesh Refinement for Astrophysics Applications with ParalleX", 
    "arxiv-id": "1110.1131v1", 
    "author": "Thomas Sterling", 
    "publish": "2011-10-06T01:27:28Z", 
    "summary": "Several applications in astrophysics require adequately resolving many\nphysical and temporal scales which vary over several orders of magnitude.\nAdaptive mesh refinement techniques address this problem effectively but often\nresult in constrained strong scaling performance. The ParalleX execution model\nis an experimental execution model that aims to expose new forms of program\nparallelism and eliminate any global barriers present in a scaling-impaired\napplication such as adaptive mesh refinement. We present two astrophysics\napplications using the ParalleX execution model: a tabulated equation of state\ncomponent for neutron star evolutions and a cosmology model evolution.\nPerformance and strong scaling results from both simulations are presented. The\ntabulated equation of state data are distributed with transparent access over\nthe nodes of the cluster. This allows seamless overlapping of computation with\nthe latencies introduced by the remote access to the table. Because of the\nexpected size increases to the equation of state table, this type of table\npartitioning for neutron star simulations is essential while the implementation\nis greatly simplified by ParalleX semantics."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1553v1", 
    "other_authors": "Jack Dongarra, Mathieu Faverge, Thomas Herault, Julien Langou, and Yves Robert", 
    "title": "Hierarchical QR factorization algorithms for multi-core cluster systems", 
    "arxiv-id": "1110.1553v1", 
    "author": "and Yves Robert", 
    "publish": "2011-10-07T14:51:08Z", 
    "summary": "This paper describes a new QR factorization algorithm which is especially\ndesigned for massively parallel platforms combining parallel distributed\nmulti-core nodes. These platforms make the present and the foreseeable future\nof high-performance computing. Our new QR factorization algorithm falls in the\ncategory of the tile algorithms which naturally enables good data locality for\nthe sequential kernels executed by the cores (high sequential performance), low\nnumber of messages in a parallel distributed setting (small latency term), and\nfine granularity (high parallelism)."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1991v1", 
    "other_authors": "Resat Umit Payli, Kayhan Erciyes, Orhan Dagdeviren", 
    "title": "Cluster-Based Load Balancing Algorithms for Grids", 
    "arxiv-id": "1110.1991v1", 
    "author": "Orhan Dagdeviren", 
    "publish": "2011-10-10T10:32:29Z", 
    "summary": "E-science applications may require huge amounts of data and high processing\npower where grid infrastructures are very suitable for meeting these\nrequirements. The load distribution in a grid may vary leading to the\nbottlenecks and overloaded sites. We describe a hierarchical dynamic load\nbalancing protocol for Grids. The Grid consists of clusters and each cluster is\nrepresented by a coordinator. Each coordinator first attempts to balance the\nload in its cluster and if this fails, communicates with the other coordinators\nto perform transfer or reception of load. This process is repeated\nperiodically. We analyze the correctness, performance and scalability of the\nproposed protocol and show from the simulation results that our algorithm\nbalances the load by decreasing the number of high loaded nodes in a grid\nenvironment."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.2561v1", 
    "other_authors": "Alexey A. Peretyatko, Ivan A. Bogatyrev, Vitaliy Yu. Kapitan, Yury V. Kirienko, Konstantin V. Nefedev, Valery I. Belokon", 
    "title": "Rigorous Calculation of the Partition Function for the Finite Number of   Ising Spins", 
    "arxiv-id": "1110.2561v1", 
    "author": "Valery I. Belokon", 
    "publish": "2011-10-12T03:34:51Z", 
    "summary": "The high-performance scalable parallel algorithm for rigorous calculation of\npartition function of lattice systems with finite number Ising spins was\ndeveloped. The parallel calculations run by C++ code with using of Message\nPassing Interface and massive parallel instructions. The algorithm can be used\nfor the research of the interacting spin systems in the Ising models of 2D and\n3D. The processing power and scalability is analyzed for different parallel and\ndistributed systems. Different methods of the speed up measuring allow obtain\nthe super-linear speeding up for the small number of processes. Program code\ncould be useful also for research by exact method of different Ising spin\nsystems, e.g. system with competition interactions."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.2677v1", 
    "other_authors": "Simplice Donfack, Laura Grigori, William D. Gropp, Vivek Kale", 
    "title": "Hybrid static/dynamic scheduling for already optimized dense matrix   factorization", 
    "arxiv-id": "1110.2677v1", 
    "author": "Vivek Kale", 
    "publish": "2011-10-12T15:09:45Z", 
    "summary": "We present the use of a hybrid static/dynamic scheduling strategy of the task\ndependency graph for direct methods used in dense numerical linear algebra.\nThis strategy provides a balance of data locality, load balance, and low\ndequeue overhead. We show that the usage of this scheduling in communication\navoiding dense factorization leads to significant performance gains. On a 48\ncore AMD Opteron NUMA machine, our experiments show that we can achieve up to\n64% improvement over a version of CALU that uses fully dynamic scheduling, and\nup to 30% improvement over the version of CALU that uses fully static\nscheduling. On a 16-core Intel Xeon machine, our hybrid static/dynamic\nscheduling approach is up to 8% faster than the version of CALU that uses a\nfully static scheduling or fully dynamic scheduling. Our algorithm leads to\nspeedups over the corresponding routines for computing LU factorization in well\nknown libraries. On the 48 core AMD NUMA machine, our best implementation is up\nto 110% faster than MKL, while on the 16 core Intel Xeon machine, it is up to\n82% faster than MKL. Our approach also shows significant speedups compared with\nPLASMA on both of these systems."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.4854v1", 
    "other_authors": "Nikos Chondros, Konstantinos Kokordelis, Mema Roussopoulos", 
    "title": "On the Practicality of `Practical' Byzantine Fault Tolerance", 
    "arxiv-id": "1110.4854v1", 
    "author": "Mema Roussopoulos", 
    "publish": "2011-10-21T18:02:22Z", 
    "summary": "Byzantine Fault Tolerant (BFT) systems are considered by the systems research\ncommunity to be state of the art with regards to providing reliability in\ndistributed systems. BFT systems provide safety and liveness guarantees with\nreasonable assumptions, amongst a set of nodes where at most f nodes display\narbitrarily incorrect behaviors, known as Byzantine faults. Despite this, BFT\nsystems are still rarely used in practice. In this paper we describe our\nexperience, from an application developer's perspective, trying to leverage the\npublicly available and highly-tuned PBFT middleware (by Castro and Liskov), to\nprovide provable reliability guarantees for an electronic voting application\nwith high security and robustness needs. We describe several obstacles we\nencountered and drawbacks we identified in the PBFT approach. These include\nsome that we tackled, such as lack of support for dynamic client management and\nleaving state management completely up to the application. Others still\nremaining include the lack of robust handling of non-determinism, lack of\nsupport for web-based applications, lack of support for stronger cryptographic\nprimitives, and others. We find that, while many of the obstacles could be\novercome with a revised BFT middleware implementation that is tuned\nspecifically for the needs of the particular application, they require\nsignificant engineering effort and time and their performance implications for\nthe end-application are unclear. An application developer is thus unlikely to\nbe willing to invest the time and effort to do so to leverage the BFT approach.\nWe conclude that the research community needs to focus on the usability of BFT\nalgorithms for real world applications, from the end-developer perspective, in\naddition to continuing to improve the BFT middleware performance, robustness\nand deployment layouts."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.5969v1", 
    "other_authors": "William Voorsluys, Rajkumar Buyya", 
    "title": "Reliable Provisioning of Spot Instances for Compute-intensive   Applications", 
    "arxiv-id": "1110.5969v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-10-27T03:12:07Z", 
    "summary": "Cloud computing providers are now offering their unused resources for leasing\nin the spot market, which has been considered the first step towards a\nfull-fledged market economy for computational resources. Spot instances are\nvirtual machines (VMs) available at lower prices than their standard on-demand\ncounterparts. These VMs will run for as long as the current price is lower than\nthe maximum bid price users are willing to pay per hour. Spot instances have\nbeen increasingly used for executing compute-intensive applications. In spite\nof an apparent economical advantage, due to an intermittent nature of biddable\nresources, application execution times may be prolonged or they may not finish\nat all. This paper proposes a resource allocation strategy that addresses the\nproblem of running compute-intensive jobs on a pool of intermittent virtual\nmachines, while also aiming to run applications in a fast and economical way.\nTo mitigate potential unavailability periods, a multifaceted fault-aware\nresource provisioning policy is proposed. Our solution employs price and\nruntime estimation mechanisms, as well as three fault tolerance techniques,\nnamely checkpointing, task duplication and migration. We evaluate our\nstrategies using trace-driven simulations, which take as input real price\nvariation traces, as well as an application trace from the Parallel Workload\nArchive. Our results demonstrate the effectiveness of executing applications on\nspot instances, respecting QoS constraints, despite occasional failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.5972v1", 
    "other_authors": "William Voorsluys, Saurabh Kumar Garg, Rajkumar Buyya", 
    "title": "Provisioning Spot Market Cloud Resources to Create Cost-effective   Virtual Clusters", 
    "arxiv-id": "1110.5972v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-10-27T03:22:25Z", 
    "summary": "Infrastructure-as-a-Service providers are offering their unused resources in\nthe form of variable-priced virtual machines (VMs), known as \"spot instances\",\nat prices significantly lower than their standard fixed-priced resources. To\nlease spot instances, users specify a maximum price they are willing to pay per\nhour and VMs will run only when the current price is lower than the user's bid.\nThis paper proposes a resource allocation policy that addresses the problem of\nrunning deadline-constrained compute-intensive jobs on a pool of composed\nsolely of spot instances, while exploiting variations in price and performance\nto run applications in a fast and economical way. Our policy relies on job\nruntime estimations to decide what are the best types of VMs to run each job\nand when jobs should run. Several estimation methods are evaluated and\ncompared, using trace-based simulations, which take real price variation traces\nobtained from Amazon Web Services as input, as well as an application trace\nfrom the Parallel Workload Archive. Results demonstrate the effectiveness of\nrunning computational jobs on spot instances, at a fraction (up to 60% lower)\nof the price that would normally cost on fixed priced resources."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.6231v1", 
    "other_authors": "Agnieszka \u0141upi\u0144ska", 
    "title": "Parallel implematation of flow and matching algorithms", 
    "arxiv-id": "1110.6231v1", 
    "author": "Agnieszka \u0141upi\u0144ska", 
    "publish": "2011-10-28T01:44:45Z", 
    "summary": "In our work we present two parallel algorithms and their lock-free\nimplementations using a popular GPU environment Nvidia CUDA. The first\nalgorithm is the push-relabel method for the flow problem in grid graphs. The\nsecond is the cost scaling algorithm for the assignment problem in complete\nbipartite graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.6288v1", 
    "other_authors": "Ian P. Gent, Lars Kotthoff", 
    "title": "Reliability of Computational Experiments on Virtualised Hardware", 
    "arxiv-id": "1110.6288v1", 
    "author": "Lars Kotthoff", 
    "publish": "2011-10-28T10:21:23Z", 
    "summary": "We present preliminary results of an investigation into the suitability of\nvirtualised hardware -- in particular clouds -- for running computational\nexperiments. Our main concern was that the reported CPU time would not be\nreliable and reproducible. The results demonstrate that while this is true in\ncases where many virtual machines are running on the same physical hardware,\nthere is no inherent variation introduced by using virtualised hardware\ncompared to non-virtualised hardware."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1112.0384v1", 
    "other_authors": "Chinmoy Dutta, Gopal Pandurangan, Rajmohan Rajaraman, Zhifeng Sun", 
    "title": "Information Spreading in Dynamic Networks", 
    "arxiv-id": "1112.0384v1", 
    "author": "Zhifeng Sun", 
    "publish": "2011-12-02T04:56:12Z", 
    "summary": "We study the fundamental problem of information spreading (also known as\ngossip) in dynamic networks. In gossip, or more generally, $k$-gossip, there\nare $k$ pieces of information (or tokens) that are initially present in some\nnodes and the problem is to disseminate the $k$ tokens to all nodes. The goal\nis to accomplish the task in as few rounds of distributed computation as\npossible. The problem is especially challenging in dynamic networks where the\nnetwork topology can change from round to round and can be controlled by an\non-line adversary.\n  The focus of this paper is on the power of token-forwarding algorithms, which\ndo not manipulate tokens in any way other than storing and forwarding them. We\nfirst consider a worst-case adversarial model first studied by Kuhn, Lynch, and\nOshman~\\cite{kuhn+lo:dynamic} in which the communication links for each round\nare chosen by an adversary, and nodes do not know who their neighbors for the\ncurrent round are before they broadcast their messages. Our main result is an\n$\\Omega(nk/\\log n)$ lower bound on the number of rounds needed for any\ndeterministic token-forwarding algorithm to solve $k$-gossip. This resolves an\nopen problem raised in~\\cite{kuhn+lo:dynamic}, improving their lower bound of\n$\\Omega(n \\log k)$, and matching their upper bound of $O(nk)$ to within a\nlogarithmic factor.\n  We next show that token-forwarding algorithms can achieve subquadratic time\nin the offline version of the problem where the adversary has to commit all the\ntopology changes in advance at the beginning of the computation, and present\ntwo polynomial-time offline token-forwarding algorithms. Our results are a step\ntowards understanding the power and limitation of token-forwarding algorithms\nin dynamic networks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.1336v5", 
    "other_authors": "Guodong Shi, Brian D. O. Anderson, Karl Henrik Johansson", 
    "title": "Consensus over Random Graph Processes: Network Borel-Cantelli Lemmas for   Almost Sure Convergence", 
    "arxiv-id": "1112.1336v5", 
    "author": "Karl Henrik Johansson", 
    "publish": "2011-12-06T16:33:04Z", 
    "summary": "Distributed consensus computation over random graph processes is considered.\nThe random graph process is defined as a sequence of random variables which\ntake values from the set of all possible digraphs over the node set. At each\ntime step, every node updates its state based on a Bernoulli trial, independent\nin time and among different nodes: either averaging among the neighbor set\ngenerated by the random graph, or sticking with its current state.\nConnectivity-independence and arc-independence are introduced to capture the\nfundamental influence of the random graphs on the consensus convergence.\nNecessary and/or sufficient conditions are presented on the success\nprobabilities of the Bernoulli trials for the network to reach a global almost\nsure consensus, with some sharp threshold established revealing a consensus\nzero-one law. Convergence rates are established by lower and upper bounds of\nthe $\\epsilon$-computation time. We also generalize the concepts of\nconnectivity/arc independence to their analogues from the $*$-mixing point of\nview, so that our results apply to a very wide class of graphical models,\nincluding the majority of random graph models in the literature, e.g.,\nErd\\H{o}s-R\\'{e}nyi, gossiping, and Markovian random graphs. We show that under\n$*$-mixing, our convergence analysis continues to hold and the corresponding\nalmost sure consensus conditions are established. Finally, we further\ninvestigate almost sure finite-time convergence of random gossiping algorithms,\nand prove that the Bernoulli trials play a key role in ensuring finite-time\nconvergence. These results add to the understanding of the interplay between\nrandom graphs, random computations, and convergence probability for distributed\ninformation processing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.1851v2", 
    "other_authors": "Michael Menzel, Marten Sch\u00f6nherr, Jens Nimis, Stefan Tai", 
    "title": "(MC2)2: A Generic Decision-Making Framework and its Application to Cloud   Computing", 
    "arxiv-id": "1112.1851v2", 
    "author": "Stefan Tai", 
    "publish": "2011-12-08T14:46:08Z", 
    "summary": "Cloud computing is a disruptive technology, representing a new model for\ninformation technology (IT) solution engineering and management that promises\nto introduce significant cost savings and other benefits. The adoption of Cloud\ncomputing requires a detailed comparison of infrastructure alternatives, taking\na number of aspects into careful consideration. Existing methods of evaluation,\nhowever, limit decision making to the relative costs of cloud computing, but do\nnot take a broader range of criteria into account. In this paper, we introduce\na generic, multi-criteria-based decision framework and an application for Cloud\nComputing, the Multi-Criteria Comparison Method for Cloud Computing ((MC2)2).\nThe framework and method allow organizations to determine what infrastructure\nbest suits their needs by evaluating and ranking infrastructure alternatives\nusing multiple criteria. Therefore, (MC2)2 offers a way to differentiate\ninfrastructures not only by costs, but also in terms of benefits, opportunities\nand risks. (MC2)2 can be adapted to facilitate a wide array of decision-making\nscenarios within the domain of information technology infrastructures,\ndepending on the criteria selected to support the framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.2025v1", 
    "other_authors": "Tin Tin Yee, Thinn Thu Naing", 
    "title": "PC-Cluster based Storage System Architecture for Cloud Storage", 
    "arxiv-id": "1112.2025v1", 
    "author": "Thinn Thu Naing", 
    "publish": "2011-12-09T06:58:13Z", 
    "summary": "Design and architecture of cloud storage system plays a vital role in cloud\ncomputing infrastructure in order to improve the storage capacity as well as\ncost effectiveness. Usually cloud storage system provides users to efficient\nstorage space with elasticity feature. One of the challenges of cloud storage\nsystem is difficult to balance the providing huge elastic capacity of storage\nand investment of expensive cost for it. In order to solve this issue in the\ncloud storage infrastructure, low cost PC cluster based storage server is\nconfigured to be activated for large amount of data to provide cloud users.\nMoreover, one of the contributions of this system is proposed an analytical\nmodel using M/M/1 queuing network model, which is modeled on intended\narchitecture to provide better response time, utilization of storage as well as\npending time when the system is running. According to the analytical result on\nexperimental testing, the storage can be utilized more than 90% of storage\nspace. In this paper, two parts have been described such as (i) design and\narchitecture of PC cluster based cloud storage system. On this system, related\nto cloud applications, services configurations are explained in detailed. (ii)\nAnalytical model has been enhanced to be increased the storage utilization on\nthe target architecture."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.2431v2", 
    "other_authors": "Arash Mohammadi, Amir Asif", 
    "title": "Distributed Particle Filter Implementation with Intermittent/Irregular   Consensus Convergence", 
    "arxiv-id": "1112.2431v2", 
    "author": "Amir Asif", 
    "publish": "2011-12-12T03:23:47Z", 
    "summary": "Motivated by non-linear, non-Gaussian, distributed multi-sensor/agent\nnavigation and tracking applications, we propose a multi-rate consensus/fusion\nbased framework for distributed implementation of the particle filter (CF/DPF).\nThe CF/DPF framework is based on running localized particle filters to estimate\nthe overall state vector at each observation node. Separate fusion filters are\ndesigned to consistently assimilate the local filtering distributions into the\nglobal posterior by compensating for the common past information between\nneighbouring nodes. The CF/DPF offers two distinct advantages over its\ncounterparts. First, the CF/DPF framework is suitable for scenarios where\nnetwork connectivity is intermittent and consensus can not be reached between\ntwo consecutive observations. Second, the CF/DPF is not limited to the Gaussian\napproximation for the global posterior density. A third contribution of the\npaper is the derivation of the exact expression for computing the posterior\nCramer-Rao lower bound (PCRLB) for the distributed architecture based on a\nrecursive procedure involving the local Fisher information matrices (FIM) of\nthe distributed estimators. The performance of the CF/DPF algorithm closely\nfollows the centralized particle filter approaching the PCRLB at the signal to\nnoise ratios that we tested."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.3725v1", 
    "other_authors": "Mamoun M. Jamous, Safaai Bin Deris", 
    "title": "Web Services Non-Functional Classification to Enhance Discovery Speed", 
    "arxiv-id": "1112.3725v1", 
    "author": "Safaai Bin Deris", 
    "publish": "2011-12-16T07:39:02Z", 
    "summary": "Recently, the use and deployment of web services has dramatically increased.\nThis is due to the easiness, interoperability, and flexibility that web\nservices offer to the software systems, which other software structures don't\nsupport or support poorly. Web services discovery became more important and\nresearch conducted in this area became more critical. With the increasing\nnumber of published and publicly available web services, speed in web service\ndiscovery process is becoming an issue which cannot be neglected. This paper\nproposes a generic non-functional based web services classification algorithm.\nClassification algorithm depends on information supplied by web service\nprovider at the registration time. Authors have proved mathematically and\nexperimentally the usefulness and efficiency of proposed algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.3880v2", 
    "other_authors": "Michael Menzel, Rajiv Ranjan", 
    "title": "CloudGenius: Automated Decision Support for Migrating Multi-Component   Enterprise Applications to Clouds", 
    "arxiv-id": "1112.3880v2", 
    "author": "Rajiv Ranjan", 
    "publish": "2011-12-16T16:24:34Z", 
    "summary": "One of the key problems in migrating multi-component enterprise applications\nto Clouds is selecting the best mix of VM images and Cloud infrastructure\nservices. A migration process has to ensure that Quality of Service (QoS)\nrequirements are met, while satisfying conflicting selection criteria, e.g.\nthroughput and cost. When selecting Cloud services, application engineers must\nconsider heterogeneous sets of criteria and complex dependencies across\nmultiple layers impossible to resolve manually. To overcome this challenge, we\npresent the generic recommender framework CloudGenius and an implementation\nthat leverage well known multi-criteria decision making technique Analytic\nHierarchy Process to automate the selection process based on a model, factors,\nand QoS requirements related to enterprise applications. In particular, we\nintroduce a structured migration process for multi-component enterprise\napplications, clearly identify the most important criteria relevant to the\nselection problem and present a multi-criteria-based selection algorithm.\nExperiments with the software prototype CumulusGenius show time complexities."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.4980v1", 
    "other_authors": "Meni Rosenfeld", 
    "title": "Analysis of Bitcoin Pooled Mining Reward Systems", 
    "arxiv-id": "1112.4980v1", 
    "author": "Meni Rosenfeld", 
    "publish": "2011-12-21T10:40:38Z", 
    "summary": "In this paper we describe the various scoring systems used to calculate\nrewards of participants in Bitcoin pooled mining, explain the problems each\nwere designed to solve and analyze their respective advantages and\ndisadvantages."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.6254v1", 
    "other_authors": "Soumitra Pal, Abhiram Ranade", 
    "title": "Scheduling Light-trails in WDM Rings", 
    "arxiv-id": "1112.6254v1", 
    "author": "Abhiram Ranade", 
    "publish": "2011-12-29T08:56:42Z", 
    "summary": "We consider the problem of scheduling communication on optical WDM\n(wavelength division multiplexing) networks using the light-trails technology.\nWe seek to design scheduling algorithms such that the given transmission\nrequests can be scheduled using minimum number of wavelengths (optical\nchannels). We provide algorithms and close lower bounds for two versions of the\nproblem on an $n$ processor linear array/ring network. In the {\\em stationary}\nversion, the pattern of transmissions (given) is assumed to not change over\ntime. For this, a simple lower bound is $c$, the congestion or the maximum\ntotal traffic required to pass through any link. We give an algorithm that\nschedules the transmissions using $O(c+\\log{n})$ wavelengths. We also show a\npattern for which $\\Omega(c+\\log{n}/\\log\\log{n})$ wavelengths are needed. In\nthe {\\em on-line} version, the transmissions arrive and depart dynamically, and\nmust be scheduled without upsetting the previously scheduled transmissions. For\nthis case we give an on-line algorithm which has competitive ratio\n$\\Theta(\\log{n})$. We show that this is optimal in the sense that every on-line\nalgorithm must have competitive ratio $\\Omega(\\log{n})$. We also give an\nalgorithm that appears to do well in simulation (for the classes of traffic we\nconsider), but which has competitive ratio between $\\Omega(\\log^2n/\\log\n\\log{n})$ and $O(\\log^2n)$. We present detailed simulations of both our\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1201.0360v1", 
    "other_authors": "Toni Stojanovski, Ljupco Krstevski", 
    "title": "On the Performance of Exhaustive Search with Cooperating agents", 
    "arxiv-id": "1201.0360v1", 
    "author": "Ljupco Krstevski", 
    "publish": "2012-01-01T16:03:49Z", 
    "summary": "Despite the occurrence of elegant algorithms for solving complex problem,\nexhaustive search has retained its significance since many real-life problems\nexhibit no regular structure and exhaustive search is the only possible\nsolution. The advent of high-performance computing either via multicore\nprocessors or distributed processors emphasizes the possibility for exhaustive\nsearch by multiple search agents. Here we analyse the performance of exhaustive\nsearch when it is conducted by multiple search agents. Several strategies for\ncooperation between the search agents are evaluated. We discover that the\nperformance of the search improves with the increase in the level of\ncooperation. Same search performance can be achieved with homogeneous and\nheterogeneous search agents provided that the length of subregions allocated to\nindividual search regions follow the differences in the speeds of heterogeneous\nsearch agents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1201.2118v1", 
    "other_authors": "Marek Blazewicz, Steven R. Brandt, Peter Diener, David M. Koppelman, Krzysztof Kurowski, Frank L\u00f6ffler, Erik Schnetter, Jian Tao", 
    "title": "A Massive Data Parallel Computational Framework for Petascale/Exascale   Hybrid Computer Systems", 
    "arxiv-id": "1201.2118v1", 
    "author": "Jian Tao", 
    "publish": "2012-01-10T17:20:17Z", 
    "summary": "Heterogeneous systems are becoming more common on High Performance Computing\n(HPC) systems. Even using tools like CUDA and OpenCL it is a non-trivial task\nto obtain optimal performance on the GPU. Approaches to simplifying this task\ninclude Merge (a library based framework for heterogeneous multi-core systems),\nZippy (a framework for parallel execution of codes on multiple GPUs), BSGP (a\nnew programming language for general purpose computation on the GPU) and\nCUDA-lite (an enhancement to CUDA that transforms code based on annotations).\nIn addition, efforts are underway to improve compiler tools for automatic\nparallelization and optimization of affine loop nests for GPUs and for\nautomatic translation of OpenMP parallelized codes to CUDA.\n  In this paper we present an alternative approach: a new computational\nframework for the development of massively data parallel scientific codes\napplications suitable for use on such petascale/exascale hybrid systems built\nupon the highly scalable Cactus framework. As the first non-trivial\ndemonstration of its usefulness, we successfully developed a new 3D CFD code\nthat achieves improved performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijngn.2011.3403", 
    "link": "http://arxiv.org/pdf/1201.2125v1", 
    "other_authors": "P. Suresh Kumar, P. Sateesh Kumar, S. Ramachandram", 
    "title": "Purging of untrustworthy recommendations from a grid", 
    "arxiv-id": "1201.2125v1", 
    "author": "S. Ramachandram", 
    "publish": "2012-01-10T17:46:16Z", 
    "summary": "In grid computing, trust has massive significance. There is lot of research\nto propose various models in providing trusted resource sharing mechanisms. The\ntrust is a belief or perception that various researchers have tried to\ncorrelate with some computational model. Trust on any entity can be direct or\nindirect. Direct trust is the impact of either first impression over the entity\nor acquired during some direct interaction. Indirect trust is the trust may be\ndue to either reputation gained or recommendations received from various\nrecommenders of a particular domain in a grid or any other domain outside that\ngrid or outside that grid itself. Unfortunately, malicious indirect trust leads\nto the misuse of valuable resources of the grid. This paper proposes the\nmechanism of identifying and purging the untrustworthy recommendations in the\ngrid environment. Through the obtained results, we show the way of purging of\nuntrustworthy entities."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijngn.2011.3403", 
    "link": "http://arxiv.org/pdf/1201.2360v2", 
    "other_authors": "Matteo Dell'Amico, Pietro Michiardi, Laszlo Toka, Pasquale Cataldi", 
    "title": "Adaptive Redundancy Management for Durable P2P Backup", 
    "arxiv-id": "1201.2360v2", 
    "author": "Pasquale Cataldi", 
    "publish": "2012-01-11T17:56:56Z", 
    "summary": "We design and analyze the performance of a redundancy management mechanism\nfor Peer-to-Peer backup applications. Armed with the realization that a backup\nsystem has peculiar requirements -- namely, data is read over the network only\nduring restore processes caused by data loss -- redundancy management targets\ndata durability rather than attempting to make each piece of information\navailabile at any time.\n  In our approach each peer determines, in an on-line manner, an amount of\nredundancy sufficient to counter the effects of peer deaths, while preserving\nacceptable data restore times. Our experiments, based on trace-driven\nsimulations, indicate that our mechanism can reduce the redundancy by a factor\nbetween two and three with respect to redundancy policies aiming for data\navailability. These results imply an according increase in storage capacity and\ndecrease in time to complete backups, at the expense of longer times required\nto restore data. We believe this is a very reasonable price to pay, given the\nnature of the application.\n  We complete our work with a discussion on practical issues, and their\nsolutions, related to which encoding technique is more suitable to support our\nscheme."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.3804v1", 
    "other_authors": "Mads Ruben Burgdorff Kristensen, Brian Vinter", 
    "title": "Managing Communication Latency-Hiding at Runtime for Parallel   Programming Languages and Libraries", 
    "arxiv-id": "1201.3804v1", 
    "author": "Brian Vinter", 
    "publish": "2012-01-18T14:43:43Z", 
    "summary": "This work introduces a runtime model for managing communication with support\nfor latency-hiding. The model enables non-computer science researchers to\nexploit communication latency-hiding techniques seamlessly. For compiled\nlanguages, it is often possible to create efficient schedules for\ncommunication, but this is not the case for interpreted languages. By\nmaintaining data dependencies between scheduled operations, it is possible to\naggressively initiate communication and lazily evaluate tasks to allow maximal\ntime for the communication to finish before entering a wait state. We implement\na heuristic of this model in DistNumPy, an auto-parallelizing version of\nnumerical Python that allows sequential NumPy programs to run on distributed\nmemory architectures. Furthermore, we present performance comparisons for eight\nbenchmarks with and without automatic latency-hiding. The results shows that\nour model reduces the time spent on waiting for communication as much as 27\ntimes, from a maximum of 54% to only 2% of the total execution time, in a\nstencil application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.4183v2", 
    "other_authors": "Nitin Vaidya, Lewis Tseng, Guanfeng Liang", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs", 
    "arxiv-id": "1201.4183v2", 
    "author": "Guanfeng Liang", 
    "publish": "2012-01-19T22:05:08Z", 
    "summary": "In this paper, we explore the problem of iterative approximate Byzantine\nconsensus in arbitrary directed graphs. In particular, we prove a necessary and\nsufficient condition for the existence of iterative byzantine consensus\nalgorithms. Additionally, we use our sufficient condition to examine whether\nsuch algorithms exist for some specific graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.6652v3", 
    "other_authors": "Danny Dolev, Christoph Lenzen, Shir Peled", 
    "title": "\"Tri, Tri again\": Finding Triangles and Small Subgraphs in a Distributed   Setting", 
    "arxiv-id": "1201.6652v3", 
    "author": "Shir Peled", 
    "publish": "2012-01-31T19:02:49Z", 
    "summary": "Let G = (V,E) be an n-vertex graph and M_d a d-vertex graph, for some\nconstant d. Is M_d a subgraph of G? We consider this problem in a model where\nall n processes are connected to all other processes, and each message contains\nup to O(log n) bits. A simple deterministic algorithm that requires\nO(n^((d-2)/d) / log n) communication rounds is presented. For the special case\nthat M_d is a triangle, we present a probabilistic algorithm that requires an\nexpected O(ceil(n^(1/3) / (t^(2/3) + 1))) rounds of communication, where t is\nthe number of triangles in the graph, and O(min{n^(1/3) log^(2/3) n / (t^(2/3)\n+ 1), n^(1/3)}) with high probability.\n  We also present deterministic algorithms specially suited for sparse graphs.\nIn any graph of maximum degree Delta, we can test for arbitrary subgraphs of\ndiameter D in O(ceil(Delta^(D+1) / n)) rounds. For triangles, we devise an\nalgorithm featuring a round complexity of O(A^2 / n + log_(2+n/A^2) n), where A\ndenotes the arboricity of G."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1203.0429v1", 
    "other_authors": "Theo Dimitrakos, David Brossard, Pierre de Leusse", 
    "title": "Securing business operations in an SOA", 
    "arxiv-id": "1203.0429v1", 
    "author": "Pierre de Leusse", 
    "publish": "2012-03-02T11:51:20Z", 
    "summary": "Service-oriented infrastructures pose new challenges in a number of areas,\nnotably with regard to security and dependability. BT has developed a\ncombination of innovative security solutions and governance frameworks that can\naddress these challenges. They include advances in identity federation;\ndistributed usage and access management; context-aware secure messaging,\nrouting and transformation; and (security) policy governance for\nservice-oriented architectures. This paper discusses these developments and the\nsteps being taken to validate their functionality and performance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1203.0432v1", 
    "other_authors": "Pierre de Leusse, Krzysztof Zielinski", 
    "title": "Toward Governance of Cross-Cloud Application Deployment", 
    "arxiv-id": "1203.0432v1", 
    "author": "Krzysztof Zielinski", 
    "publish": "2012-03-02T12:03:02Z", 
    "summary": "In this article, the authors introduce the main ideas around the governance\nof cross-Cloud application deployment and their related concepts. It is argued\nthat, due to the increasing complexity and nature of the Cloud market, an\nintermediary specialized in brokering the deployment of different components of\na same application onto different Cloud products could both facilitate said\ndeployment and in some cases improve its quality in terms of cost, security &\nreliability and QoS. In order to fulfill these objectives, the authors propose\na high level architecture that relies on their previous work on governance of\npolicy & rule driven distributed systems. This architecture aims at supplying\nfive main functions of 1) translation of Service Level Agreements (SLAs) and\npricing into a common shared DSL, 2) correlation of analytical data (e.g.\nmonitoring, metering), 3) combination of Cloud products, 4) information from\nthird parties regarding different aspects of Quality of Service (QoS) and 5)\ncross-Cloud application deployment specification and governance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1203.0435v1", 
    "other_authors": "Pierre de Leusse, Bartosz Kwolek, Krzysztof Zielinski", 
    "title": "A common interface for multi-rule-engine distributed systems", 
    "arxiv-id": "1203.0435v1", 
    "author": "Krzysztof Zielinski", 
    "publish": "2012-03-02T12:07:57Z", 
    "summary": "The rule technological landscape is becoming ever more complex, with an\nextended number of specifications and products. It is therefore becoming\nincreasingly difficult to integrate rule-driven components and manage\ninteroperability in multi-rule engine environments. The described work presents\nthe possibility to provide a common interface for rule-driven components in a\ndistributed system. The authors' approach leverages on a set of discovery\nprotocol, rule interchange and user interface to alleviate the environment's\ncomplexity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0443v1", 
    "other_authors": "Pierre de Leusse, Panos Periorellis, Paul Watson, Andreas Maierhofer", 
    "title": "Secure & Rapid Composition of Infrastructure Services in the Cloud", 
    "arxiv-id": "1203.0443v1", 
    "author": "Andreas Maierhofer", 
    "publish": "2012-03-02T12:25:09Z", 
    "summary": "A fundamental ambition of grid and distributed systems is to be capable of\nsustaining evolution and allowing for adaptability ((F. Losavio et al., 2002),\n(S. Radhakrishnan, 2005)). Furthermore, as the complexity and sophistication of\ntheses structures increases, so does the need for adaptability of each\ncomponent. One of the primary benefits of service oriented architecture (SOA)\nis the ability to compose applications, processes or more complex services from\nother services which increases the capacity for adaptation. This document\nproposes a novel infrastructure composition model that aims at increasing the\nadaptability of the capabilities exposed through it by dynamically managing\ntheir non functional requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0651v1", 
    "other_authors": "Nikzad Babaii Rizvandi, Albert Y. Zomaya, Ali Javadzadeh Boloori, Javid Taheri", 
    "title": "On Modeling Dependency between MapReduce Configuration Parameters and   Total Execution Time", 
    "arxiv-id": "1203.0651v1", 
    "author": "Javid Taheri", 
    "publish": "2012-03-03T13:18:51Z", 
    "summary": "In this paper, we propose an analytical method to model the dependency\nbetween configuration parameters and total execution time of Map-Reduce\napplications. Our approach has three key phases: profiling, modeling, and\nprediction. In profiling, an application is run several times with different\nsets of MapReduce configuration parameters to profile the execution time of the\napplication on a given platform. Then in modeling, the relation between these\nparameters and total execution time is modeled by multivariate linear\nregression. Among the possible configuration parameters, two main parameters\nhave been used in this study: the number of Mappers, and the number of\nReducers. For evaluation, two standard applications (WordCount, and Exim\nMainlog parsing) are utilized to evaluate our technique on a 4-node MapReduce\nplatform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0740v1", 
    "other_authors": "Bo Li, Yijian Pei, Bin Shen, Hao Wu, Min He, Jundong Yang", 
    "title": "Resource Availability-Aware Advance Reservation for Parallel Jobs with   Deadlines", 
    "arxiv-id": "1203.0740v1", 
    "author": "Jundong Yang", 
    "publish": "2012-03-04T14:11:56Z", 
    "summary": "Advance reservation is important to guarantee the quality of services of jobs\nby allowing exclusive access to resources over a defined time interval on\nresources. It is a challenge for the scheduler to organize available resources\nefficiently and to allocate them for parallel AR jobs with deadline constraint\nappropriately. This paper provides a slot-based data structure to organize\navailable resources of multiprocessor systems in a way that enables efficient\nsearch and update operations, and formulates a suite of scheduling policies to\nallocate resources for dynamically arriving AR requests. The performance of the\nscheduling algorithms were investigated by simulations with different job sizes\nand durations, system loads and scheduling flexibilities. Simulation results\nshow that job sizes and durations, system load and the flexibility of\nscheduling will impact the performance metrics of all the scheduling\nalgorithms, and the PE-Worst-Fit algorithm becomes the best algorithm for the\nscheduler with the highest acceptance rate of AR requests, and the jobs with\nthe First-Fit algorithm experience the lowest average slowdown. The data\nstructure and scheduling policies can be used to organize and allocate\nresources for parallel AR jobs with deadline constraint in large-scale\ncomputing systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.1466v1", 
    "other_authors": "Riccardo Murri, Sergio Maffioletti", 
    "title": "Batch-oriented software appliances", 
    "arxiv-id": "1203.1466v1", 
    "author": "Sergio Maffioletti", 
    "publish": "2012-03-07T13:45:24Z", 
    "summary": "This paper presents AppPot, a system for creating Linux software appliances.\nAppPot can be run as a regular batch or grid job and executed in user space,\nand requires no special virtualization support in the infrastructure.\n  The main design goal of AppPot is to bring the benefits of a\nvirtualization-based IaaS cloud to existing batch-oriented computing\ninfrastructures.\n  In particular, AppPot addresses the application deployment and configuration\non large heterogeneous computing infrastructures: users are enabled to prepare\ntheir own customized virtual appliance for providing a safe execution\nenvironment for their applications. These appliances can then be executed on\nvirtually any computing infrastructure being in a private or public cloud as\nwell as any batch-controlled computing clusters the user may have access to.\n  We give an overview of AppPot and its features, the technology that makes it\npossible, and report on experiences running it in production use within the\nSwiss National Grid infrastructure SMSCG."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.1888v1", 
    "other_authors": "Nitin Vaidya", 
    "title": "Matrix Representation of Iterative Approximate Byzantine Consensus in   Directed Graphs", 
    "arxiv-id": "1203.1888v1", 
    "author": "Nitin Vaidya", 
    "publish": "2012-03-08T19:00:19Z", 
    "summary": "This paper presents a proof of correctness of an iterative approximate\nByzantine consensus (IABC) algorithm for directed graphs. The iterative\nalgorithm allows fault- free nodes to reach approximate conensus despite the\npresence of up to f Byzantine faults. Necessary conditions on the underlying\nnetwork graph for the existence of a correct IABC algorithm were shown in our\nrecent work [15, 16]. [15] also analyzed a specific IABC algorithm and showed\nthat it performs correctly in any network graph that satisfies the necessary\ncondition, proving that the necessary condition is also sufficient. In this\npaper, we present an alternate proof of correctness of the IABC algorithm,\nusing a familiar technique based on transition matrices [9, 3, 17, 19].\n  The key contribution of this paper is to exploit the following observation:\nfor a given evolution of the state vector corresponding to the state of the\nfault-free nodes, many alternate state transition matrices may be chosen to\nmodel that evolution cor- rectly. For a given state evolution, we identify one\napproach to suitably \"design\" the transition matrices so that the standard\ntools for proving convergence can be applied to the Byzantine fault-tolerant\nalgorithm as well. In particular, the transition matrix for each iteration is\ndesigned such that each row of the matrix contains a large enough number of\nelements that are bounded away from 0."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.2081v2", 
    "other_authors": "Matthew Felice Pace", 
    "title": "BSP vs MapReduce", 
    "arxiv-id": "1203.2081v2", 
    "author": "Matthew Felice Pace", 
    "publish": "2012-03-09T13:42:03Z", 
    "summary": "The MapReduce framework has been generating a lot of interest in a wide range\nof areas. It has been widely adopted in industry and has been used to solve a\nnumber of non-trivial problems in academia. Putting MapReduce on strong\ntheoretical foundations is crucial in understanding its capabilities. This work\nlinks MapReduce to the BSP model of computation, underlining the relevance of\nBSP to modern parallel algorithm design and defining a subclass of BSP\nalgorithms that can be efficiently implemented in MapReduce."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.2366v1", 
    "other_authors": "Franck Michel, Johan Montagnat, Tristan Glatard", 
    "title": "Technical support for Life Sciences communities on a production grid   infrastructure", 
    "arxiv-id": "1203.2366v1", 
    "author": "Tristan Glatard", 
    "publish": "2012-03-11T19:22:51Z", 
    "summary": "Production operation of large distributed computing infrastructures (DCI)\nstill requires a lot of human intervention to reach acceptable quality of\nservice. This may be achievable for scientific communities with solid IT\nsupport, but it remains a show-stopper for others. Some application execution\nenvironments are used to hide runtime technical issues from end users. But they\nmostly aim at fault-tolerance rather than incident resolution, and their\noperation still requires substantial manpower. A longer-term support activity\nis thus needed to ensure sustained quality of service for Virtual Organisations\n(VO). This paper describes how the biomed VO has addressed this challenge by\nsetting up a technical support team. Its organisation, tooling, daily tasks,\nand procedures are described. Results are shown in terms of resource usage by\nend users, amount of reported incidents, and developed software tools. Based on\nour experience, we suggest ways to measure the impact of the technical support,\nperspectives to decrease its human cost and make it more community-specific."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3013v1", 
    "other_authors": "Marin Bertier, Marko Obrovac, C\u00e9dric Tedeschi", 
    "title": "A Protocol for the Atomic Capture of Multiple Molecules at Large Scale", 
    "arxiv-id": "1203.3013v1", 
    "author": "C\u00e9dric Tedeschi", 
    "publish": "2012-03-14T07:30:11Z", 
    "summary": "With the rise of service-oriented computing, applications are more and more\nbased on coordination of autonomous services. Envisioned over largely\ndistributed and highly dynamic platforms, expressing this coordination calls\nfor alternative programming models. The chemical programming paradigm, which\nmodels applications as chemical solutions where molecules representing digital\nentities involved in the computation, react together to produce a result, has\nbeen recently shown to provide the needed abstractions for autonomic\ncoordination of services. However, the execution of such programs over large\nscale platforms raises several problems hindering this paradigm to be actually\nleveraged. Among them, the atomic capture of molecules participating in concur-\nrent reactions is one of the most significant. In this paper, we propose a\nprotocol for the atomic capture of these molecules distributed and evolving\nover a large scale platform. As the density of possible reactions is crucial\nfor the liveness and efficiency of such a capture, the protocol proposed is\nmade up of two sub-protocols, each of them aimed at addressing different levels\nof densities of potential reactions in the solution. While the decision to\nchoose one or the other is local to each node participating in a program's\nexecution, a global coherent behaviour is obtained. Proof of liveness, as well\nas intensive simulation results showing the efficiency and limited overhead of\nthe protocol are given."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3098v1", 
    "other_authors": "Ahmet A. Husainov, Ekaterina S. Kudryashova", 
    "title": "Generalized Asynchronous Systems", 
    "arxiv-id": "1203.3098v1", 
    "author": "Ekaterina S. Kudryashova", 
    "publish": "2012-03-14T14:37:25Z", 
    "summary": "The paper is devoted to a mathematical model of concurrency the special case\nof which is asynchronous system. Distributed asynchronous automata are\nintroduced here. It is proved that the Petri nets and transition systems with\nindependence can be considered like the distributed asynchronous automata. Time\ndistributed asynchronous automata are defined in standard way by the map which\nassigns time intervals to events. It is proved that the time distributed\nasynchronous automata are generalized the time Petri nets and asynchronous\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3575v1", 
    "other_authors": "Swan Dubois, S\u00e9bastien Tixeuil, Nini Zhu", 
    "title": "The Byzantine Brides Problem", 
    "arxiv-id": "1203.3575v1", 
    "author": "Nini Zhu", 
    "publish": "2012-03-15T21:26:34Z", 
    "summary": "We investigate the hardness of establishing as many stable marriages (that\nis, marriages that last forever) in a population whose memory is placed in some\narbitrary state with respect to the considered problem, and where traitors try\nto jeopardize the whole process by behaving in a harmful manner. On the\nnegative side, we demonstrate that no solution that is completely insensitive\nto traitors can exist, and we propose a protocol for the problem that is\noptimal with respect to the traitor containment radius."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3885v1", 
    "other_authors": "Bogdan Alexandru Caprarescu, Dana Petcu", 
    "title": "Decentralized Probabilistic Auto-Scaling for Heterogeneous Systems", 
    "arxiv-id": "1203.3885v1", 
    "author": "Dana Petcu", 
    "publish": "2012-03-17T18:48:23Z", 
    "summary": "The DEPAS (Decentralized Probabilistic Auto-Scaling) algorithm assumes an\noverlay network of computing nodes where each node probabilistically decides to\nshut down, allocate one or more other nodes or do nothing. DEPAS was\nformulated, tested, and theoretically analyzed for the simplified case of\nhomogenous systems. In this paper, we extend DEPAS to heterogeneous systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4257v1", 
    "other_authors": "Mahdi Abdelkafi, Lotfi Bouzguenda, Faiez Gargouri", 
    "title": "DiscopFlow: A new Tool for Discovering Organizational Structures and   Interaction Protocols in WorkFlow", 
    "arxiv-id": "1203.4257v1", 
    "author": "Faiez Gargouri", 
    "publish": "2012-03-19T20:53:03Z", 
    "summary": "This work deals with Workflow Mining (WM) a very active and promising\nresearch area. First, in this paper we give a critical and comparative study of\nthree representative WM systems of this area: the ProM, InWolve and\nWorkflowMiner systems. The comparison is made according to quality criteria\nthat we have defined such as the capacity to filter and convert a Workflow log,\nthe capacity to discover workflow perspectives and the capacity to support\nMulti-Analysis of processes. The major drawback of these systems is the non\npossibility to deal with organizational perspective discovering issue. We mean\nby organizational perspective, the organizational structures (federation,\ncoalition, market or hierarchy) and interaction protocols (contract net,\nauction or vote). This paper defends the idea that organizational dimension in\nMulti-Agent System is an appropriate approach to support the discovering of\nthis organizational perspective. Second, the paper proposes a Workflow log\nmeta-model which extends the classical one by considering the interactions\namong actors thanks to the FIPA-ACL Performatives. Third, it describes in\ndetails our DiscopFlow tool which validates our contribution."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4367v1", 
    "other_authors": "Hamidreza Barati, Nasrin Jaberi", 
    "title": "Thesis Report: Resource Utilization Provisioning in MapReduce", 
    "arxiv-id": "1203.4367v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-03-20T10:06:24Z", 
    "summary": "In this thesis report, we have a survey on state-of-the-art methods for\nmodelling resource utilization of MapReduce applications regard to its\nconfiguration parameters. After implementation of one of the algorithms in\nliterature, we tried to find that if CPU usage modelling of a MapReduce\napplication can be used to predict CPU usage of another MapReduce application."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4751v8", 
    "other_authors": "Vincent Gramoli, Petr Kuznetsov, Srivatsan Ravi", 
    "title": "Optimism for Boosting Concurrency", 
    "arxiv-id": "1203.4751v8", 
    "author": "Srivatsan Ravi", 
    "publish": "2012-03-21T14:44:40Z", 
    "summary": "Modern concurrent programming benefits from a large variety of\nsynchronization techniques. These include conventional pessimistic locking, as\nwell as optimistic techniques based on conditional synchronization primitives\nor transactional memory. Yet, it is unclear which of these approaches better\nleverage the concurrency inherent to multi-cores.\n  In this paper, we compare the level of concurrency one can obtain by\nconverting a sequential program into a concurrent one using optimistic or\npessimistic techniques. To establish fair comparison of such implementations,\nwe introduce a new correctness criterion for concurrent programs, defined\nindependently of the synchronization techniques they use.\n  We treat a program's concurrency as its ability to accept a concurrent\nschedule, a metric inspired by the theories of both databases and transactional\nmemory. We show that pessimistic locking can provide strictly higher\nconcurrency than transactions for some applications whereas transactions can\nprovide strictly higher concurrency than pessimistic locks for others. Finally,\nwe show that combining the benefits of the two synchronization techniques can\nprovide strictly more concurrency than any of them individually. We propose a\nlist-based set algorithm that is optimal in the sense that it accepts all\ncorrect concurrent schedules. As we show via experimentation, the optimality in\nterms of concurrency is reflected by scalability gains."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4938v2", 
    "other_authors": "Luis Cabellos", 
    "title": "Advanced Programming Platform for efficient use of Data Parallel   Hardware", 
    "arxiv-id": "1203.4938v2", 
    "author": "Luis Cabellos", 
    "publish": "2012-03-22T09:54:58Z", 
    "summary": "Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5004v2", 
    "other_authors": "Colm O. Dunlaing", 
    "title": "CUDA implementation of Wagener's 2D convex hull PRAM algorithm", 
    "arxiv-id": "1203.5004v2", 
    "author": "Colm O. Dunlaing", 
    "publish": "2012-03-22T14:30:25Z", 
    "summary": "This paper describes a CUDA implementation of Wagener's PRAM convex hull\nalgorithm in two dimensions. It is presented in Knuth's literate programming\nstyle."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5160v2", 
    "other_authors": "Nikzad Babaii Rizvandi, Albert Y. Zomaya, Young Choon Lee, Ali Javadzadeh Boloori, Javid Taheri", 
    "title": "Multiple Frequency Selection in DVFS-Enabled Processors to Minimize   Energy Consumption", 
    "arxiv-id": "1203.5160v2", 
    "author": "Javid Taheri", 
    "publish": "2012-03-23T02:42:38Z", 
    "summary": "In this chapter we focus on slack reclamation and propose a new slack\nreclamation technique, Multiple Frequency Selection DVFS (MFS-DVFS). The key\nidea is to execute each task with a linear combination of more than one\nfrequency such that this combination results in using the lowest energy by\ncovering the whole slack time of the task. We have tested our algorithm with\nboth random and real-world application task graphs and compared with the\nresults in previous researches in [9] and [12-13]. The experimental results\nshow that our approach can achieve energy almost identical to the optimum\nenergy saving."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5196v1", 
    "other_authors": "Rajkumar Buyya, Suraj Pandey, Christian Vecchiola", 
    "title": "Market-Oriented Cloud Computing and the Cloudbus Toolkit", 
    "arxiv-id": "1203.5196v1", 
    "author": "Christian Vecchiola", 
    "publish": "2012-03-23T08:50:57Z", 
    "summary": "Cloud computing has penetrated the Information Technology industry deep\nenough to influence major companies to adopt it into their mainstream business.\nA strong thrust on the use of virtualization technology to realize\nInfrastructure-as-a-Service (IaaS) has led enterprises to leverage\nsubscription-oriented computing capabilities of public Clouds for hosting their\napplication services. In parallel, research in academia has been investigating\ntransversal aspects such as security, software frameworks, quality of service,\nand standardization. We believe that the complete realization of the Cloud\ncomputing vision will lead to the introduction of a virtual market where Cloud\nbrokers, on behalf of end users, are in charge of selecting and composing the\nservices advertised by different Cloud vendors. In order to make this happen,\nexisting solutions and technologies have to be redesigned and extended from a\nmarket-oriented perspective and integrated together, giving rise to what we\nterm Market-Oriented Cloud Computing.\n  In this paper, we will assess the current status of Cloud computing by\nproviding a reference model, discuss the challenges that researchers and IT\npractitioners are facing and will encounter in the near future, and present the\napproach for solving them from the perspective of the Cloudbus toolkit, which\ncomprises of a set of technologies geared towards the realization of Market\nOriented Cloud Computing vision. We provide experimental results demonstrating\nmarket-oriented resource provisioning and brokering within a Cloud and across\nmultiple distributed resources. We also include an application illustrating the\nhosting of ECG analysis as SaaS on Amazon IaaS (EC2 and S3) services."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.6096v1", 
    "other_authors": "Yehuda Afek, Eli Gafni", 
    "title": "Asynchrony from Synchrony", 
    "arxiv-id": "1203.6096v1", 
    "author": "Eli Gafni", 
    "publish": "2012-03-27T22:26:02Z", 
    "summary": "We consider synchronous dynamic networks which like radio networks may have\nasymmetric communication links, and are affected by communication rather than\nprocessor failures. In this paper we investigate the minimal message\nsurvivability in a per round basis that allows for the minimal global\ncooperation, i.e., allows to solve any task that is wait-free read-write\nsolvable. The paper completely characterizes this survivability requirement.\nMessage survivability is formalized by considering adversaries that have a\nlimited power to remove messages in a round. Removal of a message on a link in\none direction does not necessarily imply the removal of the message on that\nlink in the other direction. Surprisingly there exist a single strongest\nadversary which solves any wait-free read/write task. Any different adversary\nthat solves any wait-free read/write task is weaker, and any stronger adversary\nwill not solve any wait-free read/write task. ABD \\cite{ABD} who considered\nprocessor failure, arrived at an adversary that is $n/2$ resilient,\nconsequently can solve tasks, such as $n/2$-set-consensus, which are not\nread/write wait-free solvable. With message adversaries, we arrive at an\nadversary which has exactly the read-write wait-free power. Furthermore, this\nadversary allows for a considerably simpler (simplest that we know of) proof\nthat the protocol complex of any read/write wait-free task is a subdivided\nsimplex, finally making this proof accessible for students with no\nalgebraic-topology prerequisites, and alternatively dispensing with the\nassumption that the Immediate Snapshot complex is a subdivided simplex."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1302.1939v1", 
    "other_authors": "R. Sobie, A. Agarwal, I. Gable, C. Leavett-Brown, M. Paterson, R. Taylor, A. Charbonneau, R. Impey, W. Podiama", 
    "title": "HTC Scientific Computing in a Distributed Cloud Environment", 
    "arxiv-id": "1302.1939v1", 
    "author": "W. Podiama", 
    "publish": "2013-02-08T04:29:16Z", 
    "summary": "This paper describes the use of a distributed cloud computing system for\nhigh-throughput computing (HTC) scientific applications. The distributed cloud\ncomputing system is composed of a number of separate\nInfrastructure-as-a-Service (IaaS) clouds that are utilized in a unified\ninfrastructure. The distributed cloud has been in production-quality operation\nfor two years with approximately 500,000 completed jobs where a typical\nworkload has 500 simultaneous embarrassingly-parallel jobs that run for\napproximately 12 hours. We review the design and implementation of the system\nwhich is based on pre-existing components and a number of custom components. We\ndiscuss the operation of the system, and describe our plans for the expansion\nto more sites and increased computing capacity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2203v1", 
    "other_authors": "Zheng Li, Liam O'Brien, He Zhang, Rainbow Cai", 
    "title": "A Factor Framework for Experimental Design for Performance Evaluation of   Commercial Cloud Services", 
    "arxiv-id": "1302.2203v1", 
    "author": "Rainbow Cai", 
    "publish": "2013-02-09T06:18:04Z", 
    "summary": "Given the diversity of commercial Cloud services, performance evaluations of\ncandidate services would be crucial and beneficial for both service customers\n(e.g. cost-benefit analysis) and providers (e.g. direction of service\nimprovement). Before an evaluation implementation, the selection of suitable\nfactors (also called parameters or variables) plays a prerequisite role in\ndesigning evaluation experiments. However, there seems a lack of systematic\napproaches to factor selection for Cloud services performance evaluation. In\nother words, evaluators randomly and intuitively concerned experimental factors\nin most of the existing evaluation studies. Based on our previous taxonomy and\nmodeling work, this paper proposes a factor framework for experimental design\nfor performance evaluation of commercial Cloud services. This framework\ncapsules the state-of-the-practice of performance evaluation factors that\npeople currently take into account in the Cloud Computing domain, and in turn\ncan help facilitate designing new experiments for evaluating Cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2217v1", 
    "other_authors": "Swan Dubois, Rachid Guerraoui", 
    "title": "Introducing Speculation in Self-Stabilization - An Application to Mutual   Exclusion", 
    "arxiv-id": "1302.2217v1", 
    "author": "Rachid Guerraoui", 
    "publish": "2013-02-09T11:09:22Z", 
    "summary": "Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits. Speculation consists in\nguaranteeing that the system satisfies its requirements for any execution but\nexhibits significantly better performances for a subset of executions that are\nmore probable. A speculative protocol is in this sense supposed to be both\nrobust and efficient in practice. We introduce the notion of speculative\nstabilization which we illustrate through the mutual exclusion problem. We then\npresent a novel speculatively stabilizing mutual exclusion protocol. Our\nprotocol is self-stabilizing for any asynchronous execution. We prove that its\nstabilization time for synchronous executions is diam(g)/2 steps (where diam(g)\ndenotes the diameter of the system). This complexity result is of independent\ninterest. The celebrated mutual exclusion protocol of Dijkstra stabilizes in n\nsteps (where n is the number of processes) in synchronous executions and the\nquestion whether the stabilization time could be strictly smaller than the\ndiameter has been open since then (almost 40 years). We show that this is\nindeed possible for any underlying topology. We also provide a lower bound\nproof that shows that our new stabilization time of diam(g)/2 steps is optimal\nfor synchronous executions, even if asynchronous stabilization is not required."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2227v1", 
    "other_authors": "Sina Esfandiarpoor, Ali Pahlavan, Maziar Goudarzi", 
    "title": "Virtual Machine Consolidation for Datacenter Energy Improvement", 
    "arxiv-id": "1302.2227v1", 
    "author": "Maziar Goudarzi", 
    "publish": "2013-02-09T12:23:21Z", 
    "summary": "Rapid growth and proliferation of cloud computing services around the world\nhas increased the necessity and significance of improving the energy efficiency\nof could implementations. Virtual machines (VM) comprise the backend of most,\nif not all, cloud computing services. Several VMs are often consolidated on a\nphysical machine to better utilize its resources. We take into account the\ncooling and network structure of the datacenter hosting the physical machines\nwhen consolidating the VMs so that fewer racks and routers are employed,\nwithout compromising the service-level agreements, so that unused routing and\ncooling equipment can be turned off to reduce energy consumption. Our\nexperimental results on four benchmarks shows that our technique improves\nenergy consumption of servers, network equipment, and cooling systems by 2.5%,\n18.8%, and 28.2% respectively, resulting in a total of 14.7% improvement on\naverage in the entire datacenter."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2529v2", 
    "other_authors": "Tyanko Aleksiev, Simon Barkow, Peter Kunszt, Sergio Maffioletti, Riccardo Murri, Christian Panse", 
    "title": "VM-MAD: a cloud/cluster software for service-oriented academic   environments", 
    "arxiv-id": "1302.2529v2", 
    "author": "Christian Panse", 
    "publish": "2013-02-11T16:49:49Z", 
    "summary": "The availability of powerful computing hardware in IaaS clouds makes cloud\ncomputing attractive also for computational workloads that were up to now\nalmost exclusively run on HPC clusters.\n  In this paper we present the VM-MAD Orchestrator software: an open source\nframework for cloudbursting Linux-based HPC clusters into IaaS clouds but also\ncomputational grids. The Orchestrator is completely modular, allowing flexible\nconfigurations of cloudbursting policies. It can be used with any batch system\nor cloud infrastructure, dynamically extending the cluster when needed. A\ndistinctive feature of our framework is that the policies can be tested and\ntuned in a simulation mode based on historical or synthetic cluster accounting\ndata.\n  In the paper we also describe how the VM-MAD Orchestrator was used in a\nproduction environment at the FGCZ to speed up the analysis of mass\nspectrometry-based protein data by cloudbursting to the Amazon EC2. The\nadvantages of this hybrid system are shown with a large evaluation run using\nabout hundred large EC2 nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2543v1", 
    "other_authors": "Nitin H. Vaidya, Vijay K. Garg", 
    "title": "Byzantine Vector Consensus in Complete Graphs", 
    "arxiv-id": "1302.2543v1", 
    "author": "Vijay K. Garg", 
    "publish": "2013-02-11T17:25:24Z", 
    "summary": "Consider a network of n processes each of which has a d-dimensional vector of\nreals as its input. Each process can communicate directly with all the\nprocesses in the system; thus the communication network is a complete graph.\nAll the communication channels are reliable and FIFO (first-in-first-out). The\nproblem of Byzantine vector consensus (BVC) requires agreement on a\nd-dimensional vector that is in the convex hull of the d-dimensional input\nvectors at the non-faulty processes. We obtain the following results for\nByzantine vector consensus in complete graphs while tolerating up to f\nByzantine failures:\n  * We prove that in a synchronous system, n >= max(3f+1, (d+1)f+1) is\nnecessary and sufficient for achieving Byzantine vector consensus.\n  * In an asynchronous system, it is known that exact consensus is impossible\nin presence of faulty processes. For an asynchronous system, we prove that n >=\n(d+2)f+1 is necessary and sufficient to achieve approximate Byzantine vector\nconsensus.\n  Our sufficiency proofs are constructive. We show sufficiency by providing\nexplicit algorithms that solve exact BVC in synchronous systems, and\napproximate BVC in asynchronous systems.\n  We also obtain tight bounds on the number of processes for achieving BVC\nusing algorithms that are restricted to a simpler communication pattern."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2749v2", 
    "other_authors": "Mario Pastorelli, Antonio Barbuzzi, Damiano Carra, Matteo Dell'Amico, Pietro Michiardi", 
    "title": "Practical Size-based Scheduling for MapReduce Workloads", 
    "arxiv-id": "1302.2749v2", 
    "author": "Pietro Michiardi", 
    "publish": "2013-02-12T10:11:29Z", 
    "summary": "We present the Hadoop Fair Sojourn Protocol (HFSP) scheduler, which\nimplements a size-based scheduling discipline for Hadoop. The benefits of\nsize-based scheduling disciplines are well recognized in a variety of contexts\n(computer networks, operating systems, etc...), yet, their practical\nimplementation for a system such as Hadoop raises a number of important\nchallenges. With HFSP, which is available as an open-source project, we address\nissues related to job size estimation, resource management and study the\neffects of a variety of preemption strategies. Although the architecture\nunderlying HFSP is suitable for any size-based scheduling discipline, in this\nwork we revisit and extend the Fair Sojourn Protocol, which solves problems\nrelated to job starvation that affect FIFO, Processor Sharing and a range of\nsize-based disciplines. Our experiments, in which we compare HFSP to standard\nHadoop schedulers, pinpoint at a significant decrease in average job sojourn\ntimes - a metric that accounts for the total time a job spends in the system,\nincluding waiting and serving times - for realistic workloads that we generate\naccording to production traces available in literature."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.3344v2", 
    "other_authors": "Runhui Li, Jian Lin, Patrick P. C. Lee", 
    "title": "CORE: Augmenting Regenerating-Coding-Based Recovery for Single and   Concurrent Failures in Distributed Storage Systems", 
    "arxiv-id": "1302.3344v2", 
    "author": "Patrick P. C. Lee", 
    "publish": "2013-02-14T09:08:38Z", 
    "summary": "Data availability is critical in distributed storage systems, especially when\nnode failures are prevalent in real life. A key requirement is to minimize the\namount of data transferred among nodes when recovering the lost or unavailable\ndata of failed nodes. This paper explores recovery solutions based on\nregenerating codes, which are shown to provide fault-tolerant storage and\nminimum recovery bandwidth. Existing optimal regenerating codes are designed\nfor single node failures. We build a system called CORE, which augments\nexisting optimal regenerating codes to support a general number of failures\nincluding single and concurrent failures. We theoretically show that CORE\nachieves the minimum possible recovery bandwidth for most cases. We implement\nCORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to\n20 storage nodes. We demonstrate that our CORE prototype conforms to our\ntheoretical findings and achieves recovery bandwidth saving when compared to\nthe conventional recovery approach based on erasure codes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.3752v2", 
    "other_authors": "Guillaume Aupy, Yves Robert, Fr\u00e9d\u00e9ric Vivien, Dounia Zaidouni", 
    "title": "Checkpointing algorithms and fault prediction", 
    "arxiv-id": "1302.3752v2", 
    "author": "Dounia Zaidouni", 
    "publish": "2013-02-15T13:52:50Z", 
    "summary": "This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We extend the classical first-order analysis of Young\nand Daly in the presence of a fault prediction system, characterized by its\nrecall and its precision. In this framework, we provide an optimal algorithm to\ndecide when to take predictions into account, and we derive the optimal value\nof the checkpointing period. These results allow to analytically assess the key\nparameters that impact the performance of fault predictors at very large scale."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4059v1", 
    "other_authors": "Tomasz Jurdzinski, Dariusz R. Kowalski, Grzegorz Stachowiak", 
    "title": "Distributed Deterministic Broadcasting in Uniform-Power Ad Hoc Wireless   Networks", 
    "arxiv-id": "1302.4059v1", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2013-02-17T11:36:51Z", 
    "summary": "Development of many futuristic technologies, such as MANET, VANET, iThings,\nnano-devices, depend on efficient distributed communication protocols in\nmulti-hop ad hoc networks. A vast majority of research in this area focus on\ndesign heuristic protocols, and analyze their performance by simulations on\nnetworks generated randomly or obtained in practical measurements of some\n(usually small-size) wireless networks. %some library. Moreover, they often\nassume access to truly random sources, which is often not reasonable in case of\nwireless devices. In this work we use a formal framework to study the problem\nof broadcasting and its time complexity in any two dimensional Euclidean\nwireless network with uniform transmission powers. For the analysis, we\nconsider two popular models of ad hoc networks based on the\nSignal-to-Interference-and-Noise Ratio (SINR): one with opportunistic links,\nand the other with randomly disturbed SINR. In the former model, we show that\none of our algorithms accomplishes broadcasting in $O(D\\log^2 n)$ rounds, where\n$n$ is the number of nodes and $D$ is the diameter of the network. If nodes\nknow a priori the granularity $g$ of the network, i.e., the inverse of the\nmaximum transmission range over the minimum distance between any two stations,\na modification of this algorithm accomplishes broadcasting in $O(D\\log g)$\nrounds.\n  Finally, we modify both algorithms to make them efficient in the latter model\nwith randomly disturbed SINR, with only logarithmic growth of performance.\n  Ours are the first provably efficient and well-scalable, under the two\nmodels, distributed deterministic solutions for the broadcast task."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4414v2", 
    "other_authors": "Olivier Beaumont, Philippe Duchon, Paul Renaud-Goud", 
    "title": "Approximation Algorithms for Energy Minimization in Cloud Service   Allocation under Reliability Constraints", 
    "arxiv-id": "1302.4414v2", 
    "author": "Paul Renaud-Goud", 
    "publish": "2013-02-18T20:33:15Z", 
    "summary": "We consider allocation problems that arise in the context of service\nallocation in Clouds. More specifically, we assume on the one part that each\ncomputing resource is associated to a capacity constraint, that can be chosen\nusing Dynamic Voltage and Frequency Scaling (DVFS) method, and to a probability\nof failure. On the other hand, we assume that the service runs as a set of\nindependent instances of identical Virtual Machines. Moreover, there exists a\nService Level Agreement (SLA) between the Cloud provider and the client that\ncan be expressed as follows: the client comes with a minimal number of service\ninstances which must be alive at the end of the day, and the Cloud provider\noffers a list of pairs (price,compensation), this compensation being paid by\nthe Cloud provider if it fails to keep alive the required number of services.\nOn the Cloud provider side, each pair corresponds actually to a guaranteed\nsuccess probability of fulfilling the constraint on the minimal number of\ninstances. In this context, given a minimal number of instances and a\nprobability of success, the question for the Cloud provider is to find the\nnumber of necessary resources, their clock frequency and an allocation of the\ninstances (possibly using replication) onto machines. This solution should\nsatisfy all types of constraints during a given time period while minimizing\nthe energy consumption of used resources. We consider two energy consumption\nmodels based on DVFS techniques, where the clock frequency of physical\nresources can be changed. For each allocation problem and each energy model, we\nprove deterministic approximation ratios on the consumed energy for algorithms\nthat provide guaranteed probability failures, as well as an efficient\nheuristic, whose energy ratio is not guaranteed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4558v1", 
    "other_authors": "Guillaume Aupy, Yves Robert, Fr\u00e9d\u00e9ric Vivien, Dounia Zaidouni", 
    "title": "Checkpointing strategies with prediction windows", 
    "arxiv-id": "1302.4558v1", 
    "author": "Dounia Zaidouni", 
    "publish": "2013-02-19T09:50:38Z", 
    "summary": "This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We suppose that the fault-prediction system provides\nprediction windows instead of exact predictions, which dramatically complicates\nthe analysis of the checkpointing strategies. We propose a new approach based\nupon two periodic modes, a regular mode outside prediction windows, and a\nproactive mode inside prediction windows, whenever the size of these windows is\nlarge enough. We are able to compute the best period for any size of the\nprediction windows, thereby deriving the scheduling strategy that minimizes\nplatform waste. In addition, the results of this analytical evaluation are\nnicely corroborated by a comprehensive set of simulations, which demonstrate\nthe validity of the model and the accuracy of the approach."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4779v1", 
    "other_authors": "Charng-Da Lu", 
    "title": "Failure Data Analysis of HPC Systems", 
    "arxiv-id": "1302.4779v1", 
    "author": "Charng-Da Lu", 
    "publish": "2013-02-20T00:21:44Z", 
    "summary": "Continuous availability of HPC systems built from commodity components have\nbecome a primary concern as system size grows to thousands of processors. In\nthis paper, we present the analysis of 8-24 months of real failure data\ncollected from three HPC systems at the National Center for Supercomputing\nApplications (NCSA) during 2001-2004. The results show that the availability is\n98.7-99.8% and most outages are due to software halts. On the other hand, the\ndowntime are mostly contributed by hardware halts or scheduled maintenance. We\nalso used failure clustering analysis to identify several correlated failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.4808v3", 
    "other_authors": "Christian Cachin, Olga Ohrimenko", 
    "title": "Verifying the Consistency of Remote Untrusted Services with Commutative   Operations", 
    "arxiv-id": "1302.4808v3", 
    "author": "Olga Ohrimenko", 
    "publish": "2013-02-20T05:22:35Z", 
    "summary": "A group of mutually trusting clients outsources a computation service to a\nremote server, which they do not fully trust and that may be subject to\nattacks. The clients do not communicate with each other and would like to\nverify the correctness of the remote computation and the consistency of the\nserver's responses. This paper first presents the Commutative-Operation\nverification Protocol (COP) that ensures linearizability when the server is\ncorrect and preserves fork-linearizability in any other case. All clients that\nobserve each other's operations are consistent, in the sense that their own\noperations and those operations of other clients that they see are\nlinearizable. Second, this work extends COP through authenticated data\nstructures to Authenticated COP, which allows consistency verification of\noutsourced services whose state is kept only remotely, by the server. This\nyields the first fork-linearizable consistency verification protocol for\ngeneric outsourced services that (1) relieves clients from storing the state,\n(2) supports wait-free client operations, and (3) handles sequences of\narbitrary commutative operations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5161v3", 
    "other_authors": "Hengfeng Wei, Marzio De Biasi, Yu Huang, Jiannong Cao, Jian Lu", 
    "title": "Verifying PRAM Consistency over Read/Write Traces of Data Replicas", 
    "arxiv-id": "1302.5161v3", 
    "author": "Jian Lu", 
    "publish": "2013-02-21T02:45:25Z", 
    "summary": "Data replication technologies enable efficient and highly-available data\naccess, thus gaining more and more interests in both the academia and the\nindustry. However, data replication introduces the problem of data consistency.\nModern commercial data replication systems often provide weak consistency for\nhigh availability under certain failure scenarios. An important weak\nconsistency is Pipelined-RAM (PRAM) consistency. It allows different processes\nto hold different views of data. To determine whether a data replication system\nindeed provides PRAM consistency, we study the problem of Verifying PRAM\nConsistency over read/write traces (or VPC, for short).\n  We first identify four variants of VPC according to a) whether there are\nMultiple shared variables (or one Single variable), and b) whether write\noperations can assign Duplicate values (or only Unique values) for each shared\nvariable; the four variants are labeled VPC-SU, VPC-MU, VPC-SD, and VPC-MD.\nSecond, we present a simple VPC-MU algorithm, called RW-CLOSURE. It constructs\nan operation graph $\\mathcal{G}$ by iteratively adding edges according to three\nrules. Its time complexity is $O(n^5)$, where n is the number of operations in\nthe trace. Third, we present an improved VPC-MU algorithm, called READ-CENTRIC,\nwith time complexity $O(n^4)$. Basically it attempts to construct the operation\ngraph $\\mathcal{G}$ in an incremental and efficient way. Its correctness is\nbased on that of RW-CLOSURE. Finally, we prove that VPC-SD (so is VPC-MD) is\n$\\sf{NP}$-complete by reducing the strongly $\\sf{NP}$-complete problem\n3-PARTITION to it."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5192v4", 
    "other_authors": "Kyumars Sheykh Esmaili, Lluis Pamies-Juarez, Anwitaman Datta", 
    "title": "The CORE Storage Primitive: Cross-Object Redundancy for Efficient Data   Repair & Access in Erasure Coded Storage", 
    "arxiv-id": "1302.5192v4", 
    "author": "Anwitaman Datta", 
    "publish": "2013-02-21T06:14:04Z", 
    "summary": "Erasure codes are an integral part of many distributed storage systems aimed\nat Big Data, since they provide high fault-tolerance for low overheads.\nHowever, traditional erasure codes are inefficient on reading stored data in\ndegraded environments (when nodes might be unavailable), and on replenishing\nlost data (vital for long term resilience). Consequently, novel codes optimized\nto cope with distributed storage system nuances are vigorously being\nresearched. In this paper, we take an engineering alternative, exploring the\nuse of simple and mature techniques -juxtaposing a standard erasure code with\nRAID-4 like parity. We carry out an analytical study to determine the efficacy\nof this approach over traditional as well as some novel codes. We build upon\nthis study to design CORE, a general storage primitive that we integrate into\nHDFS. We benchmark this implementation in a proprietary cluster and in EC2. Our\nexperiments show that compared to traditional erasure codes, CORE uses 50% less\nbandwidth and is up to 75% faster while recovering a single failed node, while\nthe gains are respectively 15% and 60% for double node failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5481v1", 
    "other_authors": "Harshad B. Prajapati, Vipul K. Dabhi", 
    "title": "Classification and Characterization of Core Grid Protocols for Global   Grid Computing", 
    "arxiv-id": "1302.5481v1", 
    "author": "Vipul K. Dabhi", 
    "publish": "2013-02-22T04:20:36Z", 
    "summary": "Grid computing has attracted many researchers over a few years, and as a\nresult many new protocols have emerged and also evolved since its inception a\ndecade ago. Grid protocols play major role in implementing services that\nfacilitate coordinated resource sharing across diverse organizations. In this\npaper, we provide comprehensive coverage of different core Grid protocols that\ncan be used in Global Grid Computing. We establish the classification of core\nGrid protocols into i) Grid network communication and Grid data transfer\nprotocols, ii) Grid information security protocols, iii) Grid resource\ninformation protocols, iv) Grid management protocols, and v) Grid interface\nprotocols, depending upon the kind of activities handled by these protocols.\nAll the classified protocols are also organized into layers of the Hourglass\nmodel of Grid architecture to understand dependency among these protocols. We\nalso present the characteristics of each protocol. For better understanding of\nthese protocols, we also discuss applied protocols as examples from either\nGlobus toolkit or other popular Grid middleware projects. We believe that our\nclassification and characterization of Grid protocols will enable better\nunderstanding of core Grid protocols and will motivate further research in the\narea of Global Grid Computing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5646v1", 
    "other_authors": "Pranava K. Jha", 
    "title": "Comments on \"Resource placement in Cartesian product of networks\"   [Imani, Sarbazi-Azad and Zomaya, J. Parallel Distrib. Comput., 70 (2010)   481-495]", 
    "arxiv-id": "1302.5646v1", 
    "author": "Pranava K. Jha", 
    "publish": "2013-02-16T17:51:31Z", 
    "summary": "The present note points out a number of errors, omissions, redundancies and\narbitrary deviations from the standard terminology in the paper \"Resource\nplacement in Cartesian product of networks,\" by N. Imani, H. Sarbazi-Azad and\nA.Y. Zomaya [J. Parallel Distrib. Comput. 70 (2010) 481-495]."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5679v1", 
    "other_authors": "Juliana M. N. Silva, Cristina Boeres, L\u00facia M. A. Drummond, Artur A. Pessoa", 
    "title": "Memory Aware Load Balance Strategy on a Parallel Branch-and-Bound   Application", 
    "arxiv-id": "1302.5679v1", 
    "author": "Artur A. Pessoa", 
    "publish": "2013-02-22T19:17:13Z", 
    "summary": "The latest trends in high-performance computing systems show an increasing\ndemand on the use of a large scale multicore systems in a efficient way, so\nthat high compute-intensive applications can be executed reasonably well.\nHowever, the exploitation of the degree of parallelism available at each\nmulticore component can be limited by the poor utilization of the memory\nhierarchy available. Actually, the multicore architecture introduces some\ndistinct features that are already observed in shared memory and distributed\nenvironments. One example is that subsets of cores can share different subsets\nof memory. In order to achieve high performance it is imperative that a careful\nallocation scheme of an application is carried out on the available cores,\nbased on a scheduling model that considers the main performance bottlenecks, as\nfor example, memory contention. In this paper, the {\\em Multicore Cluster\nModel} (MCM) is proposed, which captures the most relevant performance\ncharacteristics in multicores systems such as the influence of memory hierarchy\nand contention. Better performance was achieved when a load balance strategy\nfor a Branch-and-Bound application applied to the Partitioning Sets Problem is\nbased on MCM, showing its efficiency and applicability to modern systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5999v1", 
    "other_authors": "Srimugunthan, K. Gopinath", 
    "title": "Distributed Wear levelling of Flash Memories", 
    "arxiv-id": "1302.5999v1", 
    "author": "K. Gopinath", 
    "publish": "2013-02-25T05:55:57Z", 
    "summary": "For large scale distributed storage systems, flash memories are an excellent\nchoice because flash memories consume less power, take lesser floor space for a\ntarget throughput and provide faster access to data. In a traditional\ndistributed filesystem, even distribution is required to ensure load-balancing,\nbalanced space utilisation and failure tolerance. In the presence of flash\nmemories, in addition, we should also ensure that the number of writes to these\ndifferent flash storage nodes are evenly distributed, to ensure even wear of\nflash storage nodes, so that unpredictable failures of storage nodes are\navoided. This requires that we distribute updates and do garbage collection,\nacross the flash storage nodes. We have motivated the distributed wearlevelling\nproblem considering the replica placement algorithm for HDFS. Viewing the\nwearlevelling across flash storage nodes as a distributed co-ordination\nproblem, we present an alternate design, to reduce the message communication\ncost across participating nodes. We demonstrate the effectiveness of our design\nthrough simulation"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.6224v5", 
    "other_authors": "Hammurabi Mendes, Christine Tasson, Maurice Herlihy", 
    "title": "Distributed Computability in Byzantine Asynchronous Systems", 
    "arxiv-id": "1302.6224v5", 
    "author": "Maurice Herlihy", 
    "publish": "2013-02-25T20:50:25Z", 
    "summary": "In this work, we extend the topology-based approach for characterizing\ncomputability in asynchronous crash-failure distributed systems to asynchronous\nByzantine systems. We give the first theorem with necessary and sufficient\nconditions to solve arbitrary tasks in asynchronous Byzantine systems where an\nadversary chooses faulty processes. In our adversarial formulation, outputs of\nnon-faulty processes are constrained in terms of inputs of non-faulty processes\nonly. For colorless tasks, an important subclass of distributed problems, the\ngeneral result reduces to an elegant model that effectively captures the\nrelation between the number of processes, the number of failures, as well as\nthe topological structure of the task's simplicial complexes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.1379v1", 
    "other_authors": "Mehmet Deveci, Kamer Kaya, Bora Ucar, Umit V. Catalyurek", 
    "title": "GPU accelerated maximum cardinality matching algorithms for bipartite   graphs", 
    "arxiv-id": "1303.1379v1", 
    "author": "Umit V. Catalyurek", 
    "publish": "2013-03-06T16:38:37Z", 
    "summary": "We design, implement, and evaluate GPU-based algorithms for the maximum\ncardinality matching problem in bipartite graphs. Such algorithms have a\nvariety of applications in computer science, scientific computing,\nbioinformatics, and other areas. To the best of our knowledge, ours is the\nfirst study which focuses on GPU implementation of the maximum cardinality\nmatching algorithms. We compare the proposed algorithms with serial and\nmulticore implementations from the literature on a large set of real-life\nproblems where in majority of the cases one of our GPU-accelerated algorithms\nis demonstrated to be faster than both the sequential and multicore\nimplementations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2043v1", 
    "other_authors": "Bernadette Charron-Bost", 
    "title": "Orientation and Connectivity Based Criteria for Asymptotic Consensus", 
    "arxiv-id": "1303.2043v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2013-03-08T16:22:46Z", 
    "summary": "In this article, we establish orientation and connectivity based criteria for\nthe agreement algorithm to achieve asymptotic consensus in the context of\ntime-varying topology and communication delays. These criteria unify and extend\nmany earlier convergence results on the agreement algorithm for deterministic\nand discrete-time multiagent systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2171v1", 
    "other_authors": "Kishore Kothapalli, Dip Sankar Banerjee, P. J. Narayanan, Surinder Sood, Aman Kumar Bahl, Shashank Sharma, Shrenik Lad, Krishna Kumar Singh, Kiran Matam, Sivaramakrishna Bharadwaj, Rohit Nigam, Parikshit Sakurikar, Aditya Deshpande, Ishan Misra, Siddharth Choudhary, Shubham Gupta", 
    "title": "CPU and/or GPU: Revisiting the GPU Vs. CPU Myth", 
    "arxiv-id": "1303.2171v1", 
    "author": "Shubham Gupta", 
    "publish": "2013-03-09T05:35:31Z", 
    "summary": "Parallel computing using accelerators has gained widespread research\nattention in the past few years. In particular, using GPUs for general purpose\ncomputing has brought forth several success stories with respect to time taken,\ncost, power, and other metrics. However, accelerator based computing has\nsignifi- cantly relegated the role of CPUs in computation. As CPUs evolve and\nalso offer matching computational resources, it is important to also include\nCPUs in the computation. We call this the hybrid computing model. Indeed, most\ncomputer systems of the present age offer a degree of heterogeneity and\ntherefore such a model is quite natural.\n  We reevaluate the claim of a recent paper by Lee et al.(ISCA 2010). We argue\nthat the right question arising out of Lee et al. (ISCA 2010) should be how to\nuse a CPU+GPU platform efficiently, instead of whether one should use a CPU or\na GPU exclusively. To this end, we experiment with a set of 13 diverse\nworkloads ranging from databases, image processing, sparse matrix kernels, and\ngraphs. We experiment with two different hybrid platforms: one consisting of a\n6-core Intel i7-980X CPU and an NVidia Tesla T10 GPU, and another consisting of\nan Intel E7400 dual core CPU with an NVidia GT520 GPU. On both these platforms,\nwe show that hybrid solutions offer good advantage over CPU or GPU alone\nsolutions. On both these platforms, we also show that our solutions are 90%\nresource efficient on average.\n  Our work therefore suggests that hybrid computing can offer tremendous\nadvantages at not only research-scale platforms but also the more realistic\nscale systems with significant performance gains and resource efficiency to the\nlarge scale user community."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2619v1", 
    "other_authors": "Russell Power", 
    "title": "Making Systems More Robust with Flexible RPC Lookup", 
    "arxiv-id": "1303.2619v1", 
    "author": "Russell Power", 
    "publish": "2013-03-11T19:05:37Z", 
    "summary": "Modern distributed systems use names everywhere. Lockservices such as Chubby\nand ZooKeeper provide an effective mechanism for mapping from application names\nto server instances, but proper usage of them requires a large amount of\nerror-prone boiler-plate code.\n  Application programmers often try to write wrappers to abstract away this\nlogic, but it turns out there is a more general and easier way of handling the\nissue. We show that by extending the existing name resolution capabilities of\nRPC libraries, we can remove the need for such annoying boiler-plate code while\nat the same time making our services more robust."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.3626v1", 
    "other_authors": "Niloufar Shafiei", 
    "title": "Non-blocking Patricia Tries with Replace Operations", 
    "arxiv-id": "1303.3626v1", 
    "author": "Niloufar Shafiei", 
    "publish": "2013-03-14T22:14:36Z", 
    "summary": "This paper presents a non-blocking Patricia trie implementation for an\nasynchronous shared-memory system using Compare&Swap. The trie implements a\nlinearizable set and supports three update operations: insert adds an element,\ndelete removes an element and replace replaces one element by another. The\nreplace operation is interesting because it changes two different locations of\ntree atomically. If all update operations modify different parts of the trie,\nthey run completely concurrently. The implementation also supports a wait-free\nfind operation, which only reads shared memory and never changes the data\nstructure. Empirically, we compare our algorithms to some existing set\nimplementations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.4191v4", 
    "other_authors": "Surabhi Jain, N. Sadagopan", 
    "title": "Parallel Search with Extended Fibonacci Primitive", 
    "arxiv-id": "1303.4191v4", 
    "author": "N. Sadagopan", 
    "publish": "2013-03-18T09:03:56Z", 
    "summary": "Search pattern experienced by the processor to search an element in secondary\nstorage devices follows a random sequence. Formally, it is a random walk and\nits modeling is crucial in studying performance metrics like memory access\ntime. In this paper, we first model the random walk using extended Fibonacci\nseries. Our simulation is done on a parallel computing model (PRAM) with EREW\nstrategy. Three search primitives are proposed under parallel computing model\nand each primitive is thoroughly tested on an array of size $10^7$ with the\nsize of random walk being $10^4$. Our findings reveal that search primitive\nwith pointer jumping is better than the other two primitives. Our key\ncontribution lies in modeling random walk as an extended Fibonacci series\ngenerator and simulating the same with various search primitives."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.4312v2", 
    "other_authors": "Christian Siebert, Jesper Larsson Tr\u00e4ff", 
    "title": "Perfectly load-balanced, optimal, stable, parallel merge", 
    "arxiv-id": "1303.4312v2", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2013-03-18T16:40:58Z", 
    "summary": "We present a simple, work-optimal and synchronization-free solution to the\nproblem of stably merging in parallel two given, ordered arrays of m and n\nelements into an ordered array of m+n elements. The main contribution is a new,\nsimple, fast and direct algorithm that determines, for any prefix of the stably\nmerged output sequence, the exact prefixes of each of the two input sequences\nneeded to produce this output prefix. More precisely, for any given index\n(rank) in the resulting, but not yet constructed output array representing an\noutput prefix, the algorithm computes the indices (co-ranks) in each of the two\ninput arrays representing the required input prefixes without having to merge\nthe input arrays. The co-ranking algorithm takes O(log min(m,n)) time steps.\nThe algorithm is used to devise a perfectly load-balanced, stable, parallel\nmerge algorithm where each of p processing elements has exactly the same number\nof input elements to merge. Compared to other approaches to the parallel merge\nproblem, our algorithm is considerably simpler and can be faster up to a factor\nof two. Compared to previous algorithms for solving the co-ranking problem, the\nalgorithm given here is direct and maintains stability in the presence of\nrepeated elements at no extra space or time cost. When the number of processing\nelements p does not exceed (m+n)/log min(m,n), the parallel merge algorithm has\noptimal speedup. It is easy to implement on both shared and distributed memory\nparallel systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.5164v1", 
    "other_authors": "Jianlong Zhong, Bingsheng He", 
    "title": "Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and   Scheduling", 
    "arxiv-id": "1303.5164v1", 
    "author": "Bingsheng He", 
    "publish": "2013-03-21T04:50:48Z", 
    "summary": "Graphics processors, or GPUs, have recently been widely used as accelerators\nin the shared environments such as clusters and clouds. In such shared\nenvironments, many kernels are submitted to GPUs from different users, and\nthroughput is an important metric for performance and total ownership cost.\nDespite the recently improved runtime support for concurrent GPU kernel\nexecutions, the GPU can be severely underutilized, resulting in suboptimal\nthroughput. In this paper, we propose Kernelet, a runtime system with dynamic\nslicing and scheduling techniques to improve the throughput of concurrent\nkernel executions on the GPU. With slicing, Kernelet divides a GPU kernel into\nmultiple sub-kernels (namely slices). Each slice has tunable occupancy to allow\nco-scheduling with other slices and to fully utilize the GPU resources. We\ndevelop a novel and effective Markov chain based performance model to guide the\nscheduling decision. Our experimental results demonstrate up to 31.1% and 23.4%\nperformance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5275v1", 
    "other_authors": "Michael Lange, Gerard Gorman, Michele Weiland, Lawrence Mitchell, James Southern", 
    "title": "Achieving Efficient Strong Scaling with PETSc using Hybrid MPI/OpenMP   Optimisation", 
    "arxiv-id": "1303.5275v1", 
    "author": "James Southern", 
    "publish": "2013-03-21T14:56:02Z", 
    "summary": "The increasing number of processing elements and decreas- ing memory to core\nratio in modern high-performance platforms makes efficient strong scaling a key\nrequirement for numerical algorithms. In order to achieve efficient scalability\non massively parallel systems scientific software must evolve across the entire\nstack to exploit the multiple levels of parallelism exposed in modern\narchitectures. In this paper we demonstrate the use of hybrid MPI/OpenMP\nparallelisation to optimise parallel sparse matrix-vector multiplication in\nPETSc, a widely used scientific library for the scalable solution of partial\ndifferential equations. Using large matrices generated by Fluidity, an open\nsource CFD application code which uses PETSc as its linear solver engine, we\nevaluate the effect of explicit communication overlap using task-based\nparallelism and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. We demonstrate a significant speedup\nover the pure-MPI mode and efficient strong scaling of sparse matrix-vector\nmultiplication on Fujitsu PRIMEHPC FX10 and Cray XE6 systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5837v1", 
    "other_authors": "Laura Grigori, Mathias Jacquelin, Amal Khabou", 
    "title": "Multilevel communication optimal LU and QR factorizations for   hierarchical platforms", 
    "arxiv-id": "1303.5837v1", 
    "author": "Amal Khabou", 
    "publish": "2013-03-23T11:40:50Z", 
    "summary": "This study focuses on the performance of two classical dense linear algebra\nalgorithms, the LU and the QR factorizations, on multilevel hierarchical\nplatforms. We first introduce a new model called Hierarchical Cluster Platform\n(HCP), encapsulating the characteristics of such platforms. The focus is set on\nreducing the communication requirements of studied algorithms at each level of\nthe hierarchy. Lower bounds on communications are therefore extended with\nrespect to the HCP model. We then introduce multilevel LU and QR algorithms\ntailored for those platforms, and provide a detailed performance analysis. We\nalso provide a set of numerical experiments and performance predictions\ndemonstrating the need for such algorithms on large platforms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5891v1", 
    "other_authors": "Bharath Balasubramanian, Vijay K. Garg", 
    "title": "Fault Tolerance in Distributed Systems using Fused State Machines", 
    "arxiv-id": "1303.5891v1", 
    "author": "Vijay K. Garg", 
    "publish": "2013-03-23T23:03:34Z", 
    "summary": "Replication is a standard technique for fault tolerance in distributed\nsystems modeled as deterministic finite state machines (DFSMs or machines). To\ncorrect f crash or f/2 Byzantine faults among n different machines, replication\nrequires nf additional backup machines. We present a solution called fusion\nthat requires just f additional backup machines. First, we build a framework\nfor fault tolerance in DFSMs based on the notion of Hamming distances. We\nintroduce the concept of an (f,m)-fusion, which is a set of m backup machines\nthat can correct f crash faults or f/2 Byzantine faults among a given set of\nmachines. Second, we present an algorithm to generate an (f,f)-fusion for a\ngiven set of machines. We ensure that our backups are efficient in terms of the\nsize of their state and event sets. Our evaluation of fusion on the widely used\nMCNC'91 benchmarks for DFSMs show that the average state space savings in\nfusion (over replication) is 38% (range 0-99%). To demonstrate the practical\nuse of fusion, we describe its potential application to the MapReduce\nframework. Using a simple case study, we compare replication and fusion as\napplied to this framework. While a pure replication-based solution requires 1.8\nmillion map tasks, our fusion-based solution requires only 1.4 million map\ntasks with minimal overhead during normal operation or recovery. Hence, fusion\nresults in considerable savings in state space and other resources such as the\npower needed to run the backup tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.7195v1", 
    "other_authors": "Marc Bux, Ulf Leser", 
    "title": "Parallelization in Scientific Workflow Management Systems", 
    "arxiv-id": "1303.7195v1", 
    "author": "Ulf Leser", 
    "publish": "2013-03-28T18:20:17Z", 
    "summary": "Over the last two decades, scientific workflow management systems (SWfMS)\nhave emerged as a means to facilitate the design, execution, and monitoring of\nreusable scientific data processing pipelines. At the same time, the amounts of\ndata generated in various areas of science outpaced enhancements in\ncomputational power and storage capabilities. This is especially true for the\nlife sciences, where new technologies increased the sequencing throughput from\nkilobytes to terabytes per day. This trend requires current SWfMS to adapt:\nNative support for parallel workflow execution must be provided to increase\nperformance; dynamically scalable \"pay-per-use\" compute infrastructures have to\nbe integrated to diminish hardware costs; adaptive scheduling of workflows in\ndistributed compute environments is required to optimize resource utilization.\nIn this survey we give an overview of parallelization techniques for SWfMS,\nboth in theory and in their realization in concrete systems. We find that\ncurrent systems leave considerable room for improvement and we propose key\nadvancements to the landscape of SWfMS."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1303.7270v1", 
    "other_authors": "Reza Moraveji, Javid Taheri, MohammadReza HosseinyFarahabady, Nikzad Babaii Rizvandi, Albert Y. Zomaya", 
    "title": "Data-Intensive Workload Consolidation on Hadoop Distributed File System", 
    "arxiv-id": "1303.7270v1", 
    "author": "Albert Y. Zomaya", 
    "publish": "2013-03-28T23:15:36Z", 
    "summary": "Workload consolidation, sharing physical resources among multiple workloads,\nis a promising technique to save cost and energy in cluster computing systems.\nThis paper highlights a few challenges of workload consolidation for Hadoop as\none of the current state-of-the-art data-intensive cluster computing system.\nThrough a systematic step-by-step procedure, we investigate challenges for\nefficient server consolidation in Hadoop environments. To this end, we first\ninvestigate the inter-relationship between last level cache (LLC) contention\nand throughput degradation for consolidated workloads on a single physical\nserver employing Hadoop distributed file system (HDFS). We then investigate the\ngeneral case of consolidation on multiple physical servers so that their\nthroughput never falls below a desired/predefined utilization level. We use our\nempirical results to model consolidation as a classic two-dimensional bin\npacking problem and then design a computationally efficient greedy algorithm to\nachieve minimum throughput degradation on multiple servers. Results are very\npromising and show that our greedy approach is able to achieve near optimal\nsolution in all experimented cases."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1303.7300v1", 
    "other_authors": "Giddaluru Madhavi, M. K. Kaushik", 
    "title": "Queuing Methodology Based Power Efficient Routing Protocol for Reliable   Data Communications in Manets", 
    "arxiv-id": "1303.7300v1", 
    "author": "M. K. Kaushik", 
    "publish": "2013-03-29T06:01:08Z", 
    "summary": "A mobile ad hoc network (MANET) is a wireless network that uses multi-hop\npeer-to- peer routing instead of static network infrastructure to provide\nnetwork connectivity. MANETs have applications in rapidly deployed and dynamic\nmilitary and civilian systems. The network topology in a MANET usually changes\nwith time. Therefore, there are new challenges for routing protocols in MANETs\nsince traditional routing protocols may not be suitable for MANETs. In recent\nyears, a variety of new routing protocols targeted specifically at this\nenvironment have been developed, but little performance information on each\nprotocol and no realistic performance comparison between them is available.\nThis paper presents the results of a detailed packet-level simulation comparing\nthree multi-hop wireless ad hoc network routing protocols that cover a range of\ndesign choices: DSR, NFPQR, and clustered NFPQR. By applying queuing\nmethodology to the introduced routing protocol the reliability and throughput\nof the network is increased."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1308.0083v1", 
    "other_authors": "Wei Wang, Baochun Li, Ben Liang", 
    "title": "Dominant Resource Fairness in Cloud Computing Systems with Heterogeneous   Servers", 
    "arxiv-id": "1308.0083v1", 
    "author": "Ben Liang", 
    "publish": "2013-08-01T03:08:22Z", 
    "summary": "We study the multi-resource allocation problem in cloud computing systems\nwhere the resource pool is constructed from a large number of heterogeneous\nservers, representing different points in the configuration space of resources\nsuch as processing, memory, and storage. We design a multi-resource allocation\nmechanism, called DRFH, that generalizes the notion of Dominant Resource\nFairness (DRF) from a single server to multiple heterogeneous servers. DRFH\nprovides a number of highly desirable properties. With DRFH, no user prefers\nthe allocation of another user; no one can improve its allocation without\ndecreasing that of the others; and more importantly, no user has an incentive\nto lie about its resource demand. As a direct application, we design a simple\nheuristic that implements DRFH in real-world systems. Large-scale simulations\ndriven by Google cluster traces show that DRFH significantly outperforms the\ntraditional slot-based scheduler, leading to much higher resource utilization\nwith substantially shorter job completion times."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1308.0148v1", 
    "other_authors": "Omer Demirel, Ivo F. Sbalzarini", 
    "title": "Balancing indivisible real-valued loads in arbitrary networks", 
    "arxiv-id": "1308.0148v1", 
    "author": "Ivo F. Sbalzarini", 
    "publish": "2013-08-01T10:32:33Z", 
    "summary": "In parallel computing, a problem is divided into a set of smaller tasks that\nare distributed across multiple processing elements. Balancing the load of the\nprocessing elements is key to achieving good performance and scalability. If\nthe computational costs of the individual tasks vary over time in an\nunpredictable way, dynamic load balancing aims at migrating them between\nprocessing elements so as to maintain load balance. During dynamic load\nbalancing, the tasks amount to indivisible work packets with a real-valued\ncost. For this case of indivisible, real- valued loads, we analyze the\nbalancing circuit model, a local dynamic load-balancing scheme that does not\nrequire global communication. We extend previous analyses to the present case\nand provide a probabilistic bound for the achievable load balance. Based on an\nanalogy with the offline balls-into-bins problem, we further propose a novel\nalgorithm for dynamic balancing of indivisible, real-valued loads. We benchmark\nthe proposed algorithm in numerical experiments and compare it with the\nclassical greedy algorithm, both in terms of solution quality and communication\ncost. We find that the increased communication cost of the proposed algorithm\nis compensated by a higher solution quality, leading on average to about an\norder of magnitude gain in overall performance."
},{
    "category": "cs.DC", 
    "doi": "10.5120/12908-0036", 
    "link": "http://arxiv.org/pdf/1308.0568v1", 
    "other_authors": "M A Awad, M Z Rashad, M A Elsoud, M A El-dosuky", 
    "title": "Visualization of Job Scheduling in Grid Computers", 
    "arxiv-id": "1308.0568v1", 
    "author": "M A El-dosuky", 
    "publish": "2013-08-02T18:09:23Z", 
    "summary": "One of the hot problems in grid computing is job scheduling. It is known that\nthe job scheduling is NP-complete, and thus the use of heuristics is the de\nfacto approach to deal with this practice in its difficulty. The proposed is an\nimagination to fish swarm, job dispatcher and Visualization gridsim to execute\nsome jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1031v1", 
    "other_authors": "Bj\u00f6rn Lohrmann, Daniel Warneke, Odej Kao", 
    "title": "Nephele Streaming: Stream Processing Under QoS Constraints At Scale", 
    "arxiv-id": "1308.1031v1", 
    "author": "Odej Kao", 
    "publish": "2013-08-05T16:15:58Z", 
    "summary": "The ability to process large numbers of continuous data streams in a\nnear-real-time fashion has become a crucial prerequisite for many scientific\nand industrial use cases in recent years. While the individual data streams are\nusually trivial to process, their aggregated data volumes easily exceed the\nscalability of traditional stream processing systems. At the same time,\nmassively-parallel data processing systems like MapReduce or Dryad currently\nenjoy a tremendous popularity for data-intensive applications and have proven\nto scale to large numbers of nodes. Many of these systems also provide\nstreaming capabilities. However, unlike traditional stream processors, these\nsystems have disregarded QoS requirements of prospective stream processing\napplications so far. In this paper we address this gap. First, we analyze\ncommon design principles of today's parallel data processing frameworks and\nidentify those principles that provide degrees of freedom in trading off the\nQoS goals latency and throughput. Second, we propose a highly distributed\nscheme which allows these frameworks to detect violations of user-defined QoS\nconstraints and optimize the job execution without manual interaction. As a\nproof of concept, we implemented our approach for our massively-parallel data\nprocessing framework Nephele and evaluated its effectiveness through a\ncomparison with Hadoop Online. For an example streaming application from the\nmultimedia domain running on a cluster of 200 nodes, our approach improves the\nprocessing latency by a factor of at least 13 while preserving high data\nthroughput when needed."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1303v1", 
    "other_authors": "Arokia Paul Rajan, Shanmugapriyaa", 
    "title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service", 
    "arxiv-id": "1308.1303v1", 
    "author": "Shanmugapriyaa", 
    "publish": "2013-08-05T06:11:12Z", 
    "summary": "Enterprises are driving towards less cost, more availability, agility,\nmanaged risk - all of which is accelerated towards Cloud Computing. Cloud is\nnot a particular product, but a way of delivering IT services that are\nconsumable on demand, elastic to scale up and down as needed, and follow a\npay-for-usage model. Out of the three common types of cloud computing service\nmodels, Infrastructure as a Service (IaaS) is a service model that provides\nservers, computing power, network bandwidth and Storage capacity, as a service\nto their subscribers. Cloud can relate to many things but without the\nfundamental storage pieces, which is provided as a service namely Cloud\nStorage, none of the other applications is possible. This paper introduces\nCloud Storage, which covers the key technologies in cloud computing and Cloud\nStorage, management insights about cloud computing, different types of cloud\nservices, driving forces of cloud computing and cloud storage, advantages and\nchallenges of cloud storage and concludes by pinpointing few challenges to be\naddressed by the cloud storage providers."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1343v1", 
    "other_authors": "Erik Schnetter", 
    "title": "Performance and Optimization Abstractions for Large Scale Heterogeneous   Systems in the Cactus/Chemora Framework", 
    "arxiv-id": "1308.1343v1", 
    "author": "Erik Schnetter", 
    "publish": "2013-08-06T16:40:47Z", 
    "summary": "We describe a set of lower-level abstractions to improve performance on\nmodern large scale heterogeneous systems. These provide portable access to\nsystem- and hardware-dependent features, automatically apply dynamic\noptimizations at run time, and target stencil-based codes used in finite\ndifferencing, finite volume, or block-structured adaptive mesh refinement\ncodes.\n  These abstractions include a novel data structure to manage refinement\ninformation for block-structured adaptive mesh refinement, an iterator\nmechanism to efficiently traverse multi-dimensional arrays in stencil-based\ncodes, and a portable API and implementation for explicit SIMD vectorization.\n  These abstractions can either be employed manually, or be targeted by\nautomated code generation, or be used via support libraries by compilers during\ncode generation. The implementations described below are available in the\nCactus framework, and are used e.g. in the Einstein Toolkit for relativistic\nastrophysics simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1358v1", 
    "other_authors": "Gustavo M. D. Vieira, Luiz E. Buzato", 
    "title": "The Performance of Paxos and Fast Paxos", 
    "arxiv-id": "1308.1358v1", 
    "author": "Luiz E. Buzato", 
    "publish": "2013-08-06T17:44:13Z", 
    "summary": "Paxos and Fast Paxos are optimal consensus algorithms that are simple and\nelegant, while suitable for efficient implementation. In this paper, we compare\nthe performance of both algorithms in failure-free and failure-prone runs using\nTreplica, a general replication toolkit that implements these algorithms in a\nmodular and efficient manner. We have found that Paxos outperforms Fast Paxos\nfor small number of replicas and that collisions are not the cause of this\nperformance difference."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1763v1", 
    "other_authors": "Nitin Rakesh, Vipin Tyagi", 
    "title": "Linear Network Coding on Multi-Mesh of Trees (MMT) using All to All   Broadcast (AAB)", 
    "arxiv-id": "1308.1763v1", 
    "author": "Vipin Tyagi", 
    "publish": "2013-08-08T06:21:21Z", 
    "summary": "We introduce linear network coding on parallel architecture for multi-source\nfinite acyclic network. In this problem, different messages in diverse time\nperiods are broadcast and every nonsource node in the network decodes and\nencodes the message based on further communication.We wish to minimize the\ncommunication steps and time complexity involved in transfer of data from\nnode-to-node during parallel communication.We have used Multi-Mesh of Trees\n(MMT) topology for implementing network coding. To envisage our result, we use\nall-to-all broadcast as communication algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1806v1", 
    "other_authors": "Gaurav Mittal, Dr. Nishtha Kesswani, Kuldeep Goswami", 
    "title": "A Survey of Current Trends in Distributed, Grid and Cloud Computing", 
    "arxiv-id": "1308.1806v1", 
    "author": "Kuldeep Goswami", 
    "publish": "2013-08-08T10:23:09Z", 
    "summary": "Through the 1990s to 2012 the internet changed the world of computing\ndrastically. It started its journey with parallel computing after it advanced\nto distributed computing and further to grid computing. And in present scenario\nit creates a new world which is pronounced as a Cloud Computing [1]. These all\nthree terms have different meanings. Cloud computing is based on backward\ncomputing schemes like cluster computing, distributed computing, grid computing\nand utility computing. The basic concept of cloud computing is virtualization.\nIt provides virtual hardware and software resources to various requesting\nprograms. This paper gives a detailed description about cluster computing, grid\ncomputing and cloud computing and gives an insight of some implementations of\nthe same. We try to list the inspirations for the advent of all these\ntechnologies. We also account for some present scenario faults of grid\ncomputing and also discuss new cloud computing projects which are being managed\nby the Government of India for learning. The paper also reviews the existing\nwork and covers (analytically), to some extent, some innovative ideas that can\nbe implemented."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2480v1", 
    "other_authors": "Georgios Rokos, Gerard J. Gorman, James Southern, Paul H. J. Kelly", 
    "title": "A thread-parallel algorithm for anisotropic mesh adaptation", 
    "arxiv-id": "1308.2480v1", 
    "author": "Paul H. J. Kelly", 
    "publish": "2013-08-12T07:45:15Z", 
    "summary": "Anisotropic mesh adaptation is a powerful way to directly minimise the\ncomputational cost of mesh based simulation. It is particularly important for\nmulti-scale problems where the required number of floating-point operations can\nbe reduced by orders of magnitude relative to more traditional static mesh\napproaches.\n  Increasingly, finite element and finite volume codes are being optimised for\nmodern multi-core architectures. Typically, decomposition methods implemented\nthrough the Message Passing Interface (MPI) are applied for inter-node\nparallelisation, while a threaded programming model, such as OpenMP, is used\nfor intra-node parallelisation. Inter-node parallelism for mesh adaptivity has\nbeen successfully implemented by a number of groups. However, thread-level\nparallelism is significantly more challenging because the underlying data\nstructures are extensively modified during mesh adaptation and a greater degree\nof parallelism must be realised.\n  In this paper we describe a new thread-parallel algorithm for anisotropic\nmesh adaptation algorithms. For each of the mesh optimisation phases\n(refinement, coarsening, swapping and smoothing) we describe how independent\nsets of tasks are defined. We show how a deferred updates strategy can be used\nto update the mesh data structures in parallel and without data contention. We\nshow that despite the complex nature of mesh adaptation and inherent load\nimbalances in the mesh adaptivity, a parallel efficiency of 60% is achieved on\nan 8 core Intel Xeon Sandybridge, and a 40% parallel efficiency is achieved\nusing 16 cores in a 2 socket Intel Xeon Sandybridge ccNUMA system."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2694v1", 
    "other_authors": "James Hegeman, Sriram V. Pemmaraju", 
    "title": "A Super-Fast Distributed Algorithm for Bipartite Metric Facility   Location", 
    "arxiv-id": "1308.2694v1", 
    "author": "Sriram V. Pemmaraju", 
    "publish": "2013-08-12T20:45:17Z", 
    "summary": "The \\textit{facility location} problem consists of a set of\n\\textit{facilities} $\\mathcal{F}$, a set of \\textit{clients} $\\mathcal{C}$, an\n\\textit{opening cost} $f_i$ associated with each facility $x_i$, and a\n\\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client\n$y_j$. The goal is to find a subset of facilities to \\textit{open}, and to\nconnect each client to an open facility, so as to minimize the total facility\nopening costs plus connection costs. This paper presents the first\nexpected-sub-logarithmic-round distributed O(1)-approximation algorithm in the\n$\\mathcal{CONGEST}$ model for the \\textit{metric} facility location problem on\nthe complete bipartite network with parts $\\mathcal{F}$ and $\\mathcal{C}$. Our\nalgorithm has an expected running time of $O((\\log \\log n)^3)$ rounds, where $n\n= |\\mathcal{F}| + |\\mathcal{C}|$. This result can be viewed as a continuation\nof our recent work (ICALP 2012) in which we presented the first\nsub-logarithmic-round distributed O(1)-approximation algorithm for metric\nfacility location on a \\textit{clique} network. The bipartite setting presents\nseveral new challenges not present in the problem on a clique network. We\npresent two new techniques to overcome these challenges. (i) In order to deal\nwith the problem of not being able to choose appropriate probabilities (due to\nlack of adequate knowledge), we design an algorithm that performs a random walk\nover a probability space and analyze the progress our algorithm makes as the\nrandom walk proceeds. (ii) In order to deal with a problem of quickly\ndisseminating a collection of messages, possibly containing many duplicates,\nover the bipartite network, we design a probabilistic hashing scheme that\ndelivers all of the messages in expected-$O(\\log \\log n)$ rounds."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2979v5", 
    "other_authors": "Flavio P. Junqueira, Marco Serafini", 
    "title": "On Barriers and the Gap between Active and Passive Replication (Full   Version)", 
    "arxiv-id": "1308.2979v5", 
    "author": "Marco Serafini", 
    "publish": "2013-08-13T21:09:27Z", 
    "summary": "Active replication is commonly built on top of the atomic broadcast\nprimitive. Passive replication, which has been recently used in the popular\nZooKeeper coordination system, can be naturally built on top of the\nprimary-order atomic broadcast primitive. Passive replication differs from\nactive replication in that it requires processes to cross a barrier before they\nbecome primaries and start broadcasting messages. In this paper, we propose a\nbarrier function tau that explains and encapsulates the differences between\nexisting primary-order atomic broadcast algorithms, namely semi-passive\nreplication and Zookeeper atomic broadcast (Zab), as well as the differences\nbetween Paxos and Zab. We also show that implementing primary-order atomic\nbroadcast on top of a generic consensus primitive and tau inherently results in\nhigher time complexity than atomic broadcast, as witnessed by existing\nalgorithms. We overcome this problem by presenting an alternative,\nprimary-order atomic broadcast implementation that builds on top of a generic\nconsensus primitive and uses consensus itself to form a barrier. This algorithm\nis modular and matches the time complexity of existing tau-based algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.3648v1", 
    "other_authors": "Sascha Hunold, Jesper Larsson Tr\u00e4ff", 
    "title": "On the State and Importance of Reproducible Experimental Research in   Parallel Computing", 
    "arxiv-id": "1308.3648v1", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2013-08-16T15:11:35Z", 
    "summary": "Computer science is also an experimental science. This is particularly the\ncase for parallel computing, which is in a total state of flux, and where\nexperiments are necessary to substantiate, complement, and challenge\ntheoretical modeling and analysis. Here, experimental work is as important as\nare advances in theory, that are indeed often driven by the experimental\nfindings. In parallel computing, scientific contributions presented in research\narticles are therefore often based on experimental data, with a substantial\npart devoted to presenting and discussing the experimental findings. As in all\nof experimental science, experiments must be presented in a way that makes\nreproduction by other researchers possible, in principle. Despite appearance to\nthe contrary, we contend that reproducibility plays a small role, and is\ntypically not achieved. As can be found, articles often do not have a\nsufficiently detailed description of their experiments, and do not make\navailable the software used to obtain the claimed results. As a consequence,\nparallel computational results are most often impossible to reproduce, often\nquestionable, and therefore of little or no scientific value. We believe that\nthe description of how to reproduce findings should play an important part in\nevery serious, experiment-based parallel computing research article. We aim to\ninitiate a discussion of the reproducibility issue in parallel computing, and\nelaborate on the importance of reproducible research for (1) better and sounder\ntechnical/scientific papers, (2) a sounder and more efficient review process\nand (3) more effective collective work. This paper expresses our current view\non the subject and should be read as a position statement for discussion and\nfuture work. We do not consider the related (but no less important) issue of\nthe quality of the experimental design."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.4166v1", 
    "other_authors": "Carlos Cardonha, Marcos D. Assun\u00e7\u00e3o, Marco A. S. Netto, Renato L. F. Cunha, Carlos Queiroz", 
    "title": "Patience-aware Scheduling for Cloud Services: Freeing Users from the   Chains of Boredom", 
    "arxiv-id": "1308.4166v1", 
    "author": "Carlos Queiroz", 
    "publish": "2013-08-19T20:30:38Z", 
    "summary": "Scheduling of service requests in Cloud computing has traditionally focused\non the reduction of pre-service wait, generally termed as waiting time. Under\ncertain conditions such as peak load, however, it is not always possible to\ngive reasonable response times to all users. This work explores the fact that\ndifferent users may have their own levels of tolerance or patience with\nresponse delays. We introduce scheduling strategies that produce better\nassignment plans by prioritising requests from users who expect to receive the\nresults earlier and by postponing servicing jobs from those who are more\ntolerant to response delays. Our analytical results show that the behaviour of\nusers' patience plays a key role in the evaluation of scheduling techniques,\nand our computational evaluation demonstrates that, under peak load, the new\nalgorithms typically provide better user experience than the traditional FIFO\nstrategy."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.4208v1", 
    "other_authors": "Jose Fernando S. Carvalho, Paulo Anselmo da Mota Silveira Neto, Vincius Cardoso Garcia, Rodrigo Elia Assad, Frederico Durao", 
    "title": "A Systematic Mapping Study on Cloud Computing", 
    "arxiv-id": "1308.4208v1", 
    "author": "Frederico Durao", 
    "publish": "2013-08-20T02:17:27Z", 
    "summary": "Cloud Computing emerges from the global economic crisis as an option to use\ncomputing resources from a more rational point of view. In other words, a\ncheaper way to have IT resources. However, issues as security and privacy, SLA\n(Service Layer Agreement), resource sharing, and billing has left open\nquestions about the real gains of that model. This study aims to investigate\nstate-of-the-art in Cloud Computing, identify gaps, challenges, synthesize\navailable evidences both its use and development, and provides relevant\ninformation, clarifying open questions and common discussed issues about that\nmodel through literature. The good practices of systematic map- ping study\nmethodology were adopted in order to reach those objectives. Al- though Cloud\nComputing is based on a business model with over 50 years of existence,\nevidences found in this study indicate that Cloud Computing still presents\nlimitations that prevent the full use of the proposal on-demand."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.6058v1", 
    "other_authors": "A. S. Syed Navaz, C. Prabhadevi, V. Sangeetha", 
    "title": "Data Grid Concepts for Data Security in Distributed Computing", 
    "arxiv-id": "1308.6058v1", 
    "author": "V. Sangeetha", 
    "publish": "2013-08-28T04:51:08Z", 
    "summary": "Data grid is a distributed computing architecture that integrates a large\nnumber of data and computing resources into a single virtual data management\nsystem. It enables the sharing and coordinated use of data from various\nresources and provides various services to fit the needs of high performance\ndistributed and data-intensive computing. Here data partitioning and dynamic\nreplication in data grid are considered. In which security and access\nperformance of a system are efficient. There are several important requirements\nfor data grids, including information survivability, security, and access\nperformance. More specifically, the investigation is the problem of optimal\nallocation of sensitive data objects that are partitioned by using secret\nsharing scheme or erasure coding scheme and replicated. DATA PARTITIONING is\nknown as the single data can be divided into multiple objects. REPLICATION is\nknown as process of sharing information. storing same data in multiple systems.\nReplication techniques are frequently used to improve data availability. Single\npoint failure does not affect this system. Where the data will be secured."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.6464v1", 
    "other_authors": "Buddhadeb Sau, Krishnendu Mukhopadhyaya", 
    "title": "Localizability of Wireless Sensor Networks: Beyond Wheel Extension", 
    "arxiv-id": "1308.6464v1", 
    "author": "Krishnendu Mukhopadhyaya", 
    "publish": "2013-08-29T13:37:44Z", 
    "summary": "A network is called localizable if the positions of all the nodes of the\nnetwork can be computed uniquely. If a network is localizable and embedded in\nplane with generic configuration, the positions of the nodes may be computed\nuniquely in finite time. Therefore, identifying localizable networks is an\nimportant function. If the complete information about the network is available\nat a single place, localizability can be tested in polynomial time. In a\ndistributed environment, networks with trilateration orderings (popular in real\napplications) and wheel extensions (a specific class of localizable networks)\nembedded in plane can be identified by existing techniques. We propose a\ndistributed technique which efficiently identifies a larger class of\nlocalizable networks. This class covers both trilateration and wheel\nextensions. In reality, exact distance is almost impossible or costly. The\nproposed algorithm based only on connectivity information. It requires no\ndistance information."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1402.0264v1", 
    "other_authors": "Sardar Anisul Haque, Marc Moreno Maza, Ning Xie", 
    "title": "A Many-core Machine Model for Designing Algorithms with Minimum   Parallelism Overheads", 
    "arxiv-id": "1402.0264v1", 
    "author": "Ning Xie", 
    "publish": "2014-02-03T00:22:59Z", 
    "summary": "We present a model of multithreaded computation, combining fork-join and\nsingle-instruction-multiple-data parallelisms, with an emphasis on estimating\nparallelism overheads of programs written for modern many-core architectures.\nWe establish a Graham-Brent theorem for this model so as to estimate execution\ntime of programs running on a given number of streaming multiprocessors. We\nevaluate the benefits of our model with four fundamental algorithms from\nscientific computing. In each case, our model is used to minimize parallelism\noverheads by determining an appropriate value range for a given program\nparameter; moreover experimentation confirms the model's prediction."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1402.0696v1", 
    "other_authors": "Shafii Muhammad Abdulhamid, Muhammad Shafie Abd Latiff, Mohammed Bakri Bashir", 
    "title": "On-Demand Grid Provisioning Using Cloud Infrastructures and Related   Virtualization Tools: A Survey and Taxonomy", 
    "arxiv-id": "1402.0696v1", 
    "author": "Mohammed Bakri Bashir", 
    "publish": "2014-02-04T11:27:56Z", 
    "summary": "Recent researches have shown that grid resources can be accessed by client\non-demand, with the help of virtualization technology in the Cloud. The virtual\nmachines hosted by the hypervisors are being utilized to build the grid network\nwithin the cloud environment. The aim of this study is to survey some concepts\nused for the on-demand grid provisioning using Infrastructure as a Service\nCloud and the taxonomy of its related components. This paper, discusses the\ndifferent approaches for on-demand grid using infrastructural Cloud, the issues\nit tries to address and the implementation tools. The paper also, proposed an\nextended classification for the virtualization technology used and a new\nclassification for the Grid-Cloud integration which was based on the\narchitecture, communication flow and the user demand for the Grid resources.\nThis survey, tools and taxonomies presented here will contribute as a guide in\nthe design of future architectures for further researches."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1402.1309v1", 
    "other_authors": "Yanik Ngoko, Christophe C\u00e9rin, Alfredo Goldman, Dejan Milojicic", 
    "title": "Backtracking algorithms for service selection", 
    "arxiv-id": "1402.1309v1", 
    "author": "Dejan Milojicic", 
    "publish": "2014-02-06T10:38:26Z", 
    "summary": "In this paper, we explore the automation of services' compositions. We focus\non the service selection problem. In the formulation that we consider, the\nproblem's inputs are constituted by a behavioral composition whose abstract\nservices must be bound to concrete ones. The objective is to find the binding\nthat optimizes the {\\it utility} of the composition under some services level\nagreements. We propose a complete solution. Firstly, we show that the service\nselection problem can be mapped onto a Constraint Satisfaction Problem (CSP).\nThe benefit of this mapping is that the large know-how in the resolution of the\nCSP can be used for the service selection problem. Among the existing\ntechniques for solving CSP, we consider the backtracking. Our second\ncontribution is to propose various backtracking-based algorithms for the\nservice selection problem. The proposed variants are inspired by existing\nheuristics for the CSP. We analyze the runtime gain of our framework over an\nintuitive resolution based on exhaustive search. Our last contribution is an\nexperimental evaluation in which we demonstrate that there is an effective gain\nin using backtracking instead of some comparable approaches. The experiments\nalso show that our proposal can be used for finding in real time, optimal\nsolutions on small and medium services' compositions."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.1932v1", 
    "other_authors": "Dr. Rahul Malhotra, Prince Jain", 
    "title": "An EMUSIM Technique and its Components in Cloud Computing- A Review", 
    "arxiv-id": "1402.1932v1", 
    "author": "Prince Jain", 
    "publish": "2014-02-09T10:10:48Z", 
    "summary": "Recent efforts to design and develop Cloud technologies focus on defining\nnovel methods, policies and mechanisms for efficiently managing Cloud\ninfrastructures. One key challenge potential Cloud customers have before\nrenting resources is to know how their services will behave in a set of\nresources and the costs involved when growing and shrinking their resource\npool. Most of the studies in this area rely on simulation-based experiments,\nwhich consider simplified modeling of applications and computing environment.\nIn order to better predict service's behavior on Cloud platforms, an integrated\narchitecture that is based on both simulation and emulation. The proposed\narchitecture, named EMUSIM, automatically extracts information from application\nbehavior via emulation and then uses this information to generate the\ncorresponding simulation model. This paper presents brief overview of the\nEMUSIM technique and its components. The work in this paper focuses on\narchitecture and operation details of Automated Emulation Framework (AEF),\nQAppDeployer and proposes Cloud Sim Application for Simulation techniques."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2090v1", 
    "other_authors": "Piotr Skowron, Krzysztof Rzadca", 
    "title": "We Are Impatient: Algorithms for Geographically Distributed Load   Balancing with (Almost) Arbitrary Load Functions", 
    "arxiv-id": "1402.2090v1", 
    "author": "Krzysztof Rzadca", 
    "publish": "2014-02-10T10:33:52Z", 
    "summary": "In geographically-distributed systems, communication latencies are\nnon-negligible. The perceived processing time of a request is thus composed of\nthe time needed to route the request to the server and the true processing\ntime. Once a request reaches a target server, the processing time depends on\nthe total load of that server; this dependency is described by a load function.\nWe consider a broad class of load functions; we just require that they are\nconvex and two times differentiable. In particular our model can be applied to\nheterogeneous systems in which every server has a different load function. This\napproach allows us not only to generalize results for queuing theory and for\nbatches of requests, but also to use empirically-derived load functions,\nmeasured in a system under stress-testing. The optimal assignment of requests\nto servers is communication-balanced, i.e. for any pair of non\nperfectly-balanced servers, the reduction of processing time resulting from\nmoving a single request from the overloaded to underloaded server is smaller\nthan the additional communication latency. We present a centralized and a\ndecentralized algorithm for optimal load balancing. We prove bounds on the\nalgorithms' convergence. To the best of our knowledge these bounds were not\nknown even for the special cases studied previously (queuing theory and batches\nof requests). Both algorithms are any-time algorithms. In the decentralized\nalgorithm, each server balances the load with a randomly chosen peer. Such\nalgorithm is very robust to failures. We prove that the decentralized algorithm\nperforms locally-optimal steps. Our work extends the currently known results by\nconsidering a broad class of load functions and by establishing theoretical\nbounds on the algorithms' convergence. These results are applicable for servers\nwhose characteristics under load cannot be described by a standard mathematical\nmodels."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2107v1", 
    "other_authors": "Mario Pastorelli, Matteo Dell'Amico, Pietro Michiardi", 
    "title": "OS-Assisted Task Preemption for Hadoop", 
    "arxiv-id": "1402.2107v1", 
    "author": "Pietro Michiardi", 
    "publish": "2014-02-10T11:14:19Z", 
    "summary": "This work introduces a new task preemption primitive for Hadoop, that allows\ntasks to be suspended and resumed exploiting existing memory management\nmechanisms readily available in modern operating systems. Our technique fills\nthe gap that exists between the two extremes cases of killing tasks (which\nwaste work) or waiting for their completion (which introduces latency):\nexperimental results indicate superior performance and very small overheads\nwhen compared to existing alternatives."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2491v1", 
    "other_authors": "M. Uthaya Banu, K. Saravanan", 
    "title": "Optimizing the Cost for Resource Subscription Policy in IaaS Cloud", 
    "arxiv-id": "1402.2491v1", 
    "author": "K. Saravanan", 
    "publish": "2014-02-11T14:11:18Z", 
    "summary": "Cloud computing allow the users to efficiently and dynamically provision\ncomputing resource to meet their IT needs. Cloud Provider offers two\nsubscription plan to the customer namely reservation and on-demand. The\nreservation plan is typically cheaper than on-demand plan. If the actual\ncomputing demand is known in advance reserving the resource would be\nstraightforward. The challenge is how to make properly resource provisioning\nand how the customers efficiently purchase the provisioning options under\nreservation and on-demand. To address this issue, two-phase algorithm are\nproposed to minimize service provision cost in both reservation and on-demand\nplan. To reserve the correct and optimal amount of resources during\nreservation, proposed a mathematical formulae in the first phase. To predict\nresource demand, use kalman filter in the second phase. The evaluation result\nshows that the two-phase algorithm can significantly reduce the provision cost\nand the prediction is of reasonable accuracy."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2552v1", 
    "other_authors": "Juhana Laurinharju, Jukka Suomela", 
    "title": "Linial's Lower Bound Made Easy", 
    "arxiv-id": "1402.2552v1", 
    "author": "Jukka Suomela", 
    "publish": "2014-02-11T16:33:07Z", 
    "summary": "Linial's seminal result shows that any deterministic distributed algorithm\nthat finds a $3$-colouring of an $n$-cycle requires at least $\\log^*(n)/2 - 1$\ncommunication rounds. We give a new simpler proof of this theorem."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2810v1", 
    "other_authors": "Evripidis Bampis, Vincent Chau, Dimitrios Letsios, Giorgio Lucarelli, Ioannis Milis, Georgios Zois", 
    "title": "Energy Efficient Scheduling of MapReduce Jobs", 
    "arxiv-id": "1402.2810v1", 
    "author": "Georgios Zois", 
    "publish": "2014-02-12T13:16:16Z", 
    "summary": "MapReduce is emerged as a prominent programming model for data-intensive\ncomputation. In this work, we study power-aware MapReduce scheduling in the\nspeed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on\nthe minimization of the total weighted completion time of a set of MapReduce\njobs under a given budget of energy. Using a linear programming relaxation of\nour problem, we derive a polynomial time constant-factor approximation\nalgorithm. We also propose a convex programming formulation that we combine\nwith standard list scheduling policies, and we evaluate their performance using\nsimulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3034v1", 
    "other_authors": "Sukhpal Singh, Inderveer Chana", 
    "title": "Formal Specification Language Based IaaS Cloud Workload Regression   Analysis", 
    "arxiv-id": "1402.3034v1", 
    "author": "Inderveer Chana", 
    "publish": "2014-02-13T05:24:37Z", 
    "summary": "Cloud Computing is an emerging area for accessing computing resources. In\ngeneral, Cloud service providers offer services that can be clustered into\nthree categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload\nanalysis. The efficient Cloud workload resource mapping technique is proposed.\nThis paper aims to provide a means of understanding and investigating IaaS\nCloud workloads and the resources. In this paper, regression analysis is used\nto analyze the Cloud workloads and identifies the relationship between Cloud\nworkloads and available resources. The effective organization of dynamic nature\nresources can be done with the help of Cloud workloads. Till Cloud workload is\nconsidered a vital talent, the Cloud resources cannot be consumed in an\neffective style. The proposed technique has been validated by Z Formal\nspecification language. This approach is effective in minimizing the cost and\nsubmission burst time of Cloud workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3305v1", 
    "other_authors": "Martin Klein, Robert Sanderson, Herbert Van de Sompel, Michael L. Nelson", 
    "title": "Real-Time Notification for Resource Synchronization", 
    "arxiv-id": "1402.3305v1", 
    "author": "Michael L. Nelson", 
    "publish": "2014-02-11T21:04:54Z", 
    "summary": "Web applications frequently leverage resources made available by remote web\nservers. As resources are created, updated, deleted, or moved, these\napplications face challenges to remain in lockstep with the server's change\ndynamics. Several approaches exist to help meet this challenge for use cases\nwhere \"good enough\" synchronization is acceptable. But when strict resource\ncoverage or low synchronization latency is required, commonly accepted\nWeb-based solutions remain elusive. This paper details characteristics of an\napproach that aims at decreasing synchronization latency while maintaining\ndesired levels of accuracy. The approach builds on pushing change notifications\nand pulling changed resources and it is explored with an experiment based on a\nDBpedia Live instance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3319v2", 
    "other_authors": "Boris Skoric, Sebastiaan J. A. de Hoogh, Nicola Zannone", 
    "title": "Flow-based reputation with uncertainty: Evidence-Based Subjective Logic", 
    "arxiv-id": "1402.3319v2", 
    "author": "Nicola Zannone", 
    "publish": "2014-02-13T21:49:52Z", 
    "summary": "The concept of reputation is widely used as a measure of trustworthiness\nbased on ratings from members in a community. The adoption of reputation\nsystems, however, relies on their ability to capture the actual trustworthiness\nof a target. Several reputation models for aggregating trust information have\nbeen proposed in the literature. The choice of model has an impact on the\nreliability of the aggregated trust information as well as on the procedure\nused to compute reputations. Two prominent models are flow-based reputation\n(e.g., EigenTrust, PageRank) and Subjective Logic based reputation. Flow-based\nmodels provide an automated method to aggregate trust information, but they are\nnot able to express the level of uncertainty in the information. In contrast,\nSubjective Logic extends probabilistic models with an explicit notion of\nuncertainty, but the calculation of reputation depends on the structure of the\ntrust network and often requires information to be discarded. These are severe\ndrawbacks.\n  In this work, we observe that the `opinion discounting' operation in\nSubjective Logic has a number of basic problems. We resolve these problems by\nproviding a new discounting operator that describes the flow of evidence from\none party to another. The adoption of our discounting rule results in a\nconsistent Subjective Logic algebra that is entirely based on the handling of\nevidence. We show that the new algebra enables the construction of an automated\nreputation assessment procedure for arbitrary trust networks, where the\ncalculation no longer depends on the structure of the network, and does not\nneed to throw away any information. Thus, we obtain the best of both worlds:\nflow-based reputation and consistent handling of uncertainties."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3781v1", 
    "other_authors": "H I Alzeini, Sh A Hameed, M H Habaebi", 
    "title": "A Framework for Developing Real-Time OLAP algorithm using Multi-core   processing and GPU: Heterogeneous Computing", 
    "arxiv-id": "1402.3781v1", 
    "author": "M H Habaebi", 
    "publish": "2014-02-16T10:11:57Z", 
    "summary": "The overwhelmingly increasing amount of stored data has spurred researchers\nseeking different methods in order to optimally take advantage of it which\nmostly have faced a response time problem as a result of this enormous size of\ndata. Most of solutions have suggested materialization as a favourite solution.\nHowever, such a solution cannot attain Real- Time answers anyhow. In this paper\nwe propose a framework illustrating the barriers and suggested solutions in the\nway of achieving Real-Time OLAP answers that are significantly used in decision\nsupport systems and data warehouses."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3788v1", 
    "other_authors": "Natalya Litvinenko", 
    "title": "Using of GPUs for cluster analysis of large data by K-means method", 
    "arxiv-id": "1402.3788v1", 
    "author": "Natalya Litvinenko", 
    "publish": "2014-02-16T11:36:39Z", 
    "summary": "This problem was solved within the framework of the grant project \"Solving of\nproblems of cluster analysis with application of parallel algorithms and cloud\ntechnologies\" in the Institute of Mathematics and Mathematical Modelling in\nAlmaty. The problem of cluster analysis for the large amount of data is very\nimportant in different areas of science - genetics, biology, sociology etc. At\nthe same time, such statistical known packages as STATISTICA, STADIA, SYSTAT\nand others do not allow to solve large problems. The new algorithm that uses\nthe high processing power of GPUs for solving clustering problems by the\nK-means method was developed. This algorithm is implemented as a C++\napplication in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce\n660. The developed software package for solving clustering problems by the\nmethod of K - means with using GPUs allows us to handle up to 2 million records\nwith number of features up to 25. The gain in the computing time is in factor\n5. We plan to increase this factor up to 20-30 after improving the algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3789v1", 
    "other_authors": "Natalya Litvinenko", 
    "title": "Parallel algorithms for problems of cluster analysis with very large   amount of data", 
    "arxiv-id": "1402.3789v1", 
    "author": "Natalya Litvinenko", 
    "publish": "2014-02-16T11:53:30Z", 
    "summary": "In this paper we solve on GPUs massive problems with large amount of data,\nwhich are not appropriate for solution with the SIMD technology. For the given\nproblem we consider a three-level parallelization. The multithreading of CPU is\nused at the top level and graphic processors for massive computing. For solving\nproblems of cluster analysis on GPUs the nearest neighbor method (NNM) is\ndeveloped. This algorithm allows us to handle up to 2 millions records with\nnumber of features up to 25. Since sequential and parallel algorithms are\nfundamentally different, it is difficult to compare the computation times.\nHowever, some comparisons are made. The gain in the computing time is about 10\ntimes. We plan to increase this factor up to 50-100 after fine tuning of\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1080/13658816.2013.879151", 
    "link": "http://arxiv.org/pdf/1402.4010v1", 
    "other_authors": "Lucas Benedi\u010di\u010d, Felipe A. Cruz, Tsuyoshi Hamada, Peter Koro\u0161ec", 
    "title": "A GRASS GIS parallel module for radio-propagation predictions", 
    "arxiv-id": "1402.4010v1", 
    "author": "Peter Koro\u0161ec", 
    "publish": "2014-02-17T14:10:12Z", 
    "summary": "Geographical information systems are ideal candidates for the application of\nparallel programming techniques, mainly because they usually handle large data\nsets. To help us deal with complex calculations over such data sets, we\ninvestigated the performance constraints of a classic master-worker parallel\nparadigm over a message-passing communication model. To this end, we present a\nnew approach that employs an external database in order to improve the\ncalculation/communication overlap, thus reducing the idle times for the worker\nprocesses. The presented approach is implemented as part of a parallel\nradio-coverage prediction tool for the GRASS environment. The prediction\ncalculation employs digital elevation models and land-usage data in order to\nanalyze the radio coverage of a geographical area. We provide an extended\nanalysis of the experimental results, which are based on real data from an LTE\nnetwork currently deployed in Slovenia. Based on the results of the\nexperiments, which were performed on a computer cluster, the new approach\nexhibits better scalability than the traditional master-worker approach. We\nsuccessfully tackled real-world data sets, while greatly reducing the\nprocessing time and saturating the hardware utilization."
},{
    "category": "cs.DC", 
    "doi": "10.1080/13658816.2013.879151", 
    "link": "http://arxiv.org/pdf/1402.4247v1", 
    "other_authors": "Jae-Hyeon Parq, Erik Sevre, Sang-Mook Lee", 
    "title": "Effects of Easy Hybrid Parallelization with CUDA for   Numerical-Atomic-Orbital Density Functional Theory Calculation", 
    "arxiv-id": "1402.4247v1", 
    "author": "Sang-Mook Lee", 
    "publish": "2014-02-18T08:25:47Z", 
    "summary": "We modified a MPI-friendly density functional theory (DFT) source code within\nhybrid parallelization including CUDA. Our objective is to find out how simple\nconversions within the hybrid parallelization with mid-range GPUs affect DFT\ncode not originally suitable to CUDA. We settled several rules of hybrid\nparallelization for numerical-atomic-orbital (NAO) DFT codes. The test was\nperformed on a magnetite material system with OpenMX code by utilizing a\nhardware system containing 2 Xeon E5606 CPUs and 2 Quadro 4000 GPUs. 3-way\nhybrid routines obtained a speedup of 7.55 while 2-way hybrid speedup by 10.94.\nGPUs with CUDA complement the efficiency of OpenMP and compensate CPUs'\nexcessive competition within MPI."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22315381/IJETT-V8P273", 
    "link": "http://arxiv.org/pdf/1402.4758v2", 
    "other_authors": "Rachel Householder, Scott Arnold, Robert Green", 
    "title": "On Cloud-based Oversubscription", 
    "arxiv-id": "1402.4758v2", 
    "author": "Robert Green", 
    "publish": "2014-02-19T18:25:54Z", 
    "summary": "Rising trends in the number of customers turning to the cloud for their\ncomputing needs has made effective resource allocation imperative for cloud\nservice providers. In order to maximize profits and reduce waste, providers\nhave started to explore the role of oversubscribing cloud resources. However,\nthe benefits of cloud-based oversubscription are not without inherent risks.\nThis paper attempts to unveil the incentives, risks, and techniques behind\noversubscription in a cloud infrastructure. Additionally, an overview of the\ncurrent research that has been completed on this highly relevant topic is\nreviewed, and suggestions are made regarding potential avenues for future work."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22315381/IJETT-V8P273", 
    "link": "http://arxiv.org/pdf/1402.4958v1", 
    "other_authors": "Elli Androulaki, Christian Cachin, Dan Dobre, Marko Vukolic", 
    "title": "Erasure-Coded Byzantine Storage with Separate Metadata", 
    "arxiv-id": "1402.4958v1", 
    "author": "Marko Vukolic", 
    "publish": "2014-02-20T10:53:07Z", 
    "summary": "Although many distributed storage protocols have been introduced, a solution\nthat combines the strongest properties in terms of availability, consistency,\nfault-tolerance, storage complexity and the supported level of concurrency, has\nbeen elusive for a long time. Combining these properties is difficult,\nespecially if the resulting solution is required to be efficient and incur low\ncost. We present AWE, the first erasure-coded distributed implementation of a\nmulti-writer multi-reader read/write storage object that is, at the same time:\n(1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (i.e., with data\nnodes storing a bounded number of values) and (5) Byzantine fault-tolerant\n(BFT) using the optimal number of nodes. Furthermore, AWE is efficient since it\ndoes not use public-key cryptography and requires data nodes that support only\nreads and writes, further reducing the cost of deployment and ownership of a\ndistributed storage solution. Notably, AWE stores metadata separately from\n$k$-out-of-$n$ erasure-coded fragments. This enables AWE to be the first BFT\nprotocol that uses as few as $2t+k$ data nodes to tolerate $t$ Byzantine nodes,\nfor any $k \\ge 1$."
},{
    "category": "cs.DC", 
    "doi": "10.1186/s40064-016-1731-6", 
    "link": "http://arxiv.org/pdf/1402.4986v1", 
    "other_authors": "Gang Mei, Hong Tian", 
    "title": "Performance Impact of Data Layout on the GPU-accelerated IDW   Interpolation", 
    "arxiv-id": "1402.4986v1", 
    "author": "Hong Tian", 
    "publish": "2014-02-20T12:44:09Z", 
    "summary": "This paper focuses on evaluating the performance impact of different data\nlayouts on the GPU-accelerated IDW interpolation. First, we redesign and\nimprove our previous GPU implementation that was performed by exploiting the\nfeature CUDA Dynamic Parallel (CDP). And then, we implement three versions of\nGPU implementations, i.e., the naive version, the tiled version, and the\nimproved CDP version, based on five layouts including the Structure of Arrays\n(SoA), the Array of Sturcutes (AoS), the Array of aligned Sturcutes (AoaS), the\nStructure of Arrays of aligned Structures (SoAoS), and the Hybrid layout.\nExperimental results show that: the layouts AoS and AoaS achieve better\nperformance than the layout SoA for both the naive version and tiled version,\nwhile the layout SoA is the best choice for the improved CDP version. We also\nobserve that: for the two combined data layouts (the SoAoS and the Hybrid),\nthere are no notable performance gains when compared to other three basic\nlayouts. We recommend that: in practical applications, the layout AoaS is the\nbest choice since the tiled version is the fastest one among the three versions\nof GPU implementations, especially on single precision."
},{
    "category": "cs.DC", 
    "doi": "10.1186/s40064-016-1731-6", 
    "link": "http://arxiv.org/pdf/1402.5642v1", 
    "other_authors": "Ankur Sahai", 
    "title": "VM Power Prediction in Distributed Systems for Maximizing Renewable   Energy Usage", 
    "arxiv-id": "1402.5642v1", 
    "author": "Ankur Sahai", 
    "publish": "2014-02-23T17:56:00Z", 
    "summary": "In the context of GreenPAD project it is important to predict the energy\nconsumption of individual (and mixture of) VMs / workload for optimal\nscheduling (running those VMs which require higher energy when there is more\ngreen energy available and vice-versa) in order to maximize green energy\nutilization.\n  For this we execute the following experiments on an Openstack cloud testbed\nconsisting of Fujitsu servers: VM energy measurement for different\nconfigurations (flavor + workload) and VM energy prediction for a new\nconfiguration. The automation framework for running these experiments uses bash\nscripts which call tools like 'stress' (simulating workloads), 'collected'\n(resource usage) and 'IPMI' (power measurement).\n  We propose a linear model for predicting the power usage of the VMs based on\nregression. We first collect the resource usage (using collected) and the\nassociated power usage (using IPMI) for different VM configurations and use\nthis to build a (multi-) regression model (between resource usage and VM energy\nconsumption). Then we use the information about the resource usage patterns of\nthe new workload to predict the power usage. For predicting power for mix of\nworkloads we execute (build a regression model based on) experiments with\nrandom workloads. We observe the highest energy usage for CPU-intensive\nworkloads followed by memory-intensive workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1402.6601v2", 
    "other_authors": "Rapha\u00ebl Bleuse, Thierry Gautier, Jo\u00e3o V. F. Lima, Gr\u00e9gory Mouni\u00e9, Denis Trystram", 
    "title": "Scheduling data flow program in xkaapi: A new affinity based Algorithm   for Heterogeneous Architectures", 
    "arxiv-id": "1402.6601v2", 
    "author": "Denis Trystram", 
    "publish": "2014-02-26T16:37:01Z", 
    "summary": "Efficient implementations of parallel applications on heterogeneous hybrid\narchitectures require a careful balance between computations and communications\nwith accelerator devices. Even if most of the communication time can be\noverlapped by computations, it is essential to reduce the total volume of\ncommunicated data. The literature therefore abounds with ad-hoc methods to\nreach that balance, but that are architecture and application dependent. We\npropose here a generic mechanism to automatically optimize the scheduling\nbetween CPUs and GPUs, and compare two strategies within this mechanism: the\nclassical Heterogeneous Earliest Finish Time (HEFT) algorithm and our new,\nparametrized, Distributed Affinity Dual Approximation algorithm (DADA), which\nconsists in grouping the tasks by affinity before running a fast dual\napproximation. We ran experiments on a heterogeneous parallel machine with six\nCPU cores and eight NVIDIA Fermi GPUs. Three standard dense linear algebra\nkernels from the PLASMA library have been ported on top of the Xkaapi runtime.\nWe report their performances. It results that HEFT and DADA perform well for\nvarious experimental conditions, but that DADA performs better for larger\nsystems and number of GPUs, and, in most cases, generates much lower data\ntransfers than HEFT to achieve the same performance."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.0050v1", 
    "other_authors": "P. Armstrong, A. Agarwal, A. Bishop, A. Charbonneau, R. Desmarais, K. Fransham, N. Hill, I. Gable, S. Gaudet, S. Goliath, R. Impey, C. Leavett-Brown, J. Ouellete, M. Paterson, C. Pritchet, D. Penfold-Brown, W. Podaima, D. Schade, R. J. Sobie", 
    "title": "Cloud Scheduler: a resource manager for distributed compute clouds", 
    "arxiv-id": "1007.0050v1", 
    "author": "R. J. Sobie", 
    "publish": "2010-06-30T23:54:01Z", 
    "summary": "The availability of Infrastructure-as-a-Service (IaaS) computing clouds gives\nresearchers access to a large set of new resources for running complex\nscientific applications. However, exploiting cloud resources for large numbers\nof jobs requires significant effort and expertise. In order to make it simple\nand transparent for researchers to deploy their applications, we have developed\na virtual machine resource manager (Cloud Scheduler) for distributed compute\nclouds. Cloud Scheduler boots and manages the user-customized virtual machines\nin response to a user's job submission. We describe the motivation and design\nof the Cloud Scheduler and present results on its use on both science and\ncommercial clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.0066v2", 
    "other_authors": "Anton Beloglazov, Rajkumar Buyya, Young Choon Lee, Albert Zomaya", 
    "title": "A Taxonomy and Survey of Energy-Efficient Data Centers and Cloud   Computing Systems", 
    "arxiv-id": "1007.0066v2", 
    "author": "Albert Zomaya", 
    "publish": "2010-07-01T04:44:27Z", 
    "summary": "Traditionally, the development of computing systems has been focused on\nperformance improvements driven by the demand of applications from consumer,\nscientific and business domains. However, the ever increasing energy\nconsumption of computing systems has started to limit further performance\ngrowth due to overwhelming electricity bills and carbon dioxide footprints.\nTherefore, the goal of the computer system design has been shifted to power and\nenergy efficiency. To identify open challenges in the area and facilitate\nfuture advancements it is essential to synthesize and classify the research on\npower and energy-efficient design conducted to date. In this work we discuss\ncauses and problems of high power / energy consumption, and present a taxonomy\nof energy-efficient design of computing systems covering the hardware,\noperating system, virtualization and data center levels. We survey various key\nworks in the area and map them to our taxonomy to guide future design and\ndevelopment efforts. This chapter is concluded with a discussion of\nadvancements identified in energy-efficient computing and our vision on future\nresearch directions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.0107v1", 
    "other_authors": "Graham Kirby, Alan Dearle, Andrew McCarthy, Ron Morrison, Kevin Mullen, Yanyan Yang, Richard Connor, Paula Welen, Andy Wilson", 
    "title": "First Smart Spaces", 
    "arxiv-id": "1007.0107v1", 
    "author": "Andy Wilson", 
    "publish": "2010-07-01T09:11:42Z", 
    "summary": "This document describes the Gloss software currently implemented. The\ndescription of the Gloss demonstrator for multi-surface interaction can be\nfound in D17. The ongoing integration activity for the work described in D17\nand D8 constitutes our development of infrastructure for a first smart space.\nIn this report, the focus is on infrastructure to support the implementation of\nlocation aware services. A local architecture provides a framework for\nconstructing Gloss applications, termed assemblies, that run on individual\nphysical nodes. A global architecture defines an overlay network for linking\nindividual assemblies. Both local and global architectures are under active\ndevelopment."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.0328v1", 
    "other_authors": "Markus Tauber", 
    "title": "Autonomic Management in a Distributed Storage System", 
    "arxiv-id": "1007.0328v1", 
    "author": "Markus Tauber", 
    "publish": "2010-07-02T10:08:57Z", 
    "summary": "This thesis investigates the application of autonomic management to a\ndistributed storage system. Effects on performance and resource consumption\nwere measured in experiments, which were carried out in a local area test-bed.\nThe experiments were conducted with components of one specific distributed\nstorage system, but seek to be applicable to a wide range of such systems, in\nparticular those exposed to varying conditions. The perceived characteristics\nof distributed storage systems depend on their configuration parameters and on\nvarious dynamic conditions. For a given set of conditions, one specific\nconfiguration may be better than another with respect to measures such as\nresource consumption and performance. Here, configuration parameter values were\nset dynamically and the results compared with a static configuration. It was\nhypothesised that under non-changing conditions this would allow the system to\nconverge on a configuration that was more suitable than any that could be set a\npriori. Furthermore, the system could react to a change in conditions by\nadopting a more appropriate configuration. Autonomic management was applied to\nthe peer-to-peer (P2P) and data retrieval components of ASA, a distributed\nstorage system. The effects were measured experimentally for various workload\nand churn patterns. The management policies and mechanisms were implemented\nusing a generic autonomic management framework developed during this work. The\nexperimental evaluations of autonomic management show promising results, and\nsuggest several future research topics. The findings of this thesis could be\nexploited in building other distributed storage systems that focus on\nharnessing storage on user workstations, since these are particularly likely to\nbe exposed to varying, unpredictable conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.1049v3", 
    "other_authors": "Michael Ben-Or, Danny Dolev, Ezra N. Hoch", 
    "title": "Simple Gradecast Based Algorithms", 
    "arxiv-id": "1007.1049v3", 
    "author": "Ezra N. Hoch", 
    "publish": "2010-07-07T04:50:43Z", 
    "summary": "Gradecast is a simple three-round algorithm presented by Feldman and Micali.\nThe current work presents a very simple algorithm that utilized Gradecast to\nachieve Byzantine agreement. Two small variations of the presented algorithm\nlead to improved algorithms for solving the Approximate agreement problem and\nthe Multi-consensus problem.\n  An optimal approximate agreement algorithm was presented by Fekete, which\nsupports up to 1/4 n Byzantine nodes and has message complexity of O(n^k),\nwhere n is the number of nodes and k is the number of rounds.\n  Our solution to the approximate agreement problem is optimal, simple and\nreduces the message complexity to O(k * n^3), while supporting up to 1/3 n\nByzantine nodes.\n  Multi consensus was first presented by Bar-Noy et al. It consists of\nconsecutive executions of l Byzantine consensuses. Bar-Noy et al., show an\noptimal amortized solution to this problem, assuming that all nodes start each\nconsensus instance at the same time, a property that cannot be guaranteed with\nearly stopping. Our solution is simpler, preserves round complexity optimality,\nallows early stopping and does not require synchronized starts of the consensus\ninstances."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.1086v1", 
    "other_authors": "Michael Okun", 
    "title": "On the Power of Impersonation Attacks", 
    "arxiv-id": "1007.1086v1", 
    "author": "Michael Okun", 
    "publish": "2010-07-07T09:33:58Z", 
    "summary": "In this paper we consider a synchronous message passing system in which in\nevery round an external adversary is able to send each processor up to k\nmessages with falsified sender identities and arbitrary content. It is formally\nshown that this impersonation model is slightly stronger than the asynchronous\nmessage passing model with crash failures. In particular, we prove that\n(k+1)-set agreement can be solved in this model, while k-set agreement is\nimpossible, for any k>=1. The different strength of the asynchronous and\nimpersonation models is exhibited by the order preserving renaming problem, for\nwhich an algorithm with n+k target namespace exists in the impersonation model,\nwhile an exponentially larger namespace is required in case of asynchrony."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.1189v3", 
    "other_authors": "Andrea Richa, Christian Scheideler, Stefan Schmid, Jin Zhang", 
    "title": "A Jamming-Resistant MAC Protocol for Multi-Hop Wireless Networks", 
    "arxiv-id": "1007.1189v3", 
    "author": "Jin Zhang", 
    "publish": "2010-07-07T17:25:53Z", 
    "summary": "This paper presents a simple local medium access control protocol, called\n\\textsc{Jade}, for multi-hop wireless networks with a single channel that is\nprovably robust against adaptive adversarial jamming. The wireless network is\nmodeled as a unit disk graph on a set of nodes distributed arbitrarily in the\nplane. In addition to these nodes, there are adversarial jammers that know the\nprotocol and its entire history and that are allowed to jam the wireless\nchannel at any node for an arbitrary $(1-\\epsilon)$-fraction of the time steps,\nwhere $0<\\epsilon<1$ is an arbitrary constant. We assume that the nodes cannot\ndistinguish between jammed transmissions and collisions of regular messages.\nNevertheless, we show that \\textsc{Jade} achieves an asymptotically optimal\nthroughput if there is a sufficiently dense distribution of nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.1261v1", 
    "other_authors": "Collin Bennett, Robert L. Grossman, David Locke, Jonathan Seidman, Steve Vejcik", 
    "title": "MalStone: Towards A Benchmark for Analytics on Large Data Clouds", 
    "arxiv-id": "1007.1261v1", 
    "author": "Steve Vejcik", 
    "publish": "2010-07-07T23:26:22Z", 
    "summary": "Developing data mining algorithms that are suitable for cloud computing\nplatforms is currently an active area of research, as is developing cloud\ncomputing platforms appropriate for data mining. Currently, the most common\nbenchmark for cloud computing is the Terasort (and related) benchmarks.\nAlthough the Terasort Benchmark is quite useful, it was not designed for data\nmining per se. In this paper, we introduce a benchmark called MalStone that is\nspecifically designed to measure the performance of cloud computing middleware\nthat supports the type of data intensive computing common when building data\nmining models. We also introduce MalGen, which is a utility for generating data\non clouds that can be used with MalStone."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.1709v2", 
    "other_authors": "Ezra N. Hoch, Michael Ben-Or, Danny Dolev", 
    "title": "A Fault-Resistant Asynchronous Clock Function", 
    "arxiv-id": "1007.1709v2", 
    "author": "Danny Dolev", 
    "publish": "2010-07-10T09:45:11Z", 
    "summary": "Consider an asynchronous network in a shared-memory environment consisting of\nn nodes. Assume that up to f of the nodes might be Byzantine (n > 12f), where\nthe adversary is full-information and dynamic (sometimes called adaptive). In\naddition, the non-Byzantine nodes may undergo transient failures. Nodes advance\nin atomic steps, which consist of reading all registers, performing some\ncalculation and writing to all registers.\n  This paper contains three main contributions. First, the clock-function\nproblem is defined, which is a generalization of the clock synchronization\nproblem. This generalization encapsulates previous clock synchronization\nproblem definitions while extending them to the current paper's model. Second,\na randomized asynchronous self-stabilizing Byzantine tolerant clock\nsynchronization algorithm is presented.\n  In the construction of the clock synchronization algorithm, a building block\nthat ensures different nodes advance at similar rates is developed. This\nfeature is the third contribution of the paper. It is self-stabilizing and\nByzantine tolerant and can be used as a building block for different algorithms\nthat operate in an asynchronous self-stabilizing Byzantine model.\n  The convergence time of the presented algorithm is exponential. Observe that\nin the asynchronous setting the best known full-information dynamic Byzantine\nagreement also has expected exponential convergence time, even though currently\nthere is no known reduction between the two."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.2902v1", 
    "other_authors": "Guoqiang Zhang, Suqi Cheng, Guoqing Zhang", 
    "title": "LANC: locality-aware network coding for better P2P traffic localization", 
    "arxiv-id": "1007.2902v1", 
    "author": "Guoqing Zhang", 
    "publish": "2010-07-17T06:25:34Z", 
    "summary": "As ISPs begin to cooperate to expose their network locality information as\nservices, e.g., P4P, solutions based on locality information provision for P2P\ntraffic localization will soon approach their capability limits. A natural\nquestion is: can we do any better provided that no further locality information\nimprovement can be made? This paper shows how the utility of locality\ninformation could be limited by conventional P2P data scheduling algorithms,\neven as sophisticated as the local rarest first policy.\n  Network coding's simplified data scheduling makes it competent for improving\nP2P application's throughput. Instead of only using locality information in the\ntopology construction, this paper proposes the locality-aware network coding\n(LANC) that uses locality information in both the topology construction and\ndownloading decision, and demonstrates its exceptional ability for P2P traffic\nlocalization. The randomization introduced by network coding enhances the\nchance for a peer to find innovative blocks in its neighborhood. Aided by\nproper locality-awareness, the probability for a peer to get innovative blocks\nfrom its proximity will increase as well, resulting in more efficient use of\nnetwork resources. Extensive simulation results show that LANC can\nsignificantly reduce P2P traffic redundancy without sacrificing\napplication-level performance. Aided by the same locality knowledge, the\ntraffic redundancies of LANC in most cases are less than 50\\% of the current\nbest approach that does not use network coding."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1007.4057v1", 
    "other_authors": "Bo Sang, Jianfeng Zhan, Zhihong Zhang, Lei Wang, Dongyan Xu, Yabing Huang, Dan Meng", 
    "title": "Precise, Scalable and Online Request Tracing for Multi-tier Services of   Black Boxes", 
    "arxiv-id": "1007.4057v1", 
    "author": "Dan Meng", 
    "publish": "2010-07-23T07:37:32Z", 
    "summary": "As more and more multi-tier services are developed from commercial\noff-the-shelf components or heterogeneous middleware without source code\navailable, both developers and administrators need a request tracing tool to\n(1) exactly know how a user request of interest travels through services of\nblack boxes; (2) obtain macro-level user request behavior information of\nservices without the necessity of inundating within massive logs. Previous\nresearch efforts either accept imprecision of probabilistic correlation methods\nor present precise but unscalable tracing approaches that have to collect and\nanalyze large amount of logs; Besides, previous precise request tracing\napproaches of black boxes fail to propose macro-level abstractions that enables\ndebugging performance-in-the-large, and hence users have to manually interpret\nmassive logs. This paper introduces a precise, scalable and online request\ntracing tool, named PreciseTracer, for multi-tier services of black boxes. Our\ncontributions are four-fold: first, we propose a precise request tracing\nalgorithm for multi-tier services of black boxes, which only uses\napplication-independent knowledge; second, we respectively present micro-level\nand macro-level abstractions: component activity graphs and dominated causal\npath patterns to represent causal paths of each individual request and\nrepeatedly executed causal paths that account for significant fractions; third,\nwe present two mechanisms: tracing on demand and sampling to significantly\nincrease system scalability; fourth, we design and implement an online request\ntracing tool. PreciseTracer's fast response, low overhead and scalability make\nit a promising tracing tool for large-scale production systems."
},{
    "category": "cs.DC", 
    "doi": "10.1017/S1471068410000190", 
    "link": "http://arxiv.org/pdf/1007.4438v1", 
    "other_authors": "V\u00edtor Santos Costa, In\u00eas Dutra, Ricardo Rocha", 
    "title": "Threads and Or-Parallelism Unified", 
    "arxiv-id": "1007.4438v1", 
    "author": "Ricardo Rocha", 
    "publish": "2010-07-26T12:36:31Z", 
    "summary": "One of the main advantages of Logic Programming (LP) is that it provides an\nexcellent framework for the parallel execution of programs. In this work we\ninvestigate novel techniques to efficiently exploit parallelism from real-world\napplications in low cost multi-core architectures. To achieve these goals, we\nrevive and redesign the YapOr system to exploit or-parallelism based on a\nmulti-threaded implementation. Our new approach takes full advantage of the\nstate-of-the-art fast and optimized YAP Prolog engine and shares the underlying\nexecution environment, scheduler and most of the data structures used to\nsupport YapOr's model. Initial experiments with our new approach consistently\nachieve almost linear speedups for most of the applications, proving itself as\na good alternative for exploiting implicit parallelism in the currently\navailable low cost multi-core architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1063/1.3462759", 
    "link": "http://arxiv.org/pdf/1007.4723v1", 
    "other_authors": "A. A. Waskita, N. A. Prasetyo, Z. Akbar, L. T. Handoko", 
    "title": "Public Infrastructure for Monte Carlo Simulation: publicMC@BATAN", 
    "arxiv-id": "1007.4723v1", 
    "author": "L. T. Handoko", 
    "publish": "2010-07-27T13:58:10Z", 
    "summary": "The first cluster-based public computing for Monte Carlo simulation in\nIndonesia is introduced. The system has been developed to enable public to\nperform Monte Carlo simulation on a parallel computer through an integrated and\nuser friendly dynamic web interface. The beta version, so called\npublicMC@BATAN, has been released and implemented for internal users at the\nNational Nuclear Energy Agency (BATAN). In this paper the concept and\narchitecture of publicMC@BATAN are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1063/1.3462759", 
    "link": "http://arxiv.org/pdf/1007.4890v2", 
    "other_authors": "Lin Yuan, Jianfeng Zhan, Bo Sang, Lei Wang, Haining Wang", 
    "title": "PowerTracer: Tracing requests in multi-tier services to save cluster   power consumption", 
    "arxiv-id": "1007.4890v2", 
    "author": "Haining Wang", 
    "publish": "2010-07-28T08:13:24Z", 
    "summary": "As energy proportional computing gradually extends the success of DVFS\n(Dynamic voltage and frequency scaling) to the entire system, DVFS control\nalgorithms will play a key role in reducing server clusters' power consumption.\nThe focus of this paper is to provide accurate cluster-level DVFS control for\npower saving in a server cluster. To achieve this goal, we propose a request\ntracing approach that online classifies the major causal path patterns of a\nmulti-tier service and monitors their performance data as a guide for accurate\nDVFS control. The request tracing approach significantly decreases the time\ncost of performance profiling experiments that aim to establish the empirical\nperformance model. Moreover, it decreases the controller complexity so that we\ncan introduce a much simpler feedback controller, which only relies on the\nsingle-node DVFS modulation at a time as opposed to varying multiple CPU\nfrequencies simultaneously. Based on the request tracing approach, we present a\nhybrid DVFS control system that combines an empirical performance model for\nfast modulation at different load levels and a simpler feedback controller for\nadaption. We implement a prototype of the proposed system, called PowerTracer,\nand conduct extensive experiments on a 3-tier platform. Our experimental\nresults show that PowerTracer outperforms its peer in terms of power saving and\nsystem performance."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1007.5088v1", 
    "other_authors": "Jan-Mark S. Wams, Maarten van Steen", 
    "title": "Simplified Distributed Programming with Micro Objects", 
    "arxiv-id": "1007.5088v1", 
    "author": "Maarten van Steen", 
    "publish": "2010-07-29T00:12:33Z", 
    "summary": "Developing large-scale distributed applications can be a daunting task.\nobject-based environments have attempted to alleviate problems by providing\ndistributed objects that look like local objects. We advocate that this\napproach has actually only made matters worse, as the developer needs to be\naware of many intricate internal details in order to adequately handle partial\nfailures. The result is an increase of application complexity. We present an\nalternative in which distribution transparency is lessened in favor of clearer\nsemantics. In particular, we argue that a developer should always be offered\nthe unambiguous semantics of local objects, and that distribution comes from\ncopying those objects to where they are needed. We claim that it is often\nsufficient to provide only small, immutable objects, along with facilities to\ngroup objects into clusters."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.0198v1", 
    "other_authors": "Boris D. Lubachevsky", 
    "title": "Why The Results of Parallel and Serial Monte Carlo Simulations May   Differ", 
    "arxiv-id": "1104.0198v1", 
    "author": "Boris D. Lubachevsky", 
    "publish": "2011-04-01T15:26:43Z", 
    "summary": "Parallel Monte Carlo simulations often expose faults in random number\ngenerators"
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.2566v1", 
    "other_authors": "Erik Saule, Erdeniz \u00d6. Ba\u015f, \u00dcmit V. \u00c7ataly\u00fcrek", 
    "title": "Load-Balancing Spatially Located Computations using Rectangular   Partitions", 
    "arxiv-id": "1104.2566v1", 
    "author": "\u00dcmit V. \u00c7ataly\u00fcrek", 
    "publish": "2011-04-13T18:08:57Z", 
    "summary": "Distributing spatially located heterogeneous workloads is an important\nproblem in parallel scientific computing. We investigate the problem of\npartitioning such workloads (represented as a matrix of non-negative integers)\ninto rectangles, such that the load of the most loaded rectangle (processor) is\nminimized. Since finding the optimal arbitrary rectangle-based partition is an\nNP-hard problem, we investigate particular classes of solutions: rectilinear,\njagged and hierarchical. We present a new class of solutions called m-way\njagged partitions, propose new optimal algorithms for m-way jagged partitions\nand hierarchical partitions, propose new heuristic algorithms, and provide\nworst case performance analyses for some existing and new heuristics. Moreover,\nthe algorithms are tested in simulation on a wide set of instances. Results\nshow that two of the algorithms we introduce lead to a much better load balance\nthan the state-of-the-art algorithms. We also show how to design a two-phase\nalgorithm that reaches different time/quality tradeoff."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.3876v1", 
    "other_authors": "Michiel W. van Tol, Juha Koivisto", 
    "title": "Extending and Implementing the Self-adaptive Virtual Processor for   Distributed Memory Architectures", 
    "arxiv-id": "1104.3876v1", 
    "author": "Juha Koivisto", 
    "publish": "2011-04-19T20:44:19Z", 
    "summary": "Many-core architectures of the future are likely to have distributed memory\norganizations and need fine grained concurrency management to be used\neffectively. The Self-adaptive Virtual Processor (SVP) is an abstract\nconcurrent programming model which can provide this, but the model and its\ncurrent implementations assume a single address space shared memory. We\ninvestigate and extend SVP to handle distributed environments, and discuss a\nprototype SVP implementation which transparently supports execution on\nheterogeneous distributed memory clusters over TCP/IP connections, while\nretaining the original SVP programming model."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.3947v1", 
    "other_authors": "Shlomi Dolev, Swan Dubois, Maria Potop-Butucaru, S\u00e9bastien Tixeuil", 
    "title": "Communication Optimalement Stabilisante sur Canaux non Fiables et non   FIFO", 
    "arxiv-id": "1104.3947v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-20T06:08:34Z", 
    "summary": "A self-stabilizing protocol has the capacity to recover a legitimate behavior\nwhatever is its initial state. The majority of works in self-stabilization\nassume a shared memory model or a communication using reliable and FIFO\nchannels. In this article, we interest in self-stabilizing systems using\nbounded but non reliable and non FIFO channels. We propose a stabilizing\ncommunication protocol with optimal fault resilience. In more details, this\nprotocol simulates a reliable and FIFO channel and ensures a minimal number of\nlooses, duplications, creations, and re-ordering of messages."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4022v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "Auto-Stabilisation et Confinement de Fautes Malicieuses : Optimalit\u00e9   du Protocole min+1", 
    "arxiv-id": "1104.4022v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-20T13:45:26Z", 
    "summary": "A self-stabilizing is naturally resilient to transients faults (that is,\nfaults of finite duration). Recently, a new class of protocol appears. These\nprotocols are self-stabilizing and are moreover resilient to a limited number\nof permanent faults. In this article, we interest in self-stabilizing protocols\nthat tolerate very hard permanent faults: Byzantine faults. We introduce two\nnew scheme of Byzantine containment in self-stabilizing systems. We show that,\nfor the problem of BFS spanning tree construction, the well known\nself-stabilizing protocol min+1 provides without significant modification the\nbest Byzantine containment with respect to these new schemes."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4379v1", 
    "other_authors": "Rajkumar Buyya, Karthik Sukumar", 
    "title": "Platforms for Building and Deploying Applications for Cloud Computing", 
    "arxiv-id": "1104.4379v1", 
    "author": "Karthik Sukumar", 
    "publish": "2011-04-22T02:51:54Z", 
    "summary": "Cloud computing is rapidly emerging as a new paradigm for delivering IT\nservices as utlity-oriented services on subscription-basis. The rapid\ndevelopment of applications and their deployment in Cloud computing\nenvironments in efficient manner is a complex task. In this article, we give a\nbrief introduction to Cloud computing technology and Platform as a Service, we\nexamine the offerings in this category, and provide the basis for helping\nreaders to understand basic application platform opportunities in Cloud by\ntechnologies such as Microsoft Azure, Sales Force, Google App, and Aneka for\nCloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application\nPlatform (CAP) leveraging these concepts and allowing an easy development of\nCloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers\nfacilities for quickly developing Cloud applications and a modular platform\nwhere additional services can be easily integrated to extend the system\ncapabilities, thus being at pace with the rapidly evolution of Cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4475v1", 
    "other_authors": "Henricus Bouwmeester, Mathias Jacquelin, Julien Langou, Yves Robert", 
    "title": "Tiled QR factorization algorithms", 
    "arxiv-id": "1104.4475v1", 
    "author": "Yves Robert", 
    "publish": "2011-04-22T16:45:02Z", 
    "summary": "This work revisits existing algorithms for the QR factorization of\nrectangular matrices composed of p-by-q tiles, where p >= q. Within this\nframework, we study the critical paths and performance of algorithms such as\nSameh and Kuck, Modi and Clarke, Greedy, and those found within PLASMA.\nAlthough neither Modi and Clarke nor Greedy is optimal, both are shown to be\nasymptotically optimal for all matrices of size p = q^2 f(q), where f is any\nfunction such that \\lim_{+\\infty} f= 0. This novel and important complexity\nresult applies to all matrices where p and q are proportional, p = \\lambda q,\nwith \\lambda >= 1, thereby encompassing many important situations in practice\n(least squares). We provide an extensive set of experiments that show the\nsuperiority of the new algorithms for tall matrices."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.5368v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "Maximum Metric Spanning Tree made Byzantine Tolerant", 
    "arxiv-id": "1104.5368v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-28T12:14:53Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. This paper focus on systems that are both\nself-stabilizing and Byzantine tolerant. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties is known to induce many impossibility results. In this paper, we\nprovide first two impossibility results about the construction of maximum\nmetric tree in presence of transients and (permanent) Byzantine faults. Then,\nwe provide a new self-stabilizing protocol that provides optimal containment of\nan arbitrary number of Byzantine faults."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.5643v2", 
    "other_authors": "Marie Albenque, Lucas Gerin", 
    "title": "On the algebraic numbers computable by some generalized Ehrenfest urns", 
    "arxiv-id": "1104.5643v2", 
    "author": "Lucas Gerin", 
    "publish": "2011-04-29T15:00:08Z", 
    "summary": "This article deals with some stochastic population protocols, motivated by\ntheoretical aspects of distributed computing. We modelize the problem by a\nlarge urn of black and white balls from which at every time unit a fixed number\nof balls are drawn and their colors are changed according to the number of\nblack balls among them. When the time and the number of balls both tend to\ninfinity the proportion of black balls converges to an algebraic number. We\nprove that, surprisingly enough, not every algebraic number can be \"computed\"\nthis way."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.5660v1", 
    "other_authors": "Sayaka Kamei, Anissa Lamani, Fukuhito Ooshita, S\u00e9bastien Tixeuil", 
    "title": "Asynchronous mobile robot gathering from symmetric configurations   without global multiplicity detection", 
    "arxiv-id": "1104.5660v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-29T15:20:45Z", 
    "summary": "We consider a set of k autonomous robots that are endowed with visibility\nsensors (but that are otherwise unable to communicate) and motion actuators.\nThose robots must collaborate to reach a sin- gle vertex that is unknown\nbeforehand, and to remain there hereafter. Previous works on gathering in\nring-shaped networks suggest that there exists a tradeoff between the size of\nthe set of potential initial configurations, and the power of the sensing\ncapabilities of the robots (i.e. the larger the initial configuration set, the\nmost powerful the sensor needs to be). We prove that there is no such trade\noff. We propose a gathering protocol for an odd number of robots in a\nring-shaped network that allows symmetric but not periodic configurations as\ninitial configurations, yet uses only local weak multiplicity detection. Robots\nare assumed to be anonymous and oblivious, and the execution model is the\nnon-atomic CORDA model with asynchronous fair scheduling. Our protocol allows\nthe largest set of initial configurations (with respect to impossibility\nresults) yet uses the weakest multiplicity detector to date. The time\ncomplexity of our protocol is O(n2), where n denotes the size of the ring.\nCompared to previous work that also uses local weak multiplicity detection, we\ndo not have the constraint that k < n/2 (here, we simply have 2 < k < n - 3)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.0414v1", 
    "other_authors": "Lisbeth Fajstrup, Eric Goubault, Emmanuel Haucourt, Samuel Mimram, Martin Raussen", 
    "title": "Trace Spaces: an Efficient New Technique for State-Space Reduction", 
    "arxiv-id": "1204.0414v1", 
    "author": "Martin Raussen", 
    "publish": "2012-04-02T14:24:18Z", 
    "summary": "State-space reduction techniques, used primarily in model-checkers, all rely\non the idea that some actions are independent, hence could be taken in any\n(respective) order while put in parallel, without changing the semantics. It is\nthus not necessary to consider all execution paths in the interleaving\nsemantics of a concurrent program, but rather some equivalence classes. The\npurpose of this paper is to describe a new algorithm to compute such\nequivalence classes, and a representative per class, which is based on ideas\noriginating in algebraic topology. We introduce a geometric semantics of\nconcurrent languages, where programs are interpreted as directed topological\nspaces, and study its properties in order to devise an algorithm for computing\ndihomotopy classes of execution paths. In particular, our algorithm is able to\ncompute a control-flow graph for concurrent programs, possibly containing\nloops, which is \"as reduced as possible\" in the sense that it generates traces\nmodulo equivalence. A preliminary implementation was achieved, showing\npromising results towards efficient methods to analyze concurrent programs,\nwith very promising results compared to partial-order reduction techniques."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.0641v5", 
    "other_authors": "Martin Biely, Peter Robinson, Ulrich Schmid", 
    "title": "Agreement in Directed Dynamic Networks", 
    "arxiv-id": "1204.0641v5", 
    "author": "Ulrich Schmid", 
    "publish": "2012-04-03T10:00:45Z", 
    "summary": "We study distributed computation in synchronous dynamic networks where an\nomniscient adversary controls the unidirectional communication links. Its\nbehavior is modeled as a sequence of directed graphs representing the active\n(i.e. timely) communication links per round. We prove that consensus is\nimpossible under some natural weak connectivity assumptions, and introduce\nvertex-stable root components as a means for circumventing this impossibility.\nEssentially, we assume that there is a short period of time during which an\narbitrary part of the network remains strongly connected, while its\ninterconnect topology may keep changing continuously. We present a consensus\nalgorithm that works under this assumption, and prove its correctness. Our\nalgorithm maintains a local estimate of the communication graphs, and applies\ntechniques for detecting stable network properties and univalent system\nconfigurations. Our possibility results are complemented by several\nimpossibility results and lower bounds for consensus and other distributed\ncomputing problems like leader election, revealing that our algorithm is\nasymptotically optimal."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.1516v1", 
    "other_authors": "Rajesh Kumar Bawa, Gaurav Sharma", 
    "title": "Reliable Resource Selection in Grid Environment", 
    "arxiv-id": "1204.1516v1", 
    "author": "Gaurav Sharma", 
    "publish": "2012-04-06T17:09:53Z", 
    "summary": "The primary concern in area of computational grid is security and resources.\nMost of the existing grids address this problem by authenticating the users,\nhosts and their interactions in an appropriate manner. A secured system is\ncompulsory for the efficient utilization of grid services. The high degree of\nstrangeness has been identified as the problem factors in the secured selection\nof grid. Without the assurance of a higher degree of trust relationship,\ncompetent resource selection and utilization cannot be achieved. In this paper\nwe proposed an approach which is providing reliability and reputation aware\nsecurity for resource selection in grid environment. In this approach, the\nself-protection capability and reputation weightage is utilized to obtain the\nReliability Factor (RF) value. Therefore jobs are allocated to the resources\nthat posses higher RF values. Extensive experimental evaluation shows that as\nhigher trust and reliable nodes are selected the chances of failure decreased\ndrastically."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.2204v3", 
    "other_authors": "Rajiv Ranjan, Boualem Benatallah", 
    "title": "Programming Cloud Resource Orchestration Framework: Operations and   Research Challenges", 
    "arxiv-id": "1204.2204v3", 
    "author": "Boualem Benatallah", 
    "publish": "2012-04-10T16:06:46Z", 
    "summary": "The emergence of cloud computing over the past five years is potentially one\nof the breakthrough advances in the history of computing. It delivers hardware\nand software resources as virtualization-enabled services and in which\nadministrators are free from the burden of worrying about the low level\nimplementation or system administration details. Although cloud computing\noffers considerable opportunities for the users (e.g. application developers,\ngovernments, new startups, administrators, consultants, scientists, business\nanalyst, etc.) such as no up-front investment, lowering operating cost, and\ninfinite scalability, it has many unique research challenges that need to be\ncarefully addressed in the future. In this paper, we present a survey on key\ncloud computing concepts, resource abstractions, and programming operations for\norchestrating resources and associated research challenges, wherever\napplicable."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.3058v1", 
    "other_authors": "Arnaud Casteigts, Paola Flocchini, Bernard Mans, Nicola Santoro", 
    "title": "Building Fastest Broadcast Trees in Periodically-Varying Graphs", 
    "arxiv-id": "1204.3058v1", 
    "author": "Nicola Santoro", 
    "publish": "2012-04-13T17:25:28Z", 
    "summary": "Delay-tolerant networks (DTNs) are characterized by a possible absence of\nend-to-end communication routes at any instant. Still, connectivity can\ngenerally be established over time and space. The optimality of a temporal path\n(journey) in this context can be defined in several terms, including\ntopological (e.g. {\\em shortest} in hops) and temporal (e.g. {\\em fastest,\nforemost}). The combinatorial problem of computing shortest, foremost, and\nfastest journeys {\\em given full knowledge} of the network schedule was\naddressed a decade ago (Bui-Xuan {\\it et al.}, 2003). A recent line of research\nhas focused on the distributed version of this problem, where foremost,\nshortest or fastest {\\em broadcast} are performed without knowing the schedule\nbeforehand. In this paper we show how to build {\\em fastest} broadcast trees\n(i.e., trees that minimize the global duration of the broadcast, however late\nthe departure is) in Time-Varying Graphs where intermittent edges are available\nperiodically (it is known that the problem is infeasible in the general case\neven if various parameters of the graph are know). We address the general case\nwhere contacts between nodes can have arbitrary durations and thus fastest\nroutes may consist of a mixture of {\\em continuous} and {\\em discontinuous}\nsegments (a more complex scenario than when contacts are {\\em punctual} and\nthus routes are only discontinuous). Using the abstraction of \\tclocks to\ncompute the temporal distances, we solve the fastest broadcast problem by first\nlearning, at the emitter, what is its time of {\\em minimum temporal\neccentricity} (i.e. the fastest time to reach all the other nodes), and second\nby building a {\\em foremost} broadcast tree relative to this particular\nemission date."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.4044v1", 
    "other_authors": "Xavier Vila\u00e7a, Oksana Denysyuk, Lu\u00eds Rodrigues", 
    "title": "Asynchrony and Collusion in the N-party BAR Transfer Problem", 
    "arxiv-id": "1204.4044v1", 
    "author": "Lu\u00eds Rodrigues", 
    "publish": "2012-04-18T11:01:42Z", 
    "summary": "The problem of reliably transferring data from a set of $N_P$ producers to a\nset of $N_C$ consumers in the BAR model, named N-party BAR Transfer (NBART), is\nan important building block for volunteer computing systems. An algorithm to\nsolve this problem in synchronous systems, which provides a Nash equilibrium,\nhas been presented in previous work. In this paper, we propose an NBART\nalgorithm for asynchronous systems. Furthermore, we also address the\npossibility of collusion among the Rational processes. Our game theoretic\nanalysis shows that the proposed algorithm tolerates certain degree of\narbitrary collusion, while still fulfilling the NBART properties."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.4086v1", 
    "other_authors": "Jose Gracia, Christoph Niethammer, Manuel Hasert, Steffen Brinkmann, Rainer Keller, Colin W. Glass", 
    "title": "Hybrid MPI/StarSs - a case study", 
    "arxiv-id": "1204.4086v1", 
    "author": "Colin W. Glass", 
    "publish": "2012-04-18T14:06:14Z", 
    "summary": "Hybrid parallel programming models combining distributed and shared memory\nparadigms are well established in high-performance computing. The classical\nprototype of hybrid programming in HPC is MPI/OpenMP, but many other\ncombinations are being investigated. Recently, the data-dependency driven, task\nparallel model for shared memory parallelisation named StarSs has been\nsuggested for usage in combination with MPI. In this paper we apply hybrid\nMPI/StarSs to a Lattice-Boltzmann code. In particular, we present the hybrid\nprogramming model, the benefits we expect, the challenges in porting, and\nfinally a comparison of the performance of MPI/StarSs hybrid, MPI/OpenMP hybrid\nand the original MPI-only versions of the same code."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.4565v1", 
    "other_authors": "Swan Dubois, S\u00e9bastien Tixeuil, Nini Zhu", 
    "title": "Mariages et Trahisons", 
    "arxiv-id": "1204.4565v1", 
    "author": "Nini Zhu", 
    "publish": "2012-04-20T08:59:01Z", 
    "summary": "A self-stabilizing protocol tolerates by definition transient faults (faults\nof finite duration). Recently, a new class of self-stabilizing protocols that\nare able to tolerate a given number of permanent faults. In this paper, we\nfocus on self-stabilizing protocols able to tolerate Byzantine faults, that is\nfaults that introduce an arbitrary behaviour. We focus on strict-stabilization\nin which the system have to contain the effects of Byzantine faults.\nSpecificaly, we study the possibility to construct in a self-stabilizing way a\nmaximal matching in a network where an arbitrary number of process may become\nByzantine."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.5402v1", 
    "other_authors": "Marco Aldinucci, Marco Danelutto, Massimo Torquati", 
    "title": "FastFlow tutorial", 
    "arxiv-id": "1204.5402v1", 
    "author": "Massimo Torquati", 
    "publish": "2012-04-24T15:17:53Z", 
    "summary": "FastFlow is a structured parallel programming framework targeting shared\nmemory multicores. Its layered design and the optimized implementation of the\ncommunication mechanisms used to implement the FastFlow streaming networks\nprovided to the application programmer as algorithmic skeletons support the\ndevelopment of efficient fine grain parallel applications. FastFlow is\navailable (open source) at SourceForge\n(http://sourceforge.net/projects/mc-fastflow/). This work introduces FastFlow\nprogramming techniques and points out the different ways used to parallelize\nexisting C/C++ code using FastFlow as a software accelerator. In short: this is\na kind of tutorial on FastFlow."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-28869-2_14", 
    "link": "http://arxiv.org/pdf/1204.6170v2", 
    "other_authors": "Wim H. Hesselink", 
    "title": "A distributed resource allocation algorithm for many processes", 
    "arxiv-id": "1204.6170v2", 
    "author": "Wim H. Hesselink", 
    "publish": "2012-04-27T11:14:54Z", 
    "summary": "Resource allocation is the problem that a process may enter a critical\nsection CS of its code only when its resource requirements are not in conflict\nwith those of other processes in their critical sections. For each execution of\nCS, these requirements are given anew. In the resource requirements, levels can\nbe distinguished, such as e.g. read access or write access. We allow infinitely\nmany processes that communicate by reliable asynchronous messages and have\nfinite memory. A simple starvation-free solution is presented. Processes only\nwait for one another when they have conflicting resource requirements. The\ncorrectness of the solution is argued with invariants and temporal logic. It\nhas been verified with the proof assistant PVS."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/7/072043", 
    "link": "http://arxiv.org/pdf/1204.6628v1", 
    "other_authors": "Daniele Licari, Federico Calzolari", 
    "title": "The Anatomy of a Grid portal", 
    "arxiv-id": "1204.6628v1", 
    "author": "Federico Calzolari", 
    "publish": "2012-04-30T13:32:48Z", 
    "summary": "In this paper we introduce a new way to deal with Grid portals referring to\nour implementation. L-GRID is a light portal to access the EGEE/EGI Grid\ninfrastructure via Web, allowing users to submit their jobs from a common Web\nbrowser in a few minutes, without any knowledge about the Grid infrastructure.\nIt provides the control over the complete lifecycle of a Grid Job, from its\nsubmission and status monitoring, to the output retrieval. The system,\nimplemented as client-server architecture, is based on the Globus Grid\nmiddleware. The client side application is based on a java applet; the server\nrelies on a Globus User Interface. There is no need of user registration on the\nserver side, and the user needs only his own X.509 personal certificate. The\nsystem is user-friendly, secure (it uses SSL protocol, mechanism for dynamic\ndelegation and identity creation in public key infrastructures), highly\ncustomizable, open source, and easy to install. The X.509 personal certificate\ndoes not get out from the local machine. It allows to reduce the time spent for\nthe job submission, granting at the same time a higher efficiency and a better\nsecurity level in proxy delegation and management."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/7/072043", 
    "link": "http://arxiv.org/pdf/1204.6631v1", 
    "other_authors": "Federico Calzolari, Silvia Volpe", 
    "title": "A new job migration algorithm to improve data center efficiency", 
    "arxiv-id": "1204.6631v1", 
    "author": "Silvia Volpe", 
    "publish": "2012-04-30T13:44:03Z", 
    "summary": "The under exploitation of the available resources risks to be one of the main\nproblems for a computing center. The growing demand of computational power\nnecessarily entails more complex approaches in the management of the computing\nresources, with particular attention to the batch queue system scheduler. In a\nheterogeneous batch queue system, available for both serial single core\nprocesses and parallel multi core jobs, it may happen that one or more\ncomputational nodes composing the cluster are not fully occupied, running a\nnumber of jobs lower than their actual capability. A typical case is\nrepresented by more single core jobs running each one over a different multi\ncore server, while more parallel jobs - requiring all the available cores of a\nhost - are queued. A job rearrangement executed at runtime is able to free\nextra resources, in order to host new processes. We present an efficient method\nto improve the computing resources exploitation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1204.6691v1", 
    "other_authors": "Drazen Lucanin, Michael Maurer, Toni Mastelic, Ivona Brandic", 
    "title": "Energy Efficient Service Delivery in Clouds in Compliance with the Kyoto   Protocol", 
    "arxiv-id": "1204.6691v1", 
    "author": "Ivona Brandic", 
    "publish": "2012-04-30T16:28:19Z", 
    "summary": "Cloud computing is revolutionizing the ICT landscape by providing scalable\nand efficient computing resources on demand. The ICT industry - especially data\ncenters, are responsible for considerable amounts of CO2 emissions and will\nvery soon be faced with legislative restrictions, such as the Kyoto protocol,\ndefining caps at different organizational levels (country, industry branch\netc.) A lot has been done around energy efficient data centers, yet there is\nvery little work done in defining flexible models considering CO2. In this\npaper we present a first attempt of modeling data centers in compliance with\nthe Kyoto protocol. We discuss a novel approach for trading credits for\nemission reductions across data centers to comply with their constraints. CO2\ncaps can be integrated with Service Level Agreements and juxtaposed to other\ncomputing commodities (e.g. computational power, storage), setting a foundation\nfor implementing next-generation schedulers and pricing models that support\nKyoto-compliant CO2 trading schemes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.0203v1", 
    "other_authors": "Jayavardhana Gubbi, Rajkumar Buyya, Slaven Marusic, Marimuthu Palaniswami", 
    "title": "Internet of Things (IoT): A Vision, Architectural Elements, and Future   Directions", 
    "arxiv-id": "1207.0203v1", 
    "author": "Marimuthu Palaniswami", 
    "publish": "2012-07-01T13:10:15Z", 
    "summary": "Ubiquitous sensing enabled by Wireless Sensor Network (WSN) technologies cuts\nacross many areas of modern day living. This offers the ability to measure,\ninfer and understand environmental indicators, from delicate ecologies and\nnatural resources to urban environments. The proliferation of these devices in\na communicating-actuating network creates the Internet of Things (IoT),\nwherein, sensors and actuators blend seamlessly with the environment around us,\nand the information is shared across platforms in order to develop a common\noperating picture (COP). Fuelled by the recent adaptation of a variety of\nenabling device technologies such as RFID tags and readers, near field\ncommunication (NFC) devices and embedded sensor and actuator nodes, the IoT has\nstepped out of its infancy and is the the next revolutionary technology in\ntransforming the Internet into a fully integrated Future Internet. As we move\nfrom www (static pages web) to web2 (social networking web) to web3 (ubiquitous\ncomputing web), the need for data-on-demand using sophisticated intuitive\nqueries increases significantly. This paper presents a cloud centric vision for\nworldwide implementation of Internet of Things. The key enabling technologies\nand application domains that are likely to drive IoT research in the near\nfuture are discussed. A cloud implementation using Aneka, which is based on\ninteraction of private and public clouds is presented. We conclude our IoT\nvision by expanding on the need for convergence of WSN, the Internet and\ndistributed computing directed at technological research community."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.0602v2", 
    "other_authors": "Tomasz Jurdzinski, Dariusz R. Kowalski", 
    "title": "Distributed backbone structure for deterministic algorithms in the SINR   model of wireless networks", 
    "arxiv-id": "1207.0602v2", 
    "author": "Dariusz R. Kowalski", 
    "publish": "2012-07-03T08:10:21Z", 
    "summary": "The Signal-to-Interference-and-Noise-Ratio (SINR) physical model is one of\nthe legitimate models of wireless networks. Despite of the vast amount of study\ndone in design and analysis of centralized algorithms supporting wireless\ncommunication under the SINR physical model, little is known about distributed\nalgorithms in this model, especially deterministic ones. In this work we\nconstruct, in a deterministic distributed way, a backbone structure on the top\nof a given wireless network, which can be used for transforming many algorithms\ndesigned in a simpler model of ad hoc broadcast networks without interference\ninto the SINR physical model with uniform power of stations, without increasing\ntheir asymptotic time complexity. The time cost of the backbone data structure\nconstruction is only O(Delta polylog n) rounds, where Delta is roughly the\ninverse of network density and n is the number of nodes in the whole network.\nThe core of the construction is a novel combinatorial structure called\nSINR-selector, which is introduced and constructed in this paper. We\ndemonstrate the power of the backbone data structure by using it for obtaining\nefficient O(D+Delta polylog n)-round and O(D+k+Delta polylog n)-round\ndeterministic distributed solutions for leader election and multi-broadcast,\nrespectively, where D is the network diameter and k is the number of messages\nto be disseminated."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.0780v1", 
    "other_authors": "B. Thirumala Rao, L. S. S. Reddy", 
    "title": "Survey on Improved Scheduling in Hadoop MapReduce in Cloud Environments", 
    "arxiv-id": "1207.0780v1", 
    "author": "L. S. S. Reddy", 
    "publish": "2012-07-03T19:01:26Z", 
    "summary": "Cloud Computing is emerging as a new computational paradigm shift.\nHadoop-MapReduce has become a powerful Computation Model for processing large\ndata on distributed commodity hardware clusters such as Clouds. In all Hadoop\nimplementations, the default FIFO scheduler is available where jobs are\nscheduled in FIFO order with support for other priority based schedulers also.\nIn this paper we study various scheduler improvements possible with Hadoop and\nalso provided some guidelines on how to improve the scheduling in Hadoop in\nCloud Environments."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.0894v1", 
    "other_authors": "B. Thirumala Rao, N. V. Sridevi, V. Krishna Reddy, L. S. S. Reddy", 
    "title": "Performance Issues of Heterogeneous Hadoop Clusters in Cloud Computing", 
    "arxiv-id": "1207.0894v1", 
    "author": "L. S. S. Reddy", 
    "publish": "2012-07-04T04:18:03Z", 
    "summary": "Nowadays most of the cloud applications process large amount of data to\nprovide the desired results. Data volumes to be processed by cloud applications\nare growing much faster than computing power. This growth demands new\nstrategies for processing and analyzing information. Dealing with large data\nvolumes requires two things: 1) Inexpensive, reliable storage 2) New tools for\nanalyzing unstructured and structured data. Hadoop is a powerful open source\nsoftware platform that addresses both of these problems. The current Hadoop\nimplementation assumes that computing nodes in a cluster are homogeneous in\nnature. Hadoop lacks performance in heterogeneous clusters where the nodes have\ndifferent computing capacity. In this paper we address the issues that affect\nthe performance of hadoop in heterogeneous clusters and also provided some\nguidelines on how to overcome these bottlenecks"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.1337v1", 
    "other_authors": "Eddy Caron, Florent Chuffart, Anissa Lamani, Franck Petit", 
    "title": "Optimization in a Self-Stabilizing Service Discovery Framework for Large   Scale Systems", 
    "arxiv-id": "1207.1337v1", 
    "author": "Franck Petit", 
    "publish": "2012-07-05T19:38:00Z", 
    "summary": "Ability to find and get services is a key requirement in the development of\nlarge-scale distributed sys- tems. We consider dynamic and unstable\nenvironments, namely Peer-to-Peer (P2P) systems. In previous work, we designed\na service discovery solution called Distributed Lexicographic Placement Table\n(DLPT), based on a hierar- chical overlay structure. A self-stabilizing version\nwas given using the Propagation of Information with Feedback (PIF) paradigm. In\nthis paper, we introduce the self-stabilizing COPIF (for Collaborative PIF)\nscheme. An algo- rithm is provided with its correctness proof. We use this\napproach to improve a distributed P2P framework designed for the services\ndiscovery. Significantly efficient experimental results are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.1571v1", 
    "other_authors": "Tadeusz Tomczak, Katarzyna Zadarnowska, Zbigniew Koza, Maciej Matyka, \u0141ukasz Miros\u0142aw", 
    "title": "Complete PISO and SIMPLE solvers on Graphics Processing Units", 
    "arxiv-id": "1207.1571v1", 
    "author": "\u0141ukasz Miros\u0142aw", 
    "publish": "2012-07-06T10:02:26Z", 
    "summary": "We implemented the pressure-implicit with splitting of operators (PISO) and\nsemi-implicit method for pressure-linked equations (SIMPLE) solvers of the\nNavier-Stokes equations on Fermi-class graphics processing units (GPUs) using\nthe CUDA technology. We also introduced a new format of sparse matrices\noptimized for performing elementary CFD operations, like gradient or divergence\ndiscretization, on GPUs. We verified the validity of the implementation on\nseveral standard, steady and unsteady problems. Computational effciency of the\nGPU implementation was examined by comparing its double precision run times\nwith those of essentially the same algorithms implemented in OpenFOAM. The\nresults show that a GPU (Tesla C2070) can outperform a server-class 6-core,\n12-thread CPU (Intel Xeon X5670) by a factor of 4.2."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-33645-4_9", 
    "link": "http://arxiv.org/pdf/1207.1852v4", 
    "other_authors": "Christoph Lenzen", 
    "title": "Optimal Deterministic Routing and Sorting on the Congested Clique", 
    "arxiv-id": "1207.1852v4", 
    "author": "Christoph Lenzen", 
    "publish": "2012-07-08T07:49:22Z", 
    "summary": "Consider a clique of n nodes, where in each synchronous round each pair of\nnodes can exchange O(log n) bits. We provide deterministic constant-time\nsolutions for two problems in this model. The first is a routing problem where\neach node is source and destination of n messages of size O(log n). The second\nis a sorting problem where each node i is given n keys of size O(log n) and\nneeds to receive the ith batch of n keys according to the global order of the\nkeys. The latter result also implies deterministic constant-round solutions for\nrelated problems such as selection or determining modes."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2302", 
    "link": "http://arxiv.org/pdf/1207.2704v1", 
    "other_authors": "Gaurav Raj, Ankit Nischal", 
    "title": "Efficient Resource Allocation in Resource provisioning policies over   Resource Cloud Communication Paradigm", 
    "arxiv-id": "1207.2704v1", 
    "author": "Ankit Nischal", 
    "publish": "2012-07-11T16:37:51Z", 
    "summary": "Optimal resource utilization for executing tasks within the cloud is one of\nthe biggest challenges. In executing the task over a cloud, the resource\nprovisioner is responsible for providing the resources to create virtual\nmachines. To utilize the resources optimally, the resource provisioner has to\ntake care of the process of allocating resources to Virtual Machine Manager\n(VMM). In this paper, an efficient way to utilize the resources, within the\ncloud, to create virtual machines has been proposed considering optimum cost\nbased on performance factor. This performance factor depends upon the overall\ncost of the resource, communication channel cost, reliability and popularity\nfactor. We have proposed a framework for communication between resource owner\nand cloud using Resource Cloud Communication Paradigm (RCCP). We extend the\nCloudSim[2] adding provisioner policies and Efficient Resource Allocation (ERA)\nalgorithm in VMM allocation policy as a decision support for resource\nprovisioner."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2303", 
    "link": "http://arxiv.org/pdf/1207.2706v1", 
    "other_authors": "Gaurav Raj, Kamaljit Kaur", 
    "title": "Secure Cloud Communication for Effective Cost Management System through   MSBE", 
    "arxiv-id": "1207.2706v1", 
    "author": "Kamaljit Kaur", 
    "publish": "2012-07-11T16:51:01Z", 
    "summary": "In Cloud Computing Architecture, Brokers are responsible to provide services\nto the end users. An Effective Cost Management System (ECMS) which works over\nSecure Cloud Communication Paradigm (SCCP) helps in finding a communication\nlink with overall minimum cost of links. We propose an improved Broker Cloud\nCommunication Paradigm (BCCP) with integration of security issues. Two\nalgorithms are included, first is Secure Optimized Route Cost Finder (S-ORCF)\nto find optimum route between broker and cloud on the behalf of cost factor and\nsecond is Secure Optimized Route Management (S-ORM) to maintain optimum route.\nThese algorithms proposed with cryptographic integrity of the secure route\ndiscovery process in efficient routing approaches between broker and cloud.\nThere is lack in Dynamic Source Routing Approach to verify whether any\nintermediate node has been deleted, inserted or modified with no valid\nauthentication. We use symmetric cryptographic primitives, which is made\npossible due to multisource broadcast encryption scheme. This paper outlines\nthe use of secure route discovery protocol (SRDP)that employs such a security\nparadigm in cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2305", 
    "link": "http://arxiv.org/pdf/1207.2708v1", 
    "other_authors": "Gaurav Raj, Sonika Setia", 
    "title": "Effective Cost Mechanism for Cloudlet Retransmission and Prioritized VM   Scheduling Mechanism over Broker Virtual Machine Communication Framework", 
    "arxiv-id": "1207.2708v1", 
    "author": "Sonika Setia", 
    "publish": "2012-07-11T16:57:18Z", 
    "summary": "In current scenario cloud computing is most widely increasing platform for\ntask execution. Lot of research is going on to cut down the cost and execution\ntime. In this paper, we propose an efficient algorithm to have an effective and\nfast execution of task assigned by the user. We proposed an effective\ncommunication framework between broker and virtual machine for assigning the\ntask and fetching the results in optimum time and cost using Broker Virtual\nMachine Communication Framework (BVCF). We implement it over cloudsim under VM\nscheduling policies by modification based on Virtual Machine Cost. Scheduling\nover Virtual Machine as well as over Cloudlets and Retransmission of Cloudlets\nare the basic building blocks of the proposed work on which the whole\narchitecture is dependent. Execution of cloudlets is being analyzed over Round\nRobin and FCFS scheduling policy."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2305", 
    "link": "http://arxiv.org/pdf/1207.2878v1", 
    "other_authors": "Mohsen Soryani, Morteza Analoui, Ghobad Zarrinchian", 
    "title": "A Novel Process Mapping Strategy in Clustered Environments", 
    "arxiv-id": "1207.2878v1", 
    "author": "Ghobad Zarrinchian", 
    "publish": "2012-07-12T08:44:48Z", 
    "summary": "Nowadays the number of available processing cores within computing nodes\nwhich are used in recent clustered environments, are growing up with a rapid\nrate. Despite this trend, the number of available network interfaces in such\ncomputing nodes has almost been remained unchanged. This issue can lead to high\nusage of network interface in many workloads, especially in heavy-communicating\nworkloads. As a result, network interface may raise as a performance bottleneck\nand can drastically degrade the performance. The goal of this paper is to\nintroduce a new process mapping strategy in multi-core clusters aimed at\nreducing network interface contention and improving inter-node communication\nperformance of parallel applications. Performance evaluation of the new mapping\nalgorithm in synthetic and real workloads indicates that the new strategy can\nachieve 5% to 90% performance improvement in heavy communicating workloads,\ncompared to other well-known methods."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2305", 
    "link": "http://arxiv.org/pdf/1207.3037v1", 
    "other_authors": "Nandan Mirajkar, Mohan Barde, Harshal Kamble, Dr. Rahul Athale, Kumud Singh", 
    "title": "Implementation of Private Cloud using Eucalyptus and an open source   Operating System", 
    "arxiv-id": "1207.3037v1", 
    "author": "Kumud Singh", 
    "publish": "2012-07-11T15:57:29Z", 
    "summary": "Cloud computing is bringing a revolution in computing environment replacing\ntraditional software installations, licensing issues into complete on-demand\nservices through internet. Microsoft office 365 a cloud based office\napplication is available to clients online hence no need to buy and install the\nsoftware. On Facebook a social networking website, users upload videos which\nuses cloud provider's storage service so less hardware cost for\nclients.Virtualization technology has great contribution in advent of cloud\ncomputing. Paper describes implementation of Private Cloud using open source\noperating system Ubuntu 10.04 server edition, installation of Ubuntu Enterprise\nCloud with Eucalyptus 1.6.2 and providing CentOS 5.3 operating system through\ncloud."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2012.2305", 
    "link": "http://arxiv.org/pdf/1207.3099v1", 
    "other_authors": "Kishore Kothapalli, Sriram Pemmaraju", 
    "title": "Super-Fast 3-Ruling Sets", 
    "arxiv-id": "1207.3099v1", 
    "author": "Sriram Pemmaraju", 
    "publish": "2012-07-12T21:02:17Z", 
    "summary": "A $t$-ruling set of a graph $G = (V, E)$ is a vertex-subset $S \\subseteq V$\nthat is independent and satisfies the property that every vertex $v \\in V$ is\nat a distance of at most $t$ from some vertex in $S$. A \\textit{maximal\nindependent set (MIS)} is a 1-ruling set. The problem of computing an MIS on a\nnetwork is a fundamental problem in distributed algorithms and the fastest\nalgorithm for this problem is the $O(\\log n)$-round algorithm due to Luby\n(SICOMP 1986) and Alon et al. (J. Algorithms 1986) from more than 25 years ago.\nSince then the problem has resisted all efforts to yield to a sub-logarithmic\nalgorithm. There has been recent progress on this problem, most importantly an\n$O(\\log \\Delta \\cdot \\sqrt{\\log n})$-round algorithm on graphs with $n$\nvertices and maximum degree $\\Delta$, due to Barenboim et al. (Barenboim,\nElkin, Pettie, and Schneider, April 2012, arxiv 1202.1983; to appear FOCS\n2012).\n  We approach the MIS problem from a different angle and ask if O(1)-ruling\nsets can be computed much more efficiently than an MIS? As an answer to this\nquestion, we show how to compute a 2-ruling set of an $n$-vertex graph in\n$O((\\log n)^{3/4})$ rounds. We also show that the above result can be improved\nfor special classes of graphs such as graphs with high girth, trees, and graphs\nof bounded arboricity.\n  Our main technique involves randomized sparsification that rapidly reduces\nthe graph degree while ensuring that every deleted vertex is close to some\nvertex that remains. This technique may have further applications in other\ncontexts, e.g., in designing sub-logarithmic distributed approximation\nalgorithms. Our results raise intriguing questions about how quickly an MIS (or\n1-ruling sets) can be computed, given that 2-ruling sets can be computed in\nsub-logarithmic rounds."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.3732v2", 
    "other_authors": "Joan Feigenbaum, Brighten Godfrey, Aurojit Panda, Michael Schapira, Scott Shenker, Ankit Singla", 
    "title": "On the Resilience of Routing Tables", 
    "arxiv-id": "1207.3732v2", 
    "author": "Ankit Singla", 
    "publish": "2012-07-16T17:17:36Z", 
    "summary": "Many modern network designs incorporate \"failover\" paths into routers'\nforwarding tables. We initiate the theoretical study of the conditions under\nwhich such resilient routing tables can guarantee delivery of packets."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.5480v1", 
    "other_authors": "Nadir K. Salih, Tianyi Zang", 
    "title": "Survey and comparison for Open and closed sources in cloud computing", 
    "arxiv-id": "1207.5480v1", 
    "author": "Tianyi Zang", 
    "publish": "2012-07-23T18:39:49Z", 
    "summary": "Cloud computing is a new technology widely studied in recent years. Now there\nare many cloud platforms both in industry and in academic circle. How to\nunderstand and use these platforms is a big issue. A detailed comparison has\nbeen presented in this paper focused on the aspects such as the architecture,\ncharacteristics, application and so on. To know the differences between open\nsource and close source in cloud environment we mention some examples for\nSoftware-as-a-Service, Platform-as-a-Service, and Infrastructure-as-a-Service.\nWe made comparison between them. Before conclusion we demonstrate some\nconvergences and differences between open and closed platform, but we realized\nopen source should be the best option."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.5839v1", 
    "other_authors": "Konstantinos I. Tsianos, Michael G. Rabbat", 
    "title": "The Impact of Communication Delays on Distributed Consensus Algorithms", 
    "arxiv-id": "1207.5839v1", 
    "author": "Michael G. Rabbat", 
    "publish": "2012-07-24T22:15:10Z", 
    "summary": "We study the effect of communication delays on distributed consensus\nalgorithms. Two ways to model delays on a network are presented. The first\nmodel assumes that each link delivers messages with a fixed (constant) amount\nof delay, and the second model is more realistic, allowing for i.i.d.\ntime-varying bounded delays. In contrast to previous work studying the effects\nof delays on consensus algorithms, the models studied here allow for a node to\nreceive multiple messages from the same neighbor in one iteration. The analysis\nof the fixed delay model shows that convergence to a consensus is guaranteed\nand the rate of convergence is reduced by no more than a factor O(B^2) where B\nis the maximum delay on any link. For the time-varying delay model we also give\na convergence proof which, for row-stochastic consensus protocols, is not a\ntrivial consequence of ergodic matrix products. In both delay models, the\nconsensus value is no longer the average, even if the original protocol was an\naveraging protocol. For this reason, we propose the use of a different\nconsensus algorithm called Push-Sum [Kempe et al. 2003]. We model delays in the\nPush-Sum framework and show that convergence to the average consensus is\nguaranteed. This suggests that Push-Sum might be a better choice from a\npractical standpoint."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.5949v1", 
    "other_authors": "Piyush Harsh, Florian Dudouet, Roberto G. Cascella, Yvon J\u00e9gou, Christine Morin", 
    "title": "Using Open Standards for Interoperability - Issues, Solutions, and   Challenges facing Cloud Computing", 
    "arxiv-id": "1207.5949v1", 
    "author": "Christine Morin", 
    "publish": "2012-07-25T10:54:08Z", 
    "summary": "Virtualization offers several benefits for optimal resource utilization over\ntraditional non-virtualized server farms. With improvements in internetworking\ntechnologies and increase in network bandwidth speeds, a new era of computing\nhas been ushered in, that of grids and clouds. With several commercial cloud\nproviders coming up, each with their own APIs, application description formats,\nand varying support for SLAs, vendor lock-in has become a serious issue for end\nusers. This article attempts to describe the problem, issues, possible\nsolutions and challenges in achieving cloud interoperability. These issues will\nbe analyzed in the ambit of the European project Contrail that is trying to\nadopt open standards with available virtualization solutions to enhance users'\ntrust in the clouds by attempting to prevent vendor lock-ins, supporting and\nenforcing SLAs together with adequate data protection for sensitive data."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.6011v1", 
    "other_authors": "Loretta Mastroeni, Maurizio Naldi", 
    "title": "Analysis of cloud storage prices", 
    "arxiv-id": "1207.6011v1", 
    "author": "Maurizio Naldi", 
    "publish": "2012-07-25T14:45:55Z", 
    "summary": "Cloud storage is fast securing its role as a major repository for both\nconsumers and business customers. Many companies now offer storage solutions,\nsometimes for free for limited amounts of capacity. We have surveyed the\npricing plans of a selection of major cloud providers and compared them using\nthe unit price as the means of comparison. All the providers, excepting Amazon,\nadopt a bundling pricing scheme; Amazon follows instead a block-declining\npricing policy. We compare the pricing plans through a double approach: a\npointwise comparison for each value of capacity, and an overall comparison\nusing a two-part tariff approximation and a Pareto-dominance criterion. Under\nboth approaches, most providers appear to offer pricing plans that are more\nexpensive and can be excluded from a procurement selection in favour of a\nlimited number of dominant providers."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.6644v1", 
    "other_authors": "Andre Luckow, Mark Santcroos, Ole Weidner, Andre Merzky, Pradeep Mantha, Shantenu Jha", 
    "title": "P*: A Model of Pilot-Abstractions", 
    "arxiv-id": "1207.6644v1", 
    "author": "Shantenu Jha", 
    "publish": "2012-07-27T20:08:12Z", 
    "summary": "Pilot-Jobs support effective distributed resource utilization, and are\narguably one of the most widely-used distributed computing abstractions - as\nmeasured by the number and types of applications that use them, as well as the\nnumber of production distributed cyberinfrastructures that support them. In\nspite of broad uptake, there does not exist a well-defined, unifying conceptual\nmodel of Pilot-Jobs which can be used to define, compare and contrast different\nimplementations. Often Pilot-Job implementations are strongly coupled to the\ndistributed cyber-infrastructure they were originally designed for. These\nfactors present a barrier to extensibility and interoperability. This pa- per\nis an attempt to (i) provide a minimal but complete model (P*) of Pilot-Jobs,\n(ii) establish the generality of the P* Model by mapping various existing and\nwell known Pilot-Job frameworks such as Condor and DIANE to P*, (iii) derive an\ninteroperable and extensible API for the P* Model (Pilot-API), (iv) validate\nthe implementation of the Pilot-API by concurrently using multiple distinct\nPilot-Job frameworks on distinct production distributed cyberinfrastructures,\nand (v) apply the P* Model to Pilot-Data."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.6732v2", 
    "other_authors": "Tomasz Jurdzinski, Dariusz R. Kowalski, Tomasz Maciejewski, Grzegorz Stachowiak", 
    "title": "Distributed Broadcasting in Wireless Networks under the SINR Model", 
    "arxiv-id": "1207.6732v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2012-07-28T21:21:21Z", 
    "summary": "In the advent of large-scale multi-hop wireless technologies, such as MANET,\nVANET, iThings, it is of utmost importance to devise efficient distributed\nprotocols to maintain network architecture and provide basic communication\ntools. One of such fundamental communication tasks is broadcast, also known as\na 1-to-all communication. We propose several new efficient distributed\nalgorithms and evaluate their time performance both theoretically and by\nsimulations. First randomized algorithm accomplishes broadcast in O(D+log(1/d))\nrounds with probability at least 1-d on any uniform-power network of n nodes\nand diameter D, when equipped with local estimate of network density.\nAdditionally, we evaluate average performance of this protocols by simulations\non two classes of generated networks - uniform and social - and compare the\nresults with performance of exponential backoff heuristic. Ours is the first\nprovably efficient and well-scalable distributed solution for the (global)\nbroadcast task. The second randomized protocol developed in this paper does not\nrely on the estimate of local density, and achieves only slightly higher time\nperformance O((D+log(1/d))log n). Finally, we provide a deterministic algorithm\nachieving similar time O(D log^2 n), supported by theoretical analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.6744v2", 
    "other_authors": "Lluis Pamies-Juarez, Anwitaman Datta, Frederique Oggier", 
    "title": "RapidRAID: Pipelined Erasure Codes for Fast Data Archival in Distributed   Storage Systems", 
    "arxiv-id": "1207.6744v2", 
    "author": "Frederique Oggier", 
    "publish": "2012-07-29T04:27:44Z", 
    "summary": "To achieve reliability in distributed storage systems, data has usually been\nreplicated across different nodes. However the increasing volume of data to be\nstored has motivated the introduction of erasure codes, a storage efficient\nalternative to replication, particularly suited for archival in data centers,\nwhere old datasets (rarely accessed) can be erasure encoded, while replicas are\nmaintained only for the latest data. Many recent works consider the design of\nnew storage-centric erasure codes for improved repairability. In contrast, this\npaper addresses the migration from replication to encoding: traditionally\nerasure coding is an atomic operation in that a single node with the whole\nobject encodes and uploads all the encoded pieces. Although large datasets can\nbe concurrently archived by distributing individual object encodings among\ndifferent nodes, the network and computing capacity of individual nodes\nconstrain the archival process due to such atomicity.\n  We propose a new pipelined coding strategy that distributes the network and\ncomputing load of single-object encodings among different nodes, which also\nspeeds up multiple object archival. We further present RapidRAID codes, an\nexplicit family of pipelined erasure codes which provides fast archival without\ncompromising either data reliability or storage overheads. Finally, we provide\na real implementation of RapidRAID codes and benchmark its performance using\nboth a cluster of 50 nodes and a set of Amazon EC2 instances. Experiments show\nthat RapidRAID codes reduce a single object's coding time by up to 90%, while\nwhen multiple objects are encoded concurrently, the reduction is up to 20%."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1207.7055v1", 
    "other_authors": "Benjamin Heintz, Abhishek Chandra, Ramesh K. Sitaraman", 
    "title": "Optimizing MapReduce for Highly Distributed Environments", 
    "arxiv-id": "1207.7055v1", 
    "author": "Ramesh K. Sitaraman", 
    "publish": "2012-07-30T19:42:31Z", 
    "summary": "MapReduce, the popular programming paradigm for large-scale data processing,\nhas traditionally been deployed over tightly-coupled clusters where the data is\nalready locally available. The assumption that the data and compute resources\nare available in a single central location, however, no longer holds for many\nemerging applications in commercial, scientific and social networking domains,\nwhere the data is generated in a geographically distributed manner. Further,\nthe computational resources needed for carrying out the data analysis may be\ndistributed across multiple data centers or community resources such as Grids.\nIn this paper, we develop a modeling framework to capture MapReduce execution\nin a highly distributed environment comprising distributed data sources and\ndistributed computational resources. This framework is flexible enough to\ncapture several design choices and performance optimizations for MapReduce\nexecution. We propose a model-driven optimization that has two key features:\n(i) it is end-to-end as opposed to myopic optimizations that may only make\nlocally optimal but globally suboptimal decisions, and (ii) it can control\nmultiple MapReduce phases to achieve low runtime, as opposed to single-phase\noptimizations that may control only individual phases. Our model results show\nthat our optimization can provide nearly 82% and 64% reduction in execution\ntime over myopic and single-phase optimizations, respectively. We have modified\nHadoop to implement our model outputs, and using three different MapReduce\napplications over an 8-node emulated PlanetLab testbed, we show that our\noptimized Hadoop execution plan achieves 31-41% reduction in runtime over a\nvanilla Hadoop execution. Our model-driven optimization also provides several\ninsights into the choice of techniques and execution parameters based on\napplication and platform characteristics."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.1051v1", 
    "other_authors": "Lewis Tseng, Nitin Vaidya", 
    "title": "Byzantine Convex Consensus: Preliminary Version", 
    "arxiv-id": "1307.1051v1", 
    "author": "Nitin Vaidya", 
    "publish": "2013-07-03T15:49:13Z", 
    "summary": "Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [3, 7]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [8, 12]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [8, 12] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In this paper, we generalize the problem to allow the decision to be a convex\npolytope in the d-dimensional space, such that the decided polytope is within\nthe convex hull of the input vectors at the fault-free nodes. We name this\nproblem as Byzantine convex consensus (BCC), and present an asynchronous\napproximate BCC algorithm with optimal fault tolerance. Ideally, the goal here\nis to agree on a convex polytope that is as large as possible. While we do not\nclaim that our algorithm satisfies this goal, we show a bound on the output\nconvex polytope chosen by our algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.1270v1", 
    "other_authors": "Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Werner Geurts, Gert Goossens, Francesca Lo Cicero, Alessandro Lonardo, Pier Stanislao Paolucci, Davide Rossetti, Francesco Simula, Laura Tosoratto, Piero Vicini", 
    "title": "A heterogeneous many-core platform for experiments on scalable custom   interconnects and management of fault and critical events, applied to   many-process applications: Vol. II, 2012 technical report", 
    "arxiv-id": "1307.1270v1", 
    "author": "Piero Vicini", 
    "publish": "2013-07-04T10:55:43Z", 
    "summary": "This is the second of a planned collection of four yearly volumes describing\nthe deployment of a heterogeneous many-core platform for experiments on\nscalable custom interconnects and management of fault and critical events,\napplied to many-process applications. This volume covers several topics, among\nwhich: 1- a system for awareness of faults and critical events (named LO|FA|MO)\non experimental heterogeneous many-core hardware platforms; 2- the integration\nand test of the experimental hardware heterogeneous many-core platform QUoNG,\nbased on the APEnet+ custom interconnect; 3- the design of a\nSoftware-Programmable Distributed Network Processor architecture (DNP) using\nASIP technology; 4- the initial stages of design of a new DNP generation onto a\n28nm FPGA. These developments were performed in the framework of the EURETILE\nEuropean Project under the Grant Agreement no. 247846."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.1332v2", 
    "other_authors": "Lewis Tseng, Nitin Vaidya", 
    "title": "Byzantine Convex Consensus: An Optimal Algorithm", 
    "arxiv-id": "1307.1332v2", 
    "author": "Nitin Vaidya", 
    "publish": "2013-07-04T14:01:57Z", 
    "summary": "Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [4, 8]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [9, 13]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [9, 13] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In our recent work [arXiv:/1307.1051], we proposed a generalization of the\nconsensus problem, namely Byzantine convex consensus (BCC), which allows the\ndecision to be a convex polytope in the d-dimensional space, such that the\ndecided polytope is within the convex hull of the input vectors at the\nfault-free nodes. We also presented an asynchronous approximate BCC algorithm.\n  In this paper, we propose a new BCC algorithm with optimal fault-tolerance\nthat also agrees on a convex polytope that is as large as possible under\nadversarial conditions. Our prior work [arXiv:/1307.1051] does not guarantee\nthe optimality of the output polytope."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.1517v1", 
    "other_authors": "Nandan Mirajkar, Sandeep Bhujbal, Aaradhana Deshmukh", 
    "title": "Perform wordcount Map-Reduce Job in Single Node Apache Hadoop cluster   and compress data using Lempel-Ziv-Oberhumer (LZO) algorithm", 
    "arxiv-id": "1307.1517v1", 
    "author": "Aaradhana Deshmukh", 
    "publish": "2013-07-05T04:10:34Z", 
    "summary": "Applications like Yahoo, Facebook, Twitter have huge data which has to be\nstored and retrieved as per client access. This huge data storage requires huge\ndatabase leading to increase in physical storage and becomes complex for\nanalysis required in business growth. This storage capacity can be reduced and\ndistributed processing of huge data can be done using Apache Hadoop which uses\nMap-reduce algorithm and combines the repeating data so that entire data is\nstored in reduced format. The paper describes performing a wordcount Map-Reduce\nJob in Single Node Apache Hadoop cluster and compress data using\nLempel-Ziv-Oberhumer (LZO) algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.1955v1", 
    "other_authors": "Jiong He, Mian Lu, Bingsheng He", 
    "title": "Revisiting Co-Processing for Hash Joins on the Coupled CPU-GPU   Architecture", 
    "arxiv-id": "1307.1955v1", 
    "author": "Bingsheng He", 
    "publish": "2013-07-08T06:23:12Z", 
    "summary": "Query co-processing on graphics processors (GPUs) has become an effective\nmeans to improve the performance of main memory databases. However, the\nrelatively low bandwidth and high latency of the PCI-e bus are usually\nbottleneck issues for co-processing. Recently, coupled CPU-GPU architectures\nhave received a lot of attention, e.g. AMD APUs with the CPU and the GPU\nintegrated into a single chip. That opens up new opportunities for optimizing\nquery co-processing. In this paper, we experimentally revisit hash joins, one\nof the most important join algorithms for main memory databases, on a coupled\nCPU-GPU architecture. Particularly, we study the fine-grained co-processing\nmechanisms on hash joins with and without partitioning. The co-processing\noutlines an interesting design space. We extend existing cost models to\nautomatically guide decisions on the design space. Our experimental results on\na recent AMD APU show that (1) the coupled architecture enables fine-grained\nco-processing and cache reuses, which are inefficient on discrete CPU-GPU\narchitectures; (2) the cost model can automatically guide the design and tuning\nknobs in the design space; (3) fine-grained co-processing achieves up to 53%,\n35% and 28% performance improvement over CPU-only, GPU-only and conventional\nCPU-GPU co-processing, respectively. We believe that the insights and\nimplications from this study are initial yet important for further research on\nquery co-processing on coupled CPU-GPU architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.2380v2", 
    "other_authors": "Sena Seneviratne, David C. Levy, Rajkumar Buyya", 
    "title": "A Taxonomy of Performance Prediction Systems in the Parallel and   Distributed Computing Grids", 
    "arxiv-id": "1307.2380v2", 
    "author": "Rajkumar Buyya", 
    "publish": "2013-07-09T09:34:25Z", 
    "summary": "As Grids are loosely-coupled congregations of geographically distributed\nheterogeneous resources, the efficient utilization of the resources requires\nthe support of a sound Performance Prediction System (PPS). The performance\nprediction of grid resources is helpful for both Resource Management Systems\nand grid users to make optimized resource usage decisions. There have been many\nPPS projects that span over several grid resources in several dimensions. In\nthis paper the taxonomy for describing the PPS architecture is discussed. The\ntaxonomy is used to categorize and identify approaches which are followed in\nthe implementation of the existing PPSs for Grids. The taxonomy and the survey\nresults are used to identify approaches and issues that have not been fully\nexplored in research."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.2483v1", 
    "other_authors": "Nitin H. Vaidya", 
    "title": "Iterative Byzantine Vector Consensus in Incomplete Graphs", 
    "arxiv-id": "1307.2483v1", 
    "author": "Nitin H. Vaidya", 
    "publish": "2013-07-09T14:53:05Z", 
    "summary": "This work addresses Byzantine vector consensus (BVC), wherein the input at\neach process is a d-dimensional vector of reals, and each process is expected\nto decide on a decision vector that is in the convex hull of the input vectors\nat the fault-free processes [3, 8]. The input vector at each process may also\nbe viewed as a point in the d-dimensional Euclidean space R^d, where d > 0 is a\nfinite integer. Recent work [3, 8] has addressed Byzantine vector consensus in\nsystems that can be modeled by a complete graph. This paper considers Byzantine\nvector consensus in incomplete graphs. In particular, we address a particular\nclass of iterative algorithms in incomplete graphs, and prove a necessary\ncondition, and a sufficient condition, for the graphs to be able to solve the\nvector consensus problem iteratively. We present an iterative Byzantine vector\nconsensus algorithm, and prove it correct under the sufficient condition. The\nnecessary condition presented in this paper for vector consensus does not match\nwith the sufficient condition for d > 1; thus, a weaker condition may\npotentially suffice for Byzantine vector consensus."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.3207v1", 
    "other_authors": "Paulo S\u00e9rgio Almeida, Carlos Baquero", 
    "title": "Scalable Eventually Consistent Counters over Unreliable Networks", 
    "arxiv-id": "1307.3207v1", 
    "author": "Carlos Baquero", 
    "publish": "2013-07-11T18:28:06Z", 
    "summary": "Counters are an important abstraction in distributed computing, and play a\ncentral role in large scale geo-replicated systems, counting events such as web\npage impressions or social network \"likes\". Classic distributed counters,\nstrongly consistent, cannot be made both available and partition-tolerant, due\nto the CAP Theorem, being unsuitable to large scale scenarios. This paper\ndefines Eventually Consistent Distributed Counters (ECDC) and presents an\nimplementation of the concept, Handoff Counters, that is scalable and works\nover unreliable networks. By giving up the sequencer aspect of classic\ndistributed counters, ECDC implementations can be made AP in the CAP design\nspace, while retaining the essence of counting. Handoff Counters are the first\nCRDT (Conflict-free Replicated Data Type) based mechanism that overcomes the\nidentity explosion problem in naive CRDTs, such as G-Counters (where state size\nis linear in the number of independent actors that ever incremented the\ncounter), by managing identities towards avoiding global propagation and\ngarbage collecting temporary entries. The approach used in Handoff Counters is\nnot restricted to counters, being more generally applicable to other data types\nwith associative and commutative operations."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.3306v1", 
    "other_authors": "Ashkan Paya, Dan C. Marinescu", 
    "title": "Energy-aware Application Scaling on a Cloud", 
    "arxiv-id": "1307.3306v1", 
    "author": "Dan C. Marinescu", 
    "publish": "2013-07-12T02:43:46Z", 
    "summary": "Cloud elasticity - the ability to use as much resources as needed at any\ngiven time - and low cost - a user pays only for the resources it consumes -\nrepresent solid incentives for many organizations to migrate some of their\ncomputational activities to a public cloud. As the interest in cloud computing\ngrows, so does the size of the cloud computing centers and their energy\nfootprint. The realization that power consumption of cloud computing centers is\nsignificant and it is expected to increase substantially in the future\nmotivates our interest in scheduling and scaling algorithms which minimize\npower consumption. We propose energy-aware application scaling and resource\nmanagement algorithms. Though targeting primarily the Infrastructure as a\nService (IaaS), the system models and the algorithms we propose can be applied\nto the other cloud delivery models and to private clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.4259v1", 
    "other_authors": "Shlomi Dolev, Robert Gmyr, Andrea W. Richa, Christian Scheideler", 
    "title": "Ameba-inspired Self-organizing Particle Systems", 
    "arxiv-id": "1307.4259v1", 
    "author": "Christian Scheideler", 
    "publish": "2013-07-16T12:41:05Z", 
    "summary": "Particle systems are physical systems of simple computational particles that\ncan bond to neighboring particles and use these bonds to move from one spot to\nanother (non-occupied) spot. These particle systems are supposed to be able to\nself-organize in order to adapt to a desired shape without any central control.\nSelf-organizing particle systems have many interesting applications like\ncoating objects for monitoring and repair purposes and the formation of\nnano-scale devices for surgery and molecular-scale electronic structures. While\nthere has been quite a lot of systems work in this area, especially in the\ncontext of modular self-reconfigurable robotic systems, only very little\ntheoretical work has been done in this area so far. We attempt to bridge this\ngap by proposing a model inspired by the behavior of ameba that allows rigorous\nalgorithmic research on self-organizing particle systems."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.4567v1", 
    "other_authors": "Michael Lange, Gerard Gorman, Michele Weiland, Lawrence Mitchell, Xiaohu Guo, James Southern", 
    "title": "Benchmarking mixed-mode PETSc performance on high-performance   architectures", 
    "arxiv-id": "1307.4567v1", 
    "author": "James Southern", 
    "publish": "2013-07-17T10:36:35Z", 
    "summary": "The trend towards highly parallel multi-processing is ubiquitous in all\nmodern computer architectures, ranging from handheld devices to large-scale HPC\nsystems; yet many applications are struggling to fully utilise the multiple\nlevels of parallelism exposed in modern high-performance platforms. In order to\nrealise the full potential of recent hardware advances, a mixed-mode between\nshared-memory programming techniques and inter-node message passing can be\nadopted which provides high-levels of parallelism with minimal overheads. For\nscientific applications this entails that not only the simulation code itself,\nbut the whole software stack needs to evolve. In this paper, we evaluate the\nmixed-mode performance of PETSc, a widely used scientific library for the\nscalable solution of partial differential equations. We describe the addition\nof OpenMP threaded functionality to the library, focusing on sparse\nmatrix-vector multiplication. We highlight key challenges in achieving good\nparallel performance, such as explicit communication overlap using task-based\nparallelism, and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. Using a set of matrices extracted from\nFluidity, a CFD application code which uses the library as its linear solver\nengine, we then benchmark the parallel performance of mixed-mode PETSc across\nmultiple nodes on several modern HPC architectures. We evaluate the parallel\nscalability on Uniform Memory Access (UMA) systems, such as the Fujitsu\nPRIMEHPC FX10 and IBM BlueGene/Q, as well as a Non-Uniform Memory Access (NUMA)\nCray XE6 platform. A detailed comparison is performed which highlights the\ncharacteristics of each particular architecture, before demonstrating efficient\nstrong scalability of sparse matrix-vector multiplication with significant\nspeedups over the pure-MPI mode."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.4731v1", 
    "other_authors": "Jesse Kelly, Omar Ghattas, Hari Sundar", 
    "title": "A Nested Partitioning Scheme for Parallel Heterogeneous Clusters", 
    "arxiv-id": "1307.4731v1", 
    "author": "Hari Sundar", 
    "publish": "2013-07-17T19:17:32Z", 
    "summary": "Modern supercomputers are increasingly requiring the presence of accelerators\nand co-processors. However, it has not been easy to achieve good performance on\nsuch heterogeneous clusters. The key challenge has been to ensure good load\nbalance and that neither the CPU nor the accelerator is left idle. Traditional\napproaches have offloaded entire computations to the accelerator, resulting in\nan idle CPU, or have opted for task-level parallelism requiring large data\ntransfers between the CPU and the accelerator. True work-parallelism has been\nhard as the Accelerators cannot directly communicate with other CPUs (besides\nthe host) and Accelerators. In this work, we present a new nested partition\nscheme to overcome this problem. By partitioning the work assignment on a given\nnode asymmetrically into boundary and interior work, and assigning the interior\nto the accelerator, we are able to achieve excellent efficiency while ensure\nproper utilization of both the CPU and Accelerator resources. The problem used\nfor evaluating the new partition is an $hp$ discontinuous Galerkin spectral\nelement method for a coupled elastic--acoustic wave propagation problem."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.5435v1", 
    "other_authors": "Arash Mohammadi, Amir Asif, Xionghu Zhong, A. B. Premkumar", 
    "title": "Distributed Computation of the Conditional PCRLB for Quantized   Decentralized Particle Filters", 
    "arxiv-id": "1307.5435v1", 
    "author": "A. B. Premkumar", 
    "publish": "2013-07-20T16:16:11Z", 
    "summary": "The conditional posterior Cramer-Rao lower bound (PCRLB) is an effective\nsensor resource management criteria for large, geographically distributed\nsensor networks. Existing algorithms for distributed computation of the PCRLB\n(dPCRLB) are based on raw observations leading to significant communication\noverhead to the estimation mechanism. This letter derives distributed\ncomputational techniques for determining the conditional dPCRLB for quantized,\ndecentralized sensor networks (CQ/dPCRLB). Analytical expressions for the\nCQ/dPCRLB are derived, which are particularly useful for particle filter-based\nestimators. The CQ/dPCRLB is compared for accuracy with its centralized\ncounterpart through Monte-Carlo simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.5619v1", 
    "other_authors": "Uri Abraham, Gal Amram", 
    "title": "On the Mailbox Problem", 
    "arxiv-id": "1307.5619v1", 
    "author": "Gal Amram", 
    "publish": "2013-07-22T08:27:31Z", 
    "summary": "The Mailbox Problem was described and solved by Aguilera, Gafni, and Lamport\nin their 2010 DC paper with an algorithm that uses two flag registers that\ncarry 14 values each. An interesting problem that they ask is whether there is\na mailbox algorithm with smaller flag values. We give a positive answer by\ndescribing a mailbox algorithm with 6 and 4 values in the two flag registers."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2332432.2332478", 
    "link": "http://arxiv.org/pdf/1307.6747v1", 
    "other_authors": "Sebastian Kniesburges, Andreas Koutsopoulos, Christian Scheideler", 
    "title": "CONE-DHT: A distributed self-stabilizing algorithm for a heterogeneous   storage system", 
    "arxiv-id": "1307.6747v1", 
    "author": "Christian Scheideler", 
    "publish": "2013-07-25T14:01:26Z", 
    "summary": "We consider the problem of managing a dynamic heterogeneous storage system in\na distributed way so that the amount of data assigned to a host in that system\nis related to its capacity. Two central problems have to be solved for this:\n(1) organizing the hosts in an overlay network with low degree and diameter so\nthat one can efficiently check the correct distribution of the data and route\nbetween any two hosts, and (2) distributing the data among the hosts so that\nthe distribution respects the capacities of the hosts and can easily be adapted\nas the set of hosts or their capacities change. We present distributed\nprotocols for these problems that are self-stabilizing and that do not need any\nglobal knowledge about the system such as the number of nodes or the overall\ncapacity of the system. Prior to this work no solution was known satisfying\nthese properties."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1307.7037v1", 
    "other_authors": "Dra\u017een Lu\u010danin, Ivona Brandi\u0107", 
    "title": "Take a break: cloud scheduling optimized for real-time electricity   pricing", 
    "arxiv-id": "1307.7037v1", 
    "author": "Ivona Brandi\u0107", 
    "publish": "2013-07-26T13:54:20Z", 
    "summary": "Cloud computing revolutionised the industry with its elastic, on-demand\napproach to computational resources, but has lead to a tremendous impact on the\nenvironment. Data centers constitute 1.1-1.5% of total electricity usage in the\nworld. Taking a more informed view of the electrical grid by analysing\nreal-time electricity prices, we set the foundations of a grid-conscious cloud.\nWe propose a scheduling algorithm that predicts electricity price peaks and\nthrottles energy consumption by pausing virtual machines. We evaluate the\napproach on the OpenStack cloud manager through an empirical approach and show\nreductions in energy consumption and costs. Finally, we define green instances\nin which cloud providers can offer such services to their customers under\nbetter pricing options."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1307.7976v2", 
    "other_authors": "Danny Dolev, Christoph Lenzen", 
    "title": "Node-Initiated Byzantine Consensus Without a Common Clock", 
    "arxiv-id": "1307.7976v2", 
    "author": "Christoph Lenzen", 
    "publish": "2013-07-30T13:41:29Z", 
    "summary": "The majority of the literature on consensus assumes that protocols are\njointly started at all nodes of the distributed system. We show how to remove\nthis problematic assumption in semi-synchronous systems, where messages delays\nand relative drifts of local clocks may vary arbitrarily within known bounds.\nOur framework is self-stabilizing and efficient both in terms of communication\nand time; more concretely, compared to a synchronous start in a synchronous\nmodel of a non-self-stabilizing protocol, we achieve a constant-factor increase\nin the time and communicated bits to complete an instance, plus an additive\ncommunication overhead of O(n log n) broadcasted bits per time unit and node.\nThe latter can be further reduced, at an additive increase in time complexity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1307.8228v1", 
    "other_authors": "Abu Salim, Rajesh Kumar Tiwari, Sachin Tripathi", 
    "title": "Addressing Security Challenges in Cloud Computing", 
    "arxiv-id": "1307.8228v1", 
    "author": "Sachin Tripathi", 
    "publish": "2013-07-31T05:53:00Z", 
    "summary": "Cloud computing is a new computing paradigm which allows sharing of resources\non remote server such as hardware, network, storage using internet and provides\nthe way through which application, computing power, computing infrastructure\ncan be delivered to the user as a service. Cloud computing unique attribute\npromise cost effective Information Technology Solution (IT Solution) to the\nuser. All computing needs are provided by the Cloud Service Provider (CSP) and\nthey can be increased or decreased dynamically as required by the user. As data\nand Application are located at the server and may be beyond geographical\nboundary, this leads a number of concern from the user prospective. The\nobjective of this paper is to explore the key issues of cloud computing which\nis delaying its adoption."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1307.8256v1", 
    "other_authors": "Priyanka Kumar, Sathya Peri", 
    "title": "Multi-Version Conflict Notion", 
    "arxiv-id": "1307.8256v1", 
    "author": "Sathya Peri", 
    "publish": "2013-07-31T09:17:23Z", 
    "summary": "This paper introduces the useful notion of Multi-Version Conflict notion."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1407.0442v1", 
    "other_authors": "Seda Davtyan, Kishori M. Konwar, Alexander Russell, Alexander A. Shvartsman", 
    "title": "Technical Report: Dealing with Undependable Workers in Decentralized   Network Supercomputing", 
    "arxiv-id": "1407.0442v1", 
    "author": "Alexander A. Shvartsman", 
    "publish": "2014-07-02T02:29:12Z", 
    "summary": "Internet supercomputing is an approach to solving partitionable,\ncomputation-intensive problems by harnessing the power of a vast number of\ninterconnected computers. This paper presents a new algorithm for the problem\nof using network supercomputing to perform a large collection of independent\ntasks, while dealing with undependable processors. The adversary may cause the\nprocessors to return bogus results for tasks with certain probabilities, and\nmay cause a subset $F$ of the initial set of processors $P$ to crash. The\nadversary is constrained in two ways. First, for the set of non-crashed\nprocessors $P-F$, the \\emph{average} probability of a processor returning a\nbogus result is inferior to $\\frac{1}{2}$. Second, the adversary may crash a\nsubset of processors $F$, provided the size of $P-F$ is bounded from below. We\nconsider two models: the first bounds the size of $P-F$ by a fractional\npolynomial, the second bounds this size by a poly-logarithm. Both models yield\nadversaries that are much stronger than previously studied. Our randomized\nsynchronous algorithm is formulated for $n$ processors and $t$ tasks, with\n$n\\le t$, where depending on the number of crashes each live processor is able\nto terminate dynamically with the knowledge that the problem is solved with\nhigh probability. For the adversary constrained by a fractional polynomial, the\nround complexity of the algorithm is\n$O(\\frac{t}{n^\\varepsilon}\\log{n}\\log{\\log{n}})$, its work is $O(t\\log{n}\n\\log{\\log{n}})$ and message complexity is $O(n\\log{n}\\log{\\log{n}})$. For the\npoly-log constrained adversary, the round complexity is $O(t)$, work is $O(t\nn^{\\varepsilon})$, %$O(t \\, poly \\log{n})$, and message complexity is\n$O(n^{1+\\varepsilon})$ %$O(n \\, poly \\log{n})$. All bounds are shown to hold\nwith high probability."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1407.0696v1", 
    "other_authors": "Seda Davtyan, Kishori M. Konwar, Alexander A. Shvartsman", 
    "title": "Technical Report: Estimating Reliability of Workers for Cooperative   Distributed Computing", 
    "arxiv-id": "1407.0696v1", 
    "author": "Alexander A. Shvartsman", 
    "publish": "2014-07-02T02:41:00Z", 
    "summary": "Internet supercomputing is an approach to solving partitionable,\ncomputation-intensive problems by harnessing the power of a vast number of\ninterconnected computers. For the problem of using network supercomputing to\nperform a large collection of independent tasks, prior work introduced a\ndecentralized approach and provided randomized synchronous algorithms that\nperform all tasks correctly with high probability, while dealing with\nmisbehaving or crash-prone processors. The main weaknesses of existing\nalgorithms is that they assume either that the \\emph{average} probability of a\nnon-crashed processor returning incorrect results is inferior to $\\frac{1}{2}$,\nor that the probability of returning incorrect results is known to \\emph{each}\nprocessor. Here we present a randomized synchronous distributed algorithm that\ntightly estimates the probability of each processor returning correct results.\nStarting with the set $P$ of $n$ processors, let $F$ be the set of processors\nthat crash. Our algorithm estimates the probability $p_i$ of returning a\ncorrect result for each processor $i \\in P-F$, making the estimates available\nto all these processors. The estimation is based on the $(\\epsilon,\n\\delta)$-approximation, where each estimated probability $\\tilde{p_i}$ of $p_i$\nobeys the bound ${\\sf Pr}[p_i(1-\\epsilon) \\leq \\tilde{p_i} \\leq\np_i(1+\\epsilon)] > 1 - \\delta$, for any constants $\\delta >0$ and $\\epsilon >0$\nchosen by the user. An important aspect of this algorithm is that each\nprocessor terminates without global coordination. We assess the efficiency of\nthe algorithm in three adversarial models as follows. For the model where the\nnumber of non-crashed processors $|P-F|$ is linearly bounded the time\ncomplexity $T(n)$ of the algorithm is $\\Theta(\\log{n})$, work complexity $W(n)$\nis $\\Theta(n\\log{n})$, and message complexity $M(n)$ is $\\Theta(n\\log^2n)$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1407.1237v1", 
    "other_authors": "Vinit Kumar, Ajay Agarwal", 
    "title": "HT-Paxos: High Throughput State-Machine Replication Protocol for Large   Clustered Data Centers", 
    "arxiv-id": "1407.1237v1", 
    "author": "Ajay Agarwal", 
    "publish": "2014-07-04T13:58:50Z", 
    "summary": "Paxos is a prominent theory of state machine replication. Recent data\nintensive Systems those implement state machine replication generally require\nhigh throughput. Earlier versions of Paxos as few of them are classical Paxos,\nfast Paxos and generalized Paxos have a major focus on fault tolerance and\nlatency but lacking in terms of throughput and scalability. A major reason for\nthis is the heavyweight leader. Through offloading the leader, we can further\nincrease throughput of the system. Ring Paxos, Multi Ring Paxos and S-Paxos are\nfew prominent attempts in this direction for clustered data centers. In this\npaper, we are proposing HT-Paxos, a variant of Paxos that one is the best\nsuitable for any large clustered data center. HT-Paxos further offloads the\nleader very significantly and hence increases the throughput and scalability of\nthe system. While at the same time, among high throughput state-machine\nreplication protocols, HT-Paxos provides reasonably low latency and response\ntime."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CGC.2013.25", 
    "link": "http://arxiv.org/pdf/1407.2565v2", 
    "other_authors": "L. Becchetti, A. Clementi, E. Natale, F. Pasquale, R. Silvestri", 
    "title": "Plurality Consensus in the Gossip Model", 
    "arxiv-id": "1407.2565v2", 
    "author": "R. Silvestri", 
    "publish": "2014-07-09T17:13:35Z", 
    "summary": "We study Plurality Consensus in the Gossip Model over a network of $n$\nanonymous agents. Each agent supports an initial opinion or color. We assume\nthat at the onset, the number of agents supporting the plurality color exceeds\nthat of the agents supporting any other color by a sufficiently-large bias. The\ngoal is to provide a protocol that, with high probability, brings the system\ninto the configuration in which all agents support the (initial) plurality\ncolor. We consider the Undecided-State Dynamics, a well-known protocol which\nuses just one more state (the undecided one) than those necessary to store\ncolors. We show that the speed of convergence of this protocol depends on the\ninitial color configuration as a whole, not just on the gap between the\nplurality and the second largest color community. This dependence is best\ncaptured by a novel notion we introduce, namely, the monochromatic distance\n${md}(\\bar{\\mathbf{c}})$ which measures the distance of the initial color\nconfiguration $\\bar{ \\mathbf {c}}$ from the closest monochromatic one. In the\ncomplete graph, we prove that, for a wide range of the input parameters, this\ndynamics converges within $O({md}(\\bar {\\mathbf {c}}) \\log {n})$ rounds. We\nprove that this upper bound is almost tight in the strong sense: Starting from\nany color configuration $\\bar {\\mathbf {c}}$, the convergence time is\n$\\Omega({md}(\\bar {\\mathbf {c}}))$. Finally, we adapt the Undecided-State\nDynamics to obtain a fast, random walk-based protocol for plurality consensus\non regular expanders. This protocol converges in $O({md}(\\bar {\\mathbf {c}})\n\\mathrm{polylog}(n))$ rounds using only $\\mathrm{polylog}(n)$ local memory. A\nkey-ingredient to achieve the above bounds is a new analysis of the maximum\nnode congestion that results from performing $n$ parallel random walks on\nregular expanders. All our bounds hold with high probability."
},{
    "category": "cs.DC", 
    "doi": "10.5772/7046", 
    "link": "http://arxiv.org/pdf/1407.2636v1", 
    "other_authors": "Ashok Krishnamurthy, Siddharth Samsi, Vijay Gadepally", 
    "title": "Parallel MATLAB Techniques", 
    "arxiv-id": "1407.2636v1", 
    "author": "Vijay Gadepally", 
    "publish": "2014-07-09T20:54:42Z", 
    "summary": "In this chapter, we show why parallel MATLAB is useful, provide a comparison\nof the different parallel MATLAB choices, and describe a number of applications\nin Signal and Image Processing: Audio Signal Processing, Synthetic Aperture\nRadar (SAR) Processing and Superconducting Quantum Interference Filters\n(SQIFs). Each of these applications have been parallelized using different\nmethods (Task parallel and Data parallel techniques). The applications\npresented may be considered representative of type of problems faced by signal\nand image processing researchers. This chapter will also strive to serve as a\nguide to new signal and image processing parallel programmers, by suggesting a\nparallelization strategy that can be employed when developing a general\nparallel algorithm. The objective of this chapter is to help signal and image\nprocessing algorithm developers understand the advantages of using parallel\nMATLAB to tackle larger problems while staying within the powerful environment\nof MATLAB."
},{
    "category": "cs.DC", 
    "doi": "10.5772/7046", 
    "link": "http://arxiv.org/pdf/1407.3286v1", 
    "other_authors": "Srikanth Sastry, Josef Widder", 
    "title": "Solvability-Based Comparison of Failure Detectors", 
    "arxiv-id": "1407.3286v1", 
    "author": "Josef Widder", 
    "publish": "2014-07-11T20:17:12Z", 
    "summary": "Failure detectors are oracles that have been introduced to provide processes\nin asynchronous systems with information about faults. This information can\nthen be used to solve problems otherwise unsolvable in asynchronous systems. A\nnatural question is on the \"minimum amount of information\" a failure detector\nhas to provide for a given problem. This question is classically addressed\nusing a relation that states that a failure detector D is stronger (that is,\nprovides \"more, or better, information\") than a failure detector D' if D can be\nused to implement D'. It has recently been shown that this classic\nimplementability relation has some drawbacks. To overcome this, different\nrelations have been defined, one of which states that a failure detector D is\nstronger than D' if D can solve all the time-free problems solvable by D'. In\nthis paper we compare the implementability-based hierarchy of failure detectors\nto the hierarchy based on solvability. This is done by introducing a new proof\ntechnique for establishing the solvability relation. We apply this technique to\nknown failure detectors from the literature and demonstrate significant\ndifferences between the hierarchies."
},{
    "category": "cs.DC", 
    "doi": "10.5772/7046", 
    "link": "http://arxiv.org/pdf/1407.3487v1", 
    "other_authors": "Grigori Fursin", 
    "title": "Collective Tuning Initiative", 
    "arxiv-id": "1407.3487v1", 
    "author": "Grigori Fursin", 
    "publish": "2014-07-13T17:13:17Z", 
    "summary": "Computing systems rarely deliver best possible performance due to ever\nincreasing hardware and software complexity and limitations of the current\noptimization technology. Additional code and architecture optimizations are\noften required to improve execution time, size, power consumption, reliability\nand other important characteristics of computing systems. However, it is often\na tedious, repetitive, isolated and time consuming process. In order to\nautomate, simplify and systematize program optimization and architecture\ndesign, we are developing open-source modular plugin-based Collective Tuning\nInfrastructure (CTI, http://cTuning.org) that can distribute optimization\nprocess and leverage optimization experience of multiple users. CTI provides a\nnovel fully integrated, collaborative, \"one button\" approach to improve\nexisting underperfoming computing systems ranging from embedded architectures\nto high-performance servers based on systematic iterative compilation,\nstatistical collective optimization and machine learning. Our experimental\nresults show that it is possible to reduce execution time (and code size) of\nsome programs from SPEC2006 and EEMBC among others by more than a factor of 2\nautomatically. It can also reduce development and testing time considerably.\nTogether with the first production quality machine learning enabled interactive\nresearch compiler (MILEPOST GCC) this infrastructure opens up many research\nopportunities to study and develop future realistic self-tuning and\nself-organizing adaptive intelligent computing systems based on systematic\nstatistical performance evaluation and benchmarking. Finally, using common\noptimization repository is intended to improve the quality and reproducibility\nof the research on architecture and code optimization."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ACCT.2014.32", 
    "link": "http://arxiv.org/pdf/1407.3879v1", 
    "other_authors": "Harshadkumar B. Prajapati, Vipul A. Shah", 
    "title": "Scheduling in Grid Computing Environment", 
    "arxiv-id": "1407.3879v1", 
    "author": "Vipul A. Shah", 
    "publish": "2014-07-15T04:28:09Z", 
    "summary": "Scheduling in Grid computing has been active area of research since its\nbeginning. However, beginners find very difficult to understand related\nconcepts due to a large learning curve of Grid computing. Thus, there is a need\nof concise understanding of scheduling in Grid computing area. This paper\nstrives to present concise understanding of scheduling and related\nunderstanding of Grid computing system. The paper describes overall picture of\nGrid computing and discusses important sub-systems that enable Grid computing\npossible. Moreover, the paper also discusses concepts of resource scheduling\nand application scheduling and also presents classification of scheduling\nalgorithms. Furthermore, the paper also presents methodology used for\nevaluating scheduling algorithms including both real system and simulation\nbased approaches. The presented work on scheduling in Grid containing concise\nunderstandings of scheduling system, scheduling algorithm, and scheduling\nmethodology would be very useful to users and researchers"
},{
    "category": "cs.DC", 
    "doi": "10.1109/ACCT.2014.31", 
    "link": "http://arxiv.org/pdf/1407.3881v1", 
    "other_authors": "Harshadkumar B. Prajapati, Vipul A. Shah", 
    "title": "Experimental Study of Remote Job Submission and Execution on LRM through   Grid Computing Mechanisms", 
    "arxiv-id": "1407.3881v1", 
    "author": "Vipul A. Shah", 
    "publish": "2014-07-15T04:53:12Z", 
    "summary": "Remote job submission and execution is fundamental requirement of distributed\ncomputing done using Cluster computing. However, Cluster computing limits usage\nwithin a single organization. Grid computing environment can allow use of\nresources for remote job execution that are available in other organizations.\nThis paper discusses concepts of batch-job execution using LRM and using Grid.\nThe paper discusses two ways of preparing test Grid computing environment that\nwe use for experimental testing of concepts. This paper presents experimental\ntesting of remote job submission and execution mechanisms through LRM specific\nway and Grid computing ways. Moreover, the paper also discusses various\nproblems faced while working with Grid computing environment and discusses\ntheir trouble-shootings. The understanding and experimental testing presented\nin this paper would become very useful to researchers who are new to the field\nof job management in Grid."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ACCT.2014.31", 
    "link": "http://arxiv.org/pdf/1407.4859v1", 
    "other_authors": "Deepak Majeti, Kuldeep S. Meel, Rajkishore Barik, Vivek Sarkar", 
    "title": "ADHA: Automatic Data layout framework for Heterogeneous Architectures", 
    "arxiv-id": "1407.4859v1", 
    "author": "Vivek Sarkar", 
    "publish": "2014-07-18T00:09:04Z", 
    "summary": "Data layouts play a crucial role in determining the performance of a given\napplication running on a given architecture. Existing parallel programming\nframeworks for both multicore and heterogeneous systems leave the onus of\nselecting a data layout to the programmer. Therefore, shifting the burden of\ndata layout selection to optimizing compilers can greatly enhance programmer\nproductivity and application performance. In this work, we introduce {\\ADHA}: a\ntwo-level hierarchal formulation of the data layout problem for modern\nheterogeneous architectures. We have created a reference implementation of ADHA\nin the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows\nsignificant performance benefits of up to 6.92$\\times$ compared to manually\nspecified layouts for two benchmark programs running on a CPU+GPU heterogeneous\nplatform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ACCT.2014.31", 
    "link": "http://arxiv.org/pdf/1407.4908v1", 
    "other_authors": "Bogdan Oancea, Raluca Mariana Dragoescu", 
    "title": "Integrating R and Hadoop for Big Data Analysis", 
    "arxiv-id": "1407.4908v1", 
    "author": "Raluca Mariana Dragoescu", 
    "publish": "2014-07-18T08:17:55Z", 
    "summary": "Analyzing and working with big data could be very diffi cult using classical\nmeans like relational database management systems or desktop software packages\nfor statistics and visualization. Instead, big data requires large clusters\nwith hundreds or even thousands of computing nodes. Offi cial statistics is\nincreasingly considering big data for deriving new statistics because big data\nsources could produce more relevant and timely statistics than traditional\nsources. One of the software tools successfully and wide spread used for\nstorage and processing of big data sets on clusters of commodity hardware is\nHadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS),\na resource-management platform and implements a version of the MapReduce\nprogramming model for large scale data processing. In this paper we investigate\nthe possibilities of integrating Hadoop with R which is a popular software used\nfor statistical computing and data visualization. We present three ways of\nintegrating them: R with Streaming, Rhipe and RHadoop and we emphasize the\nadvantages and disadvantages of each solution."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICICES.2013.6508316", 
    "link": "http://arxiv.org/pdf/1407.5404v1", 
    "other_authors": "A. Christy Persya, T. R. Gopalakrishnan Nair", 
    "title": "Model based design of super schedulers managing catastrophic scenario in   hard real time systems", 
    "arxiv-id": "1407.5404v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2014-07-21T07:52:42Z", 
    "summary": "The conventional design of real-time approaches depends heavily on the normal\nperformance of systems and it often becomes incapacitated in dealing with\ncatastrophic scenarios effectively. There are several investigations carried\nout to effectively tackle large scale catastrophe of a plant and how real-time\nsystems must reorganize itself to respond optimally to changing scenarios to\nreduce catastrophe and aid human intervention. The study presented here is in\nthis direction and the model accommodates catastrophe generated tasks while it\ntries to minimize the total number of deadline miss, by dynamically scheduling\nthe unusual pattern of tasks. The problem is NP hard. We prove the methods for\nan optimal scheduling algorithm. We also derive a model to maintain the\nstability of the processes. Moreover, we study the problem of minimizing the\nnumber of processors required for scheduling with a set of periodic and\nsporadic hard real time tasks with primary/backup mechanism to achieve fault\ntolerance. EDF scheduling algorithms are used on each processor to manage\nscenario changes. Finally we present a simulation of super scheduler with\nsmall, medium and large real time tasks pattern for catastrophe management."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICICES.2013.6508316", 
    "link": "http://arxiv.org/pdf/1407.5762v2", 
    "other_authors": "Graeme Smith, J. W. Sanders, Qin Li", 
    "title": "A macro-level model for investigating the effect of directional bias on   network coverage", 
    "arxiv-id": "1407.5762v2", 
    "author": "Qin Li", 
    "publish": "2014-07-22T07:14:16Z", 
    "summary": "Random walks have been proposed as a simple method of efficiently searching,\nor disseminating information throughout, communication and sensor networks. In\nnature, animals (such as ants) tend to follow correlated random walks, i.e.,\nrandom walks that are biased towards their current heading. In this paper, we\ninvestigate whether or not complementing random walks with directional bias can\ndecrease the expected discovery and coverage times in networks.\n  To do so, we develop a macro-level model of a directionally biased random\nwalk based on Markov chains. By focussing on regular, connected networks, the\nmodel allows us to efficiently calculate expected coverage times for different\nnetwork sizes and biases. Our analysis shows that directional bias can\nsignificantly reduce coverage time, but only when the bias is below a certain\nvalue which is dependent on the network size."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICICES.2013.6508316", 
    "link": "http://arxiv.org/pdf/1407.6153v1", 
    "other_authors": "Robert Obryk", 
    "title": "Write-and-f-array: implementation and an application", 
    "arxiv-id": "1407.6153v1", 
    "author": "Robert Obryk", 
    "publish": "2014-07-23T09:47:57Z", 
    "summary": "We introduce a new shared memory object: the write-and-f-array, provide its\nwait-free implementation and use it to construct an improved wait-free\nimplementation of the fetch-and-add object. The write-and-f-array generalizes\nsingle-writer write-and-snapshot object in a similar way that the f-array\ngeneralizes the multi-writer snapshot object. More specifically, a\nwrite-and-f-array is parameterized by an associative operator $f$ and is\nconceptually an array with two atomic operations:\n  - write-and-f modifies a single array's element and returns the result of\napplying $f$ to all the elements,\n  - read returns the result of applying $f$ to all the array's elements.\n  We provide a wait-free implementation of an $N$-element write-and-f-array\nwith $O(N \\log N)$ memory complexity, $O(\\log^3 N)$ step complexity of the\nwrite-and-f operation and $O(1)$ step complexity of the read operation. The\nimplementation uses CAS objects and requires their size to be $\\Omega(\\log M)$,\nwhere $M$ is the total number of write-and-f operations executed. We also show,\nhow it can be modified to achieve $O(\\log^2 N)$ step complexity of write-and-f,\nwhile increasing the memory complexity to $O(N \\log^2 N)$.\n  The write-and-f-array can be applied to create a fetch-and-add object for $P$\nprocesses with $O(P \\log P)$ memory complexity and $O(\\log^3 P)$ step\ncomplexity of the fetch-and-add operation. This is the first implementation of\nfetch-and-add with polylogarithmic step complexity and subquadratic memory\ncomplexity that can be implemented without CAS or LL/SC objects of unrealistic\nsize."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6317v1", 
    "other_authors": "Ch. Srinivasa Rao, B. Raveendra Babu", 
    "title": "A Fuzzy Differential Evolution Algorithm for Job Scheduling on   Computational Grids", 
    "arxiv-id": "1407.6317v1", 
    "author": "B. Raveendra Babu", 
    "publish": "2014-07-23T18:08:25Z", 
    "summary": "Grid computing is the recently growing area of computing that share data,\nstorage, computing across geographically dispersed area. This paper proposes a\nnovel fuzzy approach using Differential Evolution (DE) for scheduling jobs on\ncomputational grids. The fuzzy based DE generates an optimal plan to complete\nthe jobs within a minimum period of time. We evaluate the performance of the\nproposed fuzzy based DE algorithm with Genetic Algorithm (GA), Simulated\nAnnealing (SA), Differential Evolution and fuzzy PSO. Experimental results have\nshown that the new algorithm produces more optimal solutions for the job\nscheduling problems compared to other algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6603v1", 
    "other_authors": "Zaid Abdi Alkareem Alyasseri, Kadhim Al-Attar, Mazin Nasser", 
    "title": "Parallelize Bubble Sort Algorithm Using OpenMP", 
    "arxiv-id": "1407.6603v1", 
    "author": "Mazin Nasser", 
    "publish": "2014-07-24T14:47:48Z", 
    "summary": "Sorting has been a profound area for the algorithmic researchers and many\nresources are invested to suggest more works for sorting algorithms. For this\npurpose, many existing sorting algorithms were observed in terms of the\nefficiency of the algorithmic complexity. In this paper we implemented the\nbubble sort algorithm using multithreading (OpenMP). The proposed work tested\non two standard datasets (text file) with different size . The main idea of the\nproposed algorithm is distributing the elements of the input datasets into many\nadditional temporary sub-arrays according to a number of characters in each\nword. The sizes of each of these sub-arrays are decided depending on a number\nof elements with the same number of characters in the input array. We\nimplemented OpenMP using Intel core i7-3610QM ,(8 CPUs),using two approaches\n(vectors of string and array 3D) . Finally, we get the data structure effects\non the performance of the algorithm for that we choice the second approach."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6745v1", 
    "other_authors": "Ahmet Erdem Sar\u0131y\u00fcce, Erik Saule, \u00dcmit V. \u00c7ataly\u00fcrek", 
    "title": "On Distributed Graph Coloring with Iterative Recoloring", 
    "arxiv-id": "1407.6745v1", 
    "author": "\u00dcmit V. \u00c7ataly\u00fcrek", 
    "publish": "2014-07-24T22:02:53Z", 
    "summary": "Identifying the sets of operations that can be executed simultaneously is an\nimportant problem appearing in many parallel applications. By modeling the\noperations and their interactions as a graph, one can identify the independent\noperations by solving a graph coloring problem. Many efficient sequential\nalgorithms are known for this NP-Complete problem, but they are typically\nunsuitable when the operations and their interactions are distributed in the\nmemory of large parallel computers. On top of an existing distributed-memory\ngraph coloring algorithm, we investigate two compatible techniques in this\npaper for fast and scalable distributed-memory graph coloring. First, we\nintroduce an improvement for the distributed post-processing operation, called\nrecoloring, which drastically improves the number of colors. We propose a novel\nand efficient communication scheme for recoloring which enables it to scale\ngracefully. Recoloring must be seeded with an existing coloring of the graph.\nOur second contribution is to introduce a randomized color selection strategy\nfor initial coloring which quickly produces solutions of modest quality. We\nextensively evaluate the impact of our new techniques on existing distributed\nalgorithms and show the time-quality tradeoffs. We show that combining an\ninitial randomized coloring with multiple recoloring iterations yields better\nquality solutions with the smaller runtime at large scale."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6876v2", 
    "other_authors": "Petr Kuznetsov, Srivatsan Ravi", 
    "title": "On Partial Wait-Freedom in Transactional Memory", 
    "arxiv-id": "1407.6876v2", 
    "author": "Srivatsan Ravi", 
    "publish": "2014-07-25T12:52:59Z", 
    "summary": "Transactional memory (TM) is a convenient synchronization tool that allows\nconcurrent threads to declare sequences of instructions on shared data as\nspeculative \\emph{transactions} with \"all-or-nothing\" semantics. It is known\nthat dynamic transactional memory cannot provide \\emph{wait-free} progress in\nthe sense that every transaction commits in a finite number of its own steps.\nIn this paper, we explore the costs of providing wait-freedom to only a\n\\emph{subset} of transactions. Since most transactional workloads are believed\nto be read-dominated, we require that read-only transactions commit in the\nwait-free manner, while updating transactions are guaranteed to commit only if\nthey run in the absence of concurrency. We show that this kind of partial\nwait-freedom, combined with attractive requirements like read invisibility or\ndisjoint-access parallelism, incurs considerable complexity costs."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6878v1", 
    "other_authors": "Zaid Abdi Alkareem Alyasseri", 
    "title": "Survey of Parallel Computing with MATLAB", 
    "arxiv-id": "1407.6878v1", 
    "author": "Zaid Abdi Alkareem Alyasseri", 
    "publish": "2014-07-25T12:59:26Z", 
    "summary": "Matlab is one of the most widely used mathematical computing environments in\ntechnical computing. It has an interactive environment which provides high\nperformance computing (HPC) procedures and easy to use. Parallel computing with\nMatlab has been an interested area for scientists of parallel computing\nresearches for a number of years. Where there are many attempts to parallel\nMatlab. In this paper, we present most of the past,present attempts of parallel\nMatlab such as MatlabMPI, bcMPI, pMatlab, Star-P and PCT. Finally, we expect\nthe future attempts."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6915v1", 
    "other_authors": "Rostislav Tsiomenko, Bradley S. Rees", 
    "title": "Accelerating Fast Fourier Transforms Using Hadoop and CUDA", 
    "arxiv-id": "1407.6915v1", 
    "author": "Bradley S. Rees", 
    "publish": "2014-07-25T14:25:35Z", 
    "summary": "There has been considerable research into improving Fast Fourier Transform\n(FFT) performance through parallelization and optimization for specialized\nhardware. However, even with those advancements, processing of very large\nfiles, over 1TB in size, still remains prohibitively slow. Analysts performing\nsignal processing are forced to wait hours or days for results, which results\nin a disruption of their workflow and a decrease in productivity. In this paper\nwe present a unique approach that not only parallelizes the workload over\nmulti-cores, but distributes the problem over a cluster of graphics processing\nunit (GPU)-equipped servers. By utilizing Hadoop and CUDA, we can take\nadvantage of inexpensive servers while still exceeding the processing power of\na dedicated supercomputer, as demonstrated in our result using Amazon EC2."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.6932v1", 
    "other_authors": "Albert Sa\u00e0-Garriga, David Castells-Rufas, Jordi Carrabina", 
    "title": "OMP2HMPP: HMPP Source Code Generation from Programs with Pragma   Extensions", 
    "arxiv-id": "1407.6932v1", 
    "author": "Jordi Carrabina", 
    "publish": "2014-07-25T15:14:08Z", 
    "summary": "High-performance computing are based more and more in heterogeneous\narchitectures and GPGPUs have become one of the main integrated blocks in\nthese, as the recently emerged Mali GPU in embedded systems or the NVIDIA GPUs\nin HPC servers. In both GPGPUs, programming could become a hurdle that can\nlimit their adoption, since the programmer has to learn the hardware\ncapabilities and the language to work with these. We present OMP2HMPP, a tool\nthat, automatically trans-lates a high-level C source code(OpenMP) code into\nHMPP. The generated version rarely will differs from a hand-coded HMPP version,\nand will provide an important speedup, near 113%, that could be later improved\nby hand-coded CUDA. The generated code could be transported either to HPC\nservers and to embedded GPUs, due to the commonalities between them."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.7360v1", 
    "other_authors": "Amelie Chi Zhou, Bingsheng He, Shadi Ibrahim", 
    "title": "A Taxonomy and Survey on eScience as a Service in the Cloud", 
    "arxiv-id": "1407.7360v1", 
    "author": "Shadi Ibrahim", 
    "publish": "2014-07-28T09:14:35Z", 
    "summary": "Cloud computing has recently evolved as a popular computing infrastructure\nfor many applications. Scientific computing, which was mainly hosted in private\nclusters and grids, has started to migrate development and deployment to the\npublic cloud environment. eScience as a service becomes an emerging and\npromising direction for science computing. We review recent efforts in\ndeveloping and deploying scientific computing applications in the cloud. In\nparticular, we introduce a taxonomy specifically designed for scientific\ncomputing in the cloud, and further review the taxonomy with four major kinds\nof science applications, including life sciences, physics sciences, social and\nhumanities sciences, and climate and earth sciences. Our major finding is that,\ndespite existing efforts in developing cloud-based eScience, eScience still has\na long way to go to fully unlock the power of cloud computing paradigm.\nTherefore, we present the challenges and opportunities in the future\ndevelopment of cloud-based eScience services, and call for collaborations and\ninnovations from both the scientific and computer system communities to address\nthose challenges."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/1407.8433v1", 
    "other_authors": "Pierre Bertrand, Christoph Lenzen", 
    "title": "The 1-2-3-Toolkit for Building Your Own Balls-into-Bins Algorithm", 
    "arxiv-id": "1407.8433v1", 
    "author": "Christoph Lenzen", 
    "publish": "2014-07-31T14:27:10Z", 
    "summary": "In this work, we examine a generic class of simple distributed\nballs-into-bins algorithms. Exploiting the strong concentration bounds that\napply to balls-into-bins games, we provide an iterative method to compute\naccurate estimates of the remaining balls and the load distribution after each\nround. Each algorithm is classified by (i) the load that bins accept in a given\nround, (ii) the number of messages each ball sends in a given round, and (iii)\nwhether each such message is given a rank expressing the sender's inclination\nto commit to the receiving bin (if feasible). This novel ranking mechanism\nresults in notable improvements, in particular in the number of balls that may\ncommit to a bin in the first round of the algorithm. Simulations independently\nverify the correctness of the results and confirm that our approximation is\nhighly accurate even for a moderate number of $10^6$ balls and bins."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.0048v4", 
    "other_authors": "Xin Dong, Gene Cooperman", 
    "title": "A Bit-Compatible Shared Memory Parallelization for ILU(k)   Preconditioning and a Bit-Compatible Generalization to Distributed Memory", 
    "arxiv-id": "0803.0048v4", 
    "author": "Gene Cooperman", 
    "publish": "2008-03-01T07:21:27Z", 
    "summary": "ILU(k) is a commonly used preconditioner for iterative linear solvers for\nsparse, non-symmetric systems. It is often preferred for the sake of its\nstability. We present TPILU(k), the first efficiently parallelized ILU(k)\npreconditioner that maintains this important stability property. Even better,\nTPILU(k) preconditioning produces an answer that is bit-compatible with the\nsequential ILU(k) preconditioning. In terms of performance, the TPILU(k)\npreconditioning is shown to run faster whenever more cores are made available\nto it --- while continuing to be as stable as sequential ILU(k). This is in\ncontrast to some competing methods that may become unstable if the degree of\nthread parallelism is raised too far. Where Block Jacobi ILU(k) fails in an\napplication, it can be replaced by TPILU(k) in order to maintain good\nperformance, while also achieving full stability. As a further optimization,\nTPILU(k) offers an optional level-based incomplete inverse method as a fast\napproximation for the original ILU(k) preconditioned matrix. Although this\nenhancement is not bit-compatible with classical ILU(k), it is bit-compatible\nwith the output from the single-threaded version of the same algorithm. In\nexperiments on a 16-core computer, the enhanced TPILU(k)-based iterative linear\nsolver performed up to 9 times faster. As we approach an era of many-core\ncomputing, the ability to efficiently take advantage of many cores will become\never more important. TPILU(k) also demonstrates good performance on cluster or\nGrid. For example, the new algorithm achieves 50 times speedup with 80 nodes\nfor general sparse matrices of dimension 160,000 that are diagonally dominant."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.0241v2", 
    "other_authors": "Ariel Daliot, Danny Dolev, Hanna Parnas", 
    "title": "Self-Stabilizing Pulse Synchronization Inspired by Biological Pacemaker   Networks", 
    "arxiv-id": "0803.0241v2", 
    "author": "Hanna Parnas", 
    "publish": "2008-03-03T13:46:45Z", 
    "summary": "We define the ``Pulse Synchronization'' problem that requires nodes to\nachieve tight synchronization of regular pulse events, in the settings of\ndistributed computing systems. Pulse-coupled synchronization is a phenomenon\ndisplayed by a large variety of biological systems, typically overcoming a high\nlevel of noise. Inspired by such biological models, a robust and\nself-stabilizing Byzantine pulse synchronization algorithm for distributed\ncomputer systems is presented. The algorithm attains near optimal\nsynchronization tightness while tolerating up to a third of the nodes\nexhibiting Byzantine behavior concurrently. Pulse synchronization has been\npreviously shown to be a powerful building block for designing algorithms in\nthis severe fault model. We have previously shown how to stabilize general\nByzantine algorithms, using pulse synchronization. To the best of our knowledge\nthere is no other scheme to do this without the use of synchronized pulses."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.1520v1", 
    "other_authors": "Wenbing Zhao", 
    "title": "Integrity-Enhancing Replica Coordination for Byzantine Fault Tolerant   Systems", 
    "arxiv-id": "0803.1520v1", 
    "author": "Wenbing Zhao", 
    "publish": "2008-03-11T04:20:06Z", 
    "summary": "Strong replica consistency is often achieved by writing deterministic\napplications, or by using a variety of mechanisms to render replicas\ndeterministic. There exists a large body of work on how to render replicas\ndeterministic under the benign fault model. However, when replicas can be\nsubject to malicious faults, most of the previous work is no longer effective.\nFurthermore, the determinism of the replicas is often considered harmful from\nthe security perspective and for many applications, their integrity strongly\ndepends on the randomness of some of their internal operations. This calls for\nnew approaches towards achieving replica consistency while preserving the\nreplica randomness. In this paper, we present two such approaches. One is based\non Byzantine agreement and the other on threshold coin-tossing. Each approach\nhas its strength and weaknesses. We compare the performance of the two\napproaches and outline their respective best use scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.1521v1", 
    "other_authors": "Wenbing Zhao", 
    "title": "Proactive Service Migration for Long-Running Byzantine Fault Tolerant   Systems", 
    "arxiv-id": "0803.1521v1", 
    "author": "Wenbing Zhao", 
    "publish": "2008-03-11T04:34:04Z", 
    "summary": "In this paper, we describe a novel proactive recovery scheme based on service\nmigration for long-running Byzantine fault tolerant systems. Proactive recovery\nis an essential method for ensuring long term reliability of fault tolerant\nsystems that are under continuous threats from malicious adversaries. The\nprimary benefit of our proactive recovery scheme is a reduced vulnerability\nwindow. This is achieved by removing the time-consuming reboot step from the\ncritical path of proactive recovery. Our migration-based proactive recovery is\ncoordinated among the replicas, therefore, it can automatically adjust to\ndifferent system loads and avoid the problem of excessive concurrent proactive\nrecoveries that may occur in previous work with fixed watchdog timeouts.\nMoreover, the fast proactive recovery also significantly improves the system\navailability in the presence of faults."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.2219v1", 
    "other_authors": "Andrei Marculescu, Sotiris Nikoletseas, Olivier Powell, Jose Rolim", 
    "title": "Lighweight Target Tracking Using Passive Traces in Sensor Networks", 
    "arxiv-id": "0803.2219v1", 
    "author": "Jose Rolim", 
    "publish": "2008-03-14T18:01:17Z", 
    "summary": "We study the important problem of tracking moving targets in wireless sensor\nnetworks. We try to overcome the limitations of standard state of the art\ntracking methods based on continuous location tracking, i.e. the high energy\ndissipation and communication overhead imposed by the active participation of\nsensors in the tracking process and the low scalability, especially in sparse\nnetworks. Instead, our approach uses sensors in a passive way: they just record\nand judiciously spread information about observed target presence in their\nvicinity; this information is then used by the (powerful) tracking agent to\nlocate the target by just following the traces left at sensors. Our protocol is\ngreedy, local, distributed, energy efficient and very successful, in the sense\nthat (as shown by extensive simulations) the tracking agent manages to quickly\nlocate and follow the target; also, we achieve good trade-offs between the\nenergy dissipation and latency."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0803.3642v3", 
    "other_authors": "Hariharan Narayanan", 
    "title": "Distributed Averaging in the presence of a Sparse Cut", 
    "arxiv-id": "0803.3642v3", 
    "author": "Hariharan Narayanan", 
    "publish": "2008-03-25T22:04:50Z", 
    "summary": "We consider the question of averaging on a graph that has one sparse cut\nseparating two subgraphs that are internally well connected.\n  While there has been a large body of work devoted to algorithms for\ndistributed averaging, nearly all algorithms involve only {\\it convex} updates.\nIn this paper, we suggest that {\\it non-convex} updates can lead to significant\nimprovements. We do so by exhibiting a decentralized algorithm for graphs with\none sparse cut that uses non-convex averages and has an averaging time that can\nbe significantly smaller than the averaging time of known distributed\nalgorithms, such as those of \\cite{tsitsiklis, Boyd}. We use stochastic\ndominance to prove this result in a way that may be of independent interest."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V13P116", 
    "link": "http://arxiv.org/pdf/0806.0282v1", 
    "other_authors": "Patrik Flor\u00e9en, Marja Hassinen, Petteri Kaski, Jukka Suomela", 
    "title": "Local approximation algorithms for a class of 0/1 max-min linear   programs", 
    "arxiv-id": "0806.0282v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-06-02T14:27:46Z", 
    "summary": "We study the applicability of distributed, local algorithms to 0/1 max-min\nLPs where the objective is to maximise ${\\min_k \\sum_v c_{kv} x_v}$ subject to\n${\\sum_v a_{iv} x_v \\le 1}$ for each $i$ and ${x_v \\ge 0}$ for each $v$. Here\n$c_{kv} \\in \\{0,1\\}$, $a_{iv} \\in \\{0,1\\}$, and the support sets ${V_i = \\{v :\na_{iv} > 0 \\}}$ and ${V_k = \\{v : c_{kv}>0 \\}}$ have bounded size; in\nparticular, we study the case $|V_k| \\le 2$. Each agent $v$ is responsible for\nchoosing the value of $x_v$ based on information within its constant-size\nneighbourhood; the communication network is the hypergraph where the sets $V_k$\nand $V_i$ constitute the hyperedges. We present a local approximation algorithm\nwhich achieves an approximation ratio arbitrarily close to the theoretical\nlower bound presented in prior work."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.2395v1", 
    "other_authors": "Hasan Guclu, Durgesh Kumari, Murat Yuksel", 
    "title": "Ad-hoc Limited Scale-Free Models for Unstructured Peer-to-Peer Networks", 
    "arxiv-id": "0806.2395v1", 
    "author": "Murat Yuksel", 
    "publish": "2008-06-14T19:01:50Z", 
    "summary": "Several protocol efficiency metrics (e.g., scalability, search success rate,\nrouting reachability and stability) depend on the capability of preserving\nstructure even over the churn caused by the ad-hoc nodes joining or leaving the\nnetwork. Preserving the structure becomes more prohibitive due to the\ndistributed and potentially uncooperative nature of such networks, as in the\npeer-to-peer (P2P) networks. Thus, most practical solutions involve\nunstructured approaches while attempting to maintain the structure at various\nlevels of protocol stack. The primary focus of this paper is to investigate\nconstruction and maintenance of scale-free topologies in a distributed manner\nwithout requiring global topology information at the time when nodes join or\nleave. We consider the uncooperative behavior of peers by limiting the number\nof neighbors to a pre-defined hard cutoff value (i.e., no peer is a major hub),\nand the ad-hoc behavior of peers by rewiring the neighbors of nodes leaving the\nnetwork. We also investigate the effect of these hard cutoffs and rewiring of\nad-hoc nodes on the P2P search efficiency."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.3152v2", 
    "other_authors": "Stavros Kontopoulos, Athanasios K. Tsakalidis", 
    "title": "TRANS-Net: an Efficient Peer-to-Peer Overlay Network Based on a Full   Transposition Graph", 
    "arxiv-id": "0806.3152v2", 
    "author": "Athanasios K. Tsakalidis", 
    "publish": "2008-06-19T08:28:28Z", 
    "summary": "In this paper we propose a new practical P2P system based on a full\ntransposition network topology named TRANS-Net. Full transposition networks\nachieve higher fault-tolerance and lower congestion among the class of\ntransposition networks. TRANS-Net provides an efficient lookup service i.e. k\nhops with high probability, where k satisfies Theta(log_n m) less than k less\nthan Theta(log_2 m), where m denotes the number of system nodes and n is a\nsystem parameter related to the maximum number that m can take (up to n!).\nExperiments show that the look-up performance achieves the lower limit of the\ncomplexity relation. TRANS-Net also preserves data locality and provides\nefficient look-up performance for complex queries such as multi-dimensional\nqueries."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.3215v1", 
    "other_authors": "Xiahong Lin, Lin Gao, Kefei Chen, David K. Y. Chiu", 
    "title": "MOHCS: Towards Mining Overlapping Highly Connected Subgraphs", 
    "arxiv-id": "0806.3215v1", 
    "author": "David K. Y. Chiu", 
    "publish": "2008-06-19T15:13:38Z", 
    "summary": "Many networks in real-life typically contain parts in which some nodes are\nmore highly connected to each other than the other nodes of the network. The\ncollection of such nodes are usually called clusters, communities, cohesive\ngroups or modules. In graph terminology, it is called highly connected graph.\nIn this paper, we first prove some properties related to highly connected\ngraph. Based on these properties, we then redefine the highly connected\nsubgraph which results in an algorithm that determines whether a given graph is\nhighly connected in linear time. Then we present a computationally efficient\nalgorithm, called MOHCS, for mining overlapping highly connected subgraphs. We\nhave evaluated experimentally the performance of MOHCS using real and synthetic\ndata sets from computer-generated graph and yeast protein network. Our results\nshow that MOHCS is effective and reliable in finding overlapping highly\nconnected subgraphs. Keywords-component; Highly connected subgraph, clustering\nalgorithms, minimum cut, minimum degree"
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.4221v1", 
    "other_authors": "Mirela Damian, Sriram V. Pemmaraju", 
    "title": "Localized Spanners for Wireless Networks", 
    "arxiv-id": "0806.4221v1", 
    "author": "Sriram V. Pemmaraju", 
    "publish": "2008-06-26T02:09:17Z", 
    "summary": "We present a new efficient localized algorithm to construct, for any given\nquasi-unit disk graph G=(V,E) and any e > 0, a (1+e)-spanner for G of maximum\ndegree O(1) and total weight O(w(MST)), where w(MST) denotes the weight of a\nminimum spanning tree for V. We further show that similar localized techniques\ncan be used to construct, for a given unit disk graph G = (V, E), a planar\nCdel(1+e)(1+pi/2)-spanner for G of maximum degree O(1) and total weight\nO(w(MST)). Here Cdel denotes the stretch factor of the unit Delaunay\ntriangulation for V. Both constructions can be completed in O(1) communication\nrounds, and require each node to know its own coordinates."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0807.1720v1", 
    "other_authors": "Anne Benoit, Henri Casanova, Veronika Rehn-Sonigo, Yves Robert", 
    "title": "Resource Allocation Strategies for In-Network Stream Processing", 
    "arxiv-id": "0807.1720v1", 
    "author": "Yves Robert", 
    "publish": "2008-07-10T19:14:14Z", 
    "summary": "In this paper we consider the operator mapping problem for in-network stream\nprocessing applications. In-network stream processing consists in applying a\ntree of operators in steady-state to multiple data objects that are continually\nupdated at various locations on a network. Examples of in-network stream\nprocessing include the processing of data in a sensor network, or of continuous\nqueries on distributed relational databases. We study the operator mapping\nproblem in a ``constructive'' scenario, i.e., a scenario in which one builds a\nplatform dedicated to the application buy purchasing processing servers with\nvarious costs and capabilities. The objective is to minimize the cost of the\nplatform while ensuring that the application achieves a minimum steady-state\nthroughput. The first contribution of this paper is the formalization of a set\nof relevant operator-placement problems as linear programs, and a proof that\neven simple versions of the problem are NP-complete. Our second contribution is\nthe design of several polynomial time heuristics, which are evaluated via\nextensive simulations and compared to theoretical bounds for optimal solutions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0807.4609v1", 
    "other_authors": "A. B. Mutiara", 
    "title": "Analisis Kinerja Sistem Cluster Terhadapa Aplikasi Simulasi Dinamika   Molekular NAMD Memanfaatkan Pustaka CHARM++", 
    "arxiv-id": "0807.4609v1", 
    "author": "A. B. Mutiara", 
    "publish": "2008-07-29T09:18:21Z", 
    "summary": "Tingkat kompleksitas dari program simulasi dinamika molekular membutuhkan\nmesin pemroses dengan kemampuan yang sangat besar. Mesin-mesin paralel terbukti\nmemiliki potensi untuk menjawab tantangan komputasi ini. Untuk memanfaatkan\npotensi ini secara maksimal, diperlukan suatu program paralel dengan tingkat\nefisiensi, efektifitas, skalabilitas, dan ekstensibilitas yang maksimal pula.\nProgram NAMD yang dibahas pada penulisan ini dianggap mampu untuk memenuhi\nsemua kriteria yang diinginkan. Program ini dirancang dengan\nmengimplementasikan pustaka Charm++ untuk pembagian tugas perhitungan secara\nparalel. NAMD memiliki sistem automatic load balancing secara periodik yang\ncerdas, sehingga dapat memaksimalkan penggunaan kemampuan mesin yang tersedia.\nProgram ini juga dirancang secara modular, sehingga dapat dimodifikasi dan\nditambah dengan sangat mudah. NAMD menggunakan banyak kombinasi algoritma\nperhitungan dan tehnik-tehnik numerik lainnya dalam melakukan tugasnya. NAMD\n2.5 mengimplementasikan semua tehnik dan persamaan perhitungan yang digunakan\ndalam dunia simulasi dinamika molekular saat ini. NAMD dapat berjalan diatas\nberbagai mesin paralel termasuk arsitektur cluster, dengan hasil speedup yang\nmengejutkan. Tulisan ini akan menjelaskan dan membuktikan kemampuan NAMD secara\nparalel diatas lima buah mesin cluster. Penulisan ini juga akan memaparkan\nkinerja NAMD pada beberapa."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0903.0710v1", 
    "other_authors": "Anne Benoit, Henri Casanova, Veronika Rehn-Sonigo, Yves Robert", 
    "title": "Resource Allocation for Multiple Concurrent In-Network Stream-Processing   Applications", 
    "arxiv-id": "0903.0710v1", 
    "author": "Yves Robert", 
    "publish": "2009-03-04T08:15:00Z", 
    "summary": "This paper investigates the operator mapping problem for in-network\nstream-processing applications. In-network stream-processing amounts to\napplying one or more trees of operators in steady-state, to multiple data\nobjects that are continuously updated at different locations in the network.\nThe goal is to compute some final data at some desired rate. Different operator\ntrees may share common subtrees. Therefore, it may be possible to reuse some\nintermediate results in different application trees. The first contribution of\nthis work is to provide complexity results for different instances of the basic\nproblem, as well as integer linear program formulations of various problem\ninstances. The second second contribution is the design of several\npolynomial-time heuristics. One of the primary objectives of these heuristics\nis to reuse intermediate results shared by multiple applications. Our\nquantitative comparisons of these heuristics in simulation demonstrates the\nimportance of choosing appropriate processors for operator mapping. It also\nallow us to identify a heuristic that achieves good results in practice."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0903.0730v1", 
    "other_authors": "Delia Sabina Stinga", 
    "title": "Grid Technologies", 
    "arxiv-id": "0903.0730v1", 
    "author": "Delia Sabina Stinga", 
    "publish": "2009-03-04T11:08:20Z", 
    "summary": "This paper contains the most important aspects of computing grids. Grid\ncomputing allows high performance distributed systems to act as a single\ncomputer. An overview of grids structure and techniques is given in order to\nunderstand the way grids work."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0903.1386v1", 
    "other_authors": "Christian Vecchiola, Michael Kirley, Rajkumar Buyya", 
    "title": "Multi-Objective Problem Solving With Offspring on Enterprise Clouds", 
    "arxiv-id": "0903.1386v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-03-08T04:37:33Z", 
    "summary": "In this paper, we present a distributed implementation of a network based\nmulti-objective evolutionary algorithm, called EMO, by using Offspring. Network\nbased evolutionary algorithms have proven to be effective for multi-objective\nproblem solving. They feature a network of connections between individuals that\ndrives the evolution of the algorithm. Unfortunately, they require large\npopulations to be effective and a distributed implementation can leverage the\ncomputation time. Most of the existing frameworks are limited to providing\nsolutions that are basic or specific to a given algorithm. Our Offspring\nframework is a plug-in based software environment that allows rapid deployment\nand execution of evolutionary algorithms on distributed computing environments\nsuch as Enterprise Clouds. Its features and benefits are presented by\ndescribing the distributed implementation of EMO."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.1388v1", 
    "other_authors": "Chao Jin, Jayavardhana Gubbi, Rajkumar Buyya, Marimuthu Palaniswami", 
    "title": "Jeeva: Enterprise Grid-enabled Web Portal for Protein Secondary   Structure Prediction", 
    "arxiv-id": "0903.1388v1", 
    "author": "Marimuthu Palaniswami", 
    "publish": "2009-03-08T04:50:53Z", 
    "summary": "This paper presents a Grid portal for protein secondary structure prediction\ndeveloped by using services of Aneka, a .NET-based enterprise Grid technology.\nThe portal is used by research scientists to discover new prediction structures\nin a parallel manner. An SVM (Support Vector Machine)-based prediction\nalgorithm is used with 64 sample protein sequences as a case study to\ndemonstrate the potential of enterprise Grids."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.3462v1", 
    "other_authors": "Luigi Santocanale", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3 (Extended   Version)", 
    "arxiv-id": "0903.3462v1", 
    "author": "Luigi Santocanale", 
    "publish": "2009-03-20T07:54:05Z", 
    "summary": "We address the problem of finding nice labellings for event structures of\ndegree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.4961v2", 
    "other_authors": "Yunji Chen, Tianshi Chen, Weiwu Hu", 
    "title": "Global Clock, Physical Time Order and Pending Period Analysis in   Multiprocessor Systems", 
    "arxiv-id": "0903.4961v2", 
    "author": "Weiwu Hu", 
    "publish": "2009-03-29T10:34:34Z", 
    "summary": "In multiprocessor systems, various problems are treated with Lamport's\nlogical clock and the resultant logical time orders between operations.\nHowever, one often needs to face the high complexities caused by the lack of\nlogical time order information in practice. In this paper, we utilize the\n\\emph{global clock} to infuse the so-called \\emph{pending period} to each\noperation in a multiprocessor system, where the pending period is a time\ninterval that contains the performed time of the operation. Further, we define\nthe \\emph{physical time order} for any two operations with disjoint pending\nperiods. The physical time order is obeyed by any real execution in\nmultiprocessor systems due to that it is part of the truly happened operation\norders restricted by global clock, and it is then proven to be independent and\nconsistent with traditional logical time orders. The above novel yet\nfundamental concepts enables new effective approaches for analyzing\nmultiprocessor systems, which are named \\emph{pending period analysis} as a\nwhole. As a consequence of pending period analysis, many important problems of\nmultiprocessor systems can be tackled effectively. As a significant application\nexample, complete memory consistency verification, which was known as an\nNP-hard problem, can be solved with the complexity of $O(n^2)$ (where $n$ is\nthe number of operations). Moreover, the two event ordering problems, which\nwere proven to be Co-NP-Hard and NP-hard respectively, can both be solved with\nthe time complexity of O(n) if restricted by pending period information."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0909.1146v1", 
    "other_authors": "Saurabh Kumar Garg, Chee Shin Yeo, Arun Anandasivam, Rajkumar Buyya", 
    "title": "Energy-Efficient Scheduling of HPC Applications in Cloud Computing   Environments", 
    "arxiv-id": "0909.1146v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-09-07T06:13:40Z", 
    "summary": "The use of High Performance Computing (HPC) in commercial and consumer IT\napplications is becoming popular. They need the ability to gain rapid and\nscalable access to high-end computing capabilities. Cloud computing promises to\ndeliver such a computing infrastructure using data centers so that HPC users\ncan access applications and data from a Cloud anywhere in the world on demand\nand pay based on what they use. However, the growing demand drastically\nincreases the energy consumption of data centers, which has become a critical\nissue. High energy consumption not only translates to high energy cost, which\nwill reduce the profit margin of Cloud providers, but also high carbon\nemissions which is not environmentally sustainable. Hence, energy-efficient\nsolutions are required that can address the high increase in the energy\nconsumption from the perspective of not only Cloud provider but also from the\nenvironment. To address this issue we propose near-optimal scheduling policies\nthat exploits heterogeneity across multiple data centers for a Cloud provider.\nWe consider a number of energy efficiency factors such as energy cost, carbon\nemission rate, workload, and CPU power efficiency which changes across\ndifferent data center depending on their location, architectural design, and\nmanagement system. Our carbon/energy based scheduling policies are able to\nachieve on average up to 30% of energy savings in comparison to profit based\nscheduling policies leading to higher profit and less carbon emissions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-1-4419-6794-7_8", 
    "link": "http://arxiv.org/pdf/0909.1517v1", 
    "other_authors": "Marco Aldinucci, Marco Danelutto, Peter Kilpatrick", 
    "title": "Autonomic management of multiple non-functional concerns in behavioural   skeletons", 
    "arxiv-id": "0909.1517v1", 
    "author": "Peter Kilpatrick", 
    "publish": "2009-09-08T17:02:58Z", 
    "summary": "We introduce and address the problem of concurrent autonomic management of\ndifferent non-functional concerns in parallel applications build as a\nhierarchical composition of behavioural skeletons. We first define the problems\narising when multiple concerns are dealt with by independent managers, then we\npropose a methodology supporting coordinated management, and finally we discuss\nhow autonomic management of multiple concerns may be implemented in a typical\nuse case. The paper concludes with an outline of the challenges involved in\nrealizing the proposed methodology on distributed target architectures such as\nclusters and grids. Being based on the behavioural skeleton concept proposed in\nthe CoreGRID GCM, it is anticipated that the methodology will be readily\nintegrated into the current reference implementation of GCM based on Java\nProActive and running on top of major grid middleware systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-1-4419-6794-7_8", 
    "link": "http://arxiv.org/pdf/0909.1788v1", 
    "other_authors": "Pat Helland, David Campbell", 
    "title": "Building on Quicksand", 
    "arxiv-id": "0909.1788v1", 
    "author": "David Campbell", 
    "publish": "2009-09-09T18:10:57Z", 
    "summary": "Reliable systems have always been built out of unreliable components. Early\non, the reliable components were small such as mirrored disks or ECC (Error\nCorrecting Codes) in core memory. These systems were designed such that\nfailures of these small components were transparent to the application. Later,\nthe size of the unreliable components grew larger and semantic challenges crept\ninto the application when failures occurred.\n  As the granularity of the unreliable component grows, the latency to\ncommunicate with a backup becomes unpalatable. This leads to a more relaxed\nmodel for fault tolerance. The primary system will acknowledge the work request\nand its actions without waiting to ensure that the backup is notified of the\nwork. This improves the responsiveness of the system.\n  There are two implications of asynchronous state capture: 1) Everything\npromised by the primary is probabilistic. There is always a chance that an\nuntimely failure shortly after the promise results in a backup proceeding\nwithout knowledge of the commitment. Hence, nothing is guaranteed! 2)\nApplications must ensure eventual consistency. Since work may be stuck in the\nprimary after a failure and reappear later, the processing order for work\ncannot be guaranteed.\n  Platform designers are struggling to make this easier for their applications.\nEmerging patterns of eventual consistency and probabilistic execution may soon\nyield a way for applications to express requirements for a \"looser\" form of\nconsistency while providing availability in the face of ever larger failures.\n  This paper recounts portions of the evolution of these trends, attempts to\nshow the patterns that span these changes, and talks about future directions as\nwe continue to \"build on quicksand\"."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/0909.5177v3", 
    "other_authors": "Godwin Shen, Antonio Ortega", 
    "title": "Transform-based Distributed Data Gathering", 
    "arxiv-id": "0909.5177v3", 
    "author": "Antonio Ortega", 
    "publish": "2009-09-28T19:57:13Z", 
    "summary": "A general class of unidirectional transforms is presented that can be\ncomputed in a distributed manner along an arbitrary routing tree. Additionally,\nwe provide a set of conditions under which these transforms are invertible.\nThese transforms can be computed as data is routed towards the collection (or\nsink) node in the tree and exploit data correlation between nodes in the tree.\nMoreover, when used in wireless sensor networks, these transforms can also\nleverage data received at nodes via broadcast wireless communications. Various\nconstructions of unidirectional transforms are also provided for use in data\ngathering in wireless sensor networks. New wavelet transforms are also proposed\nwhich provide significant improvements over existing unidirectional transforms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/1005.0925v1", 
    "other_authors": "Asgarali Bouyer, Mohammad Javad hoseyni, Abdul Hanan Abdullah", 
    "title": "Improving Overhead Computation and pre-processing Time for Grid   Scheduling System", 
    "arxiv-id": "1005.0925v1", 
    "author": "Abdul Hanan Abdullah", 
    "publish": "2010-05-06T08:30:16Z", 
    "summary": "Computational Grid is enormous environments with heterogeneous resources and\nstable infrastructures among other Internet-based computing systems. However,\nthe managing of resources in such systems has its special problems. Scheduler\nsystems need to get last information about participant nodes from information\ncenters for the purpose of firmly job scheduling. In this paper, we focus on\nonline updating resource information centers with processed and provided data\nbased on the assumed hierarchical model. A hybrid knowledge extraction method\nhas been used to classifying grid nodes based on prediction of jobs' features.\nAn affirmative point of this research is that scheduler systems don't waste\nextra time for getting up-to-date information of grid nodes. The experimental\nresult shows the advantages of our approach compared to other conservative\nmethods, especially due to its ability to predict the behavior of nodes based\non comprehensive data tables on each node."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/1005.1195v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "The Impact of Topology on Byzantine Containment in Stabilization", 
    "arxiv-id": "1005.1195v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-07T12:33:41Z", 
    "summary": "Self-stabilization is an versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed system that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties prove difficult: we demonstrate that it is impossible to contain the\nimpact of Byzantine nodes in a self-stabilizing context for maximum metric tree\nconstruction (strict stabilization). We propose a weaker containment scheme\ncalled topology-aware strict stabilization, and present a protocol for\ncomputing maximum metric trees that is optimal for this scheme with respect to\nimpossibility result."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2010.2212", 
    "link": "http://arxiv.org/pdf/1005.1747v1", 
    "other_authors": "Salman Abdul Moiz, Lakshmi Rajamani", 
    "title": "A Real Time Optimistic Strategy to achieve Concurrency Control in Mobile   Environments Using On-demand Multicasting", 
    "arxiv-id": "1005.1747v1", 
    "author": "Lakshmi Rajamani", 
    "publish": "2010-05-11T08:17:36Z", 
    "summary": "In mobile database environments, multiple users may access similar data items\nirrespective of their physical location leading to concurrent access anomalies.\nAs disconnections and mobility are the common characteristics in mobile\nenvironment, performing concurrent access to a particular data item leads to\ninconsistency. Most of the approaches use locking mechanisms to achieve\nconcurrency control. However this leads to increase in blocking and abort rate.\nIn this paper an optimistic concurrency control strategy using on-demand\nmulticasting is proposed for mobile database environments which guarantees\nconsistency and introduces application-specific conflict detection and\nresolution strategies. The simulation results specify increase in system\nthroughput by reducing the transaction abort rates as compared to the other\noptimistic strategies proposed in literature."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.1751v1", 
    "other_authors": "N. Ch. Sriman Narayana Iyengar, Syed Mohammad Ansar Sachin kumar, Piyush Nagar, Siddharth Sharma, Akshay Atrey", 
    "title": "An Efficient and Secure Routing Protocol for Mobile Ad-Hoc Networks", 
    "arxiv-id": "1005.1751v1", 
    "author": "Akshay Atrey", 
    "publish": "2010-05-11T08:30:10Z", 
    "summary": "Efficiency and simplicity of random algorithms have made them a lucrative\nalternative for solving complex problems in the domain of communication\nnetworks. This paper presents a random algorithm for handling the routing\nproblem in Mobile Ad hoc Networks [MANETS].The performance of most existing\nrouting protocols for MANETS degrades in terms of packet delay and congestion\ncaused as the number of mobile nodes increases beyond a certain level or their\nspeed passes a certain level. As the network becomes more and more dynamic,\ncongestion in network increases due to control packets generated by the routing\nprotocols in the process of route discovery and route maintenance. Most of this\ncongestion is due to flooding mechanism used in protocols like AODV and DSDV\nfor the purpose of route discovery and route maintenance or for route discovery\nas in the case of DSR protocol. This paper introduces the concept of random\nrouting algorithm that neither maintains a routing table nor floods the entire\nnetwork as done by various known protocols thereby reducing the load on network\nin terms of number of control packets in a highly dynamic scenario. This paper\ncalculates the expected run time of the designed random algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.1904v3", 
    "other_authors": "Abhinav Pandey, Akash Pandey, Ankit Tandon, Brajesh Kr Maurya, Upendra Kushwaha, Dr. Madhvendra Mishra, Vijayshree Tiwari", 
    "title": "Cloud Computing: Exploring the scope", 
    "arxiv-id": "1005.1904v3", 
    "author": "Vijayshree Tiwari", 
    "publish": "2010-05-11T18:23:38Z", 
    "summary": "Cloud computing refers to a paradigm shift to overall IT solutions while\nraising the accessibility, scalability and effectiveness through its enabling\ntechnologies. However, migrated cloud platforms and services cost benefits as\nwell as performances are neither clear nor summarized. Globalization and the\nrecessionary economic times have not only raised the bar of a better IT\ndelivery models but also have given access to technology enabled services via\ninternet. Cloud computing has vast potential in terms of lean Retail\nmethodologies that can minimize the operational cost by using the third party\nbased IT capabilities, as a service. It will not only increase the ROI but will\nalso help in lowering the total cost of ownership. In this paper we have tried\nto compare the cloud computing cost benefits with the actual premise cost which\nan organization incurs normally. However, in spite of the cost benefits, many\nIT professional believe that the latest model i.e. \"cloud computing\" has risks\nand security concerns. This report demonstrates how to answer the following\nquestions: (1) Idea behind cloud computing. (2) Monetary cost benefits of using\ncloud with respect to traditional premise computing. (3) What are the various\nsecurity issues? We have tried to find out the cost benefit by comparing the\nMicrosoft Azure cloud cost with the prevalent premise cost."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2027v1", 
    "other_authors": "Sarbani Roy, Saikat Halder, Nandini Mukherjee", 
    "title": "A Multi-agent Framework for Performance Tuning in Distributed   Environment", 
    "arxiv-id": "1005.2027v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-05-12T09:39:18Z", 
    "summary": "This paper presents the overall design of a multi-agent framework for tuning\nthe performance of an application executing in a distributed environment. The\nmulti-agent framework provides services like resource brokering, analyzing\nperformance monitoring data, local tuning and also rescheduling in case of any\nperformance problem on a specific resource provider. The paper also briefly\ndescribes the implementation of some part of the framework. In particular, job\nmigration on the basis of performance monitoring data is particularly\nhighlighted in this paper."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2037v1", 
    "other_authors": "Ajanta De Sarkar, Sarbani Roy, Sudipto Biswas, Nandini Mukherjee", 
    "title": "An Integrated Framework for Performance Analysis and Tuning in Grid   Environment", 
    "arxiv-id": "1005.2037v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-05-12T10:13:59Z", 
    "summary": "In a heterogeneous, dynamic environment, like Grid, post-mortem analysis is\nof no use and data needs to be collected and analysed in real time. Novel\ntechniques are also required for dynamically tuning the application's\nperformance and resource brokering in order to maintain the desired QoS. The\nobjective of this paper is to propose an integrated framework for performance\nanalysis and tuning of the application, and rescheduling the application, if\nnecessary, to some other resources in order to adapt to the changing resource\nusage scenario in a dynamic environment."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2567v1", 
    "other_authors": "Alejandro Cornejo, Fabian Kuhn", 
    "title": "Deploying Wireless Networks with Beeps", 
    "arxiv-id": "1005.2567v1", 
    "author": "Fabian Kuhn", 
    "publish": "2010-05-14T16:32:02Z", 
    "summary": "We present the \\emph{discrete beeping} communication model, which assumes\nnodes have minimal knowledge about their environment and severely limited\ncommunication capabilities. Specifically, nodes have no information regarding\nthe local or global structure of the network, don't have access to synchronized\nclocks and are woken up by an adversary. Moreover, instead on communicating\nthrough messages they rely solely on carrier sensing to exchange information.\nWe study the problem of \\emph{interval coloring}, a variant of vertex coloring\nspecially suited for the studied beeping model. Given a set of resources, the\ngoal of interval coloring is to assign every node a large contiguous fraction\nof the resources, such that neighboring nodes share no resources. To highlight\nthe importance of the discreteness of the model, we contrast it against a\ncontinuous variant described in [17]. We present an O(1$ time algorithm that\nterminates with probability 1 and assigns an interval of size\n$\\Omega(T/\\Delta)$ that repeats every $T$ time units to every node of the\nnetwork. This improves an $O(\\log n)$ time algorithm with the same guarantees\npresented in \\cite{infocom09}, and accentuates the unrealistic assumptions of\nthe continuous model. Under the more realistic discrete model, we present a Las\nVegas algorithm that solves $\\Omega(T/\\Delta)$-interval coloring in $O(\\log n)$\ntime with high probability and describe how to adapt the algorithm for dynamic\nnetworks where nodes may join or leave. For constant degree graphs we prove a\nlower bound of $\\Omega(\\log n)$ on the time required to solve interval coloring\nfor this model against randomized algorithms. This lower bound implies that our\nalgorithm is asymptotically optimal for constant degree graphs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.3158v1", 
    "other_authors": "Ruhollah Tavakoli", 
    "title": "Improvement Cache Efficiency of Explicit Finite Element Procedure and   its Application to Parallel Casting Solidification Simulation", 
    "arxiv-id": "1005.3158v1", 
    "author": "Ruhollah Tavakoli", 
    "publish": "2010-05-18T11:17:34Z", 
    "summary": "A simple method for improving cache efficiency of serial and parallel\nexplicit finite procedure with application to casting solidification simulation\nover three-dimensional complex geometries is presented. The method is based on\ndivision of the global data to smaller blocks and treating each block\nindependently from others at each time step. A novel parallel finite element\nalgorithm for non-overlapped element-base decomposed domain is presented for\nimplementation of serial and parallel version of the presented method. Effect\nof mesh reordering on the efficiency is also investigated. A simple algorithm\nis presented for high quality decomposition of decoupled global mesh. Our\nresult shows 10-20 \\% performance improvement by mesh reordering and 1.2-2.2\nspeedup with application of the presented cache efficient algorithm (for serial\nand parallel versions). Also the presented parallel solver (without\ncache-efficient feature) shows nearly linear speedup on the traditional\nEthernet networked Linux cluster."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.3367v2", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "Bounding the Impact of Unbounded Attacks in Stabilization", 
    "arxiv-id": "1005.3367v2", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-19T06:34:03Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. Combining these two properties proved\ndifficult: it is impossible to contain the spatial impact of Byzantine nodes in\na self-stabilizing context for global tasks such as tree orientation and tree\nconstruction. We present and illustrate a new concept of Byzantine containment\nin stabilization. Our property, called Strong Stabilization enables to contain\nthe impact of Byzantine nodes if they actually perform too many Byzantine\nactions. We derive impossibility results for strong stabilization and present\nstrongly stabilizing protocols for tree orientation and tree construction that\nare optimal with respect to the number of Byzantine nodes that can be tolerated\nin a self-stabilizing context."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.5223v1", 
    "other_authors": "Swan Dubois, Toshimitsu Masuzawa, S\u00e9bastien Tixeuil", 
    "title": "On Byzantine Containment Properties of the $min+1$ Protocol", 
    "arxiv-id": "1005.5223v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-28T06:28:10Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a breadth-first spanning tree in this context. Combining these two\nproperties proves difficult: we demonstrate that it is impossible to contain\nthe impact of Byzantine nodes in a strictly or strongly stabilizing manner. We\nthen adopt the weaker scheme of topology-aware strict stabilization and we\npresent a similar weakening of strong stabilization. We prove that the\nclassical $min+1$ protocol has optimal Byzantine containment properties with\nrespect to these criteria."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2210", 
    "link": "http://arxiv.org/pdf/1005.5435v1", 
    "other_authors": "Y. Jayanta Singh, Yumnam Somananda Singh, Ashok Gaikwad, S. C. Mehrotra", 
    "title": "Dynamic management of transactions in distributed real-time processing   system", 
    "arxiv-id": "1005.5435v1", 
    "author": "S. C. Mehrotra", 
    "publish": "2010-05-29T07:57:30Z", 
    "summary": "Managing the transactions in real time distributed computing system is not\neasy, as it has heterogeneously networked computers to solve a single problem.\nIf a transaction runs across some different sites, it may commit at some sites\nand may failure at another site, leading to an inconsistent transaction. The\ncomplexity is increase in real time applications by placing deadlines on the\nresponse time of the database system and transactions processing. Such a system\nneeds to process Transactions before these deadlines expired. A series of\nsimulation study have been performed to analyze the performance under different\ntransaction management under conditions such as different workloads,\ndistribution methods, execution mode-distribution and parallel etc. The\nscheduling of data accesses are done in order to meet their deadlines and to\nminimize the number of transactions that missed deadlines. A new concept is\nintroduced to manage the transactions in dynamic ways rather than setting\ncomputing parameters in static ways. With this approach, the system gives a\nsignificant improvement in performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijma.2010.2202", 
    "link": "http://arxiv.org/pdf/1005.5440v1", 
    "other_authors": "Surender Kumar, R. K. Chauhan, Parveen Kumar", 
    "title": "A Low Overhead Minimum Process Global Snapshop Collection Algorithm for   Mobile Distributed System", 
    "arxiv-id": "1005.5440v1", 
    "author": "Parveen Kumar", 
    "publish": "2010-05-29T08:29:45Z", 
    "summary": "Coordinated checkpointing is an effective fault tolerant technique in\ndistributed system as it avoids the domino effect and require minimum storage\nrequirement. Most of the earlier coordinated checkpoint algorithms block their\ncomputation during checkpointing and forces minimum-process or non-blocking but\nforces all nodes to takes checkpoint even though many of them may not be\nnecessary or non-blocking minimum-process but takes useless checkpoints or\nreduced useless checkpoint but has higher synchronization message overhead or\nhas high checkpoint request propagation time. Hence in mobile distributed\nsystems there is a great need of minimizing the number of communication message\nand checkpointing overhead as it raise new issues such as mobility, low\nbandwidth of wireless channels, frequently disconnections, limited battery\npower and lack of reliable stable storage on mobile nodes. In this paper, we\npropose a minimum-process coordinated checkpointing algorithm for mobile\ndistributed system where no useless checkpoints are taken, no blocking of\nprocesses takes place and enforces a minimum-number of processes to take\ncheckpoints. Our algorithm imposes low memory and computation overheads on MH's\nand low communication overheads on wireless channels. It avoids awakening of an\nMH if it is not required to take its checkpoint and has reduced latency time as\neach process involved in a global checkpoint can forward its own decision\ndirectly to the checkpoint initiator."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1005.5606v1", 
    "other_authors": "D. Kesavaraja, R. Balasubramanian, D. Sasireka", 
    "title": "Implementation of a Cloud Data Server (CDS) for Providing Secure Service   in E-Business", 
    "arxiv-id": "1005.5606v1", 
    "author": "D. Sasireka", 
    "publish": "2010-05-31T07:08:34Z", 
    "summary": "Cloud Data Servers is the novel approach for providing secure service to\ne-business .Millions of users are surfing the Cloud for various purposes,\ntherefore they need highly safe and persistent services. Usually hackers target\nparticular Operating Systems or a Particular Controller. Inspiteof several\nongoing researches Conventional Web Servers and its Intrusion Detection System\nmight not be able to detect such attacks. So we implement a Cloud Data Server\nwith Session Controller Architecture using Redundancy and Disconnected Data\nAccess Mechanism. In this paper, we generate the hash code using MD5 algorithm.\nWith the help of which we can circumvent even the attacks, which are undefined\nby traditional Systems .we implement Cloud Data Sever using Java and Hash Code\nbackup Management using My SQL. Here we Implement AES Algorithm for providing\nmore Security for the hash Code. The CDS using the Virtual Controller controls\nand monitors the Connections and modifications of the page so as to prevent\nmalicious users from hacking the website. In the proposed approach an activity\nanalyzer takes care of intimating the administrator about possible intrusions\nand the counter measures required to tackle them. The efficiency ratio of our\napproach is 98.21% compared with similar approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1005.5630v1", 
    "other_authors": "Colette Johnen, Ivan Lavallee, Christian Lavault", 
    "title": "Reliable Self-Stabilizing Communication for Quasi Rendezvous", 
    "arxiv-id": "1005.5630v1", 
    "author": "Christian Lavault", 
    "publish": "2010-05-31T09:09:25Z", 
    "summary": "The paper presents three self-stabilizing protocols for basic fair and\nreliable link communication primitives. We assume a link-register communication\nmodel under read/write atomicity, where every process can read from but cannot\nwrite into its neighbours' registers. The first primitive guarantees that any\nprocess writes a new value in its register(s) only after all its neighbours\nhave read the previous value, whatever the initial scheduling of processes'\nactions. The second primitive implements a \"weak rendezvous\" communication\nmechanism by using an alternating bit protocol: whenever a process\nconsecutively writes n values (possibly the same ones) in a register, each\nneighbour is guaranteed to read each value from the register at least once. On\nthe basis of the previous protocol, the third primitive implements a \"quasi\nrendezvous\": in words, this primitive ensures furthermore that there exists\nexactly one reading between two writing operations All protocols are\nself-stabilizing and run in asynchronous arbitrary networks. The goal of the\npaper is in handling each primitive by a separate procedure, which can be used\nas a \"black box\" in more involved self-stabilizing protocols."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1008.0011v1", 
    "other_authors": "Heinz Kredel", 
    "title": "Parallel and distributed Gr\u00f6bner bases computation in JAS", 
    "arxiv-id": "1008.0011v1", 
    "author": "Heinz Kredel", 
    "publish": "2010-07-30T20:38:38Z", 
    "summary": "This paper considers parallel Gr\\\"obner bases algorithms on distributed\nmemory parallel computers with multi-core compute nodes. We summarize three\ndifferent Gr\\\"obner bases implementations: shared memory parallel, pure\ndistributed memory parallel and distributed memory combined with shared memory\nparallelism. The last algorithm, called distributed hybrid, uses only one\ncontrol communication channel between the master node and the worker nodes and\nkeeps polynomials in shared memory on a node. The polynomials are transported\nasynchronous to the control-flow of the algorithm in a separate distributed\ndata structure. The implementation is generic and works for all implemented\n(exact) fields. We present new performance measurements and discuss the\nperformance of the algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1008.0064v1", 
    "other_authors": "Frederique Oggier, Anwitaman Datta", 
    "title": "Self-repairing Homomorphic Codes for Distributed Storage Systems", 
    "arxiv-id": "1008.0064v1", 
    "author": "Anwitaman Datta", 
    "publish": "2010-07-31T07:37:26Z", 
    "summary": "Erasure codes provide a storage efficient alternative to replication based\nredundancy in (networked) storage systems. They however entail high\ncommunication overhead for maintenance, when some of the encoded fragments are\nlost and need to be replenished. Such overheads arise from the fundamental need\nto recreate (or keep separately) first a copy of the whole object before any\nindividual encoded fragment can be generated and replenished. There has been\nrecently intense interest to explore alternatives, most prominent ones being\nregenerating codes (RGC) and hierarchical codes (HC). We propose as an\nalternative a new family of codes to improve the maintenance process, which we\ncall self-repairing codes (SRC), with the following salient features: (a)\nencoded fragments can be repaired directly from other subsets of encoded\nfragments without having to reconstruct first the original data, ensuring that\n(b) a fragment is repaired from a fixed number of encoded fragments, the number\ndepending only on how many encoded blocks are missing and independent of which\nspecific blocks are missing. These properties allow for not only low\ncommunication overhead to recreate a missing fragment, but also independent\nreconstruction of different missing fragments in parallel, possibly in\ndifferent parts of the network. We analyze the static resilience of SRCs with\nrespect to traditional erasure codes, and observe that SRCs incur marginally\nlarger storage overhead in order to achieve the aforementioned properties. The\nsalient SRC properties naturally translate to low communication overheads for\nreconstruction of lost fragments, and allow reconstruction with lower latency\nby facilitating repairs in parallel. These desirable properties make\nself-repairing codes a good and practical candidate for networked distributed\nstorage systems."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1008.0451v1", 
    "other_authors": "Yibei Ling, Shigang Chen, Cho-Yu Jason Chiang", 
    "title": "On Optimal Deadlock Detection Scheduling", 
    "arxiv-id": "1008.0451v1", 
    "author": "Cho-Yu Jason Chiang", 
    "publish": "2010-08-03T03:46:56Z", 
    "summary": "Deadlock detection scheduling is an important, yet often overlooked problem\nthat can significantly affect the overall performance of deadlock handling.\nExcessive initiation of deadlock detection increases overall message usage,\nresulting in degraded system performance in the absence of deadlocks; while\ninsufficient initiation of deadlock detection increases the deadlock\npersistence time, resulting in an increased deadlock resolution cost in the\npresence of deadlocks. The investigation of this performance tradeoff, however,\nis missing in the literature. This paper studies the impact of deadlock\ndetection scheduling on the overall performance of deadlock handling. In\nparticular, we show that there exists an optimal deadlock detection frequency\nthat yields the minimum long-run mean average cost, which is determined by the\nmessage complexities of the deadlock detection and resolution algorithms being\nused, as well as the rate of deadlock formation, denoted as $\\lambda$. For the\nbest known deadlock detection and resolution algorithms, we show that the\nasymptotically optimal frequency of deadlock detection scheduling that\nminimizes the overall message overhead is ${\\cal O}((\\lambda n)^{1/3})$, when\nthe total number $n$ of processes is sufficiently large. Furthermore, we show\nthat in general fully distributed (uncoordinated) deadlock detection scheduling\ncannot be performed as efficiently as centralized (coordinated) deadlock\ndetection scheduling."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1008.1900v1", 
    "other_authors": "Ali Khajeh-Hosseini, David Greenwood, James W. Smith, Ian Sommerville", 
    "title": "The Cloud Adoption Toolkit: Supporting Cloud Adoption Decisions in the   Enterprise", 
    "arxiv-id": "1008.1900v1", 
    "author": "Ian Sommerville", 
    "publish": "2010-08-11T12:56:32Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper describes the challenges that\ndecision makers face when assessing the feasibility of the adoption of cloud\ncomputing in their organisations, and describes our Cloud Adoption Toolkit,\nwhich has been developed to support this process. The toolkit provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. Cost Modeling is the most mature tool in the toolkit, and this\npaper shows its effectiveness by demonstrating how practitioners can use it to\nexamine the costs of deploying their IT systems on the cloud. The Cost Modeling\ntool is evaluated using a case study of an organization that is considering the\nmigration of some of its IT systems to the cloud. The case study shows that\nrunning systems on the cloud using a traditional \"always on\" approach can be\nless cost effective, and the elastic nature of the cloud has to be used to\nreduce costs. Therefore, decision makers have to be able to model the\nvariations in resource usage and their systems deployment options to obtain\naccurate cost estimates."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1008.2767v1", 
    "other_authors": "Derek Groen, Steven Rieder, Paola Grosso, Cees de Laat, Simon Portegies Zwart", 
    "title": "A Light-Weight Communication Library for Distributed Computing", 
    "arxiv-id": "1008.2767v1", 
    "author": "Simon Portegies Zwart", 
    "publish": "2010-08-16T20:14:11Z", 
    "summary": "We present MPWide, a platform independent communication library for\nperforming message passing between computers. Our library allows coupling of\nseveral local MPI applications through a long distance network and is\nspecifically optimized for such communications. The implementation is\ndeliberately kept light-weight, platform independent and the library can be\ninstalled and used without administrative privileges. The only requirements are\na C++ compiler and at least one open port to a wide area network on each site.\nIn this paper we present the library, describe the user interface, present\nperformance tests and apply MPWide in a large scale cosmological N-body\nsimulation on a network of two computers, one in Amsterdam and the other in\nTokyo."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1008.4551v1", 
    "other_authors": "Guanfeng Liang, Nitin Vaidya", 
    "title": "Deterministic Consensus Algorithm with Linear Per-Bit Complexity", 
    "arxiv-id": "1008.4551v1", 
    "author": "Nitin Vaidya", 
    "publish": "2010-08-26T17:29:31Z", 
    "summary": "In this report, building on the deterministic multi-valued one-to-many\nByzantine agreement (broadcast) algorithm in our recent technical report [2],\nwe introduce a deterministic multi-valued all-to-all Byzantine agreement\nalgorithm (consensus), with linear complexity per bit agreed upon. The\ndiscussion in this note is not self-contained, and relies heavily on the\nmaterial in [2] - please refer to [2] for the necessary background."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1012.1131v1", 
    "other_authors": "Hien Thi Thu Truong, Claudia-Lavinia Ignat", 
    "title": "A Log Auditing Approach for Trust Management in Peer-to-Peer   Collaboration", 
    "arxiv-id": "1012.1131v1", 
    "author": "Claudia-Lavinia Ignat", 
    "publish": "2010-12-06T11:14:22Z", 
    "summary": "Nowadays we are faced with an increasing popularity of social software\nincluding wikis, blogs, micro-blogs and online social networks such as Facebook\nand MySpace. Unfortunately, the mostly used social services are centralized and\npersonal information is stored at a single vendor. This results in potential\nprivacy problems as users do not have much control over how their private data\nis disseminated. To overcome this limitation, some recent approaches envisioned\nreplacing the single authority centralization of services by a peer-to-peer\ntrust-based approach where users can decide with whom they want to share their\nprivate data. In this peer-to-peer collaboration it is very difficult to ensure\nthat after data is shared with other peers, these peers will not misbehave and\nviolate data privacy. In this paper we propose a mechanism that addresses the\nissue of data privacy violation due to data disclosure to malicious peers. In\nour approach trust values between users are adjusted according to their\nprevious activities on the shared data. Users share their private data by\nspecifying some obligations the receivers must follow. We log modifications\ndone by users on the shared data as well as the obligations that must be\nfollowed when data is shared. By a log-auditing mechanism we detect users that\nmisbehaved and we adjust their associated trust values by using any existing\ndecentralized trust model."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1012.2270v1", 
    "other_authors": "Tom\u00e1\u0161 Oberhuber, Atsushi Suzuki, Jan Vacata", 
    "title": "New Row-grouped CSR format for storing the sparse matrices on GPU with   implementation in CUDA", 
    "arxiv-id": "1012.2270v1", 
    "author": "Jan Vacata", 
    "publish": "2010-12-10T14:04:33Z", 
    "summary": "In this article we present a new format for storing sparse matrices. The\nformat is designed to perform well mainly on the GPU devices. We present its\nimplementation in CUDA. The performance has been tested on 1,600 different\ntypes of matrices and we compare our format with the Hybrid format. We give\ndetailed comparison of both formats and show their strong and weak parts."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1012.2273v1", 
    "other_authors": "D. T. Hasta, A. B. Mutiara", 
    "title": "Performance Evaluation of Parallel Message Passing and Thread   Programming Model on Multicore Architectures", 
    "arxiv-id": "1012.2273v1", 
    "author": "A. B. Mutiara", 
    "publish": "2010-12-10T14:17:19Z", 
    "summary": "The current trend of multicore architectures on shared memory systems\nunderscores the need of parallelism. While there are some programming model to\nexpress parallelism, thread programming model has become a standard to support\nthese system such as OpenMP, and POSIX threads. MPI (Message Passing Interface)\nwhich remains the dominant model used in high-performance computing today faces\nthis challenge.\n  Previous version of MPI which is MPI-1 has no shared memory concept, and\nCurrent MPI version 2 which is MPI-2 has a limited support for shared memory\nsystems. In this research, MPI-2 version of MPI will be compared with OpenMP to\nsee how well does MPI perform on multicore / SMP (Symmetric Multiprocessor)\nmachines.\n  Comparison between OpenMP for thread programming model and MPI for message\npassing programming model will be conducted on multicore shared memory machine\narchitectures to see who has a better performance in terms of speed and\nthroughput. Application used to assess the scalability of the evaluated\nparallel programming solutions is matrix multiplication with customizable\nmatrix dimension.\n  Many research done on a large scale parallel computing which using high scale\nbenchmark such as NSA Parallel Benchmark (NPB) for their testing standarization\n[1]. This research will be conducted on a small scale parallel computing that\nemphasize more on the performance evaluation between MPI and OpenMPI parallel\nprogramming model using self created benchmark."
},{
    "category": "cs.DC", 
    "doi": "10.7763/IJCTE.2010.V2.273", 
    "link": "http://arxiv.org/pdf/1012.2499v1", 
    "other_authors": "Z. Akbar, I. Firmansyah, B. Hermanto, L. T. Handoko", 
    "title": "openPC : a toolkit for public cluster with full ownership", 
    "arxiv-id": "1012.2499v1", 
    "author": "L. T. Handoko", 
    "publish": "2010-12-12T00:15:44Z", 
    "summary": "The openPC is a set of open source tools that realizes a parallel machine and\ndistributed computing environment divisible into several independent blocks of\nnodes, and each of them is remotely but fully in any means accessible for users\nwith a full ownership policy. The openPC components address fundamental issues\nrelating to security, resource access, resource allocation, compatibilities\nwith heterogeneous middlewares, user-friendly and integrated web-based\ninterfaces, hardware control and monitoring systems. These components have been\ndeployed successfully to the LIPI Public Cluster which is open for public use.\nIn this paper, the unique characteristics of openPC due to its rare\nrequirements are introduced, its components and a brief performance analysis\nare discussed."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.2860v4", 
    "other_authors": "Yi Ji, Serguei A. Mokhov, Joey Paquet", 
    "title": "Towards Refactoring the DMF to Support Jini and JMS DMS in GIPSY", 
    "arxiv-id": "1012.2860v4", 
    "author": "Joey Paquet", 
    "publish": "2010-12-13T20:58:58Z", 
    "summary": "In this paper we report on our re-engineering effort to refactor and unify\ntwo somewhat disjoint Java distributed middleware technologies -- Jini and JMS\n-- used in the implementation of the Demand Migration System (DMS). In doing\nso, we refactor their parent Demand Migration Framework (DMF), within the\nGeneral Intensional Programming System (GIPSY). The complex Java-based GIPSY\nproject is used to investigate on the intensional and hybrid programming\nparadigms."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.3347v2", 
    "other_authors": "Mikael Fernandus Simalango, Sangyoon Oh", 
    "title": "An Architectural Design for Brokered Collaborative Content Delivery   System", 
    "arxiv-id": "1012.3347v2", 
    "author": "Sangyoon Oh", 
    "publish": "2010-12-15T14:31:05Z", 
    "summary": "Advances in web technologies have driven massive content uploads and requests\nthat can be identified by the increased usage of multimedia web and social web\nservices. This situation enforces the content providers to scale their\ninfrastructure in order to cope with the extra provisioning of network traffic,\nstorage and other resources. Since the complexity and cost factors in scaling\nthe infrastructure exist, we propose a novel solution for providing and\ndelivering contents to clients by introducing a brokered collaborative content\ndelivery system. The architectural design of this system leverages content\nredundancy and content distribution mechanisms in other content providers to\ndeliver contents to the clients. With the recent emergence of cloud computing,\nwe show that this system can also be adopted to run on the cloud. In this\npaper, we focus on a brokering scheme to mediate user requests to the most\nappropriate content provider based on a ranking system. The architecture\nprovides a novel Global Rank Value (GRV) concept in estimating content provider\ncapability and transforming the QoS requirement of a content request. A\nfairness model that will bring this design to be attractive to the current\ncontent delivery regime is also introduced. Through simulation, we show that\nusing fair provider selection, contents can be provisioned by a better pool of\nqualified providers thus leveraging the collaboration and preventing potential\nQoS violation that may occur when the size of pool is smaller."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.5030v1", 
    "other_authors": "Martin Wimmer, Jesper Larsson Tr\u00e4ff", 
    "title": "Work-stealing for mixed-mode parallelism by deterministic team-building", 
    "arxiv-id": "1012.5030v1", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2010-12-22T16:43:12Z", 
    "summary": "We show how to extend classical work-stealing to deal also with data parallel\ntasks that can require any number of threads r >= 1 for their execution. We\nexplain in detail the so introduced idea of work-stealing with deterministic\nteam-building which in a natural way generalizes classical work-stealing. A\nprototype C++ implementation of the generalized work-stealing algorithm has\nbeen given and is briefly described. Building on this, a serious, well-known\ncontender for a best parallel Quicksort algorithm has been implemented, which\nnaturally relies on both task and data parallelism. For instance, sorting\n2^27-1 randomly generated integers we could improve the speed-up from 5.1 to\n8.7 on a 32-core Intel Nehalem EX system, being consistently better than the\ntuned, task-parallel Cilk++ system."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.5834v5", 
    "other_authors": "Soji Omiwade, Rong Zheng", 
    "title": "Maximum Lifetime for Data Regeneration in Wireless Sensor Networks", 
    "arxiv-id": "1012.5834v5", 
    "author": "Rong Zheng", 
    "publish": "2010-12-28T20:36:09Z", 
    "summary": "Robust distributed storage systems dedicated to wireless sensor networks\nutilize several nodes to redundantly store sensed data so that when some\nstorage nodes fail, the sensed data can still be reconstructed. For the same\nlevel of redundancy, erasure coding based approaches are known to require less\ndata storage space than replication methods.\n  To maintain the same level of redundancy when one storage node fails, erasure\ncoded data can be restored onto some other storage node by having this node\ndownload respective pieces from other live storage nodes. Previous works showed\nthat the benefits in using erasure coding for robust storage over replication\nare made unappealing by the complication in regenerating lost data. More recent\nwork has, however, shown that the bandwidth for erasure coded data can be\nfurther reduced by proposing Regenerating Coding, making erasure codes again\ndesirable for robust data storage.\n  But none of these works on regenerating coding consider how these codes will\nperform for data regeneration in wireless sensor networks. We therefore propose\nan analytical model to quantify the network lifetime gains of regenerating\ncoding over classical schemes. We also propose a distributed algorithm, TROY,\nthat determines which nodes and routes to use for data regeneration. Our\nanalytical studies show that for certain topologies, TROY achieves maximum\nnetwork lifetime. Our evaluation studies in real sensor network traces show\nthat TROY achieves near optimal lifetime and performs better than baseline\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1186/1687-1499-2012-231", 
    "link": "http://arxiv.org/pdf/1106.0736v1", 
    "other_authors": "Lei Yang, Yalin E. Sagduyu, Junshan Zhang, Jason H. Li", 
    "title": "Distributed Stochastic Power Control in Ad-hoc Networks: A Nonconvex   Case", 
    "arxiv-id": "1106.0736v1", 
    "author": "Jason H. Li", 
    "publish": "2011-06-03T19:50:22Z", 
    "summary": "Utility-based power allocation in wireless ad-hoc networks is inherently\nnonconvex because of the global coupling induced by the co-channel\ninterference. To tackle this challenge, we first show that the globally optimal\npoint lies on the boundary of the feasible region, which is utilized as a basis\nto transform the utility maximization problem into an equivalent max-min\nproblem with more structure. By using extended duality theory, penalty\nmultipliers are introduced for penalizing the constraint violations, and the\nminimum weighted utility maximization problem is then decomposed into\nsubproblems for individual users to devise a distributed stochastic power\ncontrol algorithm, where each user stochastically adjusts its target utility to\nimprove the total utility by simulated annealing. The proposed distributed\npower control algorithm can guarantee global optimality at the cost of slow\nconvergence due to simulated annealing involved in the global optimization. The\ngeometric cooling scheme and suitable penalty parameters are used to improve\nthe convergence rate. Next, by integrating the stochastic power control\napproach with the back-pressure algorithm, we develop a joint scheduling and\npower allocation policy to stabilize the queueing systems. Finally, we\ngeneralize the above distributed power control algorithms to multicast\ncommunications, and show their global optimality for multicast traffic."
},{
    "category": "cs.DC", 
    "doi": "10.1186/1687-1499-2012-231", 
    "link": "http://arxiv.org/pdf/1106.0940v1", 
    "other_authors": "Herodotos Herodotou", 
    "title": "Hadoop Performance Models", 
    "arxiv-id": "1106.0940v1", 
    "author": "Herodotos Herodotou", 
    "publish": "2011-06-06T00:02:32Z", 
    "summary": "Hadoop MapReduce is now a popular choice for performing large-scale data\nanalytics. This technical report describes a detailed set of mathematical\nperformance models for describing the execution of a MapReduce job on Hadoop.\nThe models describe dataflow and cost information at the fine granularity of\nphases within the map and reduce tasks of a job execution. The models can be\nused to estimate the performance of MapReduce jobs as well as to find the\noptimal configuration settings to use when running the jobs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1286v1", 
    "other_authors": "S. Rajeswari, Y. Venkataramani", 
    "title": "Traffic Performance Analysis of Manet Routing Protocol", 
    "arxiv-id": "1106.1286v1", 
    "author": "Y. Venkataramani", 
    "publish": "2011-06-07T09:09:17Z", 
    "summary": "The primary objective of this research work is to study and investigate the\nperformance measures of Gossip Routing protocol and Energy Efficient and\nReliable Adaptive Gossip routing protocols. We use TCP and CBR based traffic\nmodels to analyze the performance of above mentioned protocols based on the\nparameters of Packet Delivery Ratio, Average End-to-End Delay and Throughput.\nWe will investigate the effect of change in the simulation time and Number of\nnodes for the MANET routing protocols. For Simulation, we have used ns-2\nsimulator."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1845v3", 
    "other_authors": "Guanfeng Liang, Nitin Vaidya", 
    "title": "Byzantine Broadcast in Point-to-Point Networks using Local Linear Coding", 
    "arxiv-id": "1106.1845v3", 
    "author": "Nitin Vaidya", 
    "publish": "2011-06-09T16:07:57Z", 
    "summary": "The goal of Byzantine Broadcast (BB) is to allow a set of fault-free nodes to\nagree on information that a source node wants to broadcast to them, in the\npresence of Byzantine faulty nodes. We consider design of efficient algorithms\nfor BB in {\\em synchronous} point-to-point networks, where the rate of\ntransmission over each communication link is limited by its \"link capacity\".\nThe throughput of a particular BB algorithm is defined as the average number of\nbits that can be reliably broadcast to all fault-free nodes per unit time using\nthe algorithm without violating the link capacity constraints. The {\\em\ncapacity} of BB in a given network is then defined as the supremum of all\nachievable BB throughputs in the given network, over all possible BB\nalgorithms.\n  We develop NAB -- a Network-Aware Byzantine broadcast algorithm -- for\narbitrary point-to-point networks consisting of $n$ nodes, wherein the number\nof faulty nodes is at most $f$, $f<n/3$, and the network connectivity is at\nleast $2f+1$. We also prove an upper bound on the capacity of Byzantine\nbroadcast, and conclude that NAB can achieve throughput at least 1/3 of the\ncapacity. When the network satisfies an additional condition, NAB can achieve\nthroughput at least 1/2 of the capacity.\n  To the best of our knowledge, NAB is the first algorithm that can achieve a\nconstant fraction of capacity of Byzantine Broadcast (BB) in arbitrary\npoint-to-point networks."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1846v1", 
    "other_authors": "Guanfeng Liang, Nitin Vaidya", 
    "title": "New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures", 
    "arxiv-id": "1106.1846v1", 
    "author": "Nitin Vaidya", 
    "publish": "2011-06-09T16:13:54Z", 
    "summary": "In this report, we investigate the multi-valued Byzantine consensus problem.\nWe introduce two algorithms: the first one achieves traditional validity\nrequirement for consensus, and the second one achieves a stronger \"q-validity\"\nrequirement. Both algorithms are more efficient than the ones introduces in our\nrecent PODC 2011 paper titled \"Error-Free Multi-Valued Consensus with Byzantine\nFailures\"."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.2065v1", 
    "other_authors": "Yehuda Afek, Yakov Babichenko, Uriel Feige, Eli Gafni, Nati Linial, Benny Sudakov", 
    "title": "Oblivious Collaboration", 
    "arxiv-id": "1106.2065v1", 
    "author": "Benny Sudakov", 
    "publish": "2011-06-10T14:11:33Z", 
    "summary": "Communication is a crucial ingredient in every kind of collaborative work.\nBut what is the least possible amount of communication required for a given\ntask? We formalize this question by introducing a new framework for distributed\ncomputation, called {\\em oblivious protocols}.\n  We investigate the power of this model by considering two concrete examples,\nthe {\\em musical chairs} task $MC(n,m)$ and the well-known {\\em Renaming}\nproblem. The $MC(n,m)$ game is played by $n$ players (processors) with $m$\nchairs. Players can {\\em occupy} chairs, and the game terminates as soon as\neach player occupies a unique chair. Thus we say that player $P$ is {\\em in\nconflict} if some other player $Q$ is occupying the same chair, i.e.,\ntermination means there are no conflicts. By known results from distributed\ncomputing, if $m \\le 2n-2$, no strategy of the players can guarantee\ntermination. However, there is a protocol with $m = 2n-1$ chairs that always\nterminates. Here we consider an oblivious protocol where in every time step the\nonly communication is this: an adversarial {\\em scheduler} chooses an arbitrary\nnonempty set of players, and for each of them provides only one bit of\ninformation, specifying whether the player is currently in conflict or not. A\nplayer notified not to be in conflict halts and never changes its chair,\nwhereas a player notified to be in conflict changes its chair according to its\ndeterministic program. Remarkably, even with this minimal communication\ntermination can be guaranteed with only $m=2n-1$ chairs. Likewise, we obtain an\noblivious protocol for the Renaming problem whose name-space is small as that\nof the optimal nonoblivious distributed protocol.\n  Other aspects suggest themselves, such as the efficiency (program length) of\nour protocols. We make substantial progress here as well, though many\ninteresting questions remain open."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.2126v1", 
    "other_authors": "Yehuda Afek, Noga Alon, Ziv Bar-Joseph", 
    "title": "MIS on the fly", 
    "arxiv-id": "1106.2126v1", 
    "author": "Ziv Bar-Joseph", 
    "publish": "2011-06-10T17:28:35Z", 
    "summary": "Humans are very good at optimizing solutions for specific problems.\nBiological processes, on the other hand, have evolved to handle multiple\nconstrained distributed environments and so they are robust and adaptable.\nInspired by observations made in a biological system we have recently presented\na simple new randomized distributed MIS algorithm \\cite{ZScience}. Here we\nextend these results by removing a number of strong assumptions that we made,\nmaking the algorithms more practical. Specifically we present an $O(\\log^2 n)$\nrounds synchronous randomized MIS algorithm which uses only 1 bit unary\nmessages (a beeping signal with collision detection), allows for asynchronous\nwake up, does not assume any knowledge of the network topology, and assumes\nonly a loose bound on the network size. We also present an extension with no\ncollision detection in which the round complexity increases to $(\\log^3 n)$.\nFinally, we show that our algorithm is optimal under some restriction, by\npresenting a tight lower bound of $\\Omega(\\log^2 n)$ on the number of rounds\nrequired to construct a MIS for a restricted model."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.3579v2", 
    "other_authors": "Emmanuel Godard, Joseph Peters", 
    "title": "Consensus vs Broadcast in Communication Networks with Arbitrary Mobile   Omission Faults", 
    "arxiv-id": "1106.3579v2", 
    "author": "Joseph Peters", 
    "publish": "2011-06-17T21:16:51Z", 
    "summary": "We compare the solvability of the Consensus and Broadcast problems in\nsynchronous communication networks in which the delivery of messages is not\nreliable. The failure model is the mobile omission faults model. During each\nround, some messages can be lost and the set of possible simultaneous losses is\nthe same for each round. We investigate these problems for the first time for\narbitrary sets of possible failures. Previously, these sets were defined by\nbounding the numbers of failures.\n  In this setting, we present a new necessary condition for the solvability of\nConsensus that unifies previous impossibility results in this area. This\ncondition is expressed using Broadcastability properties. As a very important\napplication, we show that when the sets of omissions that can occur are defined\nby bounding the numbers of failures, counted in any way (locally, globally,\netc.), then the Consensus problem is actually equivalent to the Broadcast\nproblem."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.4213v1", 
    "other_authors": "Erlin Yao, Mingyu Chen, Rui Wang, Wenli Zhang, Guangming Tan", 
    "title": "A New and Efficient Algorithm-Based Fault Tolerance Scheme for A Million   Way Parallelism", 
    "arxiv-id": "1106.4213v1", 
    "author": "Guangming Tan", 
    "publish": "2011-06-21T14:24:43Z", 
    "summary": "Fault tolerance overhead of high performance computing (HPC) applications is\nbecoming critical to the efficient utilization of HPC systems at large scale.\nHPC applications typically tolerate fail-stop failures by checkpointing.\nAnother promising method is in the algorithm level, called algorithmic\nrecovery. These two methods can achieve high efficiency when the system scale\nis not very large, but will both lose their effectiveness when systems approach\nthe scale of Exaflops, where the number of processors including in system is\nexpected to achieve one million. This paper develops a new and efficient\nalgorithm-based fault tolerance scheme for HPC applications. When failure\noccurs during the execution, we do not stop to wait for the recovery of\ncorrupted data, but replace them with the corresponding redundant data and\ncontinue the execution. A background accelerated recovery method is also\nproposed to rebuild redundancy to tolerate multiple times of failures during\nthe execution. To demonstrate the feasibility of our new scheme, we have\nincorporated it to the High Performance Linpack. Theoretical analysis\ndemonstrates that our new fault tolerance scheme can still be effective even\nwhen the system scale achieves the Exaflops. Experiment using SiCortex SC5832\nverifies the feasibility of the scheme, and indicates that the advantage of our\nscheme can be observable even in a small scale."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.4985v1", 
    "other_authors": "Henri Casanova, Mark Stillwell, Fr\u00e9d\u00e9ric Vivien", 
    "title": "Dynamic Fractional Resource Scheduling vs. Batch Scheduling", 
    "arxiv-id": "1106.4985v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2011-06-24T14:54:51Z", 
    "summary": "We propose a novel job scheduling approach for homogeneous cluster computing\nplatforms. Its key feature is the use of virtual machine technology to share\nfractional node resources in a precise and controlled manner. Other VM-based\nscheduling approaches have focused primarily on technical issues or on\nextensions to existing batch scheduling systems, while we take a more\naggressive approach and seek to find heuristics that maximize an objective\nmetric correlated with job performance. We derive absolute performance bounds\nand develop algorithms for the online, non-clairvoyant version of our\nscheduling problem. We further evaluate these algorithms in simulation against\nboth synthetic and real-world HPC workloads and compare our algorithms to\nstandard batch scheduling approaches. We find that our approach improves over\nbatch scheduling by orders of magnitude in terms of job stretch, while leading\nto comparable or better resource utilization. Our results demonstrate that\nvirtualization technology coupled with lightweight online scheduling strategies\ncan afford dramatic improvements in performance for executing HPC workloads."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5158v1", 
    "other_authors": "Ciprian Dobre, Corina Stratan", 
    "title": "MONARC Simulation Framework", 
    "arxiv-id": "1106.5158v1", 
    "author": "Corina Stratan", 
    "publish": "2011-06-25T19:40:21Z", 
    "summary": "This paper discusses the latest generation of the MONARC (MOdels of Networked\nAnalysis at Regional Centers) simulation framework, as a design and modelling\ntool for large scale distributed systems applied to HEP experiments. A\nprocess-oriented approach for discrete event simulation is well-suited for\ndescribing concurrent running programs, as well as the stochastic arrival\npatterns that characterize how such systems are used. The simulation engine is\nbased on Threaded Objects (or Active Objects), which offer great flexibility in\nsimulating the complex behavior of distributed data processing programs. The\nengine provides an appropriate scheduling mechanism for the Active objects with\nsupport for interrupts. This approach offers a natural way of describing\ncomplex running programs that are data dependent and which concurrently compete\nfor shared resources as well as large numbers of concurrent data transfers on\nshared resources. The framework provides a complete set of basic components\n(processing nodes, data servers, network components) together with dynamically\nloadable decision units (scheduling or data replication modules) for easily\nbuilding complex Computing Model simulations. Examples of simulating complex\ndata processing systems are presented, and the way the framework is used to\ncompare different decision making algorithms or to optimize the overall Grid\narchitecture and/or the policies that govern the Grid's use."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5168v1", 
    "other_authors": "Iosif C. Legrand, Ciprian Dobre, Ramiro Voicu, Corina Stratan, Catalin Cirstoiu, Lucian Musat", 
    "title": "LISA (Localhost Information Service Agent)", 
    "arxiv-id": "1106.5168v1", 
    "author": "Lucian Musat", 
    "publish": "2011-06-25T20:49:17Z", 
    "summary": "Grid computing has gained an increasing importance in the last years,\nespecially in the academic environments, offering the possibility to rapidly\nsolve complex scientific problems. The monitoring of the Grid jobs has a vital\nimportance for analyzing the system's performance, for providing the users an\nappropriate feed-back, and for obtaining historical data which may be used for\nperformance prediction. Several monitoring systems have been developed, with\ndifferent strategies to collect and store the information. We shall present\nhere a solution based on MonALISA, a distributed service for monitoring,\ncontrol and global optimization of complex systems, and LISA, a component\napplication of MonALISA which can help in optimizing other applications by\nmeans of monitoring services. The advantages of this system are, among others,\nflexibility, dynamic configuration, high communication performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5170v2", 
    "other_authors": "Allison Lewko", 
    "title": "The Contest Between Simplicity and Efficiency in Asynchronous Byzantine   Agreement", 
    "arxiv-id": "1106.5170v2", 
    "author": "Allison Lewko", 
    "publish": "2011-06-25T20:53:20Z", 
    "summary": "In the wake of the decisive impossibility result of Fischer, Lynch, and\nPaterson for deterministic consensus protocols in the aynchronous model with\njust one failure, Ben-Or and Bracha demonstrated that the problem could be\nsolved with randomness, even for Byzantine failures. Both protocols are natural\nand intuitive to verify, and Bracha's achieves optimal resilience. However, the\nexpected running time of these protocols is exponential in general. Recently,\nKapron, Kempe, King, Saia, and Sanwalani presented the first efficient\nByzantine agreement algorithm in the asynchronous, full information model,\nrunning in polylogarithmic time. Their algorithm is Monte Carlo and drastically\ndeparts from the simple structure of Ben-Or and Bracha's Las Vegas algorithms.\n  In this paper, we begin an investigation of the question: to what extent is\nthis departure necessary? Might there be a much simpler and intuitive Las Vegas\nprotocol that runs in expected polynomial time? We will show that the\nexponential running time of Ben-Or and Bracha's algorithms is no mere accident\nof their specific details, but rather an unavoidable consequence of their\ngeneral symmetry and round structure. We define a natural class of \"fully\nsymmetric round protocols\" for solving Byzantine agreement in an asynchronous\nsetting and show that any such protocol can be forced to run in expected\nexponential time by an adversary in the full information model. We assume the\nadversary controls $t$ Byzantine processors for $t = cn$, where $c$ is an\narbitrary positive constant $< 1/3$. We view our result as a step toward\nidentifying the level of complexity required for a polynomial-time algorithm in\nthis setting, and also as a guide in the search for new efficient algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5171v1", 
    "other_authors": "Ciprian Dobre, Ramiro Voicu, Adrian Muraru, Iosif C. Legrand", 
    "title": "A Distributed Agent Based System to Control and Coordinate Large Scale   Data Transfers", 
    "arxiv-id": "1106.5171v1", 
    "author": "Iosif C. Legrand", 
    "publish": "2011-06-25T20:54:12Z", 
    "summary": "We present a distributed agent based system used to monitor, configure and\ncontrol complex, large scale data transfers in the Wide Area Network. The\nLocalhost Information Service Agent (LISA) is a lightweight dynamic service\nthat provides complete system and applications monitoring, is capable to\ndynamically configure system parameters and can help in optimizing distributed\napplications.\n  As part of the MonALISA (Monitoring Agents in A Large Integrated Services\nArchitecture) system, LISA is an end host agent capable to collect any type of\nmonitoring information, to distribute them, and to take actions based on local\nor global decision units. The system has been used for the Bandwidth Challenge\nat Supercomputing 2006 to coordinate global large scale data transfers using\nFast Data Transfer (FDT) application between hundreds of servers distributed on\nmajor Grid sites involved in processing High Energy Physics data for the future\nLarge Hadron Collider experiments."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5299v1", 
    "other_authors": "Ciprian Dobre, Florin Pop, Valentin Cristea", 
    "title": "DistHash: A robust P2P DHT-based system for replicated objects", 
    "arxiv-id": "1106.5299v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T05:49:21Z", 
    "summary": "Over the Internet today, computing and communications environments are\nsignificantly more complex and chaotic than classical distributed systems,\nlacking any centralized organization or hierarchical control. There has been\nmuch interest in emerging Peer-to-Peer (P2P) network overlays because they\nprovide a good substrate for creating large-scale data sharing, content\ndistribution and application-level multicast applications. In this paper we\npresent DistHash, a P2P overlay network designed to share large sets of\nreplicated distributed objects in the context of large-scale highly dynamic\ninfrastructures. We present original solutions to achieve optimal message\nrouting in hop-count and throughput, provide an adequate consistency approach\namong replicas, as well as provide a fault-tolerant substrate."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5302v1", 
    "other_authors": "Dacian Tudor, Florin Pop, Valentin Cristea, Vladimir Cretu", 
    "title": "Towards an IO intensive Grid application instrumentation in MedioGRID", 
    "arxiv-id": "1106.5302v1", 
    "author": "Vladimir Cretu", 
    "publish": "2011-06-27T06:03:28Z", 
    "summary": "Obtaining high performance in IO intensive applications requires systems that\nsupport reliable fast transfer, data replication, and caching. In this paper we\npresent an architecture designed for supporting IO intensive applications in\nMedioGRID, a system for real-time processing of satellite images, operating in\na Grid environment. The solution ensures that applications which are processing\ngeographical data have uniform access to data and is based on continuous\nmonitoring of the data transfers using MonALISA and its extensions. The\nMedioGRID architecture is also built on Globus, Condor and PBS and based on\nthis middleware we aim to extract information about the running systems. The\nresults obtained in testing MedioGRID system for large data transfers show that\nmonitoring system provides a very good view of system evolution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5303v1", 
    "other_authors": "Florin Pop, Valentin Cristea", 
    "title": "Intelligent strategies for DAG scheduling optimization in Grid   environments", 
    "arxiv-id": "1106.5303v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:03:42Z", 
    "summary": "The paper presents a solution to the dynamic DAG scheduling problem in Grid\nenvironments. It presents a distributed, scalable, efficient and fault-tolerant\nalgorithm for optimizing tasks assignment. The scheduler algorithm for tasks\nwith dependencies uses a heuristic model to optimize the total cost of tasks\nexecution. Also, a method based on genetic algorithms is proposed to optimize\nthe procedure of resources assignment. The experiments used the MonALISA\nmonitoring environment and its extensions. The results demonstrate very good\nbehavior in comparison with other scheduling approaches for this kind of DAG\nscheduling algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5309v1", 
    "other_authors": "Diana Moise, Eliza Moise, Florin Pop, Valentin Cristea", 
    "title": "Resource CoAllocation for Scheduling Tasks with Dependencies, in Grid", 
    "arxiv-id": "1106.5309v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:24:40Z", 
    "summary": "Scheduling applications on wide-area distributed systems is useful for\nobtaining quick and reliable results in an efficient manner. Optimized\nscheduling algorithms are fundamentally important in order to achieve optimized\nresources utilization. The existing and potential applications include many\nfields of activity like satellite image processing and medicine. The paper\nproposes a scheduling algorithm for tasks with dependencies in Grid\nenvironments. CoAllocation represents a strategy that provides a schedule for\ntask with dependencies, having as main purpose the efficiency of the schedule,\nin terms of load balancing and minimum time for the execution of the tasks."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5310v1", 
    "other_authors": "Eliza Moise, Diana Moise, Florin Pop, Valentin Cristea", 
    "title": "Advance Reservation of Resources for Task Execution in Grid Environments", 
    "arxiv-id": "1106.5310v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:24:50Z", 
    "summary": "The paper proposes a solution for the Grid scheduling problem, addressing in\nparticular the requirement of high performance an efficient algorithm must\nfulfill. Advance Reservation engages a distributed, dynamic, fault-tolerant and\nefficient strategy which reserves resources for future task execution. The\npaper presents the main features of the strategy, the functioning mechanism the\nstrategy is based on and the methods used for evaluating the algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5451v1", 
    "other_authors": "Ilango Sriram, Dave Cliff", 
    "title": "Hybrid complex network topologies are preferred for   component-subscription in large-scale data-centres", 
    "arxiv-id": "1106.5451v1", 
    "author": "Dave Cliff", 
    "publish": "2011-06-27T17:16:41Z", 
    "summary": "We report on experiments exploring the interplay between the topology of the\ncomplex network of dependent components in a large-scale data-centre, and the\nrobustness and scaling properties of that data-centre. In a previous paper [1]\nwe used the SPECI large-scale data-centre simulator [2] to compare the\nrobustness and scaling characteristics of data-centres whose dependent\ncomponents are connected via Strogatz-Watts small-world (SW) networks [3],\nversus those organized as Barabasi-Albert scale-free (SF) networks [4], and\nfound significant differences. In this paper, we present results from using the\nKlemm-Eguiliz (KE) construction method [5] to generate complex network\ntopologies for data-centre component dependencies. The KE model has a control\nparameter {\\mu}\\in[0,1]\\inR that determines whether the networks generated are\nSW (0<{\\mu}<<1) or SF ({\\mu}=1) or a \"hybrid\" network topology part-way between\nSW and SF (0<{\\mu}<1). We find that the best scores for system-level\nperformance metrics of the simulated data-centres are given by \"hybrid\" values\nof {\\mu} significantly different from pure-SW or pure-SF."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5457v1", 
    "other_authors": "John Cartlidge, Ilango Sriram", 
    "title": "Modelling Resilience in Cloud-Scale Data Centres", 
    "arxiv-id": "1106.5457v1", 
    "author": "Ilango Sriram", 
    "publish": "2011-06-27T17:31:49Z", 
    "summary": "The trend for cloud computing has initiated a race towards data centres (DC)\nof an ever-increasing size. The largest DCs now contain many hundreds of\nthousands of virtual machine (VM) services. Given the finite lifespan of\nhardware, such large DCs are subject to frequent hardware failure events that\ncan lead to disruption of service. To counter this, multiple redundant copies\nof task threads may be distributed around a DC to ensure that individual\nhardware failures do not cause entire jobs to fail. Here, we present results\ndemonstrating the resilience of different job scheduling algorithms in a\nsimulated DC with hardware failure. We use a simple model of jobs distributed\nacross a hardware network to demonstrate the relationship between resilience\nand additional communication costs of different scheduling methods."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5465v1", 
    "other_authors": "Ilango Leonardo Sriram, Dave Cliff", 
    "title": "SPECI-2: An open-source framework for predictive simulation of   cloud-scale data-centres", 
    "arxiv-id": "1106.5465v1", 
    "author": "Dave Cliff", 
    "publish": "2011-06-27T18:01:03Z", 
    "summary": "We introduce Version 2 of SPECI, a system for predictive simulation modeling\nof large-scale data-centres, i.e. warehouse-sized facilities containing\nhundreds of thousands of servers, as used to provide cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5570v1", 
    "other_authors": "Ramiro Voicu, Iosif Legrand, Harvey Newman, Nicolae Tapus, Ciprian Dobre", 
    "title": "A distributed service for on demand end to end optical circuits", 
    "arxiv-id": "1106.5570v1", 
    "author": "Ciprian Dobre", 
    "publish": "2011-06-28T06:01:49Z", 
    "summary": "In this paper we present a system for monitoring and controlling dynamic\nnetwork circuits inside the USLHCNet network. This distributed service system\nprovides in near real-time complete topological information for all the\ncircuits, resource allocation and usage, accounting, detects automatically\nfailures in the links and network equipment, generate alarms and has the\nfunctionality to take automatic actions. The system is developed based on the\nMonALISA framework, which provides a robust monitoring and controlling service\noriented architecture, with no single points of failure."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5576v1", 
    "other_authors": "Valentin Cristea, Ciprian Dobre, Florin Pop, Corina Stratan, Alexandru Costan, Catalin Leordeanu", 
    "title": "Models and Techniques for Ensuring Reliability, Safety, Availability and   Security of Large Scale Distributed Systems", 
    "arxiv-id": "1106.5576v1", 
    "author": "Catalin Leordeanu", 
    "publish": "2011-06-28T06:53:59Z", 
    "summary": "17th International Conference on Control Systems and Computer Science (CSCS\n17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 401-406, ISSN: 2066-4451."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5846v1", 
    "other_authors": "Alexandru Costan, Florin Pop, Corina Stratan, Ciprian Dobre, Catalin Leordeanu, Valentin Cristea", 
    "title": "An Architectural Model for a Grid based Workflow Management Platform in   Scientific Applications", 
    "arxiv-id": "1106.5846v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-29T06:03:11Z", 
    "summary": "With recent increasing computational and data requirements of scientific\napplications, the use of large clustered systems as well as distributed\nresources is inevitable. Although executing large applications in these\nenvironments brings increased performance, the automation of the process\nbecomes more and more challenging. While the use of complex workflow management\nsystems has been a viable solution for this automation process in business\noriented environments, the open source engines available for scientific\napplications lack some functionalities or are too difficult to use for\nnon-specialists. In this work we propose an architectural model for a grid\nbased workflow management platform providing features like an intuitive way to\ndescribe workflows, efficient data handling mechanisms and flexible fault\ntolerance support. Our integrated solution introduces a workflow engine\ncomponent based on ActiveBPEL extended with additional functionalities and a\nscheduling component providing efficient mapping between tasks and available\nresources."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.5908v1", 
    "other_authors": "Gerald Schubert, Holger Fehske, Georg Hager, Gerhard Wellein", 
    "title": "Hybrid-parallel sparse matrix-vector multiplication with explicit   communication overlap on current multicore-based systems", 
    "arxiv-id": "1106.5908v1", 
    "author": "Gerhard Wellein", 
    "publish": "2011-06-29T11:25:50Z", 
    "summary": "We evaluate optimized parallel sparse matrix-vector operations for several\nrepresentative application areas on widespread multicore-based cluster\nconfigurations. First the single-socket baseline performance is analyzed and\nmodeled with respect to basic architectural properties of standard multicore\nchips. Beyond the single node, the performance of parallel sparse matrix-vector\noperations is often limited by communication overhead. Starting from the\nobservation that nonblocking MPI is not able to hide communication cost using\nstandard MPI implementations, we demonstrate that explicit overlap of\ncommunication and computation can be achieved by using a dedicated\ncommunication thread, which may run on a virtual core. Moreover we identify\nperformance benefits of hybrid MPI/OpenMP programming due to improved load\nbalancing even without explicit communication overlap. We compare performance\nresults for pure MPI, the widely used \"vector-like\" hybrid programming\nstrategies, and explicit overlap on a modern multicore-based cluster and a Cray\nXE6 system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.6122v1", 
    "other_authors": "Dobre Ciprian, Cristea Valentin, Iosif C. Legrand", 
    "title": "Simulation Framework for Modeling Large-Scale Distributed Systems", 
    "arxiv-id": "1106.6122v1", 
    "author": "Iosif C. Legrand", 
    "publish": "2011-06-30T06:37:34Z", 
    "summary": "Simulation has become the evaluation method of choice for many areas of\ndistributing computing research. However, most existing simulation packages\nhave several limitations on the size and complexity of the system being\nmodeled. Fine grained simulation of complex systems such as Grids requires high\ncomputational effort which can only be obtained by using an underlying\ndistributed architecture. We are proposing a new distributed simulation system\nthat has the advantage of being able to model very complex distributed systems\nwhile hiding the computational effort from the end-user."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.6304v1", 
    "other_authors": "Gal Bar-Nissan, Danny Hendler, Adi Suissa", 
    "title": "A Dynamic Elimination-Combining Stack Algorithm", 
    "arxiv-id": "1106.6304v1", 
    "author": "Adi Suissa", 
    "publish": "2011-06-30T17:12:14Z", 
    "summary": "Two key synchronization paradigms for the construction of scalable concurrent\ndata-structures are software combining and elimination. Elimination-based\nconcurrent data-structures allow operations with reverse semantics (such as\npush and pop stack operations) to \"collide\" and exchange values without having\nto access a central location. Software combining, on the other hand, is\neffective when colliding operations have identical semantics: when a pair of\nthreads performing operations with identical semantics collide, the task of\nperforming the combined set of operations is delegated to one of the threads\nand the other thread waits for its operation(s) to be performed. Applying this\nmechanism iteratively can reduce memory contention and increase throughput. The\nmost highly scalable prior concurrent stack algorithm is the\nelimination-backoff stack. The elimination-backoff stack provides high\nparallelism for symmetric workloads in which the numbers of push and pop\noperations are roughly equal, but its performance deteriorates when workloads\nare asymmetric. We present DECS, a novel Dynamic Elimination-Combining Stack\nalgorithm, that scales well for all workload types. While maintaining the\nsimplicity and low-overhead of the elimination-bakcoff stack, DECS manages to\nbenefit from collisions of both identical- and reverse-semantics operations.\nOur empirical evaluation shows that DECS scales significantly better than both\nblocking and non-blocking best prior stack algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1107.0538v1", 
    "other_authors": "Antonio Wendell De Oliveira Rodrigues, Fr\u00e9d\u00e9ric Guyomarc'H, Jean-Luc Dekeyser, Yvonnick Le Menach", 
    "title": "Automatic Multi-GPU Code Generation applied to Simulation of Electrical   Machines", 
    "arxiv-id": "1107.0538v1", 
    "author": "Yvonnick Le Menach", 
    "publish": "2011-07-04T06:13:51Z", 
    "summary": "The electrical and electronic engineering has used parallel programming to\nsolve its large scale complex problems for performance reasons. However, as\nparallel programming requires a non-trivial distribution of tasks and data,\ndevelopers find it hard to implement their applications effectively. Thus, in\norder to reduce design complexity, we propose an approach to generate code for\nhybrid architectures (e.g. CPU + GPU) using OpenCL, an open standard for\nparallel programming of heterogeneous systems. This approach is based on Model\nDriven Engineering (MDE) and the MARTE profile, standard proposed by Object\nManagement Group (OMG). The aim is to provide resources to non-specialists in\nparallel programming to implement their applications. Moreover, thanks to model\nreuse capacity, we can add/change functionalities or the target architecture.\nConsequently, this approach helps industries to achieve their time-to-market\nconstraints and confirms by experimental tests, performance improvements using\nmulti-GPU environments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S1793830914500220", 
    "link": "http://arxiv.org/pdf/1107.1866v4", 
    "other_authors": "Dohan Kim", 
    "title": "Priority-based task reassignments in hierarchical 2D mesh-connected   systems using tableaux", 
    "arxiv-id": "1107.1866v4", 
    "author": "Dohan Kim", 
    "publish": "2011-07-10T15:48:34Z", 
    "summary": "Task reassignments in 2D mesh-connected systems (2D-MSs) have been researched\nfor several decades. We propose a hierarchical 2D mesh-connected system\n(2D-HMS) in order to exploit the regular nature of a 2D-MS. In our approach\npriority-based task assignments and reassignments in a 2D-HMS are represented\nby tableaux and their algorithms. We show how task relocations for a\npriority-based task reassignment in a 2D-HMS are reduced to a jeu de taquin\nslide."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.2990v2", 
    "other_authors": "Sotirios Kentros, Aggelos Kiayias", 
    "title": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness", 
    "arxiv-id": "1107.2990v2", 
    "author": "Aggelos Kiayias", 
    "publish": "2011-07-15T04:24:38Z", 
    "summary": "We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.3129v1", 
    "other_authors": "Frederique Oggier, Anwitaman Datta", 
    "title": "Homomorphic Self-repairing Codes for Agile Maintenance of Distributed   Storage Systems", 
    "arxiv-id": "1107.3129v1", 
    "author": "Anwitaman Datta", 
    "publish": "2011-07-15T18:46:33Z", 
    "summary": "Distributed data storage systems are essential to deal with the need to store\nmassive volumes of data. In order to make such a system fault-tolerant, some\nform of redundancy becomes crucial, incurring various overheads - most\nprominently in terms of storage space and maintenance bandwidth requirements.\nErasure codes, originally designed for communication over lossy channels,\nprovide a storage efficient alternative to replication based redundancy,\nhowever entailing high communication overhead for maintenance, when some of the\nencoded fragments need to be replenished in news ones after failure of some\nstorage devices. We propose as an alternative a new family of erasure codes\ncalled self-repairing codes (SRC) taking into account the peculiarities of\ndistributed storage systems, specifically the maintenance process. SRC has the\nfollowing salient features: (a) encoded fragments can be repaired directly from\nother subsets of encoded fragments by downloading less data than the size of\nthe complete object, ensuring that (b) a fragment is repaired from a fixed\nnumber of encoded fragments, the number depending only on how many encoded\nblocks are missing and independent of which specific blocks are missing. This\npaper lays the foundations by defining the novel self-repairing codes,\nelaborating why the defined characteristics are desirable for distributed\nstorage systems. Then homomorphic self-repairing codes (HSRC) are proposed as a\nconcrete instance, whose various aspects and properties are studied and\ncompared - quantitatively or qualitatively with respect to other codes\nincluding traditional erasure codes as well as other recent codes designed\nspecifically for storage applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.3734v1", 
    "other_authors": "Marc Tchiboukdjian, Nicolas Gast, Denis Trystram", 
    "title": "Decentralized List Scheduling", 
    "arxiv-id": "1107.3734v1", 
    "author": "Denis Trystram", 
    "publish": "2011-07-19T15:13:23Z", 
    "summary": "Classical list scheduling is a very popular and efficient technique for\nscheduling jobs in parallel and distributed platforms. It is inherently\ncentralized. However, with the increasing number of processors, the cost for\nmanaging a single centralized list becomes too prohibitive. A suitable approach\nto reduce the contention is to distribute the list among the computational\nunits: each processor has only a local view of the work to execute. Thus, the\nscheduler is no longer greedy and standard performance guarantees are lost.\n  The objective of this work is to study the extra cost that must be paid when\nthe list is distributed among the computational units. We first present a\ngeneral methodology for computing the expected makespan based on the analysis\nof an adequate potential function which represents the load unbalance between\nthe local lists. We obtain an equation on the evolution of the potential by\ncomputing its expected decrease in one step of the schedule. Our main theorem\nshows how to solve such equations to bound the makespan. Then, we apply this\nmethod to several scheduling problems, namely, for unit independent tasks, for\nweighted independent tasks and for tasks with precendence constraints. More\nprecisely, we prove that the time for scheduling a global workload W composed\nof independent unit tasks on m processors is equal to W/m plus an additional\nterm proportional to log_2 W. We provide a lower bound which shows that this is\noptimal up to a constant. This result is extended to the case of weighted\nindependent tasks. In the last setting, precedence task graphs, our analysis\nleads to an improvement on the bound of Arora et al. We finally provide some\nexperiments using a simulator. The distribution of the makespan is shown to fit\nexisting probability laws. The additive term is shown by simulation to be\naround 3 \\log_2 W confirming the tightness of our analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.6014v1", 
    "other_authors": "Alain Cournier, Swan Dubois, Anissa Lamani, Franck Petit, Vincent Villain", 
    "title": "Snap-Stabilizing Message Forwarding Algorithm on Tree Topologies", 
    "arxiv-id": "1107.6014v1", 
    "author": "Vincent Villain", 
    "publish": "2011-07-29T16:37:44Z", 
    "summary": "In this paper, we consider the message forwarding problem that consists in\nmanaging the network resources that are used to forward messages. Previous\nworks on this problem provide solutions that either use a significant number of\nbuffers (that is n buffers per processor, where n is the number of processors\nin the network) making the solution not scalable or, they reserve all the\nbuffers from the sender to the receiver to forward only one message %while\nusing D buffers (where D refers to the diameter of the network) . The only\nsolution that uses a constant number of buffers per link was introduced in [1].\nHowever the solution works only on a chain networks. In this paper, we propose\na snap-stabilizing algorithm for the message forwarding problem that uses the\nsame complexity on the number of buffers as [1] and works on tree topologies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.0397v2", 
    "other_authors": "Moreno Marzolla, Stefano Ferretti, Gabriele D'Angelo", 
    "title": "Auction-Based Resource Allocation in Digital Ecosystems", 
    "arxiv-id": "1109.0397v2", 
    "author": "Gabriele D'Angelo", 
    "publish": "2011-09-02T10:08:38Z", 
    "summary": "The proliferation of portable devices (PDAs, smartphones, digital multimedia\nplayers, and so forth) allows mobile users to carry around a pool of computing,\nstorage and communication resources. Sharing these resources with other users\n(\"Digital Organisms\" -- DOs) opens the door to novel interesting scenarios,\nwhere people trade resources to allow the execution, anytime and anywhere, of\napplications that require a mix of capabilities. In this paper we present a\nfully distributed approach for resource sharing among multiple devices owned by\ndifferent mobile users. Our scheme enables DOs to trade computing/networking\nfacilities through an auction-based mechanism, without the need of a central\ncontrol. We use a set of numerical experiments to compare our approach with an\noptimal (centralized) allocation strategy that, given the set of resource\ndemands and offers, maximizes the number of matches. Results confirm the\neffectiveness of our approach since it produces a fair allocation of resources\nwith low computational cost, providing DOs with the means to form an altruistic\ndigital ecosystem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.0742v1", 
    "other_authors": "Robert Louis Cloud", 
    "title": "Problems in Modern High Performance Parallel I/O Systems", 
    "arxiv-id": "1109.0742v1", 
    "author": "Robert Louis Cloud", 
    "publish": "2011-09-04T19:28:43Z", 
    "summary": "In the past couple of decades, the computational abilities of supercomput-\ners have increased tremendously. Leadership scale supercomputers now are\ncapable of petaflops. Likewise, the problem size targeted by applications\nrunning on such computers has also scaled. These large applications have I/O\nthroughput requirements on the order of tens of gigabytes per second. For a\nvariety of reasons, the I/O subsystems of such computers have not kept pace\nwith the computational increases, and the time required for I/O in an\napplication has become one of the dominant bottlenecks. Also troublesome is the\nfact that scientific applications do not attain near the peak theoretical\nbandwidth of the I/O subsystems. In addressing the two prior issues, one must\nalso question the nature of the data itself; one can ask whether contem- porary\npractices of data dumping and analysis are optimal and whether they will\ncontinue to be applicable as computers continue to scale. These three topics,\nthe I/O subsystem, the nature of scientific data output, and future possible\noptimizations are discussed in this report."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.1650v1", 
    "other_authors": "Ardhendu Mandal, Subhas Chandra Pal", 
    "title": "An Empirical Study and Analysis of the Dynamic Load Balancing Techniques   Used in Parallel Computing Systems", 
    "arxiv-id": "1109.1650v1", 
    "author": "Subhas Chandra Pal", 
    "publish": "2011-09-08T08:04:06Z", 
    "summary": "A parallel computer system is a collection of processing elements that\ncommunicate and cooperate to solve large computational problems efficiently. To\nachieve this, at first the large computational problem is partitioned into\nseveral tasks with different work-loads and then are assigned to the different\nprocessing elements for computation. Distribution of the work load is known as\nLoad Balancing. An appropriate distribution of work-loads across the various\nprocessing elements is very important as disproportional workloads can\neliminate the performance benefit of parallelizing the job. Hence, load\nbalancing on parallel systems is a critical and challenging activity. Load\nbalancing algorithms can be broadly categorized as static or dynamic. Static\nload balancing algorithms distribute the tasks to processing elements at\ncompile time, while dynamic algorithms bind tasks to processing elements at run\ntime. This paper explains only the different dynamic load balancing techniques\nin brief used in parallel systems and concluding with the comparative\nperformance analysis result of these algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.1706v1", 
    "other_authors": "Esha Ghosh, Subhas K. Ghosh, C. Pandu Rangan", 
    "title": "On the Fault Tolerance and Hamiltonicity of the Optical Transpose   Interconnection System of Non-Hamiltonian Base Graphs", 
    "arxiv-id": "1109.1706v1", 
    "author": "C. Pandu Rangan", 
    "publish": "2011-09-08T12:50:38Z", 
    "summary": "Hamiltonicity is an important property in parallel and distributed\ncomputation. Existence of Hamiltonian cycle allows efficient emulation of\ndistributed algorithms on a network wherever such algorithm exists for\nlinear-array and ring, and can ensure deadlock freedom in some routing\nalgorithms in hierarchical interconnection networks. Hamiltonicity can also be\nused for construction of independent spanning tree and leads to designing fault\ntolerant protocols. Optical Transpose Interconnection Systems or OTIS (also\nreferred to as two-level swapped network) is a widely studied interconnection\nnetwork topology which is popular due to high degree of scalability,\nregularity, modularity and package ability. Surprisingly, to our knowledge,\nonly one strong result is known regarding Hamiltonicity of OTIS - showing that\nOTIS graph built of Hamiltonian base graphs are Hamiltonian. In this work we\nconsider Hamiltonicity of OTIS networks, built on Non-Hamiltonian base and\nanswer some important questions. First, we prove that Hamiltonicity of base\ngraph is not a necessary condition for the OTIS to be Hamiltonian. We present\nan infinite family of Hamiltonian OTIS graphs composed on Non-Hamiltonian base\ngraphs. We further show that, it is not sufficient for the base graph to have\nHamiltonian path for the OTIS constructed on it to be Hamiltonian. We give\nconstructive proof of Hamiltonicity for a large family of Butterfly-OTIS. This\nproof leads to an alternate efficient algorithm for independent spanning trees\nconstruction on this class of OTIS graphs. Our algorithm is linear in the\nnumber of vertices as opposed to the generalized algorithm, which is linear in\nthe number of edges of the graph."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.3074v1", 
    "other_authors": "Alexey Lastovetsky, Ravi Reddy, Vladimir Rychkov, David Clarke", 
    "title": "Design and implementation of self-adaptable parallel algorithms for   scientific computing on highly heterogeneous HPC platforms", 
    "arxiv-id": "1109.3074v1", 
    "author": "David Clarke", 
    "publish": "2011-09-14T13:25:35Z", 
    "summary": "Traditional heterogeneous parallel algorithms, designed for heterogeneous\nclusters of workstations, are based on the assumption that the absolute speed\nof the processors does not depend on the size of the computational task. This\nassumption proved inaccurate for modern and perspective highly heterogeneous\nHPC platforms. New class of algorithms based on the functional performance\nmodel (FPM), representing the speed of the processor by a function of problem\nsize, has been recently proposed. These algorithms cannot be however employed\nin self-adaptable applications because of very high cost of construction of the\nfunctional performance model. The paper presents a new class of parallel\nalgorithms for highly heterogeneous HPC platforms. Like traditional FPM-based\nalgorithms, these algorithms assume that the speed of the processors is\ncharacterized by speed functions rather than speed constants. Unlike the\ntraditional algorithms, they do not assume the speed functions to be given.\nInstead, they estimate the speed functions of the processors for different\nproblem sizes during their execution. These algorithms do not construct the\nfull speed function for each processor but rather build and use their partial\nestimates sufficient for optimal distribution of computations with a given\naccuracy. The low execution cost of distribution of computations between\nheterogeneous processors in these algorithms make them suitable for employment\nin self-adaptable applications. Experiments with parallel matrix multiplication\napplications based on this approach are performed on local and global\nheterogeneous computational clusters. The results show that the execution time\nof optimal matrix distribution between processors is significantly less, by\norders of magnitude, than the total execution time of the optimized\napplication."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Mobilware.2013.16", 
    "link": "http://arxiv.org/pdf/1109.3561v1", 
    "other_authors": "Thibault Bernard, Alain Bui, Devan Sohier", 
    "title": "Universal adaptive self-stabilizing traversal scheme: random walk and   reloading wave", 
    "arxiv-id": "1109.3561v1", 
    "author": "Devan Sohier", 
    "publish": "2011-09-16T09:38:20Z", 
    "summary": "In this paper, we investigate random walk based token circulation in dynamic\nenvironments subject to failures. We describe hypotheses on the dynamic\nenvironment that allow random walks to meet the important property that the\ntoken visits any node infinitely often. The randomness of this scheme allows it\nto work on any topology, and require no adaptation after a topological change,\nwhich is a desirable property for applications to dynamic systems. For random\nwalks to be a traversal scheme and to answer the concurrence problem, one needs\nto guarantee that exactly one token circulates in the system. In the presence\nof transient failures, configurations with multiple tokens or with no token can\noccur. The meeting property of random walks solves the cases with multiple\ntokens. The reloading wave mechanism we propose, together with timeouts, allows\nto detect and solve cases with no token. This traversal scheme is\nself-stabilizing, and universal, meaning that it needs no assumption on the\nsystem topology. We describe conditions on the dynamicity (with a local\ndetection criterion) under which the algorithm is tolerant to dynamic\nreconfigurations. We conclude by a study on the time between two visits of the\ntoken to a node, which we use to tune the parameters of the reloading wave\nmechanism according to some system characteristics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ISWPC.2006.1613559", 
    "link": "http://arxiv.org/pdf/1109.3997v1", 
    "other_authors": "Damianos Gavalas, Grammati Pantziou, Charalampos Konstantopoulos, Basilis Mamalis", 
    "title": "Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks   Clustering Algorithm", 
    "arxiv-id": "1109.3997v1", 
    "author": "Basilis Mamalis", 
    "publish": "2011-09-19T11:11:57Z", 
    "summary": "Clustering is a promising approach for building hierarchies and simplifying\nthe routing process in mobile ad-hoc network environments. The main objective\nof clustering is to identify suitable node representatives, i.e. cluster heads\n(CHs), to store routing and topology information and maximize clusters\nstability. Traditional clustering algorithms suggest CH election exclusively\nbased on node IDs or location information and involve frequent broadcasting of\ncontrol packets, even when network topology remains unchanged. More recent\nworks take into account additional metrics (such as energy and mobility) and\noptimize initial clustering. However, in many situations (e.g. in relatively\nstatic topologies) re-clustering procedure is hardly ever invoked; hence\ninitially elected CHs soon reach battery exhaustion. Herein, we introduce an\nefficient distributed clustering algorithm that uses both mobility and energy\nmetrics to provide stable cluster formations. CHs are initially elected based\non the time and cost-efficient lowest-ID method. During clustering maintenance\nphase though, node IDs are re-assigned according to nodes mobility and energy\nstatus, ensuring that nodes with low-mobility and sufficient energy supply are\nassigned low IDs and, hence, are elected as CHs. Our algorithm also reduces\ncontrol traffic volume since broadcast period is adjusted according to nodes\nmobility pattern: we employ infrequent broadcasting for relative static network\ntopologies, and increase broadcast frequency for highly mobile network\nconfigurations. Simulation results verify that energy consumption is uniformly\ndistributed among network nodes and that signaling overhead is significantly\ndecreased."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ISWPC.2006.1613559", 
    "link": "http://arxiv.org/pdf/1109.4925v1", 
    "other_authors": "Leandro A. J. Marzulo, Tiago A. O. Alves, Felipe M. G. Fran\u00e7a, V\u00edtor Santos Costa", 
    "title": "Couillard: Parallel Programming via Coarse-Grained Data-Flow Compilation", 
    "arxiv-id": "1109.4925v1", 
    "author": "V\u00edtor Santos Costa", 
    "publish": "2011-09-22T19:44:19Z", 
    "summary": "Data-flow is a natural approach to parallelism. However, describing\ndependencies and control between fine-grained data-flow tasks can be complex\nand present unwanted overheads. TALM (TALM is an Architecture and Language for\nMulti-threading) introduces a user-defined coarse-grained parallel data-flow\nmodel, where programmers identify code blocks, called super-instructions, to be\nrun in parallel and connect them in a data-flow graph. TALM has been\nimplemented as a hybrid Von Neumann/data-flow execution system: the\n\\emph{Trebuchet}. We have observed that TALM's usefulness largely depends on\nhow programmers specify and connect super-instructions. Thus, we present\n\\emph{Couillard}, a full compiler that creates, based on an annotated\nC-program, a data-flow graph and C-code corresponding to each\nsuper-instruction. We show that our toolchain allows one to benefit from\ndata-flow execution and explore sophisticated parallel programming techniques,\nwith small effort. To evaluate our system we have executed a set of real\napplications on a large multi-core machine. Comparison with popular parallel\nprogramming methods shows competitive speedups, while providing an easier\nparallel programing approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ISWPC.2006.1613559", 
    "link": "http://arxiv.org/pdf/1109.5111v2", 
    "other_authors": "Robbert van Renesse, Fred B. Schneider, Johannes Gehrke", 
    "title": "Nerio: Leader Election and Edict Ordering", 
    "arxiv-id": "1109.5111v2", 
    "author": "Johannes Gehrke", 
    "publish": "2011-09-23T15:27:43Z", 
    "summary": "Coordination in a distributed system is facilitated if there is a unique\nprocess, the leader, to manage the other processes. The leader creates edicts\nand sends them to other processes for execution or forwarding to other\nprocesses. The leader may fail, and when this occurs a leader election protocol\nselects a replacement. This paper describes Nerio, a class of such leader\nelection protocols."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ISWPC.2006.1613559", 
    "link": "http://arxiv.org/pdf/1109.5153v1", 
    "other_authors": "Wojciech Golab", 
    "title": "A Complexity Separation Between the Cache-Coherent and Distributed   Shared Memory Models", 
    "arxiv-id": "1109.5153v1", 
    "author": "Wojciech Golab", 
    "publish": "2011-09-23T18:39:18Z", 
    "summary": "We consider asynchronous multiprocessor systems where processes communicate\nby accessing shared memory. Exchange of information among processes in such a\nmultiprocessor necessitates costly memory accesses called \\emph{remote memory\nreferences} (RMRs), which generate communication on the interconnect joining\nprocessors and main memory. In this paper we compare two popular shared memory\narchitecture models, namely the \\emph{cache-coherent} (CC) and\n\\emph{distributed shared memory} (DSM) models, in terms of their power for\nsolving synchronization problems efficiently with respect to RMRs. The\nparticular problem we consider entails one process sending a \"signal\" to a\nsubset of other processes. We show that a variant of this problem can be solved\nvery efficiently with respect to RMRs in the CC model, but not so in the DSM\nmodel, even when we consider amortized RMR complexity.\n  To our knowledge, this is the first separation in terms of amortized RMR\ncomplexity between the CC and DSM models. It is also the first separation in\nterms of RMR complexity (for asynchronous systems) that does not rely in any\nway on wait-freedom---the requirement that a process makes progress in a\nbounded number of its own steps."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1109.5190v1", 
    "other_authors": "Chirag Dekate, Matthew Anderson, Maciej Brodowicz, Hartmut Kaiser, Bryce Adelstein-Lelbach, Thomas Sterling", 
    "title": "Improving the scalability of parallel N-body applications with an event   driven constraint based execution model", 
    "arxiv-id": "1109.5190v1", 
    "author": "Thomas Sterling", 
    "publish": "2011-09-23T20:14:24Z", 
    "summary": "The scalability and efficiency of graph applications are significantly\nconstrained by conventional systems and their supporting programming models.\nTechnology trends like multicore, manycore, and heterogeneous system\narchitectures are introducing further challenges and possibilities for emerging\napplication domains such as graph applications. This paper explores the space\nof effective parallel execution of ephemeral graphs that are dynamically\ngenerated using the Barnes-Hut algorithm to exemplify dynamic workloads. The\nworkloads are expressed using the semantics of an Exascale computing execution\nmodel called ParalleX. For comparison, results using conventional execution\nmodel semantics are also presented. We find improved load balancing during\nruntime and automatic parallelism discovery improving efficiency using the\nadvanced semantics for Exascale computing."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1109.5201v1", 
    "other_authors": "Matthew Anderson, Maciej Brodowicz, Hartmut Kaiser, Thomas Sterling", 
    "title": "An Application Driven Analysis of the ParalleX Execution Model", 
    "arxiv-id": "1109.5201v1", 
    "author": "Thomas Sterling", 
    "publish": "2011-09-23T21:00:36Z", 
    "summary": "Exascale systems, expected to emerge by the end of the next decade, will\nrequire the exploitation of billion-way parallelism at multiple hierarchical\nlevels in order to achieve the desired sustained performance. The task of\nassessing future machine performance is approached by identifying the factors\nwhich currently challenge the scalability of parallel applications. It is\nsuggested that the root cause of these challenges is the incoherent coupling\nbetween the current enabling technologies, such as Non-Uniform Memory Access of\npresent multicore nodes equipped with optional hardware accelerators and the\ndecades older execution model, i.e., the Communicating Sequential Processes\n(CSP) model best exemplified by the message passing interface (MPI) application\nprogramming interface. A new execution model, ParalleX, is introduced as an\nalternative to the CSP model. In this paper, an overview of the ParalleX\nexecution model is presented along with details about a ParalleX-compliant\nruntime system implementation called High Performance ParalleX (HPX). Scaling\nand performance results for an adaptive mesh refinement numerical relativity\napplication developed using HPX are discussed. The performance results of this\nHPX-based application are compared with a counterpart MPI-based mesh refinement\ncode. The overheads associated with HPX are explored and hardware solutions are\nintroduced for accelerating the runtime system."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1109.5770v1", 
    "other_authors": "Mei Leng, Wee Peng Tay, Tony Q. S. Quek", 
    "title": "Cooperative and Distributed Localization for Wireless Sensor Networks in   Multipath Environments", 
    "arxiv-id": "1109.5770v1", 
    "author": "Tony Q. S. Quek", 
    "publish": "2011-09-27T04:38:33Z", 
    "summary": "We consider the problem of sensor localization in a wireless network in a\nmultipath environment, where time and angle of arrival information are\navailable at each sensor. We propose a distributed algorithm based on belief\npropagation, which allows sensors to cooperatively self-localize with respect\nto one single anchor in a multihop network. The algorithm has low overhead and\nis scalable. Simulations show that although the network is loopy, the proposed\nalgorithm converges, and achieves good localization accuracy."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.0875v2", 
    "other_authors": "Aditya Kurve, Christopher Griffin, David J. Miller, George Kesidis", 
    "title": "Game Theoretic Iterative Partitioning for Dynamic Load Balancing in   Distributed Network Simulation", 
    "arxiv-id": "1111.0875v2", 
    "author": "George Kesidis", 
    "publish": "2011-11-03T15:21:25Z", 
    "summary": "High fidelity simulation of large-sized complex networks can be realized on a\ndistributed computing platform that leverages the combined resources of\nmultiple processors or machines. In a discrete event driven simulation, the\nassignment of logical processes (LPs) to machines is a critical step that\naffects the computational and communication burden on the machines, which in\nturn affects the simulation execution time of the experiment. We study a\nnetwork partitioning game wherein each node (LP) acts as a selfish player. We\nderive two local node-level cost frameworks which are feasible in the sense\nthat the aggregate state information required to be exchanged between the\nmachines is independent of the size of the simulated network model. For both\ncost frameworks, we prove the existence of stable Nash equilibria in pure\nstrategies. Using iterative partition improvements, we propose game theoretic\npartitioning algorithms based on the two cost criteria and show that each\ndescends in a global cost. To exploit the distributed nature of the system, the\nalgorithm is distributed, with each node's decision based on its local\ninformation and on a few global quantities which can be communicated\nmachine-to-machine. We demonstrate the performance of our partitioning\nalgorithm on an optimistic discrete event driven simulation platform that\nmodels an actual parallel simulator."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.1129v1", 
    "other_authors": "Markus Wittmann, Thomas Zeiser, Georg Hager, Gerhard Wellein", 
    "title": "Domain decomposition and locality optimization for large-scale lattice   Boltzmann simulations", 
    "arxiv-id": "1111.1129v1", 
    "author": "Gerhard Wellein", 
    "publish": "2011-11-04T13:52:36Z", 
    "summary": "We present a simple, parallel and distributed algorithm for setting up and\npartitioning a sparse representation of a regular discretized simulation\ndomain. This method is scalable for a large number of processes even for\ncomplex geometries and ensures load balance between the domains, reasonable\ncommunication interfaces, and good data locality within the domain. Applying\nthis scheme to a list-based lattice Boltzmann flow solver can achieve similar\nor even higher flow solver performance than widely used standard graph\npartition based tools such as METIS and PT-SCOTCH."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.1170v3", 
    "other_authors": "Benjamin Morandi, Sebastian Nanz, Bertrand Meyer", 
    "title": "Record-replay debugging for the SCOOP concurrency model", 
    "arxiv-id": "1111.1170v3", 
    "author": "Bertrand Meyer", 
    "publish": "2011-11-04T16:15:14Z", 
    "summary": "To support developers in writing reliable and efficient concurrent programs,\nnovel concurrent programming abstractions have been proposed in recent years.\nProgramming with such abstractions requires new analysis tools because the\nexecution semantics often differs considerably from established models. We\npresent a record-replay technique for programs written in SCOOP, an\nobject-oriented programming model for concurrency. The resulting tool enables\ndevelopers to reproduce the nondeterministic execution of a concurrent program,\na necessary prerequisite for debugging and testing."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.2237v1", 
    "other_authors": "Elena Legchekova, Oleg Titov", 
    "title": "Choosing the best resource by method of mamdani", 
    "arxiv-id": "1111.2237v1", 
    "author": "Oleg Titov", 
    "publish": "2011-11-06T20:35:30Z", 
    "summary": "A method for selecting the best service for the storage of information by\nMamdani."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.2412v1", 
    "other_authors": "C. Dinesh", 
    "title": "Secured Data Consistency and Storage Way in Untrusted Cloud using Server   Management Algorithm", 
    "arxiv-id": "1111.2412v1", 
    "author": "C. Dinesh", 
    "publish": "2011-11-10T08:05:38Z", 
    "summary": "It is very challenging part to keep safely all required data that are needed\nin many applications for user in cloud. Storing our data in cloud may not be\nfully trustworthy. Since client doesn't have copy of all stored data, he has to\ndepend on Cloud Service Provider. But dynamic data operations, Read-Solomon and\nverification token construction methods don't tell us about total storage\ncapacity of server allocated space before and after the data addition in cloud.\nSo we have to introduce a new proposed system of efficient storage measurement\nand space comparison algorithm with time management for measuring the total\nallocated storage area before and after the data insertion in cloud. So by\nusing our proposed scheme, the value or weight of stored data before and after\nis measured by client with specified time in cloud storage area with accuracy.\nAnd here we also have proposed the multi-server restore point in server failure\ncondition. If there occurs any server failure, by using this scheme the data\ncan be recovered automatically in cloud server. Our proposed scheme efficiently\nchecks space for the in-outsourced data to maintain integrity. Here the TPA\nnecessarily doesn't have the delegation to audit user's data."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.2418v1", 
    "other_authors": "C. Dinesh", 
    "title": "Data Integrity and Dynamic Storage Way in Cloud Computing", 
    "arxiv-id": "1111.2418v1", 
    "author": "C. Dinesh", 
    "publish": "2011-11-10T08:23:47Z", 
    "summary": "It is not an easy task to securely maintain all essential data where it has\nthe need in many applications for clients in cloud. To maintain our data in\ncloud, it may not be fully trustworthy because client doesn't have copy of all\nstored data. But any authors don't tell us data integrity through its user and\nCSP level by comparison before and after the data update in cloud. So we have\nto establish new proposed system for this using our data reading protocol\nalgorithm to check the integrity of data before and after the data insertion in\ncloud. Here the security of data before and after is checked by client with the\nhelp of CSP using our \"effective automatic data reading protocol from user as\nwell as cloud level into the cloud\" with truthfulness. Also we have proposed\nthe multi-server data comparison algorithm with the calculation of overall data\nin each update before its outsourced level for server restore access point for\nfuture data recovery from cloud data server. Our proposed scheme efficiently\nchecks integrity in efficient manner so that data integrity as well as security\ncan be maintained in all cases by considering drawbacks of existing methods."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.3022v1", 
    "other_authors": "Yiling Yang, Yu Huang, Jiannong Cao, Xiaoxing Ma, Jian Lu", 
    "title": "Design of a Sliding Window over Asynchronous Event Streams", 
    "arxiv-id": "1111.3022v1", 
    "author": "Jian Lu", 
    "publish": "2011-11-13T15:20:49Z", 
    "summary": "The proliferation of sensing and monitoring applications motivates adoption\nof the event stream model of computation. Though sliding windows are widely\nused to facilitate effective event stream processing, it is greatly challenged\nwhen the event sources are distributed and asynchronous. To address this\nchallenge, we first show that the snapshots of the asynchronous event streams\nwithin the sliding window form a convex distributive lattice (denoted by\nLat-Win). Then we propose an algorithm to maintain Lat-Win at runtime. The\nLat-Win maintenance algorithm is implemented and evaluated on the open-source\ncontext-aware middleware we developed. The evaluation results first show the\nnecessity of adopting sliding windows over asynchronous event streams. Then\nthey show the performance of detecting specified predicates within Lat-Win,\neven when faced with dynamic changes in the computing environment."
},{
    "category": "cs.DC", 
    "doi": "10.1177/1094342012440585", 
    "link": "http://arxiv.org/pdf/1111.3334v1", 
    "other_authors": "Abhishek Kr. Singh, Bollibisai Giridhar, Partha Sarathi Mandal", 
    "title": "Fixing Data Anomalies with Prediction Based Algorithm in Wireless Sensor   Networks", 
    "arxiv-id": "1111.3334v1", 
    "author": "Partha Sarathi Mandal", 
    "publish": "2011-11-14T19:43:04Z", 
    "summary": "Data inconsistencies are present in the data collected over a large wireless\nsensor network (WSN), usually deployed for any kind of monitoring applications.\nBefore passing this data to some WSN applications for decision making, it is\nnecessary to ensure that the data received are clean and accurate. In this\npaper, we have used a statistical tool to examine the past data to fit in a\nhighly sophisticated prediction model i.e., ARIMA for a given sensor node and\nwith this, the model corrects the data using forecast value if any data anomaly\nexists there. Another scheme is also proposed for detecting data anomaly at\nsink among the aggregated data in the data are received from a particular\nsensor node. The effectiveness of our methods are validated by data collected\nover a real WSN application consisting of Crossbow IRIS Motes\n\\cite{Crossbow:2009}."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.4499v1", 
    "other_authors": "Somayeh Kafaie, Omid Kashefi, Mohsen Sharifi", 
    "title": "A Low-Energy Fast Cyber Foraging Mechanism for Mobile Devices", 
    "arxiv-id": "1111.4499v1", 
    "author": "Mohsen Sharifi", 
    "publish": "2011-11-18T21:37:36Z", 
    "summary": "The ever increasing demands for using resource-constrained mobile devices for\nrunning more resource intensive applications nowadays has initiated the\ndevelopment of cyber foraging solutions that offload parts or whole\ncomputational intensive tasks to more powerful surrogate stationary computers\nand run them on behalf of mobile devices as required. The choice of proper mix\nof mobile devices and surrogates has remained an unresolved challenge though.\nIn this paper, we propose a new decision-making mechanism for cyber foraging\nsystems to select the best locations to run an application, based on context\nmetrics such as the specifications of surrogates, the specifications of mobile\ndevices, application specification, and communication network specification.\nExperimental results show faster response time and lower energy consumption of\nbenched applications compared to when applications run wholly on mobile devices\nand when applications are offloaded to surrogates blindly for execution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5239v2", 
    "other_authors": "David I. Shuman, Pierre Vandergheynst, Pascal Frossard", 
    "title": "Distributed Signal Processing via Chebyshev Polynomial Approximation", 
    "arxiv-id": "1111.5239v2", 
    "author": "Pascal Frossard", 
    "publish": "2011-11-22T16:15:32Z", 
    "summary": "Unions of graph multiplier operators are an important class of linear\noperators for processing signals defined on graphs. We present a novel method\nto efficiently distribute the application of these operators. The proposed\nmethod features approximations of the graph multipliers by shifted Chebyshev\npolynomials, whose recurrence relations make them readily amenable to\ndistributed computation. We demonstrate how the proposed method can be applied\nto distributed processing tasks such as smoothing, denoising, inverse\nfiltering, and semi-supervised classification, and show that the communication\nrequirements of the method scale gracefully with the size of the network."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5391v1", 
    "other_authors": "Qiang Dong, Hui Gao, Yan Fu, Xiaofan Yang", 
    "title": "Hamiltonian Connectivity of Twisted Hypercube-Like Networks under the   Large Fault Model", 
    "arxiv-id": "1111.5391v1", 
    "author": "Xiaofan Yang", 
    "publish": "2011-11-23T03:05:45Z", 
    "summary": "Twisted hypercube-like networks (THLNs) are an important class of\ninterconnection networks for parallel computing systems, which include most\npopular variants of the hypercubes, such as crossed cubes, M\\\"obius cubes,\ntwisted cubes and locally twisted cubes. This paper deals with the\nfault-tolerant hamiltonian connectivity of THLNs under the large fault model.\nLet $G$ be an $n$-dimensional THLN and $F \\subseteq V(G)\\bigcup E(G)$, where $n\n\\geq 7$ and $|F| \\leq 2n - 10$. We prove that for any two nodes $u,v \\in V(G -\nF)$ satisfying a simple necessary condition on neighbors of $u$ and $v$, there\nexists a hamiltonian or near-hamiltonian path between $u$ and $v$ in $G-F$. The\nresult extends further the fault-tolerant graph embedding capability of THLNs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5775v2", 
    "other_authors": "Wim H. Hesselink", 
    "title": "Partial mutual exclusion for infinitely many processes", 
    "arxiv-id": "1111.5775v2", 
    "author": "Wim H. Hesselink", 
    "publish": "2011-11-24T14:17:05Z", 
    "summary": "Partial mutual exclusion is the drinking philosophers problem for complete\ngraphs. It is the problem that a process may enter a critical section CS of its\ncode only when some finite set nbh of other processes are not in their critical\nsections. For each execution of CS, the set nbh can be given by the\nenvironment. We present a starvation free solution of this problem in a setting\nwith infinitely many processes, each with finite memory, that communicate by\nasynchronous messages. The solution has the property of first-come\nfirst-served, in so far as this can be guaranteed by asynchronous messages. For\nevery execution of CS and every process in nbh, between three and six messages\nare needed. The correctness of the solution is argued with invariants and\ntemporal logic. It has been verified with the proof assistant PVS."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.6324v1", 
    "other_authors": "Ata Turk, Cevdet Aykanat, G. Vehbi Demirci, Sebastian von Alfthan, Ilja Honkonen", 
    "title": "Improving the Load Balancing Performance of Vlasiator", 
    "arxiv-id": "1111.6324v1", 
    "author": "Ilja Honkonen", 
    "publish": "2011-11-28T00:50:25Z", 
    "summary": "This whitepaper describes the load-balancing performance issues that are\nobserved and tackled during the petascaling of the Vlasiator codes. Vlasiator\nis a Vlasov-hybrid simulation code developed in Finnish Meteorological\nInstitute (FMI). Vlasiator models the communications associated with the\nspatial grid operated on as a hypergraph and partitions the grid using the\nparallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning\nframework. The result of partitioning determines the distribution of grid cells\nto processors. It is observed that the partitioning phase takes a substantial\npercentage of the overall computation time. Alternative\n(graph-partitioning-based) schemes that perform almost as well as the\nhypergraph partitioning scheme and that require less preprocessing overhead and\nbetter balance are proposed and investigated. A comparison in terms of effect\non running time, preprocessing overhead and load-balancing quality of Zoltan's\nPHG, ParMeTiS, and PT-SCOTCH are presented. Test results on J\\\"uelich\nBlueGene/P cluster are presented."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.0616v1", 
    "other_authors": "Ravi Rastogi, Amit Singh, Nikhil Singhal, Nitin, Durg Singh Chauhan", 
    "title": "Case Tool: Fast Interconnections with New 3-Disjoint Paths MIN   Simulation Module", 
    "arxiv-id": "1202.0616v1", 
    "author": "Durg Singh Chauhan", 
    "publish": "2012-02-03T07:03:49Z", 
    "summary": "Multi-stage interconnection networks (MIN) can be designed to achieve fault\ntolerance and collision solving by providing a set of disjoint paths. In this\npaper, we are discussing the new simulator added to the tool designed for\ndeveloping fault tolerant MINs. The designed tool is one of its own kind and\nwill help the user in developing 2 and 3-disjoint path networks. The java\ntechnology has been used to design the tool and have been tested on different\nsoftware platform."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.0970v1", 
    "other_authors": "Josef Spillner, Alexander Schill", 
    "title": "\u03c0-Control: A Personal Cloud Control Centre", 
    "arxiv-id": "1202.0970v1", 
    "author": "Alexander Schill", 
    "publish": "2012-02-05T15:25:02Z", 
    "summary": "Consumption of online services and cloud computing offerings is on the rise,\nlargely due to compelling advantages over traditional local applications. From\na user perspective, these include zero-maintenance of software, the always-on\nnature of such services, mashups of different applications and the networking\neffect with other users. Associated disadvantages are known, but effective\nmeans and tools to limit their effect are not yet well-established and not yet\ngenerally available to service users. We propose (1) a user-centric model of\ncloud elements beyond the conventional <SPI>aaS layers, including activities\nacross trust zones, and (2) a personal control console for all individual and\ncollaborative user activities in the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.1062v1", 
    "other_authors": "Ravi Rastogi, Nitin, Durg Singh Chauhan, Mahesh Chandra Govil", 
    "title": "On Stability Problems of Omega and 3-Disjoint Paths Omega Multi-stage   Interconnection Networks", 
    "arxiv-id": "1202.1062v1", 
    "author": "Mahesh Chandra Govil", 
    "publish": "2012-02-06T07:39:06Z", 
    "summary": "The research paper emphasizes that the Stable Matching problems are the same\nas the problems of stable configurations of Multi-stage Interconnection\nNetworks (MIN). We have discusses the Stability Problems of Existing Regular\nOmega Multi-stage Interconnection Network (OMIN) and Proposed 3-Disjoint Paths\nOmega Multi-stage Interconnection Network (3DON) using the approaches and\nsolutions provided by the Stable Matching Problem. Specifically, Stable\nMarriage Problem is used as an example of Stable Matching. On application of\nthe concept of the Stable Marriage over the MINs states that OMIN is highly\nstable in comparison to 3DON."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.1186v2", 
    "other_authors": "Yuval Emek, Jasmin Smula, Roger Wattenhofer", 
    "title": "Stone Age Distributed Computing", 
    "arxiv-id": "1202.1186v2", 
    "author": "Roger Wattenhofer", 
    "publish": "2012-02-06T16:20:06Z", 
    "summary": "The traditional models of distributed computing focus mainly on networks of\ncomputer-like devices that can exchange large messages with their neighbors and\nperform arbitrary local computations. Recently, there is a trend to apply\ndistributed computing methods to networks of sub-microprocessor devices, e.g.,\nbiological cellular networks or networks of nano-devices. However, the\nsuitability of the traditional distributed computing models to these types of\nnetworks is questionable: do tiny bio/nano nodes \"compute\" and/or \"communicate\"\nessentially the same as a computer? In this paper, we introduce a new model\nthat depicts a network of randomized finite state machines operating in an\nasynchronous environment. Although the computation and communication\ncapabilities of each individual device in the new model are, by design, much\nweaker than those of a computer, we show that some of the most important and\nextensively studied distributed computing problems can still be solved\nefficiently."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.1983v3", 
    "other_authors": "Leonid Barenboim, Michael Elkin, Seth Pettie, Johannes Schneider", 
    "title": "The Locality of Distributed Symmetry Breaking", 
    "arxiv-id": "1202.1983v3", 
    "author": "Johannes Schneider", 
    "publish": "2012-02-09T13:46:11Z", 
    "summary": "Symmetry breaking problems are among the most well studied in the field of\ndistributed computing and yet the most fundamental questions about their\ncomplexity remain open. In this paper we work in the LOCAL model (where the\ninput graph and underlying distributed network are identical) and study the\nrandomized complexity of four fundamental symmetry breaking problems on graphs:\ncomputing MISs (maximal independent sets), maximal matchings, vertex colorings,\nand ruling sets. A small sample of our results includes\n  - An MIS algorithm running in $O(\\log^2\\Delta + 2^{O(\\sqrt{\\log\\log n})})$\ntime, where $\\Delta$ is the maximum degree. This is the first MIS algorithm to\nimprove on the 1986 algorithms of Luby and Alon, Babai, and Itai, when $\\log n\n\\ll \\Delta \\ll 2^{\\sqrt{\\log n}}$, and comes close to the $\\Omega(\\log \\Delta)$\nlower bound of Kuhn, Moscibroda, and Wattenhofer.\n  - A maximal matching algorithm running in $O(\\log\\Delta + \\log^4\\log n)$\ntime. This is the first significant improvement to the 1986 algorithm of\nIsraeli and Itai. Moreover, its dependence on $\\Delta$ is provably optimal.\n  - A method for reducing symmetry breaking problems in low\narboricity/degeneracy graphs to low degree graphs. (Roughly speaking, the\narboricity or degeneracy of a graph bounds the density of any subgraph.)\nCorollaries of this reduction include an $O(\\sqrt{\\log n})$-time maximal\nmatching algorithm for graphs with arboricity up to $2^{\\sqrt{\\log n}}$ and an\n$O(\\log^{2/3} n)$-time MIS algorithm for graphs with arboricity up to $2^{(\\log\nn)^{1/3}}$.\n  Each of our algorithms is based on a simple, but powerful technique for\nreducing a randomized symmetry breaking task to a corresponding deterministic\none on a poly$(\\log n)$-size graph."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.2509v1", 
    "other_authors": "Nicolo M. Calcavecchia, Bogdan Alexandru Caprarescu, Elisabetta Di Nitto, Daniel J. Dubois, Dana Petcu", 
    "title": "DEPAS: A Decentralized Probabilistic Algorithm for Auto-Scaling", 
    "arxiv-id": "1202.2509v1", 
    "author": "Dana Petcu", 
    "publish": "2012-02-12T09:26:40Z", 
    "summary": "The dynamic provisioning of virtualized resources offered by cloud computing\ninfrastructures allows applications deployed in a cloud environment to\nautomatically increase and decrease the amount of used resources. This\ncapability is called auto-scaling and its main purpose is to automatically\nadjust the scale of the system that is running the application to satisfy the\nvarying workload with minimum resource utilization. The need for auto-scaling\nis particularly important during workload peaks, in which applications may need\nto scale up to extremely large-scale systems.\n  Both the research community and the main cloud providers have already\ndeveloped auto-scaling solutions. However, most research solutions are\ncentralized and not suitable for managing large-scale systems, moreover cloud\nproviders' solutions are bound to the limitations of a specific provider in\nterms of resource prices, availability, reliability, and connectivity.\n  In this paper we propose DEPAS, a decentralized probabilistic auto-scaling\nalgorithm integrated into a P2P architecture that is cloud provider\nindependent, thus allowing the auto-scaling of services over multiple cloud\ninfrastructures at the same time. Our simulations, which are based on real\nservice traces, show that our approach is capable of: (i) keeping the overall\nutilization of all the instantiated cloud resources in a target range, (ii)\nmaintaining service response times close to the ones obtained using optimal\ncentralized auto-scaling approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.2551v1", 
    "other_authors": "Ciprian Dobre, Florin Pop, Valentin Cristea", 
    "title": "A Simulation Model for Evaluating Distributed Systems Dependability", 
    "arxiv-id": "1202.2551v1", 
    "author": "Valentin Cristea", 
    "publish": "2012-02-12T18:17:10Z", 
    "summary": "In this paper we present a new simulation model designed to evaluate the\ndependability in distributed systems. This model extends the MONARC simulation\nmodel with new capabilities for capturing reliability, safety, availability,\nsecurity, and maintainability requirements. The model has been implemented as\nan extension of the multithreaded, process oriented simulator MONARC, which\nallows the realistic simulation of a wide-range of distributed system\ntechnologies, with respect to their specific components and characteristics.\nThe extended simulation model includes the necessary components to inject\nvarious failure events, and provides the mechanisms to evaluate different\nstrategies for replication, redundancy procedures, and security enforcement\nmechanisms, as well. The results obtained in simulation experiments presented\nin this paper probe that the use of discrete-event simulators, such as MONARC,\nin the design and development of distributed systems is appealing due to their\nefficiency and scalability."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.2981v1", 
    "other_authors": "Bogdan Alexandru Caprarescu, Eva Kaslik, Dana Petcu", 
    "title": "Theoretical Analysis and Tuning of Decentralized Probabilistic   Auto-Scaling", 
    "arxiv-id": "1202.2981v1", 
    "author": "Dana Petcu", 
    "publish": "2012-02-14T10:21:51Z", 
    "summary": "A major impediment towards the industrial adoption of decentralized\ndistributed systems comes from the difficulty to theoretically prove that these\nsystems exhibit the required behavior. In this paper, we use probability theory\nto analyze a decentralized auto-scaling algorithm in which each node\nprobabilistically decides to scale in or out. We prove that, in the context of\ndynamic workloads, the average load of the system is maintained within a\nvariation interval with a given probability, provided that the number of nodes\nand the variation interval length are higher than certain bounds. The paper\nalso proposes numerical algorithms for approximating these minimum bounds."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1202.3084v2", 
    "other_authors": "Rachid Guerraoui, Florian Huc, Anne-Marie Kermarrec", 
    "title": "On Dynamic Distributed Computing", 
    "arxiv-id": "1202.3084v2", 
    "author": "Anne-Marie Kermarrec", 
    "publish": "2012-02-14T16:50:12Z", 
    "summary": "This paper shows for the first time that distributed computing can be both\nreliable and efficient in an environment that is both highly dynamic and\nhostile. More specifically, we show how to maintain clusters of size $O(\\log\nN)$, each containing more than two thirds of honest nodes with high\nprobability, within a system whose size can vary \\textit{polynomially} with\nrespect to its initial size. Furthermore, the communication cost induced by\neach node arrival or departure is polylogarithmic with respect to $N$, the\nmaximal size of the system. Our clustering can be achieved despite the presence\nof a Byzantine adversary controlling a fraction $\\bad \\leq \\{1}{3}-\\epsilon$ of\nthe nodes, for some fixed constant $\\epsilon > 0$, independent of $N$. So far,\nsuch a clustering could only be performed for systems who size can vary\nconstantly and it was not clear whether that was at all possible for polynomial\nvariances."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TPDS.2012.239", 
    "link": "http://arxiv.org/pdf/1202.3669v2", 
    "other_authors": "Samer Al-Kiswany, Abdullah Gharaibeh, Matei Ripeanu", 
    "title": "GPUs as Storage System Accelerators", 
    "arxiv-id": "1202.3669v2", 
    "author": "Matei Ripeanu", 
    "publish": "2012-02-16T19:08:29Z", 
    "summary": "Massively multicore processors, such as Graphics Processing Units (GPUs),\nprovide, at a comparable price, a one order of magnitude higher peak\nperformance than traditional CPUs. This drop in the cost of computation, as any\norder-of-magnitude drop in the cost per unit of performance for a class of\nsystem components, triggers the opportunity to redesign systems and to explore\nnew ways to engineer them to recalibrate the cost-to-performance relation. This\nproject explores the feasibility of harnessing GPUs' computational power to\nimprove the performance, reliability, or security of distributed storage\nsystems. In this context, we present the design of a storage system prototype\nthat uses GPU offloading to accelerate a number of computationally intensive\nprimitives based on hashing, and introduce techniques to efficiently leverage\nthe processing power of GPUs. We evaluate the performance of this prototype\nunder two configurations: as a content addressable storage system that\nfacilitates online similarity detection between successive versions of the same\nfile and as a traditional system that uses hashing to preserve data integrity.\nFurther, we evaluate the impact of offloading to the GPU on competing\napplications' performance. Our results show that this technique can bring\ntangible performance gains without negatively impacting the performance of\nconcurrently running applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TPDS.2012.239", 
    "link": "http://arxiv.org/pdf/1202.3943v1", 
    "other_authors": "Daniel S. Katz, Timothy G. Armstrong, Zhao Zhang, Michael Wilde, Justin M. Wozniak", 
    "title": "Many-Task Computing and Blue Waters", 
    "arxiv-id": "1202.3943v1", 
    "author": "Justin M. Wozniak", 
    "publish": "2012-02-17T16:01:53Z", 
    "summary": "This report discusses many-task computing (MTC) generically and in the\ncontext of the proposed Blue Waters systems, which is planned to be the largest\nNSF-funded supercomputer when it begins production use in 2012. The aim of this\nreport is to inform the BW project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies.\nMany MTC applications do not neatly fit the stereotypes of high-performance\ncomputing (HPC) or high-throughput computing (HTC) applications. Like HTC\napplications, by definition MTC applications are structured as graphs of\ndiscrete tasks, with explicit input and output dependencies forming the graph\nedges. However, MTC applications have significant features that distinguish\nthem from typical HTC applications. In particular, different engineering\nconstraints for hardware and software must be met in order to support these\napplications. HTC applications have traditionally run on platforms such as\ngrids and clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time to\nsolution, may be communication intensive or data intensive, and may comprise\nvery short tasks. Therefore, hardware and software for MTC must be engineered\nto support the additional communication and I/O and must minimize task dispatch\noverheads. The hardware of large-scale HPC systems, with its high degree of\nparallelism and support for intensive communication, is well suited for MTC\napplications. However, HPC systems often lack a dynamic resource-provisioning\nfeature, are not ideal for task communication via the file system, and have an\nI/O system that is not optimized for MTC-style applications. Hence, additional\nsoftware support is likely to be required to gain full benefit from the HPC\nhardware."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3109", 
    "link": "http://arxiv.org/pdf/1202.4347v1", 
    "other_authors": "Jayshree Ghorpade, Jitendra Parande, Madhura Kulkarni, Amit Bawaskar", 
    "title": "GPGPU Processing in CUDA Architecture", 
    "arxiv-id": "1202.4347v1", 
    "author": "Amit Bawaskar", 
    "publish": "2012-02-20T15:16:40Z", 
    "summary": "The future of computation is the Graphical Processing Unit, i.e. the GPU. The\npromise that the graphics cards have shown in the field of image processing and\naccelerated rendering of 3D scenes, and the computational capability that these\nGPUs possess, they are developing into great parallel computing units. It is\nquite simple to program a graphics processor to perform general parallel tasks.\nBut after understanding the various architectural aspects of the graphics\nprocessor, it can be used to perform other taxing tasks as well. In this paper,\nwe will show how CUDA can fully utilize the tremendous power of these GPUs.\nCUDA is NVIDIA's parallel computing architecture. It enables dramatic increases\nin computing performance, by harnessing the power of the GPU. This paper talks\nabout CUDA and its architecture. It takes us through a comparison of CUDA C/C++\nwith other parallel programming languages like OpenCL and DirectCompute. The\npaper also lists out the common myths about CUDA and how the future seems to be\npromising for CUDA."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.4508v1", 
    "other_authors": "Johannes Reich", 
    "title": "Processes, Roles and Their Interactions", 
    "arxiv-id": "1202.4508v1", 
    "author": "Johannes Reich", 
    "publish": "2012-02-21T01:41:49Z", 
    "summary": "Taking an interaction network oriented perspective in informatics raises the\nchallenge to describe deterministic finite systems which take part in networks\nof nondeterministic interactions. The traditional approach to describe\nprocesses as stepwise executable activities which are not based on the\nordinarily nondeterministic interaction shows strong centralization tendencies.\nAs suggested in this article, viewing processes and their interactions as\ncomplementary can circumvent these centralization tendencies.\n  The description of both, processes and their interactions is based on the\nsame building blocks, namely finite input output automata (or transducers).\nProcesses are viewed as finite systems that take part in multiple, ordinarily\nnondeterministic interactions. The interactions between processes are described\nas protocols.\n  The effects of communication between processes as well as the necessary\ncoordination of different interactions within a processes are both based on the\nrestriction of the transition relation of product automata. The channel based\nouter coupling represents the causal relation between the output and the input\nof different systems. The coordination condition based inner coupling\nrepresents the causal relation between the input and output of a single system.\n  All steps are illustrated with the example of a network of resource\nadministration processes which is supposed to provide requesting user processes\nexclusive access to a single resource."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5261v4", 
    "other_authors": "Vladimir Savic, Henk Wymeersch, Santiago Zazo", 
    "title": "Belief Consensus Algorithms for Fast Distributed Target Tracking in   Wireless Sensor Networks", 
    "arxiv-id": "1202.5261v4", 
    "author": "Santiago Zazo", 
    "publish": "2012-02-23T18:32:59Z", 
    "summary": "In distributed target tracking for wireless sensor networks, agreement on the\ntarget state can be achieved by the construction and maintenance of a\ncommunication path, in order to exchange information regarding local likelihood\nfunctions. Such an approach lacks robustness to failures and is not easily\napplicable to ad-hoc networks. To address this, several methods have been\nproposed that allow agreement on the global likelihood through fully\ndistributed belief consensus (BC) algorithms, operating on local likelihoods in\ndistributed particle filtering (DPF). However, a unified comparison of the\nconvergence speed and communication cost has not been performed. In this paper,\nwe provide such a comparison and propose a novel BC algorithm based on belief\npropagation (BP). According to our study, DPF based on metropolis belief\nconsensus (MBC) is the fastest in loopy graphs, while DPF based on BP consensus\nis the fastest in tree graphs. Moreover, we found that BC-based DPF methods\nhave lower communication overhead than data flooding when the network is\nsufficiently sparse."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5482v2", 
    "other_authors": "Hanene Boussi Rahmouni, Kamran Munir, Mohammed Odeh, Richard McClatchey", 
    "title": "Risk-Driven Compliant Access Controls for Clouds", 
    "arxiv-id": "1202.5482v2", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T15:49:39Z", 
    "summary": "There is widespread agreement that cloud computing have proven cost cutting\nand agility benefits. However, security and regulatory compliance issues are\ncontinuing to challenge the wide acceptance of such technology both from social\nand commercial stakeholders. An important facture behind this is the fact that\nclouds and in particular public clouds are usually deployed and used within\nbroad geographical or even international domains. This implies that the\nexchange of private and other protected data within the cloud environment would\nbe governed by multiple jurisdictions. These jurisdictions have a great degree\nof harmonisation; however, they present possible conflicts that are hard to\nnegotiate at run time. So far, important efforts were played in order to deal\nwith regulatory compliance management for large distributed systems. However,\nmeasurable solutions are required for the context of cloud. In this position\npaper, we are suggesting an approach that starts with a conceptual model of\nexplicit regulatory requirements for exchanging private data on a\nmultijurisdictional environment and build on it in order to define metrics for\nnon-compliance or, in other terms, risks to compliance. These metrics will be\nintegrated within usual data access-control policies and will be checked at\npolicy analysis time before a decision to allow/deny the data access is made."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5483v1", 
    "other_authors": "Zaheer Khan, David Ludlow, Richard McClatchey, Ashiq Anjum", 
    "title": "An Architecture for Integrated Intelligence in Urban Management using   Cloud Computing", 
    "arxiv-id": "1202.5483v1", 
    "author": "Ashiq Anjum", 
    "publish": "2012-02-24T15:52:56Z", 
    "summary": "With the emergence of new methodologies and technologies it has now become\npossible to manage large amounts of environmental sensing data and apply new\nintegrated computing models to acquire information intelligence. This paper\nadvocates the application of cloud capacity to support the information,\ncommunication and decision making needs of a wide variety of stakeholders in\nthe complex business of the management of urban and regional development. The\ncomplexity lies in the interactions and impacts embodied in the concept of the\nurban-ecosystem at various governance levels. This highlights the need for more\neffective integrated environmental management systems. This paper offers a\nuser-orientated approach based on requirements for an effective management of\nthe urban-ecosystem and the potential contributions that can be supported by\nthe cloud computing community. Furthermore, the commonality of the influence of\nthe drivers of change at the urban level offers the opportunity for the cloud\ncomputing community to develop generic solutions that can serve the needs of\nhundreds of cities from Europe and indeed globally."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5512v1", 
    "other_authors": "Asif Osman, Ashiq Anjum, Naheed Batool, Richard McClatchey", 
    "title": "A Fault Tolerant, Dynamic and Low Latency BDII Architecture for Grids", 
    "arxiv-id": "1202.5512v1", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T17:51:35Z", 
    "summary": "The current BDII model relies on information gathering from agents that run\non each core node of a Grid. This information is then published into a Grid\nwide information resource known as Top BDII. The Top level BDIIs are updated\ntypically in cycles of a few minutes each. A new BDDI architecture is proposed\nand described in this paper based on the hypothesis that only a few attribute\nvalues change in each BDDI information cycle and consequently it may not be\nnecessary to update each parameter in a cycle. It has been demonstrated that\nsignificant performance gains can be achieved by exchanging only the\ninformation about records that changed during a cycle. Our investigations have\nled us to implement a low latency and fault tolerant BDII system that involves\nonly minimal data transfer and facilitates secure transactions in a Grid\nenvironment."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5519v1", 
    "other_authors": "Saad Liaquat Kiani, Ashiq Anjum, Nick Antonopoulos, Michael Knappmeyer, Nigel Baker, Richard McClatchey", 
    "title": "Context-Aware Service Utilisation in the Clouds and Energy Conservation", 
    "arxiv-id": "1202.5519v1", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T18:21:23Z", 
    "summary": "Ubiquitous computing environments are characterised by smart, interconnected\nartefacts embedded in our physical world that are projected to provide useful\nservices to human inhabitants unobtrusively. Mobile devices are becoming the\nprimary tools of human interaction with these embedded artefacts and\nutilisation of services available in smart computing environments such as\nclouds. Advancements in capabilities of mobile devices allow a number of user\nand environment related context consumers to be hosted on these devices.\nWithout a coordinating component, these context consumers and providers are a\npotential burden on device resources; specifically the effect of uncoordinated\ncomputation and communication with cloud-enabled services can negatively impact\nthe battery life. Therefore energy conservation is a major concern in realising\nthe collaboration and utilisation of mobile device based context-aware\napplications and cloud based services. This paper presents the concept of a\ncontext-brokering component to aid in coordination and communication of context\ninformation between mobile devices and services deployed in a cloud\ninfrastructure. A prototype context broker is experimentally analysed for\neffects on energy conservation when accessing and coordinating with cloud\nservices on a smart device, with results signifying reduction in energy\nconsumption."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.6094v2", 
    "other_authors": "Nitin Vaidya, Lewis Tseng, Guanfeng Liang", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -   Part II: Synchronous and Asynchronous Systems", 
    "arxiv-id": "1202.6094v2", 
    "author": "Guanfeng Liang", 
    "publish": "2012-02-28T00:01:48Z", 
    "summary": "This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1202.6134v2", 
    "other_authors": "Jianfeng Zhan, Lixin Zhang, Ninghui Sun, Lei Wang, Zhen Jia, Chunjie Luo", 
    "title": "High Volume Computing: Identifying and Characterizing Throughput   Oriented Workloads in Data Centers", 
    "arxiv-id": "1202.6134v2", 
    "author": "Chunjie Luo", 
    "publish": "2012-02-28T06:37:31Z", 
    "summary": "For the first time, this paper systematically identifies three categories of\nthroughput oriented workloads in data centers: services, data processing\napplications, and interactive real-time applications, whose targets are to\nincrease the volume of throughput in terms of processed requests or data, or\nsupported maximum number of simultaneous subscribers, respectively, and we coin\na new term high volume computing (in short HVC) to describe those workloads and\ndata center computer systems designed for them. We characterize and compare HVC\nwith other computing paradigms, e.g., high throughput computing,\nwarehouse-scale computing, and cloud computing, in terms of levels, workloads,\nmetrics, coupling degree, data scales, and number of jobs or service instances.\nWe also preliminarily report our ongoing work on the metrics and benchmarks for\nHVC systems, which is the foundation of designing innovative data center\ncomputer systems for HVC workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1202.6456v1", 
    "other_authors": "Valerie King, Jared Saia, Maxwell Young", 
    "title": "Resource-Competitive Communication", 
    "arxiv-id": "1202.6456v1", 
    "author": "Maxwell Young", 
    "publish": "2012-02-29T06:28:53Z", 
    "summary": "Consider the general scenario where Alice wishes to transmit a message m to\nBob. These two players share a communication channel; however, there exists an\nadversary, Carol, who aims to prevent the transmission of m by blocking this\nchannel. There are costs to send, receive or block m on the channel, and we\nask: How much do Alice and Bob need to spend relative to the adversary Carol in\norder to guarantee transmission of m?\n  We show that in a time-slotted network with constant costs to send, receive\nand block m in a slot, if Carol spends a total of B slots trying to block m,\nthen both Alice and Bob must be active for only O(B^{\\varphi - 1} +\n1)=O(B^{.62}+1) slots in expectation to transmit m, where \\varphi = (1 +\n\\sqrt{5})/2 is the golden ratio. Surprisingly, this result holds even if (1) B\nis unknown to either player; (2) Carol knows the algorithms of both players,\nbut not their random bits; and (3) Carol can attack using total knowledge of\npast actions of both players.\n  In the spirit of competitive analysis, approximation guarantees, and\ngame-theoretic treatments, our approach represents another notion of relative\nperformance that we call resource competitiveness. This new metric measures the\nworst-case performance of an algorithm relative to any adversarial strategy and\npertains to scenarios where all network devices are resource-constrained. Here,\nwe apply the resource-competitive results above to two concrete problems.\nFirst, we consider jamming attacks in WSNs and address the fundamental task of\npropagating m from a single device to all others in the presence of faults.\nSecond, we examine how to mitigate application-level DDoS attacks in a wired\nclient-server scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1205.0451v2", 
    "other_authors": "Saeid Abolfazli, Zohreh Sanaei, Abdullah Gani", 
    "title": "Mobile Cloud Computing: A Review on Smartphone Augmentation Approaches", 
    "arxiv-id": "1205.0451v2", 
    "author": "Abdullah Gani", 
    "publish": "2012-05-02T15:04:04Z", 
    "summary": "Smartphones have recently gained significant popularity in heavy mobile\nprocessing while users are increasing their expectations toward rich computing\nexperience. However, resource limitations and current mobile computing\nadvancements hinder this vision. Therefore, resource-intensive application\nexecution remains a challenging task in mobile computing that necessitates\ndevice augmentation. In this article, smartphone augmentation approaches are\nreviewed and classified in two main groups, namely hardware and software.\nGenerating high-end hardware is a subset of hardware augmentation approaches,\nwhereas conserving local resource and reducing resource requirements approaches\nare grouped under software augmentation methods. Our study advocates that\nconsreving smartphones' native resources, which is mainly done via task\noffloading, is more appropriate for already-developed applications than new\nones, due to costly re-development process. Cloud computing has recently\nobtained momentous ground as one of the major cornerstone technologies in\naugmenting smartphones. We present sample execution model for intensive mobile\napplications and devised taxonomy of augmentation approaches. For better\ncomprehension, the results of this study are summarized in a table."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1205.0652v2", 
    "other_authors": "Peiyan Yuan, Huadong Ma, Shaojie Tang", 
    "title": "On Exploiting Hotspot and Entropy for Data Forwarding in Delay Tolerant   Networks", 
    "arxiv-id": "1205.0652v2", 
    "author": "Shaojie Tang", 
    "publish": "2012-05-03T09:02:44Z", 
    "summary": "Performance of data forwarding in Delay Tolerant Networks (DTNs) benefits\nconsiderably if one can make use of human mobility in terms of social\nstructures. However, it is difficult and time-consuming to calculate the\ncentrality and similarity of nodes by using solutions for traditional social\nnetworks, this is mainly because of the transient node contact and the\nintermittently connected environment. In this work, we are interested in the\nfollowing question: Can we explore some other stable social attributes to\nquantify the centrality and similarity of nodes? Taking GPS traces of human\nwalks from the real world, we find that there exist two known phenomena. One is\npublic hotspot, the other is personal hotspot. Motivated by this observation,\nwe present Hoten (hotspot and entropy), a novel routing metric to improve\nrouting performance in DTNs. First, we use the relative entropy between the\npublic hotspots and the personal hotspots to compute the centrality of nodes.\nThen we utilize the inverse symmetrized entropy of the personal hotspots\nbetween two nodes to compute the similarity between them. Third, we exploit the\nentropy of personal hotspots of a node to estimate its personality. Besides, we\npropose a method to ascertain the optimized size of hotspot. Finally, we\ncompare our routing strategy with other state-of-the-art routing schemes\nthrough extensive trace-driven simulations, the results show that Hoten largely\noutperforms other solutions, especially in terms of combined overhead/packet\ndelivery ratio and the average number of hops per message."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.2546v1", 
    "other_authors": "James William Smith, Ali Khajeh-Hosseini, Jonathan Stuart Ward, Ian Sommerville", 
    "title": "CloudMonitor: Profiling Power Usage", 
    "arxiv-id": "1205.2546v1", 
    "author": "Ian Sommerville", 
    "publish": "2012-05-11T15:00:53Z", 
    "summary": "In Cloud Computing platforms the addition of hardware monitoring devices to\ngather power usage data can be impractical or uneconomical due to the large\nnumber of machines to be metered. CloudMonitor, a monitoring tool that can\ngenerate power models for software-based power estimation, can provide insights\nto the energy costs of deployments without additional hardware. Accurate power\nusage data leads to the possibility of Cloud providers creating a separate\ntariff for power and therefore incentivizing software developers to create\nenergy-efficient applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3247v1", 
    "other_authors": "Zohreh Sanaei, Saeid Abolfazli, Abdullah Gani, Rashid Hafeez Khokhar", 
    "title": "Tripod of Requirements in Horizontal Heterogeneous Mobile Cloud   Computing", 
    "arxiv-id": "1205.3247v1", 
    "author": "Rashid Hafeez Khokhar", 
    "publish": "2012-05-15T03:29:23Z", 
    "summary": "Recent trend of mobile computing is emerging toward executing\nresource-intensive applications in mobile devices regardless of underlying\nresource restrictions (e.g. limited processor and energy) that necessitate\nimminent technologies. Prosperity of cloud computing in stationary computers\nbreeds Mobile Cloud Computing (MCC) technology that aims to augment computing\nand storage capabilities of mobile devices besides conserving energy. However,\nMCC is more heterogeneous and unreliable (due to wireless connectivity) compare\nto cloud computing. Problems like variations in OS, data fragmentation, and\nsecurity and privacy discourage and decelerate implementation and pervasiveness\nof MCC. In this paper, we describe MCC as a horizontal heterogeneous ecosystem\nand identify thirteen critical metrics and approaches that influence on\nmobile-cloud solutions and success of MCC. We divide them into three major\nclasses, namely ubiquity, trust, and energy efficiency and devise a tripod of\nrequirements in MCC. Our proposed tripod shows that success of MCC is\nachievable by reducing mobility challenges (e.g. seamless connectivity,\nfragmentation), increasing trust, and enhancing energy efficiency."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3797v1", 
    "other_authors": "Ulric J. Ferner, Muriel Medard, Emina Soljanin", 
    "title": "Toward Sustainable Networking: Storage Area Networks with Network Coding", 
    "arxiv-id": "1205.3797v1", 
    "author": "Emina Soljanin", 
    "publish": "2012-05-16T20:15:49Z", 
    "summary": "This manuscript provides a model to characterize the energy savings of\nnetwork coded storage (NCS) in storage area networks (SANs). We consider\nblocking probability of drives as our measure of performance. A mapping\ntechnique to analyze SANs as independent M/G/K/K queues is presented, and\nblocking probabilities for uncoded storage schemes and NCS are derived and\ncompared. We show that coding operates differently than the amalgamation of\nfile chunks and energy savings are shown to scale well with striping number. We\nillustrate that for enterprise-level SANs energy savings of 20-50% can be\nrealized."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3809v1", 
    "other_authors": "Umit Catalyurek, John Feo, Assefaw Gebremedhin, Mahantesh Halappanavar, Alex Pothen", 
    "title": "Graph Coloring Algorithms for Muti-core and Massively Multithreaded   Architectures", 
    "arxiv-id": "1205.3809v1", 
    "author": "Alex Pothen", 
    "publish": "2012-05-16T20:59:48Z", 
    "summary": "We explore the interplay between architectures and algorithm design in the\ncontext of shared-memory platforms and a specific graph problem of central\nimportance in scientific and high-performance computing, distance-1 graph\ncoloring. We introduce two different kinds of multithreaded heuristic\nalgorithms for the stated, NP-hard, problem. The first algorithm relies on\nspeculation and iteration, and is suitable for any shared-memory system. The\nsecond algorithm uses dataflow principles, and is targeted at the\nnon-conventional, massively multithreaded Cray XMT system. We study the\nperformance of the algorithms on the Cray XMT and two multi-core systems, Sun\nNiagara 2 and Intel Nehalem. Together, the three systems represent a spectrum\nof multithreading capabilities and memory structure. As testbed, we use\nsynthetically generated large-scale graphs carefully chosen to cover a wide\nrange of input types. The results show that the algorithms have scalable\nruntime performance and use nearly the same number of colors as the underlying\nserial algorithm, which in turn is effective in practice. The study provides\ninsight into the design of high performance algorithms for irregular problems\non many-core architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.3830v2", 
    "other_authors": "Andrew Lucas, Mark Stalzer, John Feo", 
    "title": "Parallel implementation of fast randomized algorithms for the   decomposition of low rank matrices", 
    "arxiv-id": "1205.3830v2", 
    "author": "John Feo", 
    "publish": "2012-05-16T23:41:33Z", 
    "summary": "We analyze the parallel performance of randomized interpolative decomposition\nby decomposing low rank complex-valued Gaussian random matrices up to 64 GB. We\nchose a Cray XMT supercomputer as it provides an almost ideal PRAM model\npermitting quick investigation of parallel algorithms without obfuscation from\nhardware idiosyncrasies. We obtain that on non-square matrices performance\nbecomes very good, with overall runtime over 70 times faster on 128 processors.\nWe also verify that numerically discovered error bounds still hold on matrices\nnearly two orders of magnitude larger than those previously tested."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4545v1", 
    "other_authors": "Ofer Feinerman, Amos Korman", 
    "title": "Memory Lower Bounds for Randomized Collaborative Search and Applications   to Biology", 
    "arxiv-id": "1205.4545v1", 
    "author": "Amos Korman", 
    "publish": "2012-05-21T09:52:57Z", 
    "summary": "Initial knowledge regarding group size can be crucial for collective\nperformance. We study this relation in the context of the {\\em Ants Nearby\nTreasure Search (ANTS)} problem \\cite{FKLS}, which models natural cooperative\nforaging behavior such as that performed by ants around their nest. In this\nproblem, $k$ (probabilistic) agents, initially placed at some central location,\ncollectively search for a treasure on the two-dimensional grid. The treasure is\nplaced at a target location by an adversary and the goal is to find it as fast\nas possible as a function of both $k$ and $D$, where $D$ is the (unknown)\ndistance between the central location and the target. It is easy to see that\n$T=\\Omega(D+D^2/k)$ time units are necessary for finding the treasure.\nRecently, it has been established that $O(T)$ time is sufficient if the agents\nknow their total number $k$ (or a constant approximation of it), and enough\nmemory bits are available at their disposal \\cite{FKLS}. In this paper, we\nestablish lower bounds on the agent memory size required for achieving certain\nrunning time performances. To the best our knowledge, these bounds are the\nfirst non-trivial lower bounds for the memory size of probabilistic searchers.\nFor example, for every given positive constant $\\epsilon$, terminating the\nsearch by time $O(\\log^{1-\\epsilon} k \\cdot T)$ requires agents to use\n$\\Omega(\\log\\log k)$ memory bits. Such distributed computing bounds may provide\na novel, strong tool for the investigation of complex biological systems."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4809v1", 
    "other_authors": "Lewis Tseng, Nitin Vaidya", 
    "title": "Iterative Approximate Byzantine Consensus under a Generalized Fault   Model", 
    "arxiv-id": "1205.4809v1", 
    "author": "Nitin Vaidya", 
    "publish": "2012-05-22T05:43:53Z", 
    "summary": "In this work, we consider a generalized fault model that can be used to\nrepresent a wide range of failure scenarios, including correlated failures and\nnon-uniform node reliabilities. This fault model is general in the sense that\nfault models studied in prior related work, such as f -total and f -local\nmodels, are special cases of the generalized fault model. Under the generalized\nfault model, we explore iterative approximate Byzantine consensus (IABC)\nalgorithms in arbitrary directed networks. We prove a necessary and sufficient\ncondition for the existence of IABC algorithms. The use of the generalized\nfault model helps to gain a better understanding of IABC algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4883v1", 
    "other_authors": "Gang Liao, Lian Luo, Lei Liu", 
    "title": "Hybrid Parallel Bidirectional Sieve based on SMP Cluster", 
    "arxiv-id": "1205.4883v1", 
    "author": "Lei Liu", 
    "publish": "2012-05-22T11:27:10Z", 
    "summary": "In this article, hybrid parallel bidirectional sieve method is implemented by\nSMP Cluster, the individual computational units joined together by the\ncommunication network, are usually shared-memory systems with one or more\nmulticore processor. To high-efficiency optimization, we propose average divide\ndata into nodes, generating double-ended queues (deque) for sieve method that\nare able to exploit dual-cores simultaneously start sifting out primes from the\nhead and tail.And each node create a FIFO queue as dynamic data buffer to ache\ntemporary data from another nodes send to. The approach obtains huge speedup\nand efficiency on SMP Cluster."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.5055v1", 
    "other_authors": "Matthew Anderson, Maciej Brodowicz, Hartmut Kaiser, Bryce Adelstein-Lelbach, Thomas Sterling", 
    "title": "Neutron Star Evolutions using Tabulated Equations of State with a New   Execution Model", 
    "arxiv-id": "1205.5055v1", 
    "author": "Thomas Sterling", 
    "publish": "2012-05-22T20:46:11Z", 
    "summary": "The addition of nuclear and neutrino physics to general relativistic fluid\ncodes allows for a more realistic description of hot nuclear matter in neutron\nstar and black hole systems. This additional microphysics requires that each\nprocessor have access to large tables of data, such as equations of state, and\nin large simulations the memory required to store these tables locally can\nbecome excessive unless an alternative execution model is used. In this work we\npresent relativistic fluid evolutions of a neutron star obtained using a\nmessage driven multi-threaded execution model known as ParalleX. These neutron\nstar simulations would require substantial memory overhead dedicated entirely\nto the equation of state table if using a more traditional execution model. We\nintroduce a ParalleX component based on Futures for accessing large tables of\ndata, including out-of-core sized tables, which does not require substantial\nmemory overhead and effectively hides any increased network latency."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1205.6249v1", 
    "other_authors": "Dariusz Dereniowski, Andrzej Pelc", 
    "title": "Leader Election for Anonymous Asynchronous Agents in Arbitrary Networks", 
    "arxiv-id": "1205.6249v1", 
    "author": "Andrzej Pelc", 
    "publish": "2012-05-29T02:27:35Z", 
    "summary": "We study the problem of leader election among mobile agents operating in an\narbitrary network modeled as an undirected graph. Nodes of the network are\nunlabeled and all agents are identical. Hence the only way to elect a leader\namong agents is by exploiting asymmetries in their initial positions in the\ngraph. Agents do not know the graph or their positions in it, hence they must\ngain this knowledge by navigating in the graph and share it with other agents\nto accomplish leader election. This can be done using meetings of agents, which\nis difficult because of their asynchronous nature: an adversary has total\ncontrol over the speed of agents. When can a leader be elected in this\nadversarial scenario and how to do it? We give a complete answer to this\nquestion by characterizing all initial configurations for which leader election\nis possible and by constructing an algorithm that accomplishes leader election\nfor all configurations for which this can be done."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1206.0089v1", 
    "other_authors": "Chuanyou Li, Michel Hurfin, Yun Wang", 
    "title": "Reaching Approximate Byzantine Consensus in Partially-Connected Mobile   Networks", 
    "arxiv-id": "1206.0089v1", 
    "author": "Yun Wang", 
    "publish": "2012-06-01T06:14:29Z", 
    "summary": "We consider the problem of approximate consensus in mobile networks\ncontaining Byzantine nodes. We assume that each correct node can communicate\nonly with its neighbors and has no knowledge of the global topology. As all\nnodes have moving ability, the topology is dynamic. The number of Byzantine\nnodes is bounded by f and known by all correct nodes. We first introduce an\napproximate Byzantine consensus protocol which is based on the linear iteration\nmethod. As nodes are allowed to collect information during several consecutive\nrounds, moving gives them the opportunity to gather more values. We propose a\nnovel sufficient and necessary condition to guarantee the final convergence of\nthe consensus protocol. The requirement expressed by our condition is not\n\"universal\": in each phase it affects only a single correct node. More\nprecisely, at least one correct node among those that propose either the\nminimum or the maximum value which is present in the network, has to receive\nenough messages (quantity constraint) with either higher or lower values\n(quality constraint). Of course, nodes' motion should not prevent this\nrequirement to be fulfilled. Our conclusion shows that the proposed condition\ncan be satisfied if the total number of nodes is greater than 3f+1."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1206.0115v1", 
    "other_authors": "Emmanuel Agullo, B\u00e9ranger Bramas, Olivier Coulaud, Eric Darve, Matthias Messner, Takahashi Toru", 
    "title": "Pipelining the Fast Multipole Method over a Runtime System", 
    "arxiv-id": "1206.0115v1", 
    "author": "Takahashi Toru", 
    "publish": "2012-06-01T08:05:39Z", 
    "summary": "Fast Multipole Methods (FMM) are a fundamental operation for the simulation\nof many physical problems. The high performance design of such methods usually\nrequires to carefully tune the algorithm for both the targeted physics and the\nhardware. In this paper, we propose a new approach that achieves high\nperformance across architectures. Our method consists of expressing the FMM\nalgorithm as a task flow and employing a state-of-the-art runtime system,\nStarPU, in order to process the tasks on the different processing units. We\ncarefully design the task flow, the mathematical operators, their Central\nProcessing Unit (CPU) and Graphics Processing Unit (GPU) implementations, as\nwell as scheduling schemes. We compute potentials and forces of 200 million\nparticles in 48.7 seconds on a homogeneous 160 cores SGI Altix UV 100 and of 38\nmillion particles in 13.34 seconds on a heterogeneous 12 cores Intel Nehalem\nprocessor enhanced with 3 Nvidia M2090 Fermi GPUs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2011.1203", 
    "link": "http://arxiv.org/pdf/1206.0419v1", 
    "other_authors": "T. R. Gopalakrishnan Nair, P Jayarekha", 
    "title": "Pre-allocation Strategies of Computational Resources in Cloud Computing   using Adaptive Resonance Theory-2", 
    "arxiv-id": "1206.0419v1", 
    "author": "P Jayarekha", 
    "publish": "2012-06-03T04:42:23Z", 
    "summary": "One of the major challenges of cloud computing is the management of\nrequest-response coupling and optimal allocation strategies of computational\nresources for the various types of service requests. In the normal situations\nthe intelligence required to classify the nature and order of the request using\nstandard methods is insufficient because the arrival of request is at a random\nfashion and it is meant for multiple resources with different priority order\nand variety. Hence, it becomes absolutely essential that we identify the trends\nof different request streams in every category by auto classifications and\norganize preallocation strategies in a predictive way. It calls for designs of\nintelligent modes of interaction between the client request and cloud computing\nresource manager. This paper discusses about the corresponding scheme using\nAdaptive Resonance Theory-2."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2011.1203", 
    "link": "http://arxiv.org/pdf/1206.0988v1", 
    "other_authors": "Mueen Uddin, Azizah Abdul Rahman", 
    "title": "Virtualization Implementation Model for Cost Effective & Efficient Data   Centers", 
    "arxiv-id": "1206.0988v1", 
    "author": "Azizah Abdul Rahman", 
    "publish": "2012-04-07T05:19:32Z", 
    "summary": "Data centers form a key part of the infrastructure upon which a variety of\ninformation technology services are built. They provide the capabilities of\ncentralized repository for storage, management, networking and dissemination of\ndata. With the rapid increase in the capacity and size of data centers, there\nis a continuous increase in the demand for energy consumption. These data\ncenters not only consume a tremendous amount of energy but are riddled with IT\ninefficiencies. Data center are plagued with thousands of servers as major\ncomponents. These servers consume huge energy without performing useful work.\nIn an average server environment, 30% of the servers are \"dead\" only consuming\nenergy, without being properly utilized. This paper proposes a five step model\nusing an emerging technology called virtualization to achieve energy efficient\ndata centers. The proposed model helps Data Center managers to properly\nimplement virtualization technology in their data centers to make them green\nand energy efficient so as to ensure that IT infrastructure contributes as\nlittle as possible to the emission of greenhouse gases, and helps to regain\npower and cooling capacity, recapture resilience and dramatically reducing\nenergy costs and total cost of ownership."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.0992v6", 
    "other_authors": "Guodong Shi, Bo Li, Mikael Johansson, Karl Henrik Johansson", 
    "title": "Finite-time Convergent Gossiping", 
    "arxiv-id": "1206.0992v6", 
    "author": "Karl Henrik Johansson", 
    "publish": "2012-06-05T17:07:34Z", 
    "summary": "Gossip algorithms are widely used in modern distributed systems, with\napplications ranging from sensor networks and peer-to-peer networks to mobile\nvehicle networks and social networks. A tremendous research effort has been\ndevoted to analyzing and improving the asymptotic rate of convergence for\ngossip algorithms. In this work we study finite-time convergence of\ndeterministic gossiping. We show that there exists a symmetric gossip algorithm\nthat converges in finite time if and only if the number of network nodes is a\npower of two, while there always exists an asymmetric gossip algorithm with\nfinite-time convergence, independent of the number of nodes. For $n=2^m$ nodes,\nwe prove that a fastest convergence can be reached in $nm=n\\log_2 n$ node\nupdates via symmetric gossiping. On the other hand, under asymmetric gossip\namong $n=2^m+r$ nodes with $0\\leq r<2^m$, it takes at least $mn+2r$ node\nupdates for achieving finite-time convergence. It is also shown that the\nexistence of finite-time convergent gossiping often imposes strong structural\nrequirements on the underlying interaction graph. Finally, we apply our results\nto gossip algorithms in quantum networks, where the goal is to control the\nstate of a quantum system via pairwise interactions. We show that finite-time\nconvergence is never possible for such systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1118v1", 
    "other_authors": "Han Qi, Abdullah Gani", 
    "title": "Research On Mobile Cloud Computing: Review, Trend, And Perspectives", 
    "arxiv-id": "1206.1118v1", 
    "author": "Abdullah Gani", 
    "publish": "2012-06-06T03:53:39Z", 
    "summary": "Mobile Cloud Computing (MCC) which combines mobile computing and cloud\ncomputing, has become one of the industry buzz words and a major discussion\nthread in the IT world since 2009. As MCC is still at the early stage of\ndevelopment, it is necessary to grasp a thorough understanding of the\ntechnology in order to point out the direction of future research. With the\nlatter aim, this paper presents a review on the background and principle of\nMCC, characteristics, recent research work, and future research trends. A brief\naccount on the background of MCC: from mobile computing to cloud computing is\npresented and then followed with a discussion on characteristics and recent\nresearch work. It then analyses the features and infrastructure of mobile cloud\ncomputing. The rest of the paper analyses the challenges of mobile cloud\ncomputing, summary of some research projects related to this area, and points\nout promising future research directions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1290v1", 
    "other_authors": "Othon Michail, Ioannis Chatzigiannakis, Paul G. Spirakis", 
    "title": "Causality, Influence, and Computation in Possibly Disconnected Dynamic   Networks", 
    "arxiv-id": "1206.1290v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2012-06-06T18:10:34Z", 
    "summary": "In this work, we study the propagation of influence and computation in\ndynamic distributed systems. We focus on broadcasting models under a worst-case\ndynamicity assumption which have received much attention recently. We drop for\nthe first time in worst-case dynamic networks the common instantaneous\nconnectivity assumption and require a minimal temporal connectivity. Our\ntemporal connectivity constraint only requires that another causal influence\noccurs within every time-window of some given length. We establish that there\nare dynamic graphs with always disconnected instances with equivalent temporal\nconnectivity to those with always connected instances. We present a termination\ncriterion and also establish the computational equivalence with instantaneous\nconnectivity networks. We then consider another model of dynamic networks in\nwhich each node has an underlying communication neighborhood and the\nrequirement is that each node covers its local neighborhood within any\ntime-window of some given length. We discuss several properties and provide a\nprotocol for counting, that is for determining the number of nodes in the\nnetwork."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1653v2", 
    "other_authors": "Stefano Braghin, Jackson Tan, Rajesh Sharma, Anwitaman Datta", 
    "title": "PriSM: A Private Social Mesh for Leveraging Social Networking at   Workplace", 
    "arxiv-id": "1206.1653v2", 
    "author": "Anwitaman Datta", 
    "publish": "2012-06-08T02:44:05Z", 
    "summary": "In this work we describe the PriSM framework for decentralized deployment of\na federation of autonomous social networks (ASN). The individual ASNs are\ncentrally managed by organizations according to their institutional needs,\nwhile cross-ASN interactions are facilitated subject to security and\nconfidentiality requirements specified by administrators and users of the ASNs.\nSuch decentralized deployment, possibly either on private or public clouds,\nprovides control and ownership of information/flow to individual organizations.\nLack of such complete control (if third party online social networking services\nwere to be used) has so far been a great barrier in taking full advantage of\nthe novel communication mechanisms at workplace that have however become\ncommonplace for personal usage with the advent of Web 2.0 platforms and online\nsocial networks. PriSM provides a practical solution for organizations to\nharness the advantages of online social networking both in\nintra/inter-organizational settings without sacrificing autonomy, security and\nconfidentiality needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1899v3", 
    "other_authors": "Rajeev K. Shakya", 
    "title": "TTMA: Traffic-adaptive Time-division Multiple Access Protocol Wireless   Sensor Networks", 
    "arxiv-id": "1206.1899v3", 
    "author": "Rajeev K. Shakya", 
    "publish": "2012-06-09T02:06:59Z", 
    "summary": "This paper has been withdrawn by arXiv. arXiv admin note: author list\ntruncated due to disputed authorship and content. This submission repeats large\nportions of text from this http URL by other authors. Duty cycle mode in WSN\nimproves energy-efficiency, but also introduces packet delivery latency.\nSeveral duty-cycle based MAC schemes have been proposed to reduce latency, but\nthroughput is limited by duty-cycled scheduling performance. In this paper, a\nTraffic-adaptive Time-division Multiple Access (TTMA), a distributed TDMA-based\nMAC protocol is introduced to improves the throughput by traffic-adaptive\ntime-slot scheduling that increases the channel utilisation efficiency. The\nproposed time-slot scheduling method first avoids time-slots assigned to nodes\nwith no traffic through fast traffic notification. It then achieves better\nchannel utilisation among nodes having traffic through an ordered schedule\nnegotiation scheme. By decomposing traffic notification and data transmission\nscheduling into two phases leads each phase to be simple and efficient. The\nperformance evaluation shows that the two-phase design significantly improves\nthe throughput and outperforms the time division multiple access (TDMA) control\nwith slot stealing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1984v1", 
    "other_authors": "Masnida Emami, Yashar Ghiasi, Nasrin Jaberi", 
    "title": "Energy-Aware Scheduling using Dynamic Voltage-Frequency Scaling", 
    "arxiv-id": "1206.1984v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-06-10T00:13:47Z", 
    "summary": "The energy consumption issue in distributed computing systems has become\nquite critical due to environmental concerns. In response to this, many\nenergy-aware scheduling algorithms have been developed primarily by using the\ndynamic voltage-frequency scaling (DVFS) capability incorporated in recent\ncommodity processors. The majority of these algorithms involve two passes:\nschedule generation and slack reclamation. The latter is typically achieved by\nlowering processor frequency for tasks with slacks. In this article, we study\nthe latest papers in this area and develop them. This study has been evaluated\nbased on results obtained from experiments with 1,500 randomly generated task\ngraphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.2187v1", 
    "other_authors": "Lluis Pamies-Juarez, Fr\u00e9d\u00e9rique Oggier, Anwitaman Datta", 
    "title": "An Empirical Study of the Repair Performance of Novel Coding Schemes for   Networked Distributed Storage Systems", 
    "arxiv-id": "1206.2187v1", 
    "author": "Anwitaman Datta", 
    "publish": "2012-06-11T12:55:53Z", 
    "summary": "Erasure coding techniques are getting integrated in networked distributed\nstorage systems as a way to provide fault-tolerance at the cost of less storage\noverhead than traditional replication. Redundancy is maintained over time\nthrough repair mechanisms, which may entail large network resource overheads.\nIn recent years, several novel codes tailor-made for distributed storage have\nbeen proposed to optimize storage overhead and repair, such as Regenerating\nCodes that minimize the per repair traffic, or Self-Repairing Codes which\nminimize the number of nodes contacted per repair. Existing studies of these\ncoding techniques are however predominantly theoretical, under the simplifying\nassumption that only one object is stored. They ignore many practical issues\nthat real systems must address, such as data placement, de/correlation of\nmultiple stored objects, or the competition for limited network resources when\nmultiple objects are repaired simultaneously. This paper empirically studies\nthe repair performance of these novel storage centric codes with respect to\nclassical erasure codes by simulating realistic scenarios and exploring the\ninterplay of code parameters, failure characteristics and data placement with\nrespect to the trade-offs of bandwidth usage and speed of repairs."
},{
    "category": "cs.DC", 
    "doi": "10.4108/icst.simutools.2012.247736", 
    "link": "http://arxiv.org/pdf/1206.2772v3", 
    "other_authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", 
    "title": "Time Warp on the Go (Updated Version)", 
    "arxiv-id": "1206.2772v3", 
    "author": "Moreno Marzolla", 
    "publish": "2012-06-13T11:48:48Z", 
    "summary": "In this paper we deal with the impact of multi and many-core processor\narchitectures on simulation. Despite the fact that modern CPUs have an\nincreasingly large number of cores, most softwares are still unable to take\nadvantage of them. In the last years, many tools, programming languages and\ngeneral methodologies have been proposed to help building scalable applications\nfor multi-core architectures, but those solutions are somewhat limited.\nParallel and distributed simulation is an interesting application area in which\nefficient and scalable multi-core implementations would be desirable. In this\npaper we investigate the use of the Go Programming Language to implement\noptimistic parallel simulations based on the Time Warp mechanism. Specifically,\nwe describe the design, implementation and evaluation of a new parallel\nsimulator. The scalability of the simulator is studied when in presence of a\nmodern multi-core CPU and the effects of the Hyper-Threading technology on\noptimistic simulation are analyzed."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.2775v3", 
    "other_authors": "Luca Toscano, Gabriele D'Angelo, Moreno Marzolla", 
    "title": "Parallel Discrete Event Simulation with Erlang", 
    "arxiv-id": "1206.2775v3", 
    "author": "Moreno Marzolla", 
    "publish": "2012-06-13T12:12:21Z", 
    "summary": "Discrete Event Simulation (DES) is a widely used technique in which the state\nof the simulator is updated by events happening at discrete points in time\n(hence the name). DES is used to model and analyze many kinds of systems,\nincluding computer architectures, communication networks, street traffic, and\nothers. Parallel and Distributed Simulation (PADS) aims at improving the\nefficiency of DES by partitioning the simulation model across multiple\nprocessing elements, in order to enabling larger and/or more detailed studies\nto be carried out. The interest on PADS is increasing since the widespread\navailability of multicore processors and affordable high performance computing\nclusters. However, designing parallel simulation models requires considerable\nexpertise, the result being that PADS techniques are not as widespread as they\ncould be. In this paper we describe ErlangTW, a parallel simulation middleware\nbased on the Time Warp synchronization protocol. ErlangTW is entirely written\nin Erlang, a concurrent, functional programming language specifically targeted\nat building distributed systems. We argue that writing parallel simulation\nmodels in Erlang is considerably easier than using conventional programming\nlanguages. Moreover, ErlangTW allows simulation models to be executed either on\nsingle-core, multicore and distributed computing architectures. We describe the\ndesign and prototype implementation of ErlangTW, and report some preliminary\nperformance results on multicore and distributed architectures using the well\nknown PHOLD benchmark."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.4175v1", 
    "other_authors": "Anne-Marie Kermarrec, Erwan Le Merrer, Gilles Straub, Alexandre van Kempen", 
    "title": "Clustered Network Coding for Maintenance in Practical Storage Systems", 
    "arxiv-id": "1206.4175v1", 
    "author": "Alexandre van Kempen", 
    "publish": "2012-06-19T11:01:42Z", 
    "summary": "Classical erasure codes, e.g. Reed-Solomon codes, have been acknowledged as\nan efficient alternative to plain replication to reduce the storage overhead in\nreliable distributed storage systems. Yet, such codes experience high overhead\nduring the maintenance process. In this paper we propose a novel erasure-coded\nframework especially tailored for networked storage systems. Our approach\nrelies on the use of random codes coupled with a clustered placement strategy,\nenabling the maintenance of a failed machine at the granularity of multiple\nfiles. Our repair protocol leverages network coding techniques to reduce by\nhalf the amount of data transferred during maintenance, as several files can be\nrepaired simultaneously. This approach, as formally proven and demonstrated by\nour evaluation on a public experimental testbed, enables to dramatically\ndecrease the bandwidth overhead during the maintenance process, as well as the\ntime to repair a failure. In addition, the implementation is made as simple as\npossible, aiming at a deployment into practical systems."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.5938v1", 
    "other_authors": "Adamu Murtala Zungeru, Li-Minn Ang, Kah Phooi Seng", 
    "title": "Performance Evaluation of Ant-Based Routing Protocols for Wireless   Sensor Networks", 
    "arxiv-id": "1206.5938v1", 
    "author": "Kah Phooi Seng", 
    "publish": "2012-06-26T09:27:57Z", 
    "summary": "High efficient routing is an important issue in the design of limited energy\nresource Wireless Sensor Networks (WSNs). Due to the characteristic of the\nenvironment at which the sensor node is to operate, coupled with severe\nresources; on-board energy, transmission power, processing capability, and\nstorage limitations, prompt for careful resource management and new routing\nprotocol so as to counteract the differences and challenges. To this end, we\npresent an Improved Energy-Efficient Ant-Based Routing (IEEABR) Algorithm in\nwireless sensor networks. Compared to the state-of-the-art Ant-Based routing\nprotocols; Basic Ant-Based Routing (BABR) Algorithm, Sensor-driven and\nCost-aware ant routing (SC), Flooded Forward ant routing (FF), Flooded\nPiggybacked ant routing (FP), and Energy-Efficient Ant-Based Routing (EEABR),\nthe proposed IEEABR approach has advantages in terms of reduced energy usage\nwhich can effectively balance the WSN node's power consumption, and high energy\nefficiency. The performance evaluations for the algorithms on a real\napplication are conducted in a well known WSN MATLAB-based simulator (RMASE)\nusing both static and dynamic scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6016v1", 
    "other_authors": "A. Anasuya Threse Innocent", 
    "title": "Cloud Infrastructure Service Management - A Review", 
    "arxiv-id": "1206.6016v1", 
    "author": "A. Anasuya Threse Innocent", 
    "publish": "2012-05-30T09:45:54Z", 
    "summary": "The new era of computing called Cloud Computing allows the user to access the\ncloud services dynamically over the Internet wherever and whenever needed.\nCloud consists of data and resources; and the cloud services include the\ndelivery of software, infrastructure, applications, and storage over the\nInternet based on user demand through Internet. In short, cloud computing is a\nbusiness and economic model allowing the users to utilize high-end computing\nand storage virtually with minimal infrastructure on their end. Cloud has three\nservice models namely, Cloud Software-as-a-Service (SaaS), Cloud\nPlatform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS).\nThis paper talks in depth of cloud infrastructure service management."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6207v1", 
    "other_authors": "Nikos Tziritas, Samee Ullah Khan, Cheng-Zhong Xu, Jue Hong", 
    "title": "An Optimal Fully Distributed Algorithm to Minimize the Resource   Consumption of Cloud Applications", 
    "arxiv-id": "1206.6207v1", 
    "author": "Jue Hong", 
    "publish": "2012-06-27T09:02:05Z", 
    "summary": "According to the pay-per-use model adopted in clouds, the more the resources\nconsumed by an application running in a cloud computing environment, the\ngreater the amount of money the owner of the corresponding application will be\ncharged. Therefore, applying intelligent solutions to minimize the resource\nconsumption is of great importance. Because centralized solutions are deemed\nunsuitable for large-distributed systems or large-scale applications, we\npropose a fully distributed algorithm (called DRA) to overcome the scalability\nissues. Specifically, DRA migrates the inter-communicating components of an\napplication, such as processes or virtual machines, close to each other to\nminimize the total resource consumption. The migration decisions are made in a\ndynamic way and based only on local information. We prove that DRA achieves\nconvergence and results always in the optimal solution."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6225v3", 
    "other_authors": "Muhammad Shiraz, Saeid Abolfazli, Zohreh Sanaei, Abdullah Gani", 
    "title": "Virtual Machine Migration: A Resource Intensive Outsourcing Mechanism   for Mobile Cloud Computing", 
    "arxiv-id": "1206.6225v3", 
    "author": "Abdullah Gani", 
    "publish": "2012-06-27T10:23:14Z", 
    "summary": "In Mobile Cloud Computing (MCC), Virtual Machine (VM) migration based process\noffloading is a dominant approach to enhance Smart Mobile Devices (SMDs). A\nchallenging aspect of VM deployment is the additional computing resources usage\nin the deployment and management of VM which obliges computing resources for VM\ncreation and configuration. The management of VM comprises computing resources\nexploitation in the monitoring of VM in entire lifecycle and physical resources\nmanagement for VM on SMDs. Therefore, VM migration based application offloading\nrequires additional computing resource. Consequently computing resources demand\nand execution time of the application increases respectively. In this paper, we\nempirically review the impact of VM deployment and management on the execution\ntime of application in diverse scenarios. We investigate VM deployment and\nmanagement for application processing in simulation environment by employing\nCloudSim: a simulation toolkit that provides an extensible simulation framework\nto model VM deployment and management for application processing in cloud\ninfrastructure. The significance of this work is to ensure that VM deployment\nand management necessitates additional computing resources on SMD for\napplication offloading. We evaluate VM deployment and management in application\nprocessing by analyzing Key Performance Parameters (KPPs) in different\nscenarios; such as VM deployment, the execution time of applications, and total\nexecution time of the simulation. We use KPPs to assess deviations in the\nresults of diverse experimental scenarios. The empirical analysis concludes\nthat VM deployment and management oblige additional resources on computing host\nwhich make it a heavyweight approach for process offloading on smart mobile\ndevice."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0180v1", 
    "other_authors": "Othon Michail, Ioannis Chatzigiannakis, Paul G. Spirakis", 
    "title": "Naming and Counting in Anonymous Unknown Dynamic Networks", 
    "arxiv-id": "1208.0180v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2012-08-01T12:00:17Z", 
    "summary": "In this work, we study the fundamental naming and counting problems (and some\nvariations) in networks that are anonymous, unknown, and possibly dynamic. In\ncounting, nodes must determine the size of the network n and in naming they\nmust end up with unique identities. By anonymous we mean that all nodes begin\nfrom identical states apart possibly from a unique leader node and by unknown\nthat nodes have no a priori knowledge of the network (apart from some minimal\nknowledge when necessary) including ignorance of n. Network dynamicity is\nmodeled by the 1-interval connectivity model, in which communication is\nsynchronous and a worst-case adversary chooses the edges of every round subject\nto the condition that each instance is connected. We first focus on static\nnetworks with broadcast where we prove that, without a leader, counting is\nimpossible to solve and that naming is impossible to solve even with a leader\nand even if nodes know n. These impossibilities carry over to dynamic networks\nas well. We also show that a unique leader suffices in order to solve counting\nin linear time. Then we focus on dynamic networks with broadcast. We conjecture\nthat dynamicity renders nontrivial computation impossible. In view of this, we\nlet the nodes know an upper bound on the maximum degree that will ever appear\nand show that in this case the nodes can obtain an upper bound on n. Finally,\nwe replace broadcast with one-to-each, in which a node may send a different\nmessage to each of its neighbors. Interestingly, this natural variation is\nproved to be computationally equivalent to a full-knowledge model, in which\nunique names exist and the size of the network is known."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0615v2", 
    "other_authors": "Foto N. Afrati, Dimitris Fotakis, Jeffrey D. Ullman", 
    "title": "Enumerating Subgraph Instances Using Map-Reduce", 
    "arxiv-id": "1208.0615v2", 
    "author": "Jeffrey D. Ullman", 
    "publish": "2012-08-02T20:56:39Z", 
    "summary": "The theme of this paper is how to find all instances of a given \"sample\"\ngraph in a larger \"data graph,\" using a single round of map-reduce. For the\nsimplest sample graph, the triangle, we improve upon the best known such\nalgorithm. We then examine the general case, considering both the communication\ncost between mappers and reducers and the total computation cost at the\nreducers. To minimize communication cost, we exploit the techniques of (Afrati\nand Ullman, TKDE 2011)for computing multiway joins (evaluating conjunctive\nqueries) in a single map-reduce round. Several methods are shown for\ntranslating sample graphs into a union of conjunctive queries with as few\nqueries as possible. We also address the matter of optimizing computation cost.\nMany serial algorithms are shown to be \"convertible,\" in the sense that it is\npossible to partition the data graph, explore each partition in a separate\nreducer, and have the total computation cost at the reducers be of the same\norder as the computation cost of the serial algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0712v2", 
    "other_authors": "Bojan Marinkovi\u0107, Paola Glavan, Zoran Ognjanovi\u0107", 
    "title": "Description of the Chord Protocol using ASMs Formalism", 
    "arxiv-id": "1208.0712v2", 
    "author": "Zoran Ognjanovi\u0107", 
    "publish": "2012-08-03T11:17:57Z", 
    "summary": "This paper describes the overlay protocol Chord using the formalism of\nAbstract State Machines. The formalization concerns Chord actions that maintain\nring topology and manipulate distributed keys. We define a class of runs and\nprove the correctness of our formalization with respect to it."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.1793v2", 
    "other_authors": "Xiaohua Xu, Xiang-Yang Li, Min Song", 
    "title": "Real-time Data Collection Scheduling in Multi-hop Wireless Sensor   Networks", 
    "arxiv-id": "1208.1793v2", 
    "author": "Min Song", 
    "publish": "2012-08-08T22:47:10Z", 
    "summary": "We study real time periodic query scheduling for data collection in multihop\nWireless Sensor Networks (WSNs). Given a set of heterogenous data collection\nqueries in WSNs, each query requires the data from the source sensor nodes to\nbe collected to the control center within a certain end-to-end delay. We first\npropose almost-tight necessary conditions for a set of different queries to be\nschedulable by a WSN. We then develop a family of efficient and effective data\ncollection algorithms that can meet the real-time requirement under resource\nconstraints by addressing three tightly coupled tasks: (1) routing tree\nconstruction for data collection, (2) link activity scheduling, and (3)\npacket-level scheduling. Our theoretical analysis for the schedulability of\nthese algorithms show that they can achieve a constant fraction of the maximum\nschedulable load. For the case of overloaded networks where not all queries can\nbe possibly satisfied, we propose an efficient approximation algorithm to\nselect queries to maximize the total weight of selected schedulable queries.\nThe simulations corroborate our theoretical analysis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3412", 
    "link": "http://arxiv.org/pdf/1208.1922v1", 
    "other_authors": "Probir Roy, Md. Mejbah Ul Alam, Nishita Das", 
    "title": "Heuristic based task scheduling in multiprocessor systems with genetic   algorithm by choosing the eligible processor", 
    "arxiv-id": "1208.1922v1", 
    "author": "Nishita Das", 
    "publish": "2012-08-09T14:34:20Z", 
    "summary": "In multiprocessor systems, one of the main factors of systems' performance is\ntask scheduling. The well the task be distributed among the processors the well\nbe the performance. Again finding the optimal solution of scheduling the tasks\ninto the processors is NP-complete, that is, it will take a lot of time to find\nthe optimal solution. Many evolutionary algorithms (e.g. Genetic Algorithm,\nSimulated annealing) are used to reach the near optimal solution in linear\ntime. In this paper we propose a heuristic for genetic algorithm based task\nscheduling in multiprocessor systems by choosing the eligible processor on\neducated guess. From comparison it is found that this new heuristic based GA\ntakes less computation time to reach the suboptimal solution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.1942v1", 
    "other_authors": "B. Thirumala Rao, L. S. S. Reddy", 
    "title": "Scheduling Data Intensive Workloads through Virtualization on MapReduce   based Clouds", 
    "arxiv-id": "1208.1942v1", 
    "author": "L. S. S. Reddy", 
    "publish": "2012-08-09T15:07:43Z", 
    "summary": "MapReduce has become a popular programming model for running data intensive\napplications on the cloud. Completion time goals or deadlines of MapReduce jobs\nset by users are becoming crucial in existing cloud-based data processing\nenvironments like Hadoop. There is a conflict between the scheduling MR jobs to\nmeet deadlines and \"data locality\" (assigning tasks to nodes that contain their\ninput data). To meet the deadline a task may be scheduled on a node without\nlocal input data for that task causing expensive data transfer from a remote\nnode. In this paper, a novel scheduler is proposed to address the above problem\nwhich is primarily based on the dynamic resource reconfiguration approach. It\nhas two components: 1) Resource Predictor: which dynamically determines the\nrequired number of Map/Reduce slots for every job to meet completion time\nguarantee; 2) Resource Reconfigurator: that adjusts the CPU resources while not\nviolating completion time goals of the users by dynamically increasing or\ndecreasing individual VMs to maximize data locality and also to maximize the\nuse of resources within the system among the active jobs. The proposed\nscheduler has been evaluated against Fair Scheduler on virtual cluster built on\na physical cluster of 20 machines. The results demonstrate a gain of about 12%\nincrease in throughput of Jobs"
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.1975v2", 
    "other_authors": "Manuel Rodriguez Rodriguez, Bobby Philip, Zhen Wang, Mark Berrill", 
    "title": "Block-Relaxation Methods for 3D Constant-Coefficient Stencils on GPUs   and Multicore CPUs", 
    "arxiv-id": "1208.1975v2", 
    "author": "Mark Berrill", 
    "publish": "2012-08-09T17:29:16Z", 
    "summary": "Block iterative methods are extremely important as smoothers for multigrid\nmethods, as preconditioners for Krylov methods, and as solvers for diagonally\ndominant linear systems. Developing robust and efficient algorithms suitable\nfor current and evolving GPU and multicore CPU systems is a significant\nchallenge. We address this issue in the case of constant-coefficient stencils\narising in the solution of elliptic partial differential equations on\nstructured 3D uniform and adaptively refined grids. Robust, highly parallel\nimplementations of block Jacobi and chaotic block Gauss-Seidel algorithms with\nexact inversion of the blocks are developed using different parallelization\ntechniques. Experimental results for NVIDIA Fermi GPUs and AMD multicore\nsystems are presented."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2649v1", 
    "other_authors": "Daniel S. Katz, Shantenu Jha, Manish Parashar, Omer Rana, Jon Weissman", 
    "title": "Survey and Analysis of Production Distributed Computing Infrastructures", 
    "arxiv-id": "1208.2649v1", 
    "author": "Jon Weissman", 
    "publish": "2012-08-13T18:00:09Z", 
    "summary": "This report has two objectives. First, we describe a set of the production\ndistributed infrastructures currently available, so that the reader has a basic\nunderstanding of them. This includes explaining why each infrastructure was\ncreated and made available and how it has succeeded and failed. The set is not\ncomplete, but we believe it is representative.\n  Second, we describe the infrastructures in terms of their use, which is a\ncombination of how they were designed to be used and how users have found ways\nto use them. Applications are often designed and created with specific\ninfrastructures in mind, with both an appreciation of the existing capabilities\nprovided by those infrastructures and an anticipation of their future\ncapabilities. Here, the infrastructures we discuss were often designed and\ncreated with specific applications in mind, or at least specific types of\napplications. The reader should understand how the interplay between the\ninfrastructure providers and the users leads to such usages, which we call\nusage modalities. These usage modalities are really abstractions that exist\nbetween the infrastructures and the applications; they influence the\ninfrastructures by representing the applications, and they influence the ap-\nplications by representing the infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2675v1", 
    "other_authors": "Gerald Paul", 
    "title": "A GPU implementation of the Simulated Annealing Heuristic for the   Quadratic Assignment Problem", 
    "arxiv-id": "1208.2675v1", 
    "author": "Gerald Paul", 
    "publish": "2012-08-13T19:30:44Z", 
    "summary": "The quadratic assignment problem (QAP) is one of the most difficult\ncombinatorial optimization problems. An effective heuristic for obtaining\napproximate solutions to the QAP is simulated annealing (SA). Here we describe\nan SA implementation for the QAP which runs on a graphics processing unit\n(GPU). GPUs are composed of low cost commodity graphics chips which in\ncombination provide a powerful platform for general purpose parallel computing.\nFor SA runs with large numbers of iterations, we find performance 50-100 times\nbetter than that of a recent non-parallel but very efficient implementation of\nSA for the QAP"
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2849v1", 
    "other_authors": "Venkatesan T. Chakaravarthy, Naga Praveen Kumar Katta, Monu Kedia, Ramakrishnan Rajamony, Aruna Ramanan, Yogish Sabharwal", 
    "title": "Mapping Strategies for the PERCS Architecture", 
    "arxiv-id": "1208.2849v1", 
    "author": "Yogish Sabharwal", 
    "publish": "2012-08-14T12:53:24Z", 
    "summary": "The PERCS system was designed by IBM in response to a DARPA challenge that\ncalled for a high-productivity high-performance computing system. The IBM PERCS\narchitecture is a two level direct network having low diameter and high\nbisection bandwidth. Mapping and routing strategies play an important role in\nthe performance of applications on such a topology. In this paper, we study\nmapping strategies for PERCS architecture, that examine how to map tasks of a\ngiven job on to the physical processing nodes. We develop and present\nfundamental principles for designing good mapping strategies that minimize\ncongestion. This is achieved via a theoretical study of some common\ncommunication patterns under both direct and indirect routing mechanisms\nsupported by the architecture."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.3933v1", 
    "other_authors": "Melab Nouredine, Imen Chakroun, Mezmaz Mohand, Daniel Tuyttens", 
    "title": "A GPU-accelerated Branch-and-Bound Algorithm for the Flow-Shop   Scheduling Problem", 
    "arxiv-id": "1208.3933v1", 
    "author": "Daniel Tuyttens", 
    "publish": "2012-08-20T08:06:58Z", 
    "summary": "Branch-and-Bound (B&B) algorithms are time intensive tree-based exploration\nmethods for solving to optimality combinatorial optimization problems. In this\npaper, we investigate the use of GPU computing as a major complementary way to\nspeed up those methods. The focus is put on the bounding mechanism of B&B\nalgorithms, which is the most time consuming part of their exploration process.\nWe propose a parallel B&B algorithm based on a GPU-accelerated bounding model.\nThe proposed approach concentrate on optimizing data access management to\nfurther improve the performance of the bounding mechanism which uses large and\nintermediate data sets that do not completely fit in GPU memory. Extensive\nexperiments of the contribution have been carried out on well known FSP\nbenchmarks using an Nvidia Tesla C2050 GPU card. We compared the obtained\nperformances to a single and a multithreaded CPU-based execution. Accelerations\nup to x100 are achieved for large problem instances."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.4484v1", 
    "other_authors": "Sruti Gan Chaudhuri, Krishnendu Mukhopadhyaya", 
    "title": "Leader Election and Gathering for Asynchronous Transparent Fat Robots   without Chirality", 
    "arxiv-id": "1208.4484v1", 
    "author": "Krishnendu Mukhopadhyaya", 
    "publish": "2012-08-22T12:10:15Z", 
    "summary": "This paper proposes a distributed algorithm which deterministically gathers n\n(n > 4) asynchronous, fat robots. The robots are assumed to be transparent and\nthey have full visibility. The robots are initially considered to be\nstationary. A robot is visible in its motion. The robots do not store past\nactions. They are anonymous and can not be distinguished by their appearances\nand do not have common coordinate system or chirality. The robots do not\ncommunicate through message passing. In the proposed gathering algorithm one\nrobot moves at a time towards its destination. The robot which moves, is\nselected in such a way that, it will be the only robot eligible to move, until\nit reaches its destination. In case of a tie, this paper proposes a leader\nelection algorithm which produces an ordering of the robots and the first robot\nin the ordering becomes the leader. The ordering is unique in the sense that,\neach robot, characterized by its location, agrees on the same ordering. We show\nthat if a set of robots can be ordered then they can gather deterministically.\nThe paper also characterizes the cases, where ordering is not possible. This\npaper also presents an important fact that, if leader election is possible then\ngathering pattern formation is possible even with no chirality."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.4867v1", 
    "other_authors": "Lewis Tseng, Nitin H. Vaidya", 
    "title": "Parameter-independent Iterative Approximate Byzantine Consensus", 
    "arxiv-id": "1208.4867v1", 
    "author": "Nitin H. Vaidya", 
    "publish": "2012-08-23T21:44:08Z", 
    "summary": "In this work, we explore iterative approximate Byzantine consensus algorithms\nthat do not make explicit use of the global parameter of the graph, i.e., the\nupper-bound on the number of faults, f."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.5075v4", 
    "other_authors": "Lewis Tseng, Nitin Vaidya", 
    "title": "Exact Byzantine Consensus in Directed Graphs", 
    "arxiv-id": "1208.5075v4", 
    "author": "Nitin Vaidya", 
    "publish": "2012-08-24T22:17:23Z", 
    "summary": "Consider a synchronous point-to-point network of n nodes connected by\ndirected links, wherein each node has a binary input. This paper proves a tight\nnecessary and sufficient condition on the underlying communication topology for\nachieving Byzantine consensus among these nodes in the presence of up to f\nByzantine faults. We derive a necessary condition, and then we provide a\nconstructive proof of sufficiency by presenting a Byzantine consensus algorithm\nfor directed graphs that satisfy the necessary condition.\n  Prior work has developed analogous necessary and sufficient conditions for\nundirected graphs. It is known that, for undirected graphs, the following two\nconditions are together necessary and sufficient [8, 2, 6]: (i) n ? 3f + 1, and\n(ii) network connectivity greater than 2f. However, these conditions are not\nadequate to completely characterize Byzantine consensus in directed graphs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.5620v3", 
    "other_authors": "Shlomi Dolev, Omri Liba, Elad M. Schiller", 
    "title": "Self-Stabilizing Byzantine Resilient Topology Discovery and Message   Delivery", 
    "arxiv-id": "1208.5620v3", 
    "author": "Elad M. Schiller", 
    "publish": "2012-08-28T11:03:07Z", 
    "summary": "Traditional Byzantine resilient algorithms use 2f+1 vertex disjoint paths to\nensure message delivery in the presence of up to f Byzantine nodes. The\nquestion of how these paths are identified is related to the fundamental\nproblem of topology discovery. Distributed algorithms for topology discovery\ncope with a never ending task, dealing with frequent changes in the network\ntopology and unpredictable transient faults. Therefore, algorithms for topology\ndiscovery should be self-stabilizing to ensure convergence of the topology\ninformation following any such unpredictable sequence of events. We present the\nfirst such algorithm that can cope with Byzantine nodes. Starting in an\narbitrary global state, and in the presence of f Byzantine nodes, each node is\neventually aware of all the other non-Byzantine nodes and their connecting\ncommunication links. Using the topology information, nodes can, for example,\nroute messages across the network and deliver messages from one end user to\nanother. We present the first deterministic, cryptographicassumptions- free,\nself-stabilizing, Byzantine-resilient algorithms for network topology discovery\nand end-to-end message delivery. We also consider the task of r-neighborhood\ndiscovery for the case in which r and the degree of nodes are bounded by\nconstants. The use of r-neighborhood discovery facilitates polynomial time,\ncommunication and space solutions for the above tasks. The obtained algorithms\ncan be used to authenticate parties, in particular during the establishment of\nprivate secrets, thus forming public key schemes that are resistant to\nman-in-the-middle attacks of the compromised Byzantine nodes. A polynomial and\nefficient end-to-end algorithm that is based on the established private secrets\ncan be employed in between periodical re-establishments of the secrets."
},{
    "category": "cs.DC", 
    "doi": "10.1109/INDUSIS.2010.5565821", 
    "link": "http://arxiv.org/pdf/1209.0851v1", 
    "other_authors": "Ehsan Saboori, Shahriar Mohammadi, Shafigh Parsazad", 
    "title": "A new scheduling algorithm for server farms load balancing", 
    "arxiv-id": "1209.0851v1", 
    "author": "Shafigh Parsazad", 
    "publish": "2012-09-05T02:53:30Z", 
    "summary": "This paper describes a new scheduling algorithm to distribute jobs in server\nfarm systems. The proposed algorithm overcomes the starvation caused by SRPT\n(Shortest Remaining Processing Time). This algorithm is used in process\nscheduling in operating system approach. The algorithm was developed to be used\nin dispatcher scheduling. This algorithm is non-preemptive discipline, similar\nto SRPT, in which the priority of each job depends on its estimated run time,\nand also the amount of time it has spent on waiting. Tasks in the servers are\nserved in order of priority to optimize the system response time. The\nexperiments show that the mean round around time is reduced in the server farm\nsystem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/INDUSIS.2010.5565821", 
    "link": "http://arxiv.org/pdf/1209.1076v1", 
    "other_authors": "Konstantinos I. Tsianos, Sean Lawlor, Michael G. Rabbat", 
    "title": "Communication/Computation Tradeoffs in Consensus-Based Distributed   Optimization", 
    "arxiv-id": "1209.1076v1", 
    "author": "Michael G. Rabbat", 
    "publish": "2012-09-05T18:45:21Z", 
    "summary": "We study the scalability of consensus-based distributed optimization\nalgorithms by considering two questions: How many processors should we use for\na given problem, and how often should they communicate when communication is\nnot free? Central to our analysis is a problem-specific value $r$ which\nquantifies the communication/computation tradeoff. We show that organizing the\ncommunication among nodes as a $k$-regular expander graph (Reingold, Vadhan,\nand Wigderson, 2002) yields speedups, while when all pairs of nodes communicate\n(as in a complete graph), there is an optimal number of processors that depends\non $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach\na fixed level of accuracy, by communicating less and less frequently as the\ncomputation progresses. Experiments on a real cluster solving metric learning\nand non-smooth convex minimization tasks demonstrate strong agreement between\ntheory and practice."
},{
    "category": "cs.DC", 
    "doi": "10.7815/ijorcs.25.2012.044", 
    "link": "http://arxiv.org/pdf/1209.2614v1", 
    "other_authors": "K. Venkataramana, M. Padmavathamma", 
    "title": "A threshold secure data sharing scheme for federated clouds", 
    "arxiv-id": "1209.2614v1", 
    "author": "M. Padmavathamma", 
    "publish": "2012-09-12T13:44:48Z", 
    "summary": "Cloud computing allows users to view computing in a new direction, as it uses\nthe existing technologies to provide better IT services at low-cost. To offer\nhigh QOS to customers according SLA, cloud services broker or cloud service\nprovider uses individual cloud providers that work collaboratively to form a\nfederation of clouds. It is required in applications like Real-time online\ninteractive applications, weather research and forecasting etc., in which the\ndata and applications are complex and distributed. In these applications secret\ndata should be shared, so secure data sharing mechanism is required in\nFederated clouds to reduce the risk of data intrusion, the loss of service\navailability and to ensure data integrity. So In this paper we have proposed\nzero knowledge data sharing scheme where Trusted Cloud Authority (TCA) will\ncontrol federated clouds for data sharing where the secret to be exchanged for\ncomputation is encrypted and retrieved by individual cloud at the end. Our\nscheme is based on the difficulty of solving the Discrete Logarithm problem\n(DLOG) in a finite abelian group of large prime order which is NP-Hard. So our\nproposed scheme provides data integrity in transit, data availability when one\nof host providers are not available during the computation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.3356v1", 
    "other_authors": "Rajkumar Buyya, Rodrigo N. Calheiros, Xiaorong Li", 
    "title": "Autonomic Cloud Computing: Open Challenges and Architectural Elements", 
    "arxiv-id": "1209.3356v1", 
    "author": "Xiaorong Li", 
    "publish": "2012-09-15T04:40:46Z", 
    "summary": "As Clouds are complex, large-scale, and heterogeneous distributed systems,\nmanagement of their resources is a challenging task. They need automated and\nintegrated intelligent strategies for provisioning of resources to offer\nservices that are secure, reliable, and cost-efficient. Hence, effective\nmanagement of services becomes fundamental in software platforms that\nconstitute the fabric of computing Clouds. In this direction, this paper\nidentifies open issues in autonomic resource provisioning and presents\ninnovative management techniques for supporting SaaS applications hosted on\nClouds. We present a conceptual architecture and early results evidencing the\nbenefits of autonomic management of Clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.3904v1", 
    "other_authors": "Chrysovalandis Agathangelou, Chryssis Georgiou, Marios Mavronicolas", 
    "title": "A Distributed Algorithm for Gathering Many Fat Mobile Robots in the   Plane", 
    "arxiv-id": "1209.3904v1", 
    "author": "Marios Mavronicolas", 
    "publish": "2012-09-18T10:36:45Z", 
    "summary": "In this work we consider the problem of gathering autonomous robots in the\nplane. In particular, we consider non-transparent unit-disc robots (i.e., fat)\nin an asynchronous setting. Vision is the only mean of coordination. Using a\nstate-machine representation we formulate the gathering problem and develop a\ndistributed algorithm that solves the problem for any number of robots.\n  The main idea behind our algorithm is for the robots to reach a configuration\nin which all the following hold: (a) The robots' centers form a convex hull in\nwhich all robots are on the convex, (b) Each robot can see all other robots,\nand (c) The configuration is connected, that is, every robot touches another\nrobot and all robots together form a connected formation. We show that starting\nfrom any initial configuration, the robots, making only local decisions and\ncoordinate by vision, eventually reach such a configuration and terminate,\nyielding a solution to the gathering problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4408v1", 
    "other_authors": "Longfei Ma, Xue Chen, Zhouxiang Meng", 
    "title": "A performance Analysis of the Game of Life based on parallel algorithm", 
    "arxiv-id": "1209.4408v1", 
    "author": "Zhouxiang Meng", 
    "publish": "2012-09-20T02:12:03Z", 
    "summary": "In this article, Conway's Game of Life using OpenMP parallel processing to\nsimulate several different parallel methods, experimental performance results\nand compare to find the optimal solution of the parallelization of the Game of\nLife. Finally pointed out the importance of the design of parallel algorithms\nin solving the parallel problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4485v1", 
    "other_authors": "Ivan Zuzak, Ivan Benc", 
    "title": "Performance Evaluation of Hierarchical Publish-Subscribe Monitoring   Architecture for Service-Oriented Applications", 
    "arxiv-id": "1209.4485v1", 
    "author": "Ivan Benc", 
    "publish": "2012-09-20T10:15:52Z", 
    "summary": "Contemporary high-performance service-oriented applications demand a\nperformance efficient run-time monitoring. In this paper, we analyze a\nhierarchical publish-subscribe architecture for monitoring service-oriented\napplications. The analyzed architecture is based on a tree topology and\npublish-subscribe communication model for aggregation of distributed monitoring\ndata. In order to satisfy interoperability and platform independence of\nservice-orientation, monitoring reports are represented as XML documents. Since\nXML formatting introduces a significant processing and network load, we analyze\nthe performance of monitoring architecture with respect to the number of\nmonitored nodes, the load of system machines, and the overall latency of the\nmonitoring system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4620v2", 
    "other_authors": "Lewis Tseng, Nitin Vaidya, Vartika Bhandari", 
    "title": "Broadcast Using Certified Propagation Algorithm in Presence of Byzantine   Faults", 
    "arxiv-id": "1209.4620v2", 
    "author": "Vartika Bhandari", 
    "publish": "2012-09-20T19:15:50Z", 
    "summary": "We explore the correctness of the Certified Propagation Algorithm (CPA) [6,\n1, 8, 5] in solving broadcast with locally bounded Byzantine faults. CPA allows\nthe nodes to use only local information regarding the network topology. We\nprovide a tight necessary and sufficient condition on the network topology for\nthe correctness of CPA. To the best of our knowledge, this work is the first to\nsolve the open problem in [8]. We also present some simple extensions of this\nresult"
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.5319v1", 
    "other_authors": "Nourah Al-Angari, Abdullatif ALAbdullatif", 
    "title": "Multiprocessor Scheduling Using Parallel Genetic Algorithm", 
    "arxiv-id": "1209.5319v1", 
    "author": "Abdullatif ALAbdullatif", 
    "publish": "2012-09-24T16:12:20Z", 
    "summary": "Tasks scheduling is the most challenging problem in the parallel computing.\nHence, the inappropriate scheduling will reduce or even abort the utilization\nof the true potential of the parallelization. Genetic algorithm (GA) has been\nsuccessfully applied to solve the scheduling problem. The fitness evaluation is\nthe most time consuming GA operation for the CPU time, which affect the GA\nperformance. The proposed synchronous master-slave algorithm outperforms the\nsequential algorithm in case of complex and high number of generations problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.1026v2", 
    "other_authors": "Nguyen Quang-Hung, Nam Thoai, Nguyen Thanh Son", 
    "title": "Performance Constraint and Power-Aware Allocation For User Requests In   Virtual Computing Lab", 
    "arxiv-id": "1210.1026v2", 
    "author": "Nguyen Thanh Son", 
    "publish": "2012-10-03T08:38:53Z", 
    "summary": "Cloud computing is driven by economies of scale. A cloud system uses\nvirtualization technology to provide cloud resources (e.g. CPU, memory) to\nusers in form of virtual machines. Virtual machine (VM), which is a sandbox for\nuser application, fits well in the education environment to provide\ncomputational resources for teaching and research needs. In resource\nmanagement, they want to reduce costs in operations by reducing expensive cost\nof electronic bill of large-scale data center system. A lease-based model is\nsuitable for our Virtual Computing Lab, in which users ask resources on a lease\nof virtual machines. This paper proposes two host selection policies, named MAP\n(minimum of active physical hosts) and MAP-H2L, and four algorithms solving the\nlease scheduling problem. FF-MAP, FF-MAP-H2L algorithms meet a trade-off\nbetween the energy consumption and Quality of Service (e.g. performance). The\nsimulation on 7-day workload, which converted from LLNL Atlas log, showed the\nFF-MAP and FF-MAP-H2L algorithms reducing 7.24% and 7.42% energy consumption\nthan existing greedy mapping algorithm in the leasing scheduler Haizea. In\naddition, we introduce a ratio \\theta of consolidation in HalfPI-FF-MAP and\nPI-FF-MAP algorithms, in which \\theta is \\pi/2 and \\pi, and results on their\nsimulations show that energy consumption decreased by 34.87% and 63.12%\nrespectively."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.1804v2", 
    "other_authors": "Tomasz Jurdzinski, Dariusz R. Kowalski, Grzegorz Stachowiak", 
    "title": "Distributed Deterministic Broadcasting in Wireless Networks of Weak   Devices under the SINR Model", 
    "arxiv-id": "1210.1804v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2012-10-05T16:21:03Z", 
    "summary": "In this paper we initiate a study of distributed deterministic broadcasting\nin ad-hoc wireless networks with uniform transmission powers under the SINR\nmodel. We design algorithms in two settings: with and without local knowledge\nabout immediate neighborhood. In the former setting, our solution has almost\noptimal O(Dlog2 n) time cost, where n is the size of a network, D is the\neccentricity of the network and {1,...,N} is the set of possible node IDs. In\nthe latter case, we prove an Omega(n log N) lower bound and develop an\nalgorithm matching this formula, where n is the number of network nodes. As one\nof the conclusions, we derive that the inherited cost of broadcasting\ntechniques in wireless networks is much smaller, by factor around\nmin{n/D,Delta}, than the cost of learning the immediate neighborhood. Finally,\nwe develop a O(D Delta log2 N) algorithm for the setting without local\nknowledge, where Delta is the upper bound on the degree of the communication\ngraph of a network. This algorithm is close to a lower bound Omega(D Delta)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.2047v2", 
    "other_authors": "Miranda Zhang, Rajiv Ranjan, Surya Nepal, Michael Menzel, Armin Haller", 
    "title": "A Declarative Recommender System for Cloud Infrastructure Services   Selection", 
    "arxiv-id": "1210.2047v2", 
    "author": "Armin Haller", 
    "publish": "2012-10-07T12:16:09Z", 
    "summary": "The cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice..."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2401v1", 
    "other_authors": "Biao Xu, Ruair\u00ed de Fr\u00e9in, Eric Robson, M\u00edche\u00e1l \u00d3 Foghl\u00fa", 
    "title": "Distributed Formal Concept Analysis Algorithms Based on an Iterative   MapReduce Framework", 
    "arxiv-id": "1210.2401v1", 
    "author": "M\u00edche\u00e1l \u00d3 Foghl\u00fa", 
    "publish": "2012-10-05T10:28:24Z", 
    "summary": "While many existing formal concept analysis algorithms are efficient, they\nare typically unsuitable for distributed implementation. Taking the MapReduce\n(MR) framework as our inspiration we introduce a distributed approach for\nperforming formal concept mining. Our method has its novelty in that we use a\nlight-weight MapReduce runtime called Twister which is better suited to\niterative algorithms than recent distributed approaches. First, we describe the\ntheoretical foundations underpinning our distributed formal concept analysis\napproach. Second, we provide a representative exemplar of how a classic\ncentralized algorithm can be implemented in a distributed fashion using our\nmethodology: we modify Ganter's classic algorithm by introducing a family of\nMR* algorithms, namely MRGanter and MRGanter+ where the prefix denotes the\nalgorithm's lineage. To evaluate the factors that impact distributed algorithm\nperformance, we compare our MR* algorithms with the state-of-the-art.\nExperiments conducted on real datasets demonstrate that MRGanter+ is efficient,\nscalable and an appealing algorithm for distributed problems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2580v1", 
    "other_authors": "Loris Marchal, Oliver Sinnen, Fr\u00e9d\u00e9ric Vivien", 
    "title": "Scheduling tree-shaped task graphs to minimize memory and makespan", 
    "arxiv-id": "1210.2580v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2012-10-09T12:16:36Z", 
    "summary": "This paper investigates the execution of tree-shaped task graphs using\nmultiple processors. Each edge of such a tree represents a large IO file. A\ntask can only be executed if all input and output files fit into memory, and a\nfile can only be removed from memory after it has been consumed. Such trees\narise, for instance, in the multifrontal method of sparse matrix factorization.\nThe maximum amount of memory needed depends on the execution order of the\ntasks. With one processor the objective of the tree traversal is to minimize\nthe required memory. This problem was well studied and optimal polynomial\nalgorithms were proposed.\n  Here, we extend the problem by considering multiple processors, which is of\nobvious interest in the application area of matrix factorization. With the\nmultiple processors comes the additional objective to minimize the time needed\nto traverse the tree, i.e., to minimize the makespan. Not surprisingly, this\nproblem proves to be much harder than the sequential one. We study the\ncomputational complexity of this problem and provide an inapproximability\nresult even for unit weight trees. Several heuristics are proposed, each with a\ndifferent optimization focus, and they are analyzed in an extensive\nexperimental evaluation using realistic trees."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2857v1", 
    "other_authors": "Nasrin Jaberi", 
    "title": "An Introduction on Dependency Between Hardware Life Time Components and   Dynamic Voltage Scaling", 
    "arxiv-id": "1210.2857v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-10-10T10:07:59Z", 
    "summary": "The main open question is how to calculate the effect of switching between\nfrequencies in DVFS technique on the lifetime of the cluster components. As\nmoving from one frequency to another in DVFS technique always gives a shock to\nthe component and consequently decreases the component lifetime, therefore, it\nbecomes interesting to answer the question of how fast a component can change\nits speed in order to decrease power without changing its lifetime."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3077v1", 
    "other_authors": "Miranda Zhang, Rajiv Ranjan, Armin Haller, Dimitrios Georgakopoulos, Peter Strazdins", 
    "title": "Investigating Decision Support Techniques for Automating Cloud Service   Selection", 
    "arxiv-id": "1210.3077v1", 
    "author": "Peter Strazdins", 
    "publish": "2012-10-10T22:12:26Z", 
    "summary": "The compass of Cloud infrastructure services advances steadily leaving users\nin the agony of choice. To be able to select the best mix of service offering\nfrom an abundance of possibilities, users must consider complex dependencies\nand heterogeneous sets of criteria. Therefore, we present a PhD thesis proposal\non investigating an intelligent decision support system for selecting Cloud\nbased infrastructure services (e.g. storage, network, CPU)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3292v3", 
    "other_authors": "Yasaman Keshtkarjahromi, Rashid Ansari, Ashfaq Khokhar", 
    "title": "Energy Efficient Decentralized Detection Based on Bit-optimal Multi-hop   Transmission in One-dimensional Wireless Sensor Networks", 
    "arxiv-id": "1210.3292v3", 
    "author": "Ashfaq Khokhar", 
    "publish": "2012-10-11T16:40:36Z", 
    "summary": "Existing information theoretic work in decentralized detection is largely\nfocused on parallel configuration of Wireless Sensor Networks (WSNs), where an\nindividual hard or soft decision is computed at each sensor node and then\ntransmitted directly to the fusion node. Such an approach is not efficient for\nlarge networks, where communication structure is likely to comprise of multiple\nhops. On the other hand, decentralized detection problem investigated for\nmulti-hop networks is mainly concerned with reducing number and/or size of\nmessages by using compression and fusion of information at intermediate nodes.\nIn this paper an energy efficient multi-hop configuration of WSNs is proposed\nto solve the detection problem in large networks with two objectives:\nmaximizing network lifetime and minimizing probability of error in the fusion\nnode. This optimization problem is considered under the constraint of total\nconsumed energy. The two objectives mentioned are achieved simultaneously in\nthe multi-hop configuration by exploring tradeoffs between different path\nlengths and number of bits allocated to each node for quantization. Simulation\nresults show significant improvement in the proposed multi-hop configuration\ncompared with the parallel configuration in terms of energy efficiency and\ndetection accuracy for different size networks, especially in larger networks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3876v1", 
    "other_authors": "Xi Xu, Rashid Ansari, Ashfaq Khokhar", 
    "title": "Power-efficient Hierarchical Data Aggregation using Compressive Sensing   in WSN", 
    "arxiv-id": "1210.3876v1", 
    "author": "Ashfaq Khokhar", 
    "publish": "2012-10-15T02:39:12Z", 
    "summary": "Compressive Sensing (CS) method is a burgeoning technique being applied to\ndiverse areas including wireless sensor networks (WSNs). In WSNs, it has been\nstudied in the context of data gathering and aggregation, particularly aimed at\nreducing data transmission cost and improving power efficiency. Existing CS\nbased data gathering work in WSNs assume fixed and uniform compression\nthreshold across the network, regard- less of the data field characteristics.\nIn this paper, we present a novel data aggregation architecture model that\ncombines a multi- resolution structure with compressed sensing. The compression\nthresholds vary over the aggregation hierarchy, reflecting the underlying data\nfield. Compared with previous relevant work, the proposed model shows its\nsignificant energy saving from theoretical analysis. We have also implemented\nthe proposed CS- based data aggregation framework on a SIDnet SWANS platform,\ndiscrete event simulator commonly used for WSN simulations. Our experiments\nshow substantial energy savings, ranging from 37% to 77% for different nodes in\nthe networking depending on the position of hierarchy."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.4690v2", 
    "other_authors": "Nikzad Babaii Rizvandi, Albert Y. Zomaya", 
    "title": "A Primarily Survey on Energy Efficiency in Cloud and Distributed   Computing Systems", 
    "arxiv-id": "1210.4690v2", 
    "author": "Albert Y. Zomaya", 
    "publish": "2012-10-17T10:55:56Z", 
    "summary": "A survey of available techniques in hardware to reduce energy consumption"
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.5774v2", 
    "other_authors": "Christoph Lenzen, Boaz Patt-Shamir", 
    "title": "Fast Routing Table Construction Using Small Messages", 
    "arxiv-id": "1210.5774v2", 
    "author": "Boaz Patt-Shamir", 
    "publish": "2012-10-21T23:01:59Z", 
    "summary": "We describe a distributed randomized algorithm computing approximate\ndistances and routes that approximate shortest paths. Let n denote the number\nof nodes in the graph, and let HD denote the hop diameter of the graph, i.e.,\nthe diameter of the graph when all edges are considered to have unit weight.\nGiven 0 < eps <= 1/2, our algorithm runs in weak-O(n^(1/2 + eps) + HD)\ncommunication rounds using messages of O(log n) bits and guarantees a stretch\nof O(eps^(-1) log eps^(-1)) with high probability. This is the first\ndistributed algorithm approximating weighted shortest paths that uses small\nmessages and runs in weak-o(n) time (in graphs where HD in weak-o(n)). The time\ncomplexity nearly matches the lower bounds of weak-Omega(sqrt(n) + HD) in the\nsmall-messages model that hold for stateless routing (where routing decisions\ndo not depend on the traversed path) as well as approximation of the weigthed\ndiameter. Our scheme replaces the original identifiers of the nodes by labels\nof size O(log eps^(-1) log n). We show that no algorithm that keeps the\noriginal identifiers and runs for weak-o(n) rounds can achieve a\npolylogarithmic approximation ratio.\n  Variations of our techniques yield a number of fast distributed approximation\nalgorithms solving related problems using small messages. Specifically, we\npresent algorithms that run in weak-O(n^(1/2 + eps) + HD) rounds for a given 0\n< eps <= 1/2, and solve, with high probability, the following problems:\n  - O(eps^(-1))-approximation for the Generalized Steiner Forest (the running\ntime in this case has an additive weak-O(t^(1 + 2eps)) term, where t is the\nnumber of terminals);\n  - O(eps^(-2))-approximation of weighted distances, using node labels of size\nO(eps^(-1) log n) and weak-O(n^(eps)) bits of memory per node;\n  - O(eps^(-1))-approximation of the weighted diameter;\n  - O(eps^(-3))-approximate shortest paths using the labels 1,...,n."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.6384v1", 
    "other_authors": "Rafaelli de C. Coutinho, L\u00facia M. A. Drummond, Yuri Frota", 
    "title": "A Distributed Transportation Simplex Applied to a Content Distribution   Network Problem", 
    "arxiv-id": "1210.6384v1", 
    "author": "Yuri Frota", 
    "publish": "2012-10-23T21:21:06Z", 
    "summary": "A Content Distribution Network (CDN) can be defined as an overlay system that\nreplicates copies of contents at multiple points of a network, close to the\nfinal users, with the objective of improving data access. CDN technology is\nwidely used for the distribution of large-sized contents, like in video\nstreaming. In this paper we address the problem of finding the best server for\neach customer request in CDNs, in order to minimize the overall cost. We\nconsider the problem as a transportation problem and a distributed algorithm is\nproposed to solve it. The algorithm is composed of two independent phases: a\ndistributed heuristic finds an initial solution that may be later improved by a\ndistributed transportation simplex algorithm. It is compared with the\nsequential version of the transportation simplex and with an auction-based\ndistributed algorithm. Computational experiments carried out on a set of\ninstances adapted from the literature revealed that our distributed approach\nhas a performance similar to its sequential counterpart, in spite of not\nrequiring global information about the contents requests. Moreover, the results\nalso showed that the new method outperforms the based-auction distributed\nalgorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7057v1", 
    "other_authors": "Bahman Bahmani, Ashish Goel, Rajendra Shinde", 
    "title": "Efficient Distributed Locality Sensitive Hashing", 
    "arxiv-id": "1210.7057v1", 
    "author": "Rajendra Shinde", 
    "publish": "2012-10-26T05:59:55Z", 
    "summary": "Distributed frameworks are gaining increasingly widespread use in\napplications that process large amounts of data. One important example\napplication is large scale similarity search, for which Locality Sensitive\nHashing (LSH) has emerged as the method of choice, specially when the data is\nhigh-dimensional. At its core, LSH is based on hashing the data points to a\nnumber of buckets such that similar points are more likely to map to the same\nbuckets. To guarantee high search quality, the LSH scheme needs a rather large\nnumber of hash tables. This entails a large space requirement, and in the\ndistributed setting, with each query requiring a network call per hash bucket\nlook up, this also entails a big network load. The Entropy LSH scheme proposed\nby Panigrahy significantly reduces the number of required hash tables by\nlooking up a number of query offsets in addition to the query itself. While\nthis improves the LSH space requirement, it does not help with (and in fact\nworsens) the search network efficiency, as now each query offset requires a\nnetwork call. In this paper, focusing on the Euclidian space under $l_2$ norm\nand building up on Entropy LSH, we propose the distributed Layered LSH scheme,\nand prove that it exponentially decreases the network cost, while maintaining a\ngood load balance between different machines. Our experiments also verify that\nour scheme results in a significant network traffic reduction that brings about\nlarge runtime improvement in real world applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7624v1", 
    "other_authors": "Vivek Chalotra, Anju Bhasin, Anik Gupta, Sanjeev Singh Sambyal", 
    "title": "HEP Analysis Facility An Approach to Grid Computing", 
    "arxiv-id": "1210.7624v1", 
    "author": "Sanjeev Singh Sambyal", 
    "publish": "2012-10-29T11:29:52Z", 
    "summary": "HEP Analysis Facility is a cluster designed and implemented in Scientific\nLinux Cern 5.5 to grant High Energy Physics researchers one place where they\ncan go to undertake a particular task or to provide a parallel processing\narchitecture in which CPU resources are shared across a network and all\nmachines function as one large supercomputer."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7626v1", 
    "other_authors": "Vivek Chalotra, Anju Bhasin, Anik Gupta, Sanjeev Singh Sambyal", 
    "title": "Hep Cluster First Step Towards Grid Computing", 
    "arxiv-id": "1210.7626v1", 
    "author": "Sanjeev Singh Sambyal", 
    "publish": "2012-10-29T11:36:32Z", 
    "summary": "HEP Cluster is designed and implemented in Scientific Linux Cern 5.5 to grant\nHigh Energy Physics researchers one place where they can go to undertake a\nparticular task or to provide a parallel processing architecture in which CPU\nresources are shared across a network and all machines function as one large\nsupercomputer. It gives physicists a facility to access computers and data,\ntransparently, without having to consider location, operating system, account\nadministration, and other details. By using this facility researchers can\nprocess their jobs much faster than the stand alone desktop systems. Keywords:\nCluster, Network, Storage, Parallel Computing & Gris."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7935v1", 
    "other_authors": "Vivek Chalotra, Anju Bhasin, Anik Gupta, Sanjeev Singh Sambyal, Sanjay Mahajan", 
    "title": "Energy Efficient Algorithms and Power Consumption Techniques in High   Performance Computing", 
    "arxiv-id": "1210.7935v1", 
    "author": "Sanjay Mahajan", 
    "publish": "2012-10-30T09:14:40Z", 
    "summary": "High Performance Computing is an internet based computing which makes\ncomputer infrastructure and services available to the user for research\npurpose. However, an important issue which needs to be resolved before High\nPerformance Computing Cluster with large pool of servers gain widespread\nacceptance is the design of data centers with less energy consumption. It is\nonly possible when servers produce less heat and consume less power. Systems\nreliability decreases with increase in temperature due to heat generation\ncaused by large power consumption as computing in high temperature is more\nerror-prone. Here in this paper our approach is to design and implement a high\nperformance cluster for high-end research in the High Energy Physics stream.\nThis involves the usage of fine grained power gating technique in\nmicroprocessors and energy efficient algorithms that reduce the overall running\ncost of the data center."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1211.0235v1", 
    "other_authors": "Alex Scott, Peter Jeavons, Lei Xu", 
    "title": "Feedback from nature: an optimal distributed algorithm for maximal   independent set selection", 
    "arxiv-id": "1211.0235v1", 
    "author": "Lei Xu", 
    "publish": "2012-11-01T17:41:34Z", 
    "summary": "Maximal Independent Set selection is a fundamental problem in distributed\ncomputing. A novel probabilistic algorithm for this problem has recently been\nproposed by Afek et al, inspired by the study of the way that developing cells\nin the fly become specialised. The algorithm they propose is simple and robust,\nbut not as efficient as previous approaches: the expected time complexity is\nO(log^2 n). Here we first show that the approach of Afek et al cannot achieve\nbetter efficiency than this across all networks, no matter how the probability\nvalues are chosen. However, we then propose a new algorithm that incorporates\nanother important feature of the biological system: adapting the probabilities\nused at each node based on local feedback from neighbouring nodes. Our new\nalgorithm retains all the advantages of simplicity and robustness, but also\nachieves the optimal efficiency of O(log n) expected time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1211.1279v1", 
    "other_authors": "Sunirmal Khatua, Nandini Mukherjee", 
    "title": "Application-centric Resource Provisioning for Amazon EC2 Spot Instances", 
    "arxiv-id": "1211.1279v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2012-11-06T15:58:55Z", 
    "summary": "In late 2009, Amazon introduced spot instances to offer their unused\nresources at lower cost with reduced reliability. Amazon's spot instances allow\ncustomers to bid on unused Amazon EC2 capacity and run those instances for as\nlong as their bid exceeds the current spot price. The spot price changes\nperiodically based on supply and demand, and customers whose bids exceed it\ngain access to the available spot instances. Customers may expect their\nservices at lower cost with spot instances compared to on-demand or reserved.\nHowever the reliability is compromised since the instances(IaaS) providing the\nservice(SaaS) may become unavailable at any time without any notice to the\ncustomer. Checkpointing and migration schemes are of great use to cope with\nsuch situation. In this paper we study various checkpointing schemes that can\nbe used with spot instances. Also we device some algorithms for checkpointing\nscheme on top of application-centric resource provisioning framework that\nincrease the reliability while reducing the cost significantly."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.1447v1", 
    "other_authors": "Harshad B. Prajapati, Vipul A. Shah", 
    "title": "Advance Reservation based DAG Application Scheduling Simulator for Grid   Environment", 
    "arxiv-id": "1211.1447v1", 
    "author": "Vipul A. Shah", 
    "publish": "2012-11-07T04:21:58Z", 
    "summary": "In the last decade, scheduling of Directed Acyclic Graph (DAG) application in\nthe context of Grid environment has attracted attention of many researchers.\nHowever, deployment of Grid environment requires skills, efforts, budget, and\ntime. Although various simulation toolkits or frameworks are available for\nsimulating Grid environment, either they support different possible studies in\nGrid computing area or takes lot of efforts in molding them to make them\nsuitable for scheduling of DAG application. In this paper, we describe design\nand implementation of GridSim based ready to use application scheduler for\nscheduling of DAG application in Grid environment. The proposed application\nscheduler supports supplying DAG application and configuration of Grid\nresources through GUI. We also describe implementation of Min-Min static\nscheduling algorithm for scheduling of DAG application to validate the proposed\nscheduler. Our proposed DAG application scheduling simulator is useful, easy,\nand time-saver."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.1658v1", 
    "other_authors": "Prabhanjan Kambadur, Amol Ghoting, Anshul Gupta, Andrew Lumsdaine", 
    "title": "Extending Task Parallelism for Frequent Pattern Mining", 
    "arxiv-id": "1211.1658v1", 
    "author": "Andrew Lumsdaine", 
    "publish": "2012-11-07T20:18:30Z", 
    "summary": "Algorithms for frequent pattern mining, a popular informatics application,\nhave unique requirements that are not met by any of the existing parallel\ntools. In particular, such applications operate on extremely large data sets\nand have irregular memory access patterns. For efficient parallelization of\nsuch applications, it is necessary to support dynamic load balancing along with\nscheduling mechanisms that allow users to exploit data locality. Given these\nrequirements, task parallelism is the most promising of the available parallel\nprogramming models. However, existing solutions for task parallelism schedule\ntasks implicitly and hence, custom scheduling policies that can exploit data\nlocality cannot be easily employed. In this paper we demonstrate and\ncharacterize the speedup obtained in a frequent pattern mining application\nusing a custom clustered scheduling policy in place of the popular Cilk-style\npolicy. We present PFunc, a novel task parallel library whose customizable task\nscheduling and task priorities facilitated the implementation of our clustered\nscheduling policy."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.2032v1", 
    "other_authors": "Mohamed Firdhous", 
    "title": "Implementation of Security in Distributed Systems - A Comparative Study", 
    "arxiv-id": "1211.2032v1", 
    "author": "Mohamed Firdhous", 
    "publish": "2012-11-09T02:52:32Z", 
    "summary": "This paper presents a comparative study of distributed systems and the\nsecurity issues associated with those systems. Four commonly used distributed\nsystems were considered for detailed analysis in terms of technologies\ninvolved, security issues faced by them and solution proposed to circumvent\nthose issues. Finally the security issues and the solutions were summarized and\ncompared with each other."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.2038v1", 
    "other_authors": "Krishnahari Thouti, S. R. Sathe", 
    "title": "Comparison of OpenMP & OpenCL Parallel Processing Technologies", 
    "arxiv-id": "1211.2038v1", 
    "author": "S. R. Sathe", 
    "publish": "2012-11-09T04:27:32Z", 
    "summary": "This paper presents a comparison of OpenMP and OpenCL based on the parallel\nimplementation of algorithms from various fields of computer applications. The\nfocus of our study is on the performance of benchmark comparing OpenMP and\nOpenCL. We observed that OpenCL programming model is a good option for mapping\nthreads on different processing cores. Balancing all available cores and\nallocating sufficient amount of work among all computing units, can lead to\nimproved performance. In our simulation, we used Fedora operating system; a\nsystem with Intel Xeon Dual core processor having thread count 24 coupled with\nNVIDIA Quadro FX 3800 as graphical processing unit."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.4290v2", 
    "other_authors": "Muntasir Raihan Rahman, Wojciech Golab, Alvin AuYoung, Kimberly Keeton, Jay J. Wylie", 
    "title": "Toward a Principled Framework for Benchmarking Consistency", 
    "arxiv-id": "1211.4290v2", 
    "author": "Jay J. Wylie", 
    "publish": "2012-11-19T02:59:53Z", 
    "summary": "Large-scale key-value storage systems sacrifice consistency in the interest\nof dependability (i.e., partition tolerance and availability), as well as\nperformance (i.e., latency). Such systems provide eventual\nconsistency,which---to this point---has been difficult to quantify in real\nsystems. Given the many implementations and deployments of\neventually-consistent systems (e.g., NoSQL systems), attempts have been made to\nmeasure this consistency empirically, but they suffer from important drawbacks.\nFor example, state-of-the art consistency benchmarks exercise the system only\nin restricted ways and disrupt the workload, which limits their accuracy.\n  In this paper, we take the position that a consistency benchmark should paint\na comprehensive picture of the relationship between the storage system under\nconsideration, the workload, the pattern of failures, and the consistency\nobserved by clients. To illustrate our point, we first survey prior efforts to\nquantify eventual consistency. We then present a benchmarking technique that\novercomes the shortcomings of existing techniques to measure the consistency\nobserved by clients as they execute the workload under consideration. This\nmethod is versatile and minimally disruptive to the system under test. As a\nproof of concept, we demonstrate this tool on Cassandra."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.6473v1", 
    "other_authors": "Christophe C\u00e9rin, Alain Takoudjou, Nicolas Gren\u00e8che", 
    "title": "Int\u00e9gration des intergiciels de grilles de PC dans le nuage SlapOS :   le cas de BOINC", 
    "arxiv-id": "1211.6473v1", 
    "author": "Nicolas Gren\u00e8che", 
    "publish": "2012-11-27T23:05:10Z", 
    "summary": "In this article we describe the problems and solutions related to the\nintegration of desktop grid middleware in a cloud, in this case the open source\nSlapOS cloud. We focus on the issues about recipes that describe the\nintegration and the problem of the confinement of execution. They constitute\ntwo aspects of service-oriented architecture and Cloud Computing. These two\nissues solved with SlapOS are not in relation to what is traditionally done in\nthe clouds because we do not rely on virtual machines and, there is no data\ncenter (as defined in cloud). Moreover, we show that from the initial\ndeployment model we take into account not only Web applications, B2B\napplications... but also applications from the field of grids; here desktop\ngrid middleware which is a case study."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.6526v1", 
    "other_authors": "Ashish Goel, Kamesh Munagala", 
    "title": "Complexity Measures for Map-Reduce, and Comparison to Parallel Computing", 
    "arxiv-id": "1211.6526v1", 
    "author": "Kamesh Munagala", 
    "publish": "2012-11-28T06:03:24Z", 
    "summary": "The programming paradigm Map-Reduce and its main open-source implementation,\nHadoop, have had an enormous impact on large scale data processing. Our goal in\nthis expository writeup is two-fold: first, we want to present some complexity\nmeasures that allow us to talk about Map-Reduce algorithms formally, and\nsecond, we want to point out why this model is actually different from other\nmodels of parallel programming, most notably the PRAM (Parallel Random Access\nMemory) model. We are looking for complexity measures that are detailed enough\nto make fine-grained distinction between different algorithms, but which also\nabstract away many of the implementation details."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0085v1", 
    "other_authors": "Gaurav Somani, Prateek Khandelwal, Kapil Phatnani", 
    "title": "VUPIC: Virtual Machine Usage Based Placement in IaaS Cloud", 
    "arxiv-id": "1212.0085v1", 
    "author": "Kapil Phatnani", 
    "publish": "2012-12-01T08:51:48Z", 
    "summary": "Efficient resource allocation is one of the critical performance challenges\nin an Infrastructure as a Service (IaaS) cloud. Virtual machine (VM) placement\nand migration decision making methods are integral parts of these resource\nallocation mechanisms. We present a novel virtual machine placement algorithm\nwhich takes performance isolation amongst VMs and their continuous resource\nusage into account while taking placement decisions. Performance isolation is a\nform of resource contention between virtual machines interested in basic low\nlevel hardware resources (CPU, memory, storage, and networks bandwidth).\nResource contention amongst multiple co-hosted neighbouring VMs form the basis\nof the presented novel approach. Experiments are conducted to show the various\ncategories of applications and effect of performance isolation and resource\ncontention amongst them. A per-VM 3-dimensional Resource Utilization Vector\n(RUV) has been continuously calculated and used for placement decisions while\ntaking conflicting resource interests of VMs into account. Experiments using\nthe novel placement algorithm: VUPIC, show effective improvements in VM\nperformance as well as overall resource utilization of the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0156v1", 
    "other_authors": "Miranda Zhang, Rajiv Ranjan, Armin Haller, Dimitrios Georgakopoulos, Michael Menzel, Surya Nepal", 
    "title": "An Ontology based System for Cloud Infrastructure Services Discovery", 
    "arxiv-id": "1212.0156v1", 
    "author": "Surya Nepal", 
    "publish": "2012-12-01T20:39:22Z", 
    "summary": "The Cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice. As a result, Cloud service identification and discovery\nremains a hard problem due to different service descriptions, non standardised\nnaming conventions and heterogeneous types and features of Cloud services. In\nthis paper, we present an OWL based ontology, the Cloud Computing Ontology\n(CoCoOn) that defines functional and non functional concepts, attributes and\nrelations of infrastructure services. We also present a system..."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0421v1", 
    "other_authors": "Piotr Skowron, Krzysztof Rzadca", 
    "title": "Network delay-aware load balancing in selfish and cooperative   distributed systems", 
    "arxiv-id": "1212.0421v1", 
    "author": "Krzysztof Rzadca", 
    "publish": "2012-12-03T15:52:18Z", 
    "summary": "We consider a request processing system composed of organizations and their\nservers connected by the Internet.\n  The latency a user observes is a sum of communication delays and the time\nneeded to handle the request on a server. The handling time depends on the\nserver congestion, i.e. the total number of requests a server must handle. We\nanalyze the problem of balancing the load in a network of servers in order to\nminimize the total observed latency. We consider both cooperative and selfish\norganizations (each organization aiming to minimize the latency of the\nlocally-produced requests). The problem can be generalized to the task\nscheduling in a distributed cloud; or to content delivery in an\norganizationally-distributed CDNs.\n  In a cooperative network, we show that the problem is polynomially solvable.\nWe also present a distributed algorithm iteratively balancing the load. We show\nhow to estimate the distance between the current solution and the optimum based\non the amount of load exchanged by the algorithm. During the experimental\nevaluation, we show that the distributed algorithm is efficient, therefore it\ncan be used in networks with dynamically changing loads.\n  In a network of selfish organizations, we prove that the price of anarchy\n(the worst-case loss of performance due to selfishness) is low when the network\nis homogeneous and the servers are loaded (the request handling time is high\ncompared to the communication delay). After relaxing these assumptions, we\nassess the loss of performance caused by the selfishness experimentally,\nshowing that it remains low.\n  Our results indicate that a network of servers handling requests can be\nefficiently managed by a distributed algorithm. Additionally, even if the\nnetwork is organizationally distributed, with individual organizations\noptimizing performance of their requests, the network remains efficient."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0427v2", 
    "other_authors": "Piotr Skowron, Krzysztof Rzadca", 
    "title": "Exploring heterogeneity of unreliable machines for p2p backup", 
    "arxiv-id": "1212.0427v2", 
    "author": "Krzysztof Rzadca", 
    "publish": "2012-12-03T16:09:02Z", 
    "summary": "P2P architecture is a viable option for enterprise backup. In contrast to\ndedicated backup servers, nowadays a standard solution, making backups directly\non organization's workstations should be cheaper (as existing hardware is\nused), more efficient (as there is no single bottleneck server) and more\nreliable (as the machines are geographically dispersed).\n  We present the architecture of a p2p backup system that uses pairwise\nreplication contracts between a data owner and a replicator. In contrast to\nstandard p2p storage systems using directly a DHT, the contracts allow our\nsystem to optimize replicas' placement depending on a specific optimization\nstrategy, and so to take advantage of the heterogeneity of the machines and the\nnetwork. Such optimization is particularly appealing in the context of backup:\nreplicas can be geographically dispersed, the load sent over the network can be\nminimized, or the optimization goal can be to minimize the backup/restore time.\nHowever, managing the contracts, keeping them consistent and adjusting them in\nresponse to dynamically changing environment is challenging.\n  We built a scientific prototype and ran the experiments on 150 workstations\nin the university's computer laboratories and, separately, on 50 PlanetLab\nnodes. We found out that the main factor affecting the quality of the system is\nthe availability of the machines. Yet, our main conclusion is that it is\npossible to build an efficient and reliable backup system on highly unreliable\nmachines (our computers had just 13% average availability)."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.1284v1", 
    "other_authors": "M. N. Hulkury, M. R. Doomun", 
    "title": "Integrated Green Cloud Computing Architecture", 
    "arxiv-id": "1212.1284v1", 
    "author": "M. R. Doomun", 
    "publish": "2012-12-06T10:57:59Z", 
    "summary": "Arbitrary usage of cloud computing, either private or public, can lead to\nuneconomical energy consumption in data processing, storage and communication.\nHence, green cloud computing solutions aim not only to save energy but also\nreduce operational costs and carbon footprints on the environment. In this\npaper, an Integrated Green Cloud Architecture (IGCA) is proposed that comprises\nof a client-oriented Green Cloud Middleware to assist managers in better\noverseeing and configuring their overall access to cloud services in the\ngreenest or most energy-efficient way. Decision making, whether to use local\nmachine processing, private or public clouds, is smartly handled by the\nmiddleware using predefined system specifications such as service level\nagreement (SLA), Quality of service (QoS), equipment specifications and job\ndescription provided by IT department. Analytical model is used to show the\nfeasibility to achieve efficient energy consumption while choosing between\nlocal, private and public Cloud service provider (CSP)."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.3295v2", 
    "other_authors": "Aaditya Prakash", 
    "title": "Measures of Fault Tolerance in Distributed Simulated Annealing", 
    "arxiv-id": "1212.3295v2", 
    "author": "Aaditya Prakash", 
    "publish": "2012-12-13T20:00:40Z", 
    "summary": "In this paper, we examine the different measures of Fault Tolerance in a\nDistributed Simulated Annealing process. Optimization by Simulated Annealing on\na distributed system is prone to various sources of failure. We analyse\nsimulated annealing algorithm, its architecture in distributed platform and\npotential sources of failures. We examine the behaviour of tolerant distributed\nsystem for optimization task. We present possible methods to overcome the\nfailures and achieve fault tolerance for the distributed simulated annealing\nprocess. We also examine the implementation of Simulated Annealing in MapReduce\nsystem and possible ways to prevent failures in reaching the global optima.\nThis paper will be beneficial to those who are interested in implementing a\nlarge scale distributed simulated annealing optimization problem of industrial\nor academic interest. We recommend hybrid tolerance technique to optimize the\ntrade-off between efficiency and availability."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2513228.2513286", 
    "link": "http://arxiv.org/pdf/1212.4123v4", 
    "other_authors": "Sleiman Rabah, Serguei A. Mokhov, Joey Paquet", 
    "title": "An Interactive Graph-Based Automation Assistant: A Case Study to Manage   the GIPSY's Distributed Multi-tier Run-Time System", 
    "arxiv-id": "1212.4123v4", 
    "author": "Joey Paquet", 
    "publish": "2012-12-17T20:09:55Z", 
    "summary": "The GIPSY system provides a framework for a distributed multi-tier\ndemand-driven evaluation of heterogeneous programs, in which certain tiers can\ngenerate demands, while others can respond to demands to work on them. They are\nconnected through a virtual network that can be flexibly reconfigured at\nrun-time. Although the demand generator components were originally designed\nspecifically for the eductive (demand-driven) evaluation of Lucid intensional\nprograms, the GIPSY's run-time's flexible framework design enables it to\nperform the execution of various kinds of programs that can be evaluated using\nthe demand-driven computational model. Management of the GISPY networks has\nbecome a tedious (although scripted) task that took manual command-line console\nto do, which does not scale for large experiments. Therefore a new component\nhas been designed and developed to allow users to represent, visualize, and\ninteractively create, configure and seamlessly manage such a network as a\ngraph. Consequently, this work presents a Graphical GMT Manager, an interactive\ngraph-based assistant component for the GIPSY network creation and\nconfiguration management. Besides allowing the management of the nodes and\ntiers (mapped to hosts where store, workers, and generators reside), it lets\nthe user to visually control the network parameters and the interconnection\nbetween computational nodes at run-time. In this paper we motivate and present\nthe key features of this newly implemented graph-based component. We give the\ngraph representation details, mapping of the graph nodes to tiers, tier groups,\nand specific commands. We provide the requirements and design specification of\nthe tool and its implementation. Then we detail and discuss some experimental\nresults."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2513228.2513286", 
    "link": "http://arxiv.org/pdf/1212.4658v1", 
    "other_authors": "Stefano Stalio, Giuseppe Di Carlo, Sandra Parlati, Piero Spinnato", 
    "title": "Resource management on a VM based computer cluster for scientific   computing", 
    "arxiv-id": "1212.4658v1", 
    "author": "Piero Spinnato", 
    "publish": "2012-12-19T13:31:04Z", 
    "summary": "In the last ten years host virtualization has brought a revolution in the way\nalmost every activity related to information technology is thought of and\nperformed. The use of virtualization for HPC and HTC computing, while eagerly\ndesired, has probably been one of the last steps of this revolution, the\nperformance loss due to the hardware abstraction layer being the cause that\nslowed down a process that has been much faster in other fields. Nowadays the\nwidespread diffusion of virtualization and of new virtualization techniques\nseem to have helped breaking this last barrier and virtual host computing\ninfrastructures for HPC and HTC are found in many data centers. In this\ndocument the approach adopted at the INFN \"Laboratori Nazionali del Gran Sasso\"\nfor providing computational resources via a virtual host based computing\nfacility is described. Particular evidence is given to the storage layout, to\nthe middleware architecture and to resource allocation strategies, as these are\nissues for which a personalized solution was adopted. Other aspects may be\ncovered in the future within other documents."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1212.4692v1", 
    "other_authors": "Anjan K. Koundinya, Srinath N. K., K. A. K. Sharma, Kiran Kumar, Madhu M. N., Kiran U. Shanbag", 
    "title": "Map / Reduce Deisgn and Implementation of Apriori Alogirthm for handling   voluminous data-sets", 
    "arxiv-id": "1212.4692v1", 
    "author": "Kiran U. Shanbag", 
    "publish": "2012-12-19T15:04:12Z", 
    "summary": "Apriori is one of the key algorithms to generate frequent itemsets. Analyzing\nfrequent itemset is a crucial step in analysing structured data and in finding\nassociation relationship between items. This stands as an elementary foundation\nto supervised learning, which encompasses classifier and feature extraction\nmethods. Applying this algorithm is crucial to understand the behaviour of\nstructured data. Most of the structured data in scientific domain are\nvoluminous. Processing such kind of data requires state of the art computing\nmachines. Setting up such an infrastructure is expensive. Hence a distributed\nenvironment such as a clustered setup is employed for tackling such scenarios.\nApache Hadoop distribution is one of the cluster frameworks in distributed\nenvironment that helps by distributing voluminous data across a number of nodes\nin the framework. This paper focuses on map/reduce design and implementation of\nApriori algorithm for structured data analysis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1212.5956v1", 
    "other_authors": "Jingxin K. Wang, Jianrui Ding, Tian Niu", 
    "title": "Interoperability and Standardization of Intercloud Cloud Computing", 
    "arxiv-id": "1212.5956v1", 
    "author": "Tian Niu", 
    "publish": "2012-12-24T19:24:35Z", 
    "summary": "Cloud computing is getting mature, and the interoperability and\nstandardization of the clouds is still waiting to be solved. This paper\ndiscussed the interoperability among clouds about message transmission, data\ntransmission and virtual machine transfer. Starting from IEEE Pioneering Cloud\nComputing Initiative, this paper discussed about standardization of the cloud\ncomputing, especially intercloud cloud computing. This paper also discussed the\nstandardization from the market-oriented view."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.2976v1", 
    "other_authors": "Ran Wolff", 
    "title": "Local Thresholding on Distributed Hash Tables", 
    "arxiv-id": "1301.2976v1", 
    "author": "Ran Wolff", 
    "publish": "2013-01-14T13:56:17Z", 
    "summary": "We present a binary routing tree protocol for distributed hash table\noverlays. Using this protocol each peer can independently route messages to its\nparent and two descendants on the fly without any maintenance, global context,\nand synchronization. The protocol is then extended to support tree change\nnotification with similar efficiency. The resulting tree is almost perfectly\ndense and balanced, and has O(1) stretch if the distributed hash table is\nsymmetric Chord. We use the tree routing protocol to overcome the main\nimpediment for implementation of local thresholding algorithms in peer-to-peer\nsystems -- their requirement for cycle free routing. Direct comparison of a\ngossip-based algorithm and a corresponding local thresholding algorithm on a\nmajority voting problem reveals that the latter obtains superior accuracy using\na fraction of the communication overhead."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.3223v4", 
    "other_authors": "Allison Lewko, Mark Lewko", 
    "title": "On the Complexity of Asynchronous Agreement Against Powerful Adversaries", 
    "arxiv-id": "1301.3223v4", 
    "author": "Mark Lewko", 
    "publish": "2013-01-15T04:39:01Z", 
    "summary": "We introduce new techniques for proving lower bounds on the running time of\nrandomized algorithms for asynchronous agreement against powerful adversaries.\nIn particular, we define a \\emph{strongly adaptive adversary} that is\ncomputationally unbounded and has a limited ability to corrupt a dynamic subset\nof processors by erasing their memories. We demonstrate that the randomized\nagreement algorithms designed by Ben-Or and Bracha to tolerate crash or\nByzantine failures in the asynchronous setting extend to defeat a strongly\nadaptive adversary. These algorithms have essentially perfect correctness and\ntermination, but at the expense of exponential running time. In the case of the\nstrongly adaptive adversary, we show that this dismally slow running time is\n\\emph{inherent}: we prove that any algorithm with essentially perfect\ncorrectness and termination against the strongly adaptive adversary must have\nexponential running time. We additionally interpret this result as yielding an\nenhanced understanding of the tools needed to simultaneously achieving perfect\ncorrectness and termination as well as fast running time for randomized\nalgorithms tolerating crash or Byzantine failures."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.4490v1", 
    "other_authors": "Bharath Ramesh, Calvin J. Ribbens, Srinidhi Varadarajan", 
    "title": "Regional Consistency: Programmability and Performance for   Non-Cache-Coherent Systems", 
    "arxiv-id": "1301.4490v1", 
    "author": "Srinidhi Varadarajan", 
    "publish": "2013-01-18T20:40:42Z", 
    "summary": "Parallel programmers face the often irreconcilable goals of programmability\nand performance. HPC systems use distributed memory for scalability, thereby\nsacrificing the programmability advantages of shared memory programming models.\nFurthermore, the rapid adoption of heterogeneous architectures, often with\nnon-cache-coherent memory systems, has further increased the challenge of\nsupporting shared memory programming models. Our primary objective is to define\na memory consistency model that presents the familiar thread-based shared\nmemory programming model, but allows good application performance on\nnon-cache-coherent systems, including distributed memory clusters and\naccelerator-based systems. We propose regional consistency (RegC), a new\nconsistency model that achieves this objective. Results on up to 256 processors\nfor representative benchmarks demonstrate the potential of RegC in the context\nof our prototype distributed shared memory system."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.4539v1", 
    "other_authors": "Olivier Cessenat", 
    "title": "Sophie, an FDTD code on the way to multicore, getting rid of the memory   bandwidth bottleneck better using cache", 
    "arxiv-id": "1301.4539v1", 
    "author": "Olivier Cessenat", 
    "publish": "2013-01-19T08:13:27Z", 
    "summary": "FDTD codes, such as Sophie developed at CEA/DAM, no longer take advantage of\nthe processor's increased computing power, especially recently with the raising\nmulticore technology. This is rooted in the fact that low order numerical\nschemes need an important memory bandwidth to bring and store the computed\nfields. The aim of this article is to present a programming method at the\nsoftware's architecture level that improves the memory access pattern in order\nto reuse data in cache instead of constantly accessing RAM memory. We will\nexhibit a more than two computing time improvement in practical applications.\nThe target audience of this article is made of computing scientists and of\nelectrical engineers that develop simulation codes with no specific knowledge\nin computer science or electronics."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.4839v1", 
    "other_authors": "Adrian Klein, Fuyuki Ishikawa, Shinichi Honiden", 
    "title": "A Scalable Distributed Architecture for Network- and QoS-aware Service   Composition", 
    "arxiv-id": "1301.4839v1", 
    "author": "Shinichi Honiden", 
    "publish": "2013-01-21T12:25:16Z", 
    "summary": "Service-Oriented Computing (SOC) enables the composition of loosely coupled\nservice agents provided with varying Quality of Service (QoS) levels,\neffectively forming a multiagent system (MAS). Selecting a (near-)optimal set\nof services for a composition in terms of QoS is crucial when many functionally\nequivalent services are available. As the number of distributed services,\nespecially in the cloud, is rising rapidly, the impact of the network on the\nQoS keeps increasing. Despite this and opposed to most MAS approaches, current\nservice approaches depend on a centralized architecture which cannot adapt to\nthe network. Thus, we propose a scalable distributed architecture composed of a\nflexible number of distributed control nodes. Our architecture requires no\nchanges to existing services and adapts from a centralized to a completely\ndistributed realization by adding control nodes as needed. Also, we propose an\nextended QoS aggregation algorithm that allows to accurately estimate network\nQoS. Finally, we evaluate the benefits and optimality of our architecture in a\ndistributed environment."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.6195v1", 
    "other_authors": "Samer Al-Kiswany, Emalayan Vairavanathan, Lauro B. Costa, Hao Yang, Matei Ripeanu", 
    "title": "The Case for Cross-Layer Optimizations in Storage: A Workflow-Optimized   Storage System", 
    "arxiv-id": "1301.6195v1", 
    "author": "Matei Ripeanu", 
    "publish": "2013-01-26T00:53:12Z", 
    "summary": "This paper proposes using file system custom metadata as a bidirectional\ncommunication channel between applications and the storage system. This channel\ncan be used to pass hints that enable cross-layer optimizations, an option\nhindered today by the ossified file-system interface. We study this approach in\ncontext of storage system support for large-scale workflow execution systems:\nOur workflow optimized storage system (WOSS), exploits application hints to\nprovide per-file optimized operations, and exposes data location to enable\nlocation-aware scheduling.\n  This paper argues that an incremental adoption path for adopting cross-layer\noptimizations in storage systems exists, presents the system architecture for a\nworkflow-optimized storage system and its integration with a workflow runtime\nengine, and evaluates the proposed approach using synthetic as well as real\napplications workloads."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.6228v3", 
    "other_authors": "Andre Luckow, Mark Santcroos, Ashley Zebrowski, Shantenu Jha", 
    "title": "Pilot-Data: An Abstraction for Distributed Data", 
    "arxiv-id": "1301.6228v3", 
    "author": "Shantenu Jha", 
    "publish": "2013-01-26T10:06:13Z", 
    "summary": "Scientific problems that depend on processing large amounts of data require\novercoming challenges in multiple areas: managing large-scale data\ndistribution, controlling co-placement and scheduling of data with compute\nresources, and storing, transferring, and managing large volumes of data.\nAlthough there exist multiple approaches to addressing each of these\nchallenges, an integrative approach is missing; furthermore, extending existing\nfunctionality or enabling interoperable capabilities remains difficult at best.\nWe propose the concept of Pilot-Data to address the fundamental challenges of\nco-placement and scheduling of data and compute in heterogeneous and\ndistributed environments with interoperability and extensibility as first-order\nconcerns. Pilot-Data is an extension of the Pilot-Job abstraction for\nsupporting the management of data in conjunction with compute tasks. Pilot-Data\nseparates logical data units from physical storage, thereby providing the basis\nfor efficient compute/data placement and scheduling. In this paper, we discuss\nthe design and implementation of the Pilot-Data prototype, demonstrate its use\nby data-intensive applications on multiple production distributed\ncyberinfrastructure and illustrate the advantages arising from flexible\nexecution modes enabled by Pilot-Data. Our experiments utilize an\nimplementation of Pilot-Data in conjunction with a scalable Pilot-Job (BigJob)\nto establish the application performance that can be enabled by the use of\nPilot-Data. We demonstrate how the concept of Pilot-Data also provides the\nbasis upon which to build tools and support capabilities like affinity which in\nturn can be used for advanced data-compute co-placement and scheduling."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1301.6297v3", 
    "other_authors": "Hagit Attiya, Sandeep Hans, Petr Kuznetsov, Srivatsan Ravi", 
    "title": "Safety of Deferred Update in Transactional Memory", 
    "arxiv-id": "1301.6297v3", 
    "author": "Srivatsan Ravi", 
    "publish": "2013-01-26T23:35:25Z", 
    "summary": "Transactional memory allows the user to declare sequences of instructions as\nspeculative \\emph{transactions} that can either \\emph{commit} or \\emph{abort}.\nIf a transaction commits, it appears to be executed sequentially, so that the\ncommitted transactions constitute a correct sequential execution. If a\ntransaction aborts, none of its instructions can affect other transactions.\n  The popular criterion of \\emph{opacity} requires that the views of aborted\ntransactions must also be consistent with the global sequential order\nconstituted by committed ones. This is believed to be important, since\ninconsistencies observed by an aborted transaction may cause a fatal\nirrecoverable error or waste of the system in an infinite loop. Intuitively, an\nopaque implementation must ensure that no intermediate view a transaction\nobtains before it commits or aborts can be affected by a transaction that has\nnot started committing yet, so called \\emph{deferred-update} semantics.\n  In this paper, we intend to grasp this intuition formally. We propose a\nvariant of opacity that explicitly requires the sequential order to respect the\ndeferred-update semantics. We show that our criterion is a safety property,\ni.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures\nthat a serialization of a history implies serializations of its prefixes.\nFinally, we show that our property is equivalent to opacity if we assume that\nno two transactions commit identical values on the same variable, and present a\ncounter-example for scenarios when the \"unique-write\" assumption does not hold."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.0012v1", 
    "other_authors": "Russell Power", 
    "title": "Using Memory-Protection to Simplify Zero-copy Operations", 
    "arxiv-id": "1304.0012v1", 
    "author": "Russell Power", 
    "publish": "2013-03-29T20:03:47Z", 
    "summary": "High performance networks (e.g. Infiniband) rely on zero-copy operations for\nperformance. Zero-copy operations, as the name implies, avoid copying buffers\nfor sending and receiving data. Instead, hardware devices directly read and\nwrite to application specified areas of memory. Since these networks can send\nand receive at nearly the same speed as the memory bus inside machines,\nzero-copy operations are necessary to achieve peak performance for many\napplications.\n  Unfortunately, programming with zero-copy APIs is a *giant pain*. Users must\ncarefully avoid using buffers that may be accessed by a device. Typically this\neither results in spaghetti code (where every access to a buffer is checked\nbefore usage), or blocking operations (which pretty much defeat the whole point\nof zero-copy).\n  We show that by abusing memory protection hardware, we can offer the best of\nboth worlds: a simple zero-copy mechanism which allows for non-blocking send\nand receives while protecting against incorrect accesses."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.0267v1", 
    "other_authors": "Alexandre Domingues Goncalves, Lucia Maria Drummond, Artur Alves Pessoa, Peter Hahn", 
    "title": "Improving Lower Bounds for the Quadratic Assignment Problem by applying   a Distributed Dual Ascent Algorithm", 
    "arxiv-id": "1304.0267v1", 
    "author": "Peter Hahn", 
    "publish": "2013-04-01T00:14:45Z", 
    "summary": "The application of the Reformulation Linearization Technique (RLT) to the\nQuadratic Assignment Problem (QAP) leads to a tight linear relaxation with huge\ndimensions that is hard to solve. Previous works found in the literature show\nthat these relaxations combined with branch-and-bound algorithms belong to the\nstate-of-the-art of exact methods for the QAP. For the level 3 RLT (RLT3),\nusing this relaxation is prohibitive in conventional machines for instances\nwith more than 22 locations due to memory limitations. This paper presents a\ndistributed version of a dual ascent algorithm for the RLT3 QAP relaxation that\napproximately solves it for instances with up to 30 locations for the first\ntime. Although, basically, the distributed algorithm has been implemented on\ntop of its sequential conterpart, some changes, which improved not only the\nparallel performance but also the quality of solutions, were proposed here.\nWhen compared to other lower bounding methods found in the literature, our\nalgorithm generates the best known lower bounds for 26 out of the 28 tested\ninstances, reaching the optimal solution in 18 of them."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.2617v1", 
    "other_authors": "Stefano Ferretti", 
    "title": "Resilience of Dynamic Overlays through Local Interactions", 
    "arxiv-id": "1304.2617v1", 
    "author": "Stefano Ferretti", 
    "publish": "2013-04-09T14:43:02Z", 
    "summary": "This paper presents a self-organizing protocol for dynamic (unstructured P2P)\noverlay networks, which allows to react to the variability of node arrivals and\ndepartures. Through local interactions, the protocol avoids that the departure\nof nodes causes a partitioning of the overlay. We show that it is sufficient to\nhave knowledge about 1st and 2nd neighbours, plus a simple interaction P2P\nprotocol, to make unstructured networks resilient to node faults. A simulation\nassessment over different kinds of overlay networks demonstrates the viability\nof the proposal."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.2840v1", 
    "other_authors": "Ivan Rodero, Manish Parashar", 
    "title": "Cross-layer Application-aware Power/Energy Management for Extreme Scale   Science", 
    "arxiv-id": "1304.2840v1", 
    "author": "Manish Parashar", 
    "publish": "2013-04-10T04:06:30Z", 
    "summary": "High Performance Computing (HPC) has evolved over the past decades into\nincreasingly complex and powerful systems. Current HPC systems consume several\nMWs of power, enough to power small towns, and are in fact soon approaching the\nlimits of the power available to them. Estimates are with the given current\ntechnology, achieving exascale will require hundreds of MW, which is not\nfeasible from multiple perspectives. Architecture and technology researchers\nare aggressively addressing this; however as past history is shown, innovation\nat these levels are not sufficient and have to be accompanied with innovations\nat higher levels (algorithms, programming, runtime, OS) to achieve the multiple\norders of magnitude reduction - i.e., a comprehensive cross-layer and\napplication-aware strategy is required. Furthermore, energy/power-efficiency\nhas to be addressed in combination with quality of solutions, performance and\nreliability and other objectives and appropriate tradeoffs are required."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.2981v2", 
    "other_authors": "Sultan Ullah, Zheng Xuefeng", 
    "title": "Cloud Computing: a Prologue", 
    "arxiv-id": "1304.2981v2", 
    "author": "Zheng Xuefeng", 
    "publish": "2013-04-10T14:45:47Z", 
    "summary": "An emerging internet based super computing model is represented by cloud\ncomputing. Cloud computing is the convergence and evolution of several concepts\nfrom virtualization, distributed storage, grid, and automation management to\nenable a more flexible approach for deploying and scaling applications.\nHowever, cloud computing moves the application software and databases to the\nlarge data centers, where the management of the data and services may not be\nfully trustworthy. The concept of cloud computing on the basis of the various\ndefinitions available in the industry and the characteristics of cloud\ncomputing are being analyzed in this paper. The paper also describes the main\ncloud service providers and their products followed by primary cloud computing\noperating systems."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1304.3203v1", 
    "other_authors": "Sultan Ullah, Zheng Xuefeng", 
    "title": "Cloud Computing Research Challenges", 
    "arxiv-id": "1304.3203v1", 
    "author": "Zheng Xuefeng", 
    "publish": "2013-04-11T06:10:12Z", 
    "summary": "In recent times cloud computing has appeared as a new model for hosting and\nconveying services over the Internet. This model is striking to business\nvendors as it eradicates the requirement for users to plan in advance, and it\npermits the organization to start from low level and then add more resources\nonly if there is an increase in the service demand. Even though cloud computing\npresents greater opportunities not only to information technology industry, but\nevery organization involved in utilizing the computing in one way or the other,\nit is still in infancy with many problems to be fixed. The paper discusses\nresearch challenges in cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.3396v1", 
    "other_authors": "Ankit Mundra, Bhagvan K. Gupta, Geetanjali Rathee, Meenu Chawla, Nitin Rakesh, Vipin Tyagi", 
    "title": "Validated Real Time Middle Ware For Distributed Cyber Physical Systems   Using HMM", 
    "arxiv-id": "1304.3396v1", 
    "author": "Vipin Tyagi", 
    "publish": "2013-04-11T19:09:42Z", 
    "summary": "Distributed Cyber Physical Systems designed for different scenario must be\ncapable enough to perform in an efficient manner in every situation. Earlier\napproaches, such as CORBA, has performed but with different time constraints.\nTherefore, there was the need to design reconfigurable, robust, validated and\nconsistent real time middle ware systems with end-to-end timing. In the\nDCPS-HMM we have proposed the processor efficiency and data validation which\nmay proof crucial in implementing various distributed systems such as credit\ncard systems or file transfer through network."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.3978v1", 
    "other_authors": "Manan D. Shah, Harshad B. Prajapati", 
    "title": "Reallocation and Allocation of Virtual Machines in Cloud Computing", 
    "arxiv-id": "1304.3978v1", 
    "author": "Harshad B. Prajapati", 
    "publish": "2013-04-15T05:02:16Z", 
    "summary": "Cloud computing has given the new face to the distributed field. Two main\nissues are discussed in this paper, (I) the process of finding the efficient\nvirtual machine by using the concept of load balancing algorithm. (II)\nReallocation of the Virtual Machines i.e. migration of the Virtual Machines\nwhen cloud provider is not available with the required Virtual Machines. We\nhave discussed about the different load balancing algorithms which are used for\ndeciding the efficient Virtual Machine for the allocation to the client on\ndemand. While in the second issue is concern we have discuss about different\nmodules available for the migration of Virtual Machines from one source machine\nto the other target machine. At last discussion about the different simulators\navailable for the cloud are carried out in this paper."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.3980v1", 
    "other_authors": "Deepak. c. vegda, Harshad. B. Prajapati", 
    "title": "Scheduling of Dependent Tasks Application using Random Search Technique", 
    "arxiv-id": "1304.3980v1", 
    "author": "Harshad. B. Prajapati", 
    "publish": "2013-04-15T05:04:31Z", 
    "summary": "Since beginning of Grid computing, scheduling of dependent tasks application\nhas attracted attention of researchers due to NP-Complete nature of the\nproblem. In Grid environment, scheduling is deciding about assignment of tasks\nto available resources. Scheduling in Grid is challenging when the tasks have\ndependencies and resources are heterogeneous. The main objective in scheduling\nof dependent tasks is minimizing make-span. Due to NP-complete nature of\nscheduling problem, exact solutions cannot generate schedule efficiently.\nTherefore, researchers apply heuristic or random search techniques to get\noptimal or near to optimal solution of such problems. In this paper, we show\nhow Genetic Algorithm can be used to solve dependent task scheduling problem.\nWe describe how initial population can be generated using random assignment and\nheight based approaches. We also present design of crossover and mutation\noperators to enable scheduling of dependent tasks application without violating\ndependency constraints. For implementation of GA based scheduling, we explore\nand analyze SimGrid and GridSim simulation toolkits. From results, we found\nthat SimGrid is suitable, as it has support of SimDag API for DAG applications.\nWe found that GA based approach can generate schedule for dependent tasks\napplication in reasonable time while trying to minimize make-span."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.4326v3", 
    "other_authors": "Himanshu Chauhan, Vijay K. Garg, Aravind Natarajan, Neeraj Mittal", 
    "title": "Distributed Abstraction Algorithm for Online Predicate Detection", 
    "arxiv-id": "1304.4326v3", 
    "author": "Neeraj Mittal", 
    "publish": "2013-04-16T03:56:24Z", 
    "summary": "Analyzing a distributed computation is a hard problem in general due to the\ncombinatorial explosion in the size of the state-space with the number of\nprocesses in the system. By abstracting the computation, unnecessary\nexplorations can be avoided. Computation slicing is an approach for abstracting\ndis- tributed computations with respect to a given predicate. We focus on\nregular predicates, a family of predicates that covers a large number of\ncommonly used predicates for runtime verification. The existing algorithms for\ncomputation slicing are centralized in nature in which a single process is\nresponsible for computing the slice in either offline or online manner. In this\npaper, we present a distributed online algorithm for computing the slice of a\ndistributed computation with respect to a regular predicate. Our algorithm\ndistributes the work and storage requirements across the system, thus reducing\nthe space and computation complexities per process. In addition, for\nconjunctive predicates, our algorithm also reduces the message load per\nprocess."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.6994v1", 
    "other_authors": "Swan Dubois, Rachid Guerraoui", 
    "title": "Sp\u00e9culation et auto-stabilisation", 
    "arxiv-id": "1304.6994v1", 
    "author": "Rachid Guerraoui", 
    "publish": "2013-04-25T19:54:00Z", 
    "summary": "Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits a correct behaviour.\nSpeculation consists in guaranteeing that the system satisfies its requirements\nfor any execution but exhibits significantly better performances for a subset\nof executions that are more probable. A speculative protocol is in this sense\nsupposed to be both robust and efficient in practice. We introduce the notion\nof speculative stabilization which we illustrate through the mutual exclusion\nproblem. We then present a novel speculatively stabilizing mutual exclusion\nprotocol. Our protocol is self-stabilizing for any asynchronous execution. We\nprove that its stabilization time for synchronous executions is diam(g)/2 steps\n(where diam(g) denotes the diameter of the system). This complexity result is\nof independent interest. The celebrated mutual exclusion protocol of Dijkstra\nstabilizes in n steps (where n is the number of processes) in synchronous\nexecutions and the question whether the stabilization time could be strictly\nsmaller than the diameter has been open since then (almost 40 years). We show\nthat this is indeed possible for any underlying topology. We also provide a\nlower bound proof that shows that our new stabilization time of diam(g)/2 steps\nis optimal for synchronous executions, even if asynchronous stabilization is\nnot required."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1304.7654v1", 
    "other_authors": "Adrian Jackson, M. Sergio Campobasso", 
    "title": "Optimised hybrid parallelisation of a CFD code on Many Core   architectures", 
    "arxiv-id": "1304.7654v1", 
    "author": "M. Sergio Campobasso", 
    "publish": "2013-04-29T13:22:38Z", 
    "summary": "COSA is a novel CFD system based on the compressible Navier-Stokes model for\nunsteady aerodynamics and aeroelasticity of fixed structures, rotary wings and\nturbomachinery blades. It includes a steady, time domain, and harmonic balance\nflow solver.\n  COSA has primarily been parallelised using MPI, but there is also a hybrid\nparallelisation that adds OpenMP functionality to the MPI parallelisation to\nenable larger number of cores to be utilised for a given simulation as the MPI\nparallelisation is limited to the number of geometric partitions (or blocks) in\nthe simulation, or to exploit multi-threaded hardware where appropriate. This\npaper outlines the work undertaken to optimise these two parallelisation\nstrategies, improving the efficiency of both and therefore reducing the\ncomputational time required to compute simulations. We also analyse the power\nconsumption of the code on a range of leading HPC systems to further understand\nthe performance of the code."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2013.4204", 
    "link": "http://arxiv.org/pdf/1305.1121v1", 
    "other_authors": "John Augustine, Anisur Rahaman Molla, Ehab Morsy, Gopal Pandurangan, Peter Robinson, Eli Upfal", 
    "title": "Storage and Search in Dynamic Peer-to-Peer Networks", 
    "arxiv-id": "1305.1121v1", 
    "author": "Eli Upfal", 
    "publish": "2013-05-06T09:04:39Z", 
    "summary": "We study robust and efficient distributed algorithms for searching, storing,\nand maintaining data in dynamic Peer-to-Peer (P2P) networks. P2P networks are\nhighly dynamic networks that experience heavy node churn (i.e., nodes join and\nleave the network continuously over time). Our goal is to guarantee, despite\nhigh node churn rate, that a large number of nodes in the network can store,\nretrieve, and maintain a large number of data items. Our main contributions are\nfast randomized distributed algorithms that guarantee the above with high\nprobability (whp) even under high adversarial churn:\n  1. A randomized distributed search algorithm that (whp) guarantees that\nsearches from as many as $n - o(n)$ nodes ($n$ is the stable network size)\nsucceed in ${O}(\\log n)$-rounds despite ${O}(n/\\log^{1+\\delta} n)$ churn, for\nany small constant $\\delta > 0$, per round. We assume that the churn is\ncontrolled by an oblivious adversary (that has complete knowledge and control\nof what nodes join and leave and at what time, but is oblivious to the random\nchoices made by the algorithm).\n  2. A storage and maintenance algorithm that guarantees (whp) data items can\nbe efficiently stored (with only $\\Theta(\\log{n})$ copies of each data item)\nand maintained in a dynamic P2P network with churn rate up to\n${O}(n/\\log^{1+\\delta} n)$ per round. Our search algorithm together with our\nstorage and maintenance algorithm guarantees that as many as $n - o(n)$ nodes\ncan efficiently store, maintain, and search even under ${O}(n/\\log^{1+\\delta}\nn)$ churn per round. Our algorithms require only polylogarithmic in $n$ bits to\nbe processed and sent (per round) by each node.\n  To the best of our knowledge, our algorithms are the first-known,\nfully-distributed storage and search algorithms that provably work under highly\ndynamic settings (i.e., high churn rates per step)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s11227-015-1483-z", 
    "link": "http://arxiv.org/pdf/1305.1183v2", 
    "other_authors": "J. Filipovi\u010d, M. Madzin, J. Fousek, L. Matyska", 
    "title": "Optimizing CUDA Code By Kernel Fusion---Application on BLAS", 
    "arxiv-id": "1305.1183v2", 
    "author": "L. Matyska", 
    "publish": "2013-05-06T13:32:22Z", 
    "summary": "Modern GPUs are able to perform significantly more arithmetic operations than\ntransfers of a single word to or from global memory. Hence, many GPU kernels\nare limited by memory bandwidth and cannot exploit the arithmetic power of\nGPUs. However, the memory locality can be often improved by kernel fusion when\na sequence of kernels is executed and some kernels in this sequence share data.\n  In this paper, we show how kernels performing map, reduce or their nested\ncombinations can be fused automatically by our source-to-source compiler. To\ndemonstrate the usability of the compiler, we have implemented several BLAS-1\nand BLAS-2 routines and show how the performance of their sequences can be\nimproved by fusions.\n  Compared to similar sequences using CUBLAS, our compiler is able to generate\ncode that is up to 2.61x faster for the examples tested."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICWS.2013.84", 
    "link": "http://arxiv.org/pdf/1305.1842v1", 
    "other_authors": "Ward Jaradat, Alan Dearle, Adam Barker", 
    "title": "An Architecture for Decentralised Orchestration of Web Service Workflows", 
    "arxiv-id": "1305.1842v1", 
    "author": "Adam Barker", 
    "publish": "2013-05-08T15:07:08Z", 
    "summary": "Service-oriented workflows are typically executed using a centralised\norchestration approach that presents significant scalability challenges. These\nchallenges include the consumption of network bandwidth, degradation of\nperformance, and single-points of failure. We provide a decentralised\norchestration architecture that attempts to address these challenges. Our\narchitecture adopts a design model that permits the computation to be moved\n\"closer\" to services in a workflow. This is achieved by partitioning workflows\nspecified using our simple dataflow language into smaller fragments, which may\nbe sent to remote locations for execution."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2460756.2460759", 
    "link": "http://arxiv.org/pdf/1305.2319v1", 
    "other_authors": "Yehia Elkhatib, Gordon S. Blair, Bholanathsingh Surajbali", 
    "title": "Experiences of Using a Hybrid Cloud to Construct an Environmental   Virtual Observatory", 
    "arxiv-id": "1305.2319v1", 
    "author": "Bholanathsingh Surajbali", 
    "publish": "2013-05-10T12:08:43Z", 
    "summary": "Environmental science is often fragmented: data is collected using mismatched\nformats and conventions, and models are misaligned and run in isolation. Cloud\ncomputing offers a lot of potential in the way of resolving such issues by\nsupporting data from different sources and at various scales, by facilitating\nthe integration of models to create more sophisticated software services, and\nby providing a sustainable source of suitable computational and storage\nresources. In this paper, we highlight some of our experiences in building the\nEnvironmental Virtual Observatory pilot (EVOp), a tailored cloud-based\ninfrastructure and associated web-based tools designed to enable users from\ndifferent backgrounds to access data concerning different environmental issues.\nWe review our architecture design, the current deployment and prototypes. We\nalso reflect on lessons learned. We believe that such experiences are of\nbenefit to other scientific communities looking to assemble virtual\nobservatories or similar virtual research environments."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2460756.2460759", 
    "link": "http://arxiv.org/pdf/1305.3031v1", 
    "other_authors": "Ashkan Paya, Dan C. Marinescu", 
    "title": "Clustering Algorithms for Scale-free Networks and Applications to Cloud   Resource Management", 
    "arxiv-id": "1305.3031v1", 
    "author": "Dan C. Marinescu", 
    "publish": "2013-05-14T06:19:43Z", 
    "summary": "In this paper we introduce algorithms for the construction of scale-free\nnetworks and for clustering around the nerve centers, nodes with a high\nconnectivity in a scale-free networks. We argue that such overlay networks\ncould support self-organization in a complex system like a cloud computing\ninfrastructure and allow the implementation of optimal resource management\npolicies."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2460756.2460759", 
    "link": "http://arxiv.org/pdf/1305.3123v1", 
    "other_authors": "Muhammad Hilman, Heru Suhartanto, Arry Yanuar", 
    "title": "Performance Analysis of Embarassingly Parallel Application on Cluster   Computer Environment: A Case Study of Virtual Screening with Autodock Vina   1.1 on Hastinapura Cluster", 
    "arxiv-id": "1305.3123v1", 
    "author": "Arry Yanuar", 
    "publish": "2013-05-14T11:57:20Z", 
    "summary": "IT based scientific research requires high computational resources. The\nlimitation on funding and infrastructure led the high performance computing era\nfrom supercomputer to cluster and grid computing technology. Parallel\napplication running well on cluster computer as well as supercomputer, one of\nthe type is embarrassingly parallel application. Many scientist loves EP\nbecause it doesn't need any sophisticated technique but gives amazing\nperformance. This paper discuss the bioinformatics research that used\nembarrassingly application and show its performance on cluster computer."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2460756.2460759", 
    "link": "http://arxiv.org/pdf/1305.4263v1", 
    "other_authors": "Peva Blanchard, Shlomi Dolev, Joffroy Beauquier, Sylvie Dela\u00ebt", 
    "title": "Self-Stabilizing Paxos", 
    "arxiv-id": "1305.4263v1", 
    "author": "Sylvie Dela\u00ebt", 
    "publish": "2013-05-18T13:20:11Z", 
    "summary": "We present the first self-stabilizing consensus and replicated state machine\nfor asynchronous message passing systems. The scheme does not require that all\nparticipants make a certain number of steps prior to reaching a practically\ninfinite execution where the replicated state machine exhibits the desired\nbehavior. In other words, the system reaches a configuration from which it\noperates according to the specified requirements of the replicated\nstate-machine, for a long enough execution regarding all practical\nconsiderations."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.4365v1", 
    "other_authors": "Anjan K. Koundinya, Harish G., Srinath N. K., Raghavendra G. E., Pramod Y. V., Sandeep R., Punith Kumar G", 
    "title": "Performance Analysis of Parallel Pollard's Rho Algorithm", 
    "arxiv-id": "1305.4365v1", 
    "author": "Punith Kumar G", 
    "publish": "2013-05-19T14:44:57Z", 
    "summary": "Integer factorization is one of the vital algorithms discussed as a part of\nanalysis of any black-box cipher suites where the cipher algorithm is based on\nnumber theory. The origin of the problem is from Discrete Logarithmic Problem\nwhich appears under the analysis of the crypto-graphic algorithms as seen by a\ncrypt-analyst. The integer factorization algorithm poses a potential in\ncomputational science too, obtaining the factors of a very large number is\nchallenging with a limited computing infrastructure. This paper analyses the\nPollards Rho heuristic with a varying input size to evaluate the performance\nunder a multi-core environment and also to estimate the threshold for each\ncomputing infrastructure."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.4367v1", 
    "other_authors": "Rapha\u00ebl Jolly", 
    "title": "Parallelizing Stream with Future", 
    "arxiv-id": "1305.4367v1", 
    "author": "Rapha\u00ebl Jolly", 
    "publish": "2013-05-19T15:00:14Z", 
    "summary": "Stream is re-interpreted in terms of a Lazy monad. Future is substituted for\nLazy in the obtained construct, resulting in possible parallelization of any\nalgorithm expressible as a Stream computation. The principle is tested against\ntwo example algorithms. Performance is evaluated, and a way to improve it\nbriefly discussed."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.4376v1", 
    "other_authors": "Lukasz Swierczewski", 
    "title": "3DES ECB Optimized for Massively Parallel CUDA GPU Architecture", 
    "arxiv-id": "1305.4376v1", 
    "author": "Lukasz Swierczewski", 
    "publish": "2013-05-19T16:02:42Z", 
    "summary": "Modern computers have graphics cards with much higher theoretical efficiency\nthan conventional CPU. The paper presents application possibilities GPU CUDA\nacceleration for encryption of data using the new architecture tailored to the\n3DES algorithm, characterized by increased security compared to the normal DES.\nThe algorithm used in ECB mode (Electronic Codebook), in which 64-bit data\nblocks are encrypted independently by stream processors (CUDA cores)."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.4868v3", 
    "other_authors": "Christian Cachin, Dan Dobre, Marko Vukolic", 
    "title": "Asynchronous BFT Storage with 2t+1 Data Replicas", 
    "arxiv-id": "1305.4868v3", 
    "author": "Marko Vukolic", 
    "publish": "2013-05-21T16:06:23Z", 
    "summary": "The cost of Byzantine Fault Tolerant (BFT) storage is the main concern\npreventing its adoption in practice. This cost stems from the need to maintain\nat least 3t+1 replicas in different storage servers in the asynchronous model,\nso that t Byzantine replica faults can be tolerated. In this paper, we present\nMDStore, the first fully asynchronous read/write BFT storage protocol that\nreduces the number of data replicas to as few as 2t+1, maintaining 3t+1\nreplicas of metadata at (possibly) different servers. At the heart of MDStore\nstore is its metadata service that is built upon a new abstraction we call\ntimestamped storage. Timestamped storage both allows for conditional writes\n(facilitating the implementation of a metadata service) and has consensus\nnumber one (making it implementable wait-free in an asynchronous system despite\nfaults). In addition to its low data replication factor, MDStore offers very\nstrong guarantees implementing multi-writer multi-reader atomic wait-free\nsemantics and tolerating any number of Byzantine readers and crash-faulty\nwriters. We further show that MDStore data replication overhead is optimal;\nnamely, we prove a lower bound of 2t+1 on the number of data replicas that\napplies even to crash-tolerant storage with a fault-free metadata service\noracle. Finally, we prove that separating data from metadata for reducing the\ncost of BFT storage is not possible without cryptographic assumptions. However,\nour MDStore protocol uses only lightweight cryptographic hash functions."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.5608v1", 
    "other_authors": "Wei Wang, Baochun Li, Ben Liang", 
    "title": "To Reserve or Not to Reserve: Optimal Online Multi-Instance Acquisition   in IaaS Clouds", 
    "arxiv-id": "1305.5608v1", 
    "author": "Ben Liang", 
    "publish": "2013-05-24T02:58:50Z", 
    "summary": "Infrastructure-as-a-Service (IaaS) clouds offer diverse instance purchasing\noptions. A user can either run instances on demand and pay only for what it\nuses, or it can prepay to reserve instances for a long period, during which a\nusage discount is entitled. An important problem facing a user is how these two\ninstance options can be dynamically combined to serve time-varying demands at\nminimum cost. Existing strategies in the literature, however, require either\nexact knowledge or the distribution of demands in the long-term future, which\nsignificantly limits their use in practice. Unlike existing works, we propose\ntwo practical online algorithms, one deterministic and another randomized, that\ndynamically combine the two instance options online without any knowledge of\nthe future. We show that the proposed deterministic (resp., randomized)\nalgorithm incurs no more than 2-alpha (resp., e/(e-1+alpha)) times the minimum\ncost obtained by an optimal offline algorithm that knows the exact future a\npriori, where alpha is the entitled discount after reservation. Our online\nalgorithms achieve the best possible competitive ratios in both the\ndeterministic and randomized cases, and can be easily extended to cases when\nshort-term predictions are reliable. Simulations driven by a large volume of\nreal-world traces show that significant cost savings can be achieved with\nprevalent IaaS prices."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.5800v1", 
    "other_authors": "Dave Dice, Danny Hendler, Ilya Mirsky", 
    "title": "Lightweight Contention Management for Efficient Compare-and-Swap   Operations", 
    "arxiv-id": "1305.5800v1", 
    "author": "Ilya Mirsky", 
    "publish": "2013-05-24T17:05:51Z", 
    "summary": "Many concurrent data-structure implementations use the well-known\ncompare-and-swap (CAS) operation, supported in hardware by most modern\nmultiprocessor architectures for inter-thread synchronization. A key weakness\nof the CAS operation is the degradation in its performance in the presence of\nmemory contention.\n  In this work we study the following question: can software-based contention\nmanagement improve the efficiency of hardware-provided CAS operations? Our\nperformance evaluation establishes that lightweight contention management\nsupport can greatly improve performance under medium and high contention levels\nwhile typically incurring only small overhead when contention is low."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.6123v1", 
    "other_authors": "S. M. M. M Kalyan kumar, Dr S C Pradhan", 
    "title": "Building Internal Cloud at NIC : A Preview", 
    "arxiv-id": "1305.6123v1", 
    "author": "Dr S C Pradhan", 
    "publish": "2013-05-27T06:35:01Z", 
    "summary": "The most of computing environments in the IT support organization like NIC\nare designed to run in centralized datacentre. The centralized infrastructure\nof various development projects are used to deploy their services on it and\nconnecting remotely to that datacentre from all the stations of organization.\nCurrently these servers are mostly underutilized due to the static and\nconventional approaches used for accessing and utilizing of these resources.\nThe cloud patterns is much needful for optimizing resource utilization and\nreducing the investments on unnecessary costs. So, we build up and prototyped a\nprivate cloud system called nIC(NIC Internal Cloud) to leverage the benefits of\ncloud environment. For this system we adopted the combination of various\ntechniques from open source software community. The user-base of nIC consists\ndevelopers, web and database admins, service providers and desktop users from\nvarious projects in NIC. We can optimize the resource usage by customizing the\nuser based template services on these virtualized infrastructure. It will also\nincreases the flexibility of the managing and maintenance of the operations\nlike archiving, disaster recovery and scaling of resources. The open-source\napproach is further decreases the enterprise costs. In this paper, we describe\nthe design and analysis of implementing issues on internal cloud environments\nin NIC and similar organizations."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.6203v1", 
    "other_authors": "Iulia Dumitru, Grigore Stamatescu, Ioana Fagarasan, Sergiu Stelian Iliescu", 
    "title": "Dynamic Management Techniques for Increasing Energy Efficiency within a   Data Center", 
    "arxiv-id": "1305.6203v1", 
    "author": "Sergiu Stelian Iliescu", 
    "publish": "2013-05-27T13:06:36Z", 
    "summary": "In ours days data centers provide the global community an indispensable\nservice: nearly unlimited access to almost any kind of information we can\nimagine by supporting most Internet services such as: Web hosting and\nE-commerce services. Because of their capacity and their work, data centers\nhave various impacts on the environment, but those related with the electricity\nuse are by far the most important. In this paper, we present several power and\nenergy management techniques for data centers and we will focus our attention\non techniques that are explicitly tailored to servers and their workloads."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.6474v1", 
    "other_authors": "Martin Wimmer, Daniel Cederman, Jesper Larsson Tr\u00e4ff, Philippas Tsigas", 
    "title": "Configurable Strategies for Work-stealing", 
    "arxiv-id": "1305.6474v1", 
    "author": "Philippas Tsigas", 
    "publish": "2013-05-28T12:59:25Z", 
    "summary": "Work-stealing systems are typically oblivious to the nature of the tasks they\nare scheduling. For instance, they do not know or take into account how long a\ntask will take to execute or how many subtasks it will spawn. Moreover, the\nactual task execution order is typically determined by the underlying task\nstorage data structure, and cannot be changed. There are thus possibilities for\noptimizing task parallel executions by providing information on specific tasks\nand their preferred execution order to the scheduling system.\n  We introduce scheduling strategies to enable applications to dynamically\nprovide hints to the task-scheduling system on the nature of specific tasks.\nScheduling strategies can be used to independently control both local task\nexecution order as well as steal order. In contrast to conventional scheduling\npolicies that are normally global in scope, strategies allow the scheduler to\napply optimizations on individual tasks. This flexibility greatly improves\ncomposability as it allows the scheduler to apply different, specific\nscheduling choices for different parts of applications simultaneously. We\npresent a number of benchmarks that highlight diverse, beneficial effects that\ncan be achieved with scheduling strategies. Some benchmarks (branch-and-bound,\nsingle-source shortest path) show that prioritization of tasks can reduce the\ntotal amount of work compared to standard work-stealing execution order. For\nother benchmarks (triangle strip generation) qualitatively better results can\nbe achieved in shorter time. Other optimizations, such as dynamic merging of\ntasks or stealing of half the work, instead of half the tasks, are also shown\nto improve performance. Composability is demonstrated by examples that combine\ndifferent strategies, both within the same kernel (prefix sum) as well as when\nscheduling multiple kernels (prefix sum and unbalanced tree search)."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.6624v1", 
    "other_authors": "Priyanka Kumar, Sathya Peri", 
    "title": "A TimeStamp based Multi-version STM Protocol that satisfies Opacity and   Multi-Version Permissiveness", 
    "arxiv-id": "1305.6624v1", 
    "author": "Sathya Peri", 
    "publish": "2013-05-28T20:41:06Z", 
    "summary": "Software Transactional Memory Systems (STM) are a promising alternative to\nlock based systems for concurrency control in shared memory systems. In\nmultiversion STM systems, each write on a transaction object produces a new\nversion of that object. The advantage obtained by storing multiple versions is\nthat one can ensure that read operations do not fail. Opacity is a commonly\nused correctness criterion for STM systems. Multi-Version permissive STM system\nnever aborts a read-only transaction. Although many multi-version STM systems\nhave been proposed, to the best of our knowledge none of them have been\nformally proved to satisfy opacity. In this paper we present a time-stamp based\nmultiversion STM system that satisfies opacity and mv-permissiveness. We\nformally prove the correctness of the proposed STM system. We also present\ngarbage collection procedure which deletes unwanted versions of the transaction\nobjects and formally prove it correctness."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.7403v1", 
    "other_authors": "Jonathan Stuart Ward, Adam Barker", 
    "title": "Monitoring Large-Scale Cloud Systems with Layered Gossip Protocols", 
    "arxiv-id": "1305.7403v1", 
    "author": "Adam Barker", 
    "publish": "2013-05-31T14:11:15Z", 
    "summary": "Monitoring is an essential aspect of maintaining and developing computer\nsystems that increases in difficulty proportional to the size of the system.\nThe need for robust monitoring tools has become more evident with the advent of\ncloud computing. Infrastructure as a Service (IaaS) clouds allow end users to\ndeploy vast numbers of virtual machines as part of dynamic and transient\narchitectures. Current monitoring solutions, including many of those in the\nopen-source domain rely on outdated concepts including manual deployment and\nconfiguration, centralised data collection and adapt poorly to membership\nchurn.\n  In this paper we propose the development of a cloud monitoring suite to\nprovide scalable and robust lookup, data collection and analysis services for\nlarge-scale cloud systems. In lieu of centrally managed monitoring we propose a\nmulti-tier architecture using a layered gossip protocol to aggregate monitoring\ninformation and facilitate lookup, information collection and the\nidentification of redundant capacity. This allows for a resource aware data\ncollection and storage architecture that operates over the system being\nmonitored. This in turn enables monitoring to be done in-situ without the need\nfor significant additional infrastructure to facilitate monitoring services. We\nevaluate this approach against alternative monitoring paradigms and demonstrate\nhow our solution is well adapted to usage in a cloud-computing context."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1305.7429v3", 
    "other_authors": "Marco Canini, Petr Kuznetsov, Dan Levin, Stefan Schmid", 
    "title": "A Distributed SDN Control Plane for Consistent Policy Updates", 
    "arxiv-id": "1305.7429v3", 
    "author": "Stefan Schmid", 
    "publish": "2013-05-31T14:45:31Z", 
    "summary": "Software-defined networking (SDN) is a novel paradigm that out-sources the\ncontrol of packet-forwarding switches to a set of software controllers. The\nmost fundamental task of these controllers is the correct implementation of the\n\\emph{network policy}, i.e., the intended network behavior. In essence, such a\npolicy specifies the rules by which packets must be forwarded across the\nnetwork.\n  This paper studies a distributed SDN control plane that enables\n\\emph{concurrent} and \\emph{robust} policy implementation. We introduce a\nformal model describing the interaction between the data plane and a\ndistributed control plane (consisting of a collection of fault-prone\ncontrollers). Then we formulate the problem of \\emph{consistent} composition of\nconcurrent network policy updates (short: the \\emph{CPC Problem}). To\nanticipate scenarios in which some conflicting policy updates must be rejected,\nwe enable the composition via a natural \\emph{transactional} interface with\nall-or-nothing semantics.\n  We show that the ability of an $f$-resilient distributed control plane to\nprocess concurrent policy updates depends on the tag complexity, i. e., the\nnumber of policy labels (a.k.a. \\emph{tags}) available to the controllers, and\ndescribe a CPC protocol with optimal tag complexity $f+2$."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.0077v1", 
    "other_authors": "Steven Cheng, Lisa Higham, Jalal Kawash", 
    "title": "Partition Consistency: A Case Study in Modeling Systems with Weak Memory   Consistency and Proving Correctness of their Implementations", 
    "arxiv-id": "1306.0077v1", 
    "author": "Jalal Kawash", 
    "publish": "2013-06-01T04:35:02Z", 
    "summary": "Multiprocess systems, including grid systems, multiprocessors and multicore\ncomputers, incorporate a variety of specialized hardware and software\nmechanisms, which speed computation, but result in complex memory behavior. As\na consequence, the possible outcomes of a concurrent program can be unexpected.\nA memory consistency model is a description of the behaviour of such a system.\nAbstract memory consistency models aim to capture the concrete implementations\nand architectures. Therefore, formal specification of the implementation or\narchitecture is necessary, and proofs of correspondence between the abstract\nand the concrete models are required.\n  This paper provides a case study of this process. We specify a new model,\npartition consistency, that generalizes many existing consistency models. A\nconcrete message-passing network model is also specified. Implementations of\npartition consistency on this network model are then presented and proved\ncorrect. A middle level of abstraction is utilized to facilitate the proofs.\nAll three levels of abstraction are specified using the same framework. The\npaper aims to illustrate a general methodology and techniques for specifying\nmemory consistency models and proving the correctness of their implementations."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.0326v1", 
    "other_authors": "Tomasz Kajdanowicz, Przemyslaw Kazienko, Wojciech Indyk", 
    "title": "Parallel Processing of Large Graphs", 
    "arxiv-id": "1306.0326v1", 
    "author": "Wojciech Indyk", 
    "publish": "2013-06-03T08:44:32Z", 
    "summary": "More and more large data collections are gathered worldwide in various IT\nsystems. Many of them possess the networked nature and need to be processed and\nanalysed as graph structures. Due to their size they require very often usage\nof parallel paradigm for efficient computation. Three parallel techniques have\nbeen compared in the paper: MapReduce, its map-side join extension and Bulk\nSynchronous Parallel (BSP). They are implemented for two different graph\nproblems: calculation of single source shortest paths (SSSP) and collective\nclassification of graph nodes by means of relational influence propagation\n(RIP). The methods and algorithms are applied to several network datasets\ndiffering in size and structural profile, originating from three domains:\ntelecommunication, multimedia and microblog. The results revealed that\niterative graph processing with the BSP implementation always and\nsignificantly, even up to 10 times outperforms MapReduce, especially for\nalgorithms with many iterations and sparse communication. Also MapReduce\nextension based on map-side join usually noticeably presents better efficiency,\nalthough not as much as BSP. Nevertheless, MapReduce still remains the good\nalternative for enormous networks, whose data structures do not fit in local\nmemories."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.0441v1", 
    "other_authors": "Islam Elgedawy", 
    "title": "DCaaS: Data Consistency as a Service for Managing Data Uncertainty on   the Clouds", 
    "arxiv-id": "1306.0441v1", 
    "author": "Islam Elgedawy", 
    "publish": "2013-06-03T14:46:08Z", 
    "summary": "Ensuring data correctness over partitioned distributed database systems is a\nclassical problem. Classical solutions proposed to solve this problem are\nmainly adopting locking or blocking techniques. These techniques are not\nsuitable for cloud environments as they produce terrible response times; due to\nthe long latency and faultiness of wide area network connections among cloud\ndatacenters. One way to improve performance is to restrict access of\nusers-bases to specific datacenters and avoid data sharing between datacenters.\nHowever, conflicts might appear when data is replicated between datacenters;\nnevertheless change propagation timeliness is not guaranteed. Such problems\ncreated data uncertainty on cloud environments. Managing data uncertainty is\none of the main obstacles for supporting global distributed transactions on the\nclouds. To overcome this problem, this paper proposes an quota-based approach\nfor managing data uncertainty on the clouds that guarantees global data\ncorrectness without global locking or blocking. To decouple service developers\nfrom the hassles of managing data uncertainty, we propose to use a new platform\nservice (i.e. Data Consistency as a Service (DCaaS)) to encapsulate the\nproposed approach. DCaaS service also ensures SaaS services cloud portability,\nas it works as a cloud adapter between SaaS service instances. Experiments show\nthat proposed approach realized by the DCaaS service provides much better\nresponse time when compared with classical locking and blocking techniques."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.0448v1", 
    "other_authors": "W. El-Haweet, Islam Elgedawy, Ibrahim Abd El-Salam", 
    "title": "Adaptive Fixed Priority End-To-End Imprecise Scheduling In Distributed   Real Time Systems", 
    "arxiv-id": "1306.0448v1", 
    "author": "Ibrahim Abd El-Salam", 
    "publish": "2013-06-03T15:07:27Z", 
    "summary": "In end-to-end distributed real time systems, a task may be executed\nsequentially on different processors. The end-toend task response time must not\nexceed the end-to-end task deadline to consider the task a schedulable task. In\ntransient over load periods, deadlines may be missed or processors may\nsaturate. The imprecise computation technique is a way to overcome the\nmentioned problems by trading off precision and timeliness. We developed an\nimprecise integrated framework for scheduling fixed priority end-to-end tasks\nin distributed real time systems by extending an existing integrated framework\nfor the same problem. We devised a new priority assignment scheme called global\nmandatory relevance scheme to meet the concept of imprecise computation. We\ndevised an algorithm for processor utilization adjustment, this algorithm\ndecreases the processor load when the processor utilization is greater than\none. Also we extended the schedulability analysis algorithms presented in the\nold framework to allow adaptive priority assignment and to meet imprecise\ncomputation concept. Simulation results showed that our new framework is more\ndependable and predictable than the existing framework over transient overload\nperiods."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.1303v1", 
    "other_authors": "Putti Srinivasrao, V. P. C. Rao, A. Govardhan, Ambika Prasad Mohanty", 
    "title": "Scalable Distributed Job Processing with Dynamic Load Balancing", 
    "arxiv-id": "1306.1303v1", 
    "author": "Ambika Prasad Mohanty", 
    "publish": "2013-06-06T06:32:54Z", 
    "summary": "We present here a cost effective framework for a robust scalable and\ndistributed job processing system that adapts to the dynamic computing needs\neasily with efficient load balancing for heterogeneous systems. The design is\nsuch that each of the components are self contained and do not depend on each\nother. Yet, they are still interconnected through an enterprise message bus so\nas to ensure safe, secure and reliable communication based on transactional\nfeatures to avoid duplication as well as data loss. The load balancing,\nfault-tolerance and failover recovery are built into the system through a\nmechanism of health check facility and a queue based load balancing. The system\nhas a centralized repository with central monitors to keep track of the\nprogress of various job executions as well as status of processors in\nreal-time. The basic requirement of assigning a priority and processing as per\npriority is built into the framework. The most important aspect of the\nframework is that it avoids the need for job migration by computing the target\nprocessors based on the current load and the various cost factors. The\nframework will have the capability to scale horizontally as well as vertically\nto achieve the required performance, thus effectively minimizing the total cost\nof ownership."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2013.5214", 
    "link": "http://arxiv.org/pdf/1306.1394v1", 
    "other_authors": "Jonathan Stuart Ward, Adam Barker", 
    "title": "A Cloud Computing Survey: Developments and Future Trends in   Infrastructure as a Service Computing", 
    "arxiv-id": "1306.1394v1", 
    "author": "Adam Barker", 
    "publish": "2013-06-06T12:41:57Z", 
    "summary": "Cloud computing is a recent paradigm based around the notion of delivery of\nresources via a service model over the Internet. Despite being a new paradigm\nof computation, cloud computing owes its origins to a number of previous\nparadigms. The term cloud computing is well defined and no longer merits\nrigorous taxonomies to furnish a definition. Instead this survey paper\nconsiders the past, present and future of cloud computing. As an evolution of\nprevious paradigms, we consider the predecessors to cloud computing and what\nsignificance they still hold to cloud services. Additionally we examine the\ntechnologies which comprise cloud computing and how the challenges and future\ndevelopments of these technologies will influence the field. Finally we examine\nthe challenges that limit the growth, application and development of cloud\ncomputing and suggest directions required to overcome these challenges in order\nto further the success of cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.1639v1", 
    "other_authors": "Inderveer Chana, Tarandeep Kaur", 
    "title": "Delivering IT as A Utility- A Systematic Review", 
    "arxiv-id": "1306.1639v1", 
    "author": "Tarandeep Kaur", 
    "publish": "2013-06-07T07:37:31Z", 
    "summary": "Utility Computing has facilitated the creation of new markets that has made\nit possible to realize the long held dream of delivering IT as a Utility. Even\nthough utility computing is in its nascent stage today, the proponents of\nutility computing envisage that it will become a commodity business in the\nupcoming time and utility service providers will meet all the IT requests of\nthe companies. This paper takes a cross-sectional view at the emergence of\nutility computing along with different requirements needed to realize utility\nmodel. It also surveys the current trends in utility computing highlighting\ndiverse architecture models aligned towards delivering IT as a utility.\nDifferent resource management systems for proficient allocation of resources\nhave been listed together with various resource scheduling and pricing\nstrategies used by them. Further, a review of generic key perspectives closely\nrelated to the concept of delivering IT as a Utility has been taken citing the\ncontenders for the future enhancements in this technology in the form of Grid\nand Cloud Computing."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.1692v1", 
    "other_authors": "Sebastian Kniesburges, Andreas Koutsopoulos, Christian Scheideler", 
    "title": "A DeterministicWorst-Case Message Complexity Optimal Solution for   Resource Discovery", 
    "arxiv-id": "1306.1692v1", 
    "author": "Christian Scheideler", 
    "publish": "2013-06-07T11:23:40Z", 
    "summary": "We consider the problem of resource discovery in distributed systems. In\nparticular we give an algorithm, such that each node in a network discovers the\naddress of any other node in the network. We model the knowledge of the nodes\nas a virtual overlay network given by a directed graph such that complete\nknowledge of all nodes corresponds to a complete graph in the overlay network.\nAlthough there are several solutions for resource discovery, our solution is\nthe first that achieves worst-case optimal work for each node, i.e. the number\nof addresses (O(n)) or bits (O(n log n)) a node receives or sends coincides\nwith the lower bound, while ensuring only a linear runtime (O(n)) on the number\nof rounds."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.1861v1", 
    "other_authors": "Antonio Fern\u00e1ndez Anta, Chryssis Georgiou, Dariusz R. Kowalski, Elli Zavou", 
    "title": "Online Parallel Scheduling of Non-uniform Tasks: Trading Failures for   Energy", 
    "arxiv-id": "1306.1861v1", 
    "author": "Elli Zavou", 
    "publish": "2013-06-08T00:34:57Z", 
    "summary": "Consider a system in which tasks of different execution times arrive\ncontinuously and have to be executed by a set of processors that are prone to\ncrashes and restarts. In this paper we model and study the impact of\nparallelism and failures on the competitiveness of such an online system. In a\nfault-free environment, a simple Longest-in-System scheduling policy, enhanced\nby a redundancy-avoidance mechanism, guarantees optimality in a long-term\nexecution. In the presence of failures though, scheduling becomes a much more\nchallenging task. In particular, no parallel deterministic algorithm can be\ncompetitive against an offline optimal solution, even with one single processor\nand tasks of only two different execution times. We find that when additional\nenergy is provided to the system in the form of processor speedup, the\nsituation changes. Specifically, we identify thresholds on the speedup under\nwhich such competitiveness cannot be achieved by any deterministic algorithm,\nand above which competitive algorithms exist. Finally, we propose algorithms\nthat achieve small bounded competitive ratios when the speedup is over the\nthreshold."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.1888v1", 
    "other_authors": "Elarbi Badidi", 
    "title": "A Framework for Software-as-a-Service Selection and Provisioning", 
    "arxiv-id": "1306.1888v1", 
    "author": "Elarbi Badidi", 
    "publish": "2013-06-08T07:20:49Z", 
    "summary": "As cloud computing is increasingly transforming the information technology\nlandscape, organizations and businesses are exhibiting strong interest in\nSoftware-as-a-Service (SaaS) offerings that can help them increase business\nagility and reduce their operational costs. They increasingly demand services\nthat can meet their functional and non-functional requirements. Given the\nplethora and the variety of SaaS offerings, we propose, in this paper, a\nframework for SaaS provisioning, which relies on brokered Service Level\nagreements (SLAs), between service consumers and SaaS providers. The Cloud\nService Broker (CSB) helps service consumers find the right SaaS providers that\ncan fulfil their functional and non-functional requirements. The proposed\nselection algorithm ranks potential SaaS providers by matching their offerings\nagainst the requirements of the service consumer using an aggregate utility\nfunction. Furthermore, the CSB is in charge of conducting SLA negotiation with\nselected SaaS providers, on behalf of service consumers, and performing SLA\ncompliance monitoring."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.1928v1", 
    "other_authors": "Joel M. Crichlow, Stephen J. Hartley, Michael Hosein", 
    "title": "A Light-Weight Distributed System for the processing of Replicated   Counter-like Objects", 
    "arxiv-id": "1306.1928v1", 
    "author": "Michael Hosein", 
    "publish": "2013-06-08T15:05:18Z", 
    "summary": "In order to increase availability in a distributed system some or all of the\ndata items are replicated and stored at separate sites. This is an issue of key\nconcern especially since there is such a proliferation of wireless technologies\nand mobile users. However, the concurrent processing of transactions at\nseparate sites can generate inconsistencies in the stored information. We have\nbuilt a distributed service that manages updates to widely deployed\ncounter-like replicas. There are many heavy-weight distributed systems\ntargeting large information critical applications. Our system is intentionally,\nrelatively lightweight and useful for the somewhat reduced information critical\napplications. The service is built on our distributed concurrency control\nscheme which combines optimism and pessimism in the processing of transactions.\nThe service allows a transaction to be processed immediately (optimistically)\nat any individual replica as long as the transaction satisfies a cost bound.\nAll transactions are also processed in a concurrent pessimistic manner to\nensure mutual consistency."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.3906v1", 
    "other_authors": "Masoud Saeida Ardekani, Pierre Sutra, Nuno Pregui\u00e7a, Marc Shapiro", 
    "title": "Non-Monotonic Snapshot Isolation", 
    "arxiv-id": "1306.3906v1", 
    "author": "Marc Shapiro", 
    "publish": "2013-06-17T15:45:13Z", 
    "summary": "Many distributed applications require transactions. However, transactional\nprotocols that require strong synchronization are costly in large scale\nenvironments. Two properties help with scalability of a transactional system:\ngenuine partial replication (GPR), which leverages the intrinsic parallelism of\na workload, and snapshot isolation (SI), which decreases the need for\nsynchronization. We show that, under standard assumptions (data store accesses\nare not known in advance, and transactions may access arbitrary objects in the\ndata store), it is impossible to have both SI and GPR. To circumvent this\nimpossibility, we propose a weaker consistency criterion, called Non-monotonic\nSnapshot Isolation (NMSI). NMSI retains the most important properties of SI,\ni.e., read-only transactions always commit, and two write-conflicting updates\ndo not both commit. We present a GPR protocol that ensures NMSI, and has lower\nmessage cost (i.e., it contacts fewer replicas and/or commits faster) than\nprevious approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijfcst.2013.3302", 
    "link": "http://arxiv.org/pdf/1306.4161v1", 
    "other_authors": "Jean-Noel Quintin, Khalid Hasanov, Alexey Lastovetsky", 
    "title": "Hierarchical Parallel Matrix Multiplication on Large-Scale Distributed   Memory Platforms", 
    "arxiv-id": "1306.4161v1", 
    "author": "Alexey Lastovetsky", 
    "publish": "2013-06-18T12:17:36Z", 
    "summary": "Matrix multiplication is a very important computation kernel both in its own\nright as a building block of many scientific applications and as a popular\nrepresentative for other scientific applications. Cannon algorithm which dates\nback to 1969 was the first efficient algorithm for parallel matrix\nmultiplication providing theoretically optimal communication cost. However this\nalgorithm requires a square number of processors. In the mid 1990s, the SUMMA\nalgorithm was introduced. SUMMA overcomes the shortcomings of Cannon algorithm\nas it can be used on a non-square number of processors as well. Since then the\nnumber of processors in HPC platforms has increased by two orders of magnitude\nmaking the contribution of communication in the overall execution time more\nsignificant. Therefore, the state of the art parallel matrix multiplication\nalgorithms should be revisited to reduce the communication cost further. This\npaper introduces a new parallel matrix multiplication algorithm, Hierarchical\nSUMMA (HSUMMA), which is a redesign of SUMMA. Our algorithm reduces the\ncommunication cost of SUMMA by introducing a two-level virtual hierarchy into\nthe two-dimensional arrangement of processors. Experiments on an IBM BlueGene-P\ndemonstrate the reduction of communication cost up to 2.08 times on 2048 cores\nand up to 5.89 times on 16384 cores."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.4956v1", 
    "other_authors": "Saeid Abolfazli, Zohreh Sanaei, Ejaz Ahmed, Abdullah Gani, Rajkumar Buyya", 
    "title": "Cloud-Based Augmentation for Mobile Devices: Motivation, Taxonomies, and   Open Challenges", 
    "arxiv-id": "1306.4956v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2013-06-20T18:41:00Z", 
    "summary": "Recently, Cloud-based Mobile Augmentation (CMA) approaches have gained\nremarkable ground from academia and industry. CMA is the state-of-the-art\nmobile augmentation model that employs resource-rich clouds to increase,\nenhance, and optimize computing capabilities of mobile devices aiming at\nexecution of resource-intensive mobile applications. Augmented mobile devices\nenvision to perform extensive computations and to store big data beyond their\nintrinsic capabilities with least footprint and vulnerability. Researchers\nutilize varied cloud-based computing resources (e.g., distant clouds and nearby\nmobile nodes) to meet various computing requirements of mobile users. However,\nemploying cloud-based computing resources is not a straightforward panacea.\nComprehending critical factors that impact on augmentation process and optimum\nselection of cloud-based resource types are some challenges that hinder CMA\nadaptability. This paper comprehensively surveys the mobile augmentation domain\nand presents taxonomy of CMA approaches. The objectives of this study is to\nhighlight the effects of remote resources on the quality and reliability of\naugmentation processes and discuss the challenges and opportunities of\nemploying varied cloud-based resources in augmenting mobile devices. We present\naugmentation definition, motivation, and taxonomy of augmentation types,\nincluding traditional and cloud-based. We critically analyze the\nstate-of-the-art CMA approaches and classify them into four groups of distant\nfixed, proximate fixed, proximate mobile, and hybrid to present a taxonomy.\nVital decision making and performance limitation factors that influence on the\nadoption of CMA approaches are introduced and an exemplary decision making\nflowchart for future CMA approaches are presented. Impacts of CMA approaches on\nmobile computing is discussed and open challenges are presented as the future\nresearch directions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.5782v1", 
    "other_authors": "Patrizio Dazzi", 
    "title": "A Tool for Programming Embarrassingly Task Parallel Applications on CoW   and NoW", 
    "arxiv-id": "1306.5782v1", 
    "author": "Patrizio Dazzi", 
    "publish": "2013-06-24T21:05:44Z", 
    "summary": "Embarrassingly parallel problems can be split in parts that are characterized\nby a really low (or sometime absent) exchange of information during their\ncomputation in parallel. As a consequence they can be effectively computed in\nparallel exploiting commodity hardware, hence without particularly\nsophisticated interconnection networks. Basically, this means Clusters,\nNetworks of Workstations and Desktops as well as Computational Clouds. Despite\nthe simplicity of this computational model, it can be exploited to compute a\nquite large range of problems. This paper describes JJPF, a tool for developing\ntask parallel applications based on Java and Jini that showed to be an\neffective and efficient solution in environment like Clusters and Networks of\nWorkstations and Desktops."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.6023v2", 
    "other_authors": "Matteo Dell'Amico", 
    "title": "A Simulator for Data-Intensive Job Scheduling", 
    "arxiv-id": "1306.6023v2", 
    "author": "Matteo Dell'Amico", 
    "publish": "2013-06-25T16:34:05Z", 
    "summary": "Despite the fact that size-based schedulers can give excellent results in\nterms of both average response times and fairness, data-intensive computing\nexecution engines generally do not employ size-based schedulers, mainly because\nof the fact that job size is not known a priori.\n  In this work, we perform a simulation-based analysis of the performance of\nsize-based schedulers when they are employed with the workload of typical\ndata-intensive schedules and with approximated size estimations. We show\nresults that are very promising: even when size estimation is very imprecise,\nresponse times of size-based schedulers can be definitely smaller than those of\nsimple scheduling techniques such as processor sharing or FIFO."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.6192v1", 
    "other_authors": "Lukasz Swierczewski", 
    "title": "Akceleracja obliczen algebry liniowej z wykorzystaniem masywnie   rownoleglych, wielordzeniowych procesorow GPU", 
    "arxiv-id": "1306.6192v1", 
    "author": "Lukasz Swierczewski", 
    "publish": "2013-06-26T10:14:37Z", 
    "summary": "The paper presents the aspect of use of modern graphics accelerators\nsupporting CUDA technology for high-performance computing in the field of\nlinear algebra. Fully programmable graphic cards have been available for\nseveral years for both ordinary users and research units. They provide the\ncapability of performing virtually any computing with high performance, which\nis often beyond the reach of conventional CPUs. GPU architecture, also in case\nof classical problems of linear algebra which is the basis for many\ncalculations, can bring many benefits to the developer. Performance increase,\nobserved during matrix multiplication on nVidia Tesla C2050, was more than\nthousandfold compared to ordinary CPU, resulting in drastic reduction of\nlatency for some of the results, thus the cost of obtaining them."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.6397v1", 
    "other_authors": "Ankit Mundra, Nitin Rakesh, Vipin Tyagi", 
    "title": "Query Centric CPS (QCPS) Approach for Multiple Heterogeneous Systems", 
    "arxiv-id": "1306.6397v1", 
    "author": "Vipin Tyagi", 
    "publish": "2013-06-27T03:13:09Z", 
    "summary": "In modern scenario we need to have mechanisms which can provide better\ninteraction with physical world by an efficient and more effective\ncommunication and computation approach for multiple heterogeneous sensor\nnetworks. Previous work provides efficient communication approach between\nsensor nodes and a query centric approach for multiple collaborative\nheterogeneous sensor networks. Even there is energy issues involved in wireless\nsensor network operation. In this paper we have proposed Query centric Cyber\nPhysical System (QCPS)model to implement query centric user request using Cyber\nPhysical System (CPS). CPS takes both communication and computation in parallel\nto provide better interaction with physical world. This feature of CPS reduces\nsystem cost and makes it more energy efficient. This paper provides an\nefficient query processing approach for multiple heterogeneous sensor networks\nusing cyber physical system.This approach results in reduction of communication\nand computation cost as sensor network communicates using centroid of\nrespective grids which reduces cost of communication while involvement of CPS\nreduces the computation cost."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.6410v2", 
    "other_authors": "Amelie Chi Zhou, Bingsheng He, Cheng Liu", 
    "title": "Monetary Cost Optimizations for Hosting Workflow-as-a-Service in IaaS   Clouds", 
    "arxiv-id": "1306.6410v2", 
    "author": "Cheng Liu", 
    "publish": "2013-06-27T05:50:26Z", 
    "summary": "Recently, we have witnessed workflows from science and other data-intensive\napplications emerging on Infrastructure-asa-Service (IaaS) clouds, and many\nworkflow service providers offering workflow as a service (WaaS). The major\nconcern of WaaS providers is to minimize the monetary cost of executing\nworkflows in the IaaS cloud. While there have been previous studies on this\nconcern, most of them assume static task execution time and static pricing\nscheme, and have the QoS notion of satisfying a deterministic deadline.\nHowever, cloud environment is dynamic, with performance dynamics caused by the\ninterference from concurrent executions and price dynamics like spot prices\noffered by Amazon EC2. Therefore, we argue that WaaS providers should have the\nnotion of offering probabilistic performance guarantees for individual\nworkflows on IaaS clouds. We develop a probabilistic scheduling framework\ncalled Dyna to minimize the monetary cost while offering probabilistic deadline\nguarantees. The framework includes an A*-based instance configuration method\nfor performance dynamics, and a hybrid instance configuration refinement for\nutilizing spot instances. Experimental results with three real-world scientific\nworkflow applications on Amazon EC2 demonstrate (1) the accuracy of our\nframework on satisfying the probabilistic deadline guarantees required by the\nusers; (2) the effectiveness of our framework on reducing monetary cost in\ncomparison with the existing approaches."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1306.6597v3", 
    "other_authors": "Sukhpal Singh, Rishideep Singh", 
    "title": "Earthquake Disaster based Efficient Resource Utilization Technique in   IaaS Cloud", 
    "arxiv-id": "1306.6597v3", 
    "author": "Rishideep Singh", 
    "publish": "2013-06-27T18:23:06Z", 
    "summary": "Cloud Computing is an emerging area. The main aim of the initial\nsearch-and-rescue period after strong earthquakes is to reduce the whole number\nof mortalities. One main trouble rising in this period is to and the greatest\nassignment of available resources to functioning zones. For this issue a\ndynamic optimization model is presented. The model uses thorough descriptions\nof the operational zones and of the available resources to determine the\nresource performance and efficiency for different workloads related to the\nresponse. A suitable solution method for the model is offered as well. In this\npaper, Earthquake Disaster Based Resource Scheduling (EDBRS) Framework has been\nproposed. The allocation of resources to cloud workloads based on urgency\n(emergency during Earthquake Disaster). Based on this criterion, the resource\nscheduling algorithm has been proposed. The performance of the proposed\nalgorithm has been assessed with the existing common scheduling algorithms\nthrough the CloudSim. The experimental results show that the proposed algorithm\noutperforms the existing algorithms by reducing execution cost and time of\ncloud consumer workloads submitted to the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.0215v1", 
    "other_authors": "Mian Lu, Lei Zhang, Huynh Phung Huynh, Zhongliang Ong, Yun Liang, Bingsheng He, Rick Siow Mong Goh, Richard Huynh", 
    "title": "Optimizing the MapReduce Framework on Intel Xeon Phi Coprocessor", 
    "arxiv-id": "1309.0215v1", 
    "author": "Richard Huynh", 
    "publish": "2013-09-01T12:39:30Z", 
    "summary": "With the ease-of-programming, flexibility and yet efficiency, MapReduce has\nbecome one of the most popular frameworks for building big-data applications.\nMapReduce was originally designed for distributed-computing, and has been\nextended to various architectures, e,g, multi-core CPUs, GPUs and FPGAs. In\nthis work, we focus on optimizing the MapReduce framework on Xeon Phi, which is\nthe latest product released by Intel based on the Many Integrated Core\nArchitecture. To the best of our knowledge, this is the first work to optimize\nthe MapReduce framework on the Xeon Phi.\n  In our work, we utilize advanced features of the Xeon Phi to achieve high\nperformance. In order to take advantage of the SIMD vector processing units, we\npropose a vectorization friendly technique for the map phase to assist the\nauto-vectorization as well as develop SIMD hash computation algorithms.\nFurthermore, we utilize MIMD hyper-threading to pipeline the map and reduce to\nimprove the resource utilization. We also eliminate multiple local arrays but\nuse low cost atomic operations on the global array for some applications, which\ncan improve the thread scalability and data locality due to the coherent L2\ncaches. Finally, for a given application, our framework can either\nautomatically detect suitable techniques to apply or provide guideline for\nusers at compilation time. We conduct comprehensive experiments to benchmark\nthe Xeon Phi and compare our optimized MapReduce framework with a\nstate-of-the-art multi-core based MapReduce framework (Phoenix++). By\nevaluating six real-world applications, the experimental results show that our\noptimized framework is 1.2X to 38X faster than Phoenix++ for various\napplications on the Xeon Phi."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.1049v3", 
    "other_authors": "Luis Vaquero, Felix Cuadrado, Dionysios Logothetis, Claudio Martella", 
    "title": "xDGP: A Dynamic Graph Processing System with Adaptive Partitioning", 
    "arxiv-id": "1309.1049v3", 
    "author": "Claudio Martella", 
    "publish": "2013-09-04T14:36:17Z", 
    "summary": "Many real-world systems, such as social networks, rely on mining efficiently\nlarge graphs, with hundreds of millions of vertices and edges. This volume of\ninformation requires partitioning the graph across multiple nodes in a\ndistributed system. This has a deep effect on performance, as traversing edges\ncut between partitions incurs a significant performance penalty due to the cost\nof communication. Thus, several systems in the literature have attempted to\nimprove computational performance by enhancing graph partitioning, but they do\nnot support another characteristic of real-world graphs: graphs are inherently\ndynamic, their topology evolves continuously, and subsequently the optimum\npartitioning also changes over time.\n  In this work, we present the first system that dynamically repartitions\nmassive graphs to adapt to structural changes. The system optimises graph\npartitioning to prevent performance degradation without using data replication.\nThe system adopts an iterative vertex migration algorithm that relies on local\ninformation only, making complex coordination unnecessary. We show how the\nimprovement in graph partitioning reduces execution time by over 50%, while\nadapting the partitioning to a large number of changes to the graph in three\nreal-world scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.1230v1", 
    "other_authors": "Kerry A. Seitz Jr., Alex Kennedy, Owen Ransom, Bassam A. Younis, John D. Owens", 
    "title": "A GPU Implementation for Two-Dimensional Shallow Water Modeling", 
    "arxiv-id": "1309.1230v1", 
    "author": "John D. Owens", 
    "publish": "2013-09-05T04:20:02Z", 
    "summary": "In this paper, we present a GPU implementation of a two-dimensional shallow\nwater model. Water simulations are useful for modeling floods, river/reservoir\nbehavior, and dam break scenarios. Our GPU implementation shows vast\nperformance improvements over the original Fortran implementation. By taking\nadvantage of the GPU, researchers and engineers will be able to study water\nsystems more efficiently and in greater detail."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.1630v1", 
    "other_authors": "Henri Casanova, Arnaud Giersch, Arnaud Legrand, Martin Quinson, Fr\u00e9d\u00e9ric Suter", 
    "title": "SimGrid: a Sustained Effort for the Versatile Simulation of Large Scale   Distributed Systems", 
    "arxiv-id": "1309.1630v1", 
    "author": "Fr\u00e9d\u00e9ric Suter", 
    "publish": "2013-09-06T13:16:20Z", 
    "summary": "In this paper we present Simgrid, a toolkit for the versatile simulation of\nlarge scale distributed systems, whose development effort has been sustained\nfor the last fifteen years. Over this time period SimGrid has evolved from a\none-laboratory project in the U.S. into a scientific instrument developed by an\ninternational collaboration. The keys to making this evolution possible have\nbeen securing of funding, improving the quality of the software, and increasing\nthe user base. In this paper we describe how we have been able to make advances\non all three fronts, on which we plan to intensify our efforts over the\nupcoming years."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.2328v1", 
    "other_authors": "Olivier Serres, Abdullah Kayi, Ahmad Anbar, Tarek El-Ghazawi", 
    "title": "Hardware Support for Address Mapping in PGAS Languages; a UPC Case Study", 
    "arxiv-id": "1309.2328v1", 
    "author": "Tarek El-Ghazawi", 
    "publish": "2013-09-09T21:20:13Z", 
    "summary": "The Partitioned Global Address Space (PGAS) programming model strikes a\nbalance between the locality-aware, but explicit, message-passing model and the\neasy-to-use, but locality-agnostic, shared memory model. However, the PGAS rich\nmemory model comes at a performance cost which can hinder its potential for\nscalability and performance. To contain this overhead and achieve full\nperformance, compiler optimizations may not be sufficient and manual\noptimizations are typically added. This, however, can severely limit the\nproductivity advantage. Such optimizations are usually targeted at reducing\naddress translation overheads for shared data structures. This paper proposes a\nhardware architectural support for PGAS, which allows the processor to\nefficiently handle shared addresses. This eliminates the need for such\nhand-tuning, while maintaining the performance and productivity of PGAS\nlanguages. We propose to avail this hardware support to compilers by\nintroducing new instructions to efficiently access and traverse the PGAS memory\nspace. A prototype compiler is realized by extending the Berkeley Unified\nParallel C (UPC) compiler. It allows unmodified code to use the new\ninstructions without the user intervention, thereby creating a real productive\nprogramming environment. Two implementations are realized: the first is\nimplemented using the full system simulator Gem5, which allows the evaluation\nof the performance gain. The second is implemented using a softcore processor\nLeon3 on an FPGA to verify the implementability and to parameterize the cost of\nthe new hardware and its instructions. The new instructions show promising\nresults for the NAS Parallel Benchmarks implemented in UPC. A speedup of up to\n5.5x is demonstrated for unmodified and unoptimized codes. Unoptimized code\nperformance using this hardware was shown to also surpass the performance of\nmanually optimized code by up to 10%."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SURV.2013.070813.00285", 
    "link": "http://arxiv.org/pdf/1309.2772v3", 
    "other_authors": "Pierre Sutra, Etienne Rivi\u00e8re, Pascal Felber", 
    "title": "A Practical Distributed Universal Construction with Unknown Participants", 
    "arxiv-id": "1309.2772v3", 
    "author": "Pascal Felber", 
    "publish": "2013-09-11T09:35:39Z", 
    "summary": "Modern distributed systems employ atomic read-modify-write primitives to\ncoordinate concurrent operations. Such primitives are typically built on top of\na central server, or rely on an agreement protocol. Both approaches provide a\nuniversal construction, that is, a general mechanism to construct atomic and\nresponsive objects. These two techniques are however known to be inherently\ncostly. As a consequence, they may result in bottlenecks in applications using\nthem for coordination. In this paper, we investigate another direction to\nimplement a universal construction. Our idea is to delegate the implementation\nof the universal construction to the clients, and solely implement a\ndistributed shared atomic memory at the servers side. The construction we\npropose is obstruction-free. It can be implemented in a purely asynchronous\nmanner, and it does not assume the knowledge of the participants. It is built\non top of grafarius and racing objects, two novel shared abstractions that we\nintroduce in detail. To assess the benefits of our approach, we present a\nprototype implementation on top of the Cassandra data store, and compare it\nempirically to the Zookeeper coordination service."
}]