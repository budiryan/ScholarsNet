[{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9401102v1", 
    "other_authors": "Donald E. Knuth", 
    "title": "Mini-indexes for literate programs", 
    "arxiv-id": "cs/9401102v1", 
    "author": "Donald E. Knuth", 
    "publish": "1994-01-01T00:00:00Z", 
    "summary": "This paper describes how to implement a documentation technique that helps\nreaders to understand large programs or collections of programs, by providing\nlocal indexes to all identifiers that are visible on every two-page spread. A\ndetailed example is given for a program that finds all Hamiltonian circuits in\nan undirected graph."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9809008v1", 
    "other_authors": "Catuscia Palamidessi", 
    "title": "Comparing the expressive power of the Synchronous and the Asynchronous   pi-calculus", 
    "arxiv-id": "cs/9809008v1", 
    "author": "Catuscia Palamidessi", 
    "publish": "1998-09-02T17:40:46Z", 
    "summary": "The Asynchronous pi-calculus, as recently proposed by Boudol and,\nindependently, by Honda and Tokoro, is a subset of the pi-calculus which\ncontains no explicit operators for choice and output-prefixing. The\ncommunication mechanism of this calculus, however, is powerful enough to\nsimulate output-prefixing, as shown by Boudol, and input-guarded choice, as\nshown recently by Nestmann and Pierce. A natural question arises, then, whether\nor not it is possible to embed in it the full pi-calculus. We show that this is\nnot possible, i.e. there does not exist any uniform, parallel-preserving,\ntranslation from the pi-calculus into the asynchronous pi-calculus, up to any\n``reasonable'' notion of equivalence. This result is based on the incapablity\nof the asynchronous pi-calculus of breaking certain symmetries possibly present\nin the initial communication graph. By similar arguments, we prove a separation\nresult between the pi-calculus and CCS."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9809016v1", 
    "other_authors": "Gopalan Nadathur, Bharat Jayaraman, Keehang Kwon", 
    "title": "Scoping Constructs in Logic Programming: Implementation Problems and   their Solution", 
    "arxiv-id": "cs/9809016v1", 
    "author": "Keehang Kwon", 
    "publish": "1998-09-10T16:54:05Z", 
    "summary": "The inclusion of universal quantification and a form of implication in goals\nin logic programming is considered. These additions provide a logical basis for\nscoping but they also raise new implementation problems. When universal and\nexistential quantifiers are permitted to appear in mixed order in goals, the\ndevices of logic variables and unification that are employed in solving\nexistential goals must be modified to ensure that constraints arising out of\nthe order of quantification are respected. Suitable modifications that are\nbased on attaching numerical tags to constants and variables and on using these\ntags in unification are described. The resulting devices are amenable to an\nefficient implementation and can, in fact, be assimilated easily into the usual\nmachinery of the Warren Abstract Machine (WAM). The provision of implications\nin goals results in the possibility of program clauses being added to the\nprogram for the purpose of solving specific subgoals. A naive scheme based on\nasserting and retracting program clauses does not suffice for implementing such\nadditions for two reasons. First, it is necessary to also support the\nresurrection of an earlier existing program in the face of backtracking.\nSecond, the possibility for implication goals to be surrounded by quantifiers\nrequires a consideration of the parameterization of program clauses by bindings\nfor their free variables. Devices for supporting these additional requirements\nare described as also is the integration of these devices into the WAM. Further\nextensions to the machine are outlined for handling higher-order additions to\nthe language. The ideas presented here are relevant to the implementation of\nthe higher-order logic programming language lambda Prolog."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9810027v1", 
    "other_authors": "G. N. C. Kirby, R. Morrison, D. W. Stemple", 
    "title": "Linguistic Reflection in Java", 
    "arxiv-id": "cs/9810027v1", 
    "author": "D. W. Stemple", 
    "publish": "1998-10-29T14:30:59Z", 
    "summary": "Reflective systems allow their own structures to be altered from within. Here\nwe are concerned with a style of reflection, called linguistic reflection,\nwhich is the ability of a running program to generate new program fragments and\nto integrate these into its own execution. In particular we describe how this\nkind of reflection may be provided in the compiler-based, strongly typed\nobject-oriented programming language Java. The advantages of the programming\ntechnique include attaining high levels of genericity and accommodating system\nevolution. These advantages are illustrated by an example taken from persistent\nprogramming which shows how linguistic reflection allows functionality (program\ncode) to be generated on demand (Just-In-Time) from a generic specification and\nintegrated into the evolving running program. The technique is evaluated\nagainst alternative implementation approaches with respect to efficiency,\nsafety and ease of use."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9811001v1", 
    "other_authors": "Lunjin Lu", 
    "title": "A Polymorphic Groundness Analysis of Logic Programs", 
    "arxiv-id": "cs/9811001v1", 
    "author": "Lunjin Lu", 
    "publish": "1998-10-31T07:20:37Z", 
    "summary": "A polymorphic analysis is an analysis whose input and output contain\nparameters which serve as placeholders for information that is unknown before\nanalysis but provided after analysis. In this paper, we present a polymorphic\ngroundness analysis that infers parameterised groundness descriptions of the\nvariables of interest at a program point. The polymorphic groundness analysis\nis designed by replacing two primitive operators used in a monomorphic\ngroundness analysis and is shown to be as precise as the monomorphic groundness\nanalysis for any possible values for mode parameters. Experimental results of a\nprototype implementation of the polymorphic groundness analysis are given."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9901007v1", 
    "other_authors": "Alexander Yu. Vlasov", 
    "title": "Universal Object Oriented Languages and Computer Algebra", 
    "arxiv-id": "cs/9901007v1", 
    "author": "Alexander Yu. Vlasov", 
    "publish": "1999-01-15T19:28:58Z", 
    "summary": "The universal object oriented languages made programming more simple and\nefficient. In the article is considered possibilities of using similar methods\nin computer algebra. A clear and powerful universal language is useful if\nparticular problem was not implemented in standard software packages like\nREDUCE, MATHEMATICA, etc. and if the using of internal programming languages of\nthe packages looks not very efficient.\n  Functional languages like LISP had some advantages and traditions for\nalgebraic and symbolic manipulations. Functional and object oriented\nprogramming are not incompatible ones. An extension of the model of an object\nfor manipulation with pure functions and algebraic expressions is considered."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/9911001v2", 
    "other_authors": "Jan Heering, Paul Klint", 
    "title": "Semantics of Programming Languages: A Tool-Oriented Approach", 
    "arxiv-id": "cs/9911001v2", 
    "author": "Paul Klint", 
    "publish": "1999-11-04T11:14:45Z", 
    "summary": "By paying more attention to semantics-based tool generation, programming\nlanguage semantics can significantly increase its impact. Ultimately, this may\nlead to ``Language Design Assistants'' incorporating substantial amounts of\nsemantic knowledge."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0001003v1", 
    "other_authors": "Oleg Kiselyov", 
    "title": "Why C++ is not very fit for GUI programming", 
    "arxiv-id": "cs/0001003v1", 
    "author": "Oleg Kiselyov", 
    "publish": "2000-01-06T21:31:29Z", 
    "summary": "With no intent of starting a holy war, this paper lists several annoying C++\nbirthmarks that the author has come across developing GUI class libraries.\nC++'s view of classes, instances and hierarchies appears tantalizingly close to\nGUI concepts of controls, widgets, window classes and subwindows. OO models of\nC++ and of a window system are however different. C++ was designed to be a\n\"static\" language with a lexical name scoping, static type checking and\nhierarchies defined at compile time. Screen objects on the other hand are\ninherently dynamic; they usually live well beyond the procedure/block that\ncreated them; the hierarchy of widgets is defined to a large extent by layout,\nvisibility and event flow. Many GUI fundamentals such as dynamic and geometric\nhierarchies of windows and controls, broadcasting and percolation of events are\nnot supported directly by C++ syntax or execution semantics (or supported as\n\"exceptions\" -- pun intended). Therefore these features have to be emulated in\nC++ GUI code. This leads to duplication of a graphical toolkit or a window\nmanager functionality, code bloat, engaging in unsafe practices and forgoing of\nmany strong C++ features (like scoping rules and compile-time type checking).\nThis paper enumerates a few major C++/GUI sores and illustrates them on simple\nexamples."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0001009v1", 
    "other_authors": "Nikolay Mateev, Vijay Menon, Keshav Pingali", 
    "title": "Fractal Symbolic Analysis", 
    "arxiv-id": "cs/0001009v1", 
    "author": "Keshav Pingali", 
    "publish": "2000-01-12T22:15:05Z", 
    "summary": "Restructuring compilers use dependence analysis to prove that the meaning of\na program is not changed by a transformation. A well-known limitation of\ndependence analysis is that it examines only the memory locations read and\nwritten by a statement, and does not assume any particular interpretation for\nthe operations in that statement. Exploiting the semantics of these operations\nenables a wider set of transformations to be used, and is critical for\noptimizing important codes such as LU factorization with pivoting.\n  Symbolic execution of programs enables the exploitation of such semantic\nproperties, but it is intractable for all but the simplest programs. In this\npaper, we propose a new form of symbolic analysis for use in restructuring\ncompilers. Fractal symbolic analysis compares a program and its transformed\nversion by repeatedly simplifying these programs until symbolic analysis\nbecomes tractable, ensuring that equality of simplified programs is sufficient\nto guarantee equality of the original programs. We present a prototype\nimplementation of fractal symbolic analysis, and show how it can be used to\noptimize the cache performance of LU factorization with pivoting."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0003010v1", 
    "other_authors": "Burkhard D. Steinmacher-Burow", 
    "title": "TSIA: A Dataflow Model", 
    "arxiv-id": "cs/0003010v1", 
    "author": "Burkhard D. Steinmacher-Burow", 
    "publish": "2000-03-06T12:16:00Z", 
    "summary": "The Task System and Item Architecture (TSIA) is a model for transparent\napplication execution. In many real-world projects, a TSIA provides a simple\napplication with a transparent reliable, distributed, heterogeneous, adaptive,\ndynamic, real-time, parallel, secure or other execution. TSIA is suitable for\nmany applications, not just for the simple applications served to date. This\npresentation shows that TSIA is a dataflow model - a long-standing model for\ntransparent parallel execution. The advances to the dataflow model include a\nsimple semantics, as well as support for input/output, for modifiable items and\nfor other such effects."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0004006v1", 
    "other_authors": "F. Ferrucci, G. Pacini, M. I. Sessa", 
    "title": "On Redundancy Elimination Tolerant Scheduling Rules", 
    "arxiv-id": "cs/0004006v1", 
    "author": "M. I. Sessa", 
    "publish": "2000-04-17T13:34:17Z", 
    "summary": "In (Ferrucci, Pacini and Sessa, 1995) an extended form of resolution, called\nReduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is\nan SLD derivation such that redundancy elimination from resolvents is performed\nafter each rewriting step. It is intuitive that redundancy elimination may have\npositive effects on derivation process. However, undesiderable effects are also\npossible. In particular, as shown in this paper, program termination as well as\ncompleteness of loop checking mechanisms via a given selection rule may be\nlost. The study of such effects has led us to an analysis of selection rule\nbasic concepts, so that we have found convenient to move the attention from\nrules of atom selection to rules of atom scheduling. A priority mechanism for\natom scheduling is built, where a priority is assigned to each atom in a\nresolvent, and primary importance is given to the event of arrival of new atoms\nfrom the body of the applied clause at rewriting time. This new computational\nmodel proves able to address the study of redundancy elimination effects,\ngiving at the same time interesting insights into general properties of\nselection rules. As a matter of fact, a class of scheduling rules, namely the\nspecialisation independent ones, is defined in the paper by using not trivial\nsemantic arguments. As a quite surprising result, specialisation independent\nscheduling rules turn out to coincide with a class of rules which have an\nimmediate structural characterisation (named stack-queue rules). Then we prove\nthat such scheduling rules are tolerant to redundancy elimination, in the sense\nthat neither program termination nor completeness of equality loop check is\nlost passing from SLD to RSLD."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0004011v1", 
    "other_authors": "Burkhard D. Steinmacher-Burow", 
    "title": "Task Frames", 
    "arxiv-id": "cs/0004011v1", 
    "author": "Burkhard D. Steinmacher-Burow", 
    "publish": "2000-04-19T12:22:36Z", 
    "summary": "Forty years ago Dijkstra introduced the current conventional execution of\nroutines. It places activation frames onto a stack. Each frame is the internal\nstate of an executing routine. The resulting application execution is not\neasily helped by an external system. This presentation proposes an alternative\nexecution of routines. It places task frames onto the stack. A task frame is\nthe call of a routine to be executed. The feasibility of the alternative\nexecution is demonstrated by a crude implementation. As described elsewhere, an\napplication which executes in terms of tasks can be provided by an external\nsystem with a transparent reliable, distributed, heterogeneous, adaptive,\ndynamic, real-time, parallel, secure or other execution. By extending the crude\nimplementation, this presentation outlines a simple transparent parallel\nexecution."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0005002v1", 
    "other_authors": "Jan Heering", 
    "title": "Application Software, Domain-Specific Languages, and Language Design   Assistants", 
    "arxiv-id": "cs/0005002v1", 
    "author": "Jan Heering", 
    "publish": "2000-05-03T13:34:15Z", 
    "summary": "While application software does the real work, domain-specific languages\n(DSLs) are tools to help produce it efficiently, and language design assistants\nin turn are meta-tools to help produce DSLs quickly. DSLs are already in wide\nuse (HTML for web pages, Excel macros for spreadsheet applications, VHDL for\nhardware design, ...), but many more will be needed for both new as well as\nexisting application domains. Language design assistants to help develop them\ncurrently exist only in the basic form of language development systems. After a\nquick look at domain-specific languages, and especially their relationship to\napplication libraries, we survey existing language development systems and give\nan outline of future language design assistants."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0005023v1", 
    "other_authors": "Alessandro Lonardo, Emanuele Panizzi, Benedetto Proietti", 
    "title": "C++ programming language for an abstract massively parallel SIMD   architecture", 
    "arxiv-id": "cs/0005023v1", 
    "author": "Benedetto Proietti", 
    "publish": "2000-05-19T10:19:51Z", 
    "summary": "The aim of this work is to define and implement an extended C++ language to\nsupport the SIMD programming paradigm. The C++ programming language has been\nextended to express all the potentiality of an abstract SIMD machine consisting\nof a central Control Processor and a N-dimensional toroidal array of Numeric\nProcessors. Very few extensions have been added to the standard C++ with the\ngoal of minimising the effort for the programmer in learning a new language and\nto keep very high the performance of the compiled code. The proposed language\nhas been implemented as a porting of the GNU C++ Compiler on a SIMD\nsupercomputer."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0005033v1", 
    "other_authors": "Emanuele Panizzi, Bernardo Pastorelli", 
    "title": "Multimethods and separate static typechecking in a language with   C++-like object model", 
    "arxiv-id": "cs/0005033v1", 
    "author": "Bernardo Pastorelli", 
    "publish": "2000-05-31T07:54:30Z", 
    "summary": "The goal of this paper is the description and analysis of multimethod\nimplementation in a new object-oriented, class-based programming language\ncalled OOLANG. The implementation of the multimethod typecheck and selection,\ndeeply analyzed in the paper, is performed in two phases in order to allow\nstatic typechecking and separate compilation of modules. The first phase is\nperformed at compile time, while the second is executed at link time and does\nnot require the modules' source code. OOLANG has syntax similar to C++; the\nmain differences are the absence of pointers and the realization of\npolymorphism through subsumption. It adopts the C++ object model and supports\nmultiple inheritance as well as virtual base classes. For this reason, it has\nbeen necessary to define techniques for realigning argument and return value\naddresses when performing multimethod invocations."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0006034v1", 
    "other_authors": "Kevin Glynn, Martin Sulzmann, Peter J. Stuckey", 
    "title": "Type Classes and Constraint Handling Rules", 
    "arxiv-id": "cs/0006034v1", 
    "author": "Peter J. Stuckey", 
    "publish": "2000-06-26T00:39:32Z", 
    "summary": "Type classes are an elegant extension to traditional, Hindley-Milner based\ntyping systems. They are used in modern, typed languages such as Haskell to\nsupport controlled overloading of symbols. Haskell 98 supports only\nsingle-parameter and constructor type classes. Other extensions such as\nmulti-parameter type classes are highly desired but are still not officially\nsupported by Haskell. Subtle issues arise with extensions, which may lead to a\nloss of feasible type inference or ambiguous programs. A proper logical basis\nfor type class systems seems to be missing. Such a basis would allow extensions\nto be characterised and studied rigorously. We propose to employ Constraint\nHandling Rules as a tool to study and develop type class systems in a uniform\nway."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0009029v1", 
    "other_authors": "Matthew Huntbach", 
    "title": "The Concurrent Language Aldwych", 
    "arxiv-id": "cs/0009029v1", 
    "author": "Matthew Huntbach", 
    "publish": "2000-09-29T14:24:39Z", 
    "summary": "Aldwych is proposed as the foundation of a general purpose language for\nparallel applications. It works on a rule-based principle, and has aspects\nvariously of concurrent functional, logic and object-oriented languages, yet it\nforms an integrated whole. It is intended to be applicable both for small-scale\nparallel programming, and for large-scale open systems."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0010009v1", 
    "other_authors": "Riccardo Pucella", 
    "title": "An Approach to the Implementation of Overlapping Rules in Standard ML", 
    "arxiv-id": "cs/0010009v1", 
    "author": "Riccardo Pucella", 
    "publish": "2000-10-03T17:00:42Z", 
    "summary": "We describe an approach to programming rule-based systems in Standard ML,\nwith a focus on so-called overlapping rules, that is rules that can still be\nactive when other rules are fired. Such rules are useful when implementing\nrule-based reactive systems, and to that effect we show a simple implementation\nof Loyall's Active Behavior Trees, used to control goal-directed agents in the\nOz virtual environment. We discuss an implementation of our framework using a\nreactive library geared towards implementing those kind of systems."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0010016v1", 
    "other_authors": "Berthold Hoffmann, Mark Minas", 
    "title": "Towards rule-based visual programming of generic visual systems", 
    "arxiv-id": "cs/0010016v1", 
    "author": "Mark Minas", 
    "publish": "2000-10-10T10:32:00Z", 
    "summary": "This paper illustrates how the diagram programming language DiaPlan can be\nused to program visual systems. DiaPlan is a visual rule-based language that is\nfounded on the computational model of graph transformation. The language\nsupports object-oriented programming since its graphs are hierarchically\nstructured. Typing allows the shape of these graphs to be specified recursively\nin order to increase program security. Thanks to its genericity, DiaPlan allows\nto implement systems that represent and manipulate data in arbitrary diagram\nnotations. The environment for the language exploits the diagram editor\ngenerator DiaGen for providing genericity, and for implementing its user\ninterface and type checker."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0011025v1", 
    "other_authors": "Alexander Serebrenik, Danny De Schreye", 
    "title": "Termination analysis of logic programs using acceptability with general   term orders", 
    "arxiv-id": "cs/0011025v1", 
    "author": "Danny De Schreye", 
    "publish": "2000-11-17T12:45:13Z", 
    "summary": "We present a new approach to termination analysis of logic programs. The\nessence of the approach is that we make use of general term-orderings (instead\nof level mappings), like it is done in transformational approaches to logic\nprogram termination analysis, but that we apply these orderings directly to the\nlogic program and not to the term-rewrite system obtained through some\ntransformation. We define some variants of acceptability, based on general\nterm-orderings, and show how they are equivalent to LD-termination. We develop\na demand driven, constraint-based approach to verify these\nacceptability-variants.\n  The advantage of the approach over standard acceptability is that in some\ncases, where complex level mappings are needed, fairly simple term-orderings\nmay be easily generated. The advantage over transformational approaches is that\nit avoids the transformation step all together."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0011036v1", 
    "other_authors": "Nachum Dershowitz, Naomi Lindenstrauss, Yehoshua Sagiv, Alexander Serebrenik", 
    "title": "Automatic Termination Analysis of Programs Containing Arithmetic   Predicates", 
    "arxiv-id": "cs/0011036v1", 
    "author": "Alexander Serebrenik", 
    "publish": "2000-11-23T09:56:03Z", 
    "summary": "For logic programs with arithmetic predicates, showing termination is not\neasy, since the usual order for the integers is not well-founded. A new method,\neasily incorporated in the TermiLog system for automatic termination analysis,\nis presented for showing termination in this case.\n  The method consists of the following steps: First, a finite abstract domain\nfor representing the range of integers is deduced automatically. Based on this\nabstraction, abstract interpretation is applied to the program. The result is a\nfinite number of atoms abstracting answers to queries which are used to extend\nthe technique of query-mapping pairs. For each query-mapping pair that is\npotentially non-terminating, a bounded (integer-valued) termination function is\nguessed. If traversing the pair decreases the value of the termination\nfunction, then termination is established. Simple functions often suffice for\neach query-mapping pair, and that gives our approach an edge over the classical\napproach of using a single termination function for all loops, which must\ninevitably be more complicated and harder to guess automatically. It is worth\nnoting that the termination of McCarthy's 91 function can be shown\nautomatically using our method.\n  In summary, the proposed approach is based on combining a finite abstraction\nof the integers with the technique of the query-mapping pairs, and is\nessentially capable of dividing a termination proof into several cases, such\nthat a simple termination function suffices for each case. Consequently, the\nwhole process of proving termination can be done automatically in the framework\nof TermiLog and similar systems."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0012008v1", 
    "other_authors": "Nachum Dershowitz, Naomi Lindenstrauss, Yehoshua Sagiv, Alexander Serebrenik", 
    "title": "A General Framework for Automatic Termination Analysis of Logic Programs", 
    "arxiv-id": "cs/0012008v1", 
    "author": "Alexander Serebrenik", 
    "publish": "2000-12-13T13:37:17Z", 
    "summary": "This paper describes a general framework for automatic termination analysis\nof logic programs, where we understand by ``termination'' the finitenes s of\nthe LD-tree constructed for the program and a given query. A general property\nof mappings from a certain subset of the branches of an infinite LD-tree into a\nfinite set is proved. From this result several termination theorems are\nderived, by using different finite sets. The first two are formulated for the\npredicate dependency and atom dependency graphs. Then a general result for the\ncase of the query-mapping pairs relevant to a program is proved (cf.\n\\cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\\em TermiLog} system\ndescribed in \\cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this\nsystem it is not possible to prove termination for programs involving\narithmetic predicates, since the usual order for the integers is not\nwell-founded. A new method, which can be easily incorporated in {\\em TermiLog}\nor similar systems, is presented, which makes it possible to prove termination\nfor programs involving arithmetic predicates. It is based on combining a finite\nabstraction of the integers with the technique of the query-mapping pairs, and\nis essentially capable of dividing a termination proof into several cases, such\nthat a simple termination function suffices for each case. Finally several\npossible extensions are outlined."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0101025v1", 
    "other_authors": "Enea Zaffanella, Patricia M. Hill, Roberto Bagnara", 
    "title": "Decomposing Non-Redundant Sharing by Complementation", 
    "arxiv-id": "cs/0101025v1", 
    "author": "Roberto Bagnara", 
    "publish": "2001-01-23T14:20:08Z", 
    "summary": "Complementation, the inverse of the reduced product operation, is a technique\nfor systematically finding minimal decompositions of abstract domains. File'\nand Ranzato advanced the state of the art by introducing a simple method for\ncomputing a complement. As an application, they considered the extraction by\ncomplementation of the pair-sharing domain PS from the Jacobs and Langen's\nset-sharing domain SH. However, since the result of this operation was still\nSH, they concluded that PS was too abstract for this. Here, we show that the\nsource of this result lies not with PS but with SH and, more precisely, with\nthe redundant information contained in SH with respect to ground-dependencies\nand pair-sharing. In fact, a proper decomposition is obtained if the\nnon-redundant version of SH, PSD, is substituted for SH. To establish the\nresults for PSD, we define a general schema for subdomains of SH that includes\nPSD and Def as special cases. This sheds new light on the structure of PSD and\nexposes a natural though unexpected connection between Def and PSD. Moreover,\nwe substantiate the claim that complementation alone is not sufficient to\nobtain truly minimal decompositions of domains. The right solution to this\nproblem is to first remove redundancies by computing the quotient of the domain\nwith respect to the observable behavior, and only then decompose it by\ncomplementation."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0102025v2", 
    "other_authors": "Marco Bozzano, Giorgio Delzanno, Maurizio Martelli", 
    "title": "An Effective Fixpoint Semantics for Linear Logic Programs", 
    "arxiv-id": "cs/0102025v2", 
    "author": "Maurizio Martelli", 
    "publish": "2001-02-23T17:42:36Z", 
    "summary": "In this paper we investigate the theoretical foundation of a new bottom-up\nsemantics for linear logic programs, and more precisely for the fragment of\nLinLog that consists of the language LO enriched with the constant 1. We use\nconstraints to symbolically and finitely represent possibly infinite\ncollections of provable goals. We define a fixpoint semantics based on a new\noperator in the style of Tp working over constraints. An application of the\nfixpoint operator can be computed algorithmically. As sufficient conditions for\ntermination, we show that the fixpoint computation is guaranteed to converge\nfor propositional LO. To our knowledge, this is the first attempt to define an\neffective fixpoint semantics for linear logic programs. As an application of\nour framework, we also present a formal investigation of the relations between\nLO and Disjunctive Logic Programming. Using an approach based on abstract\ninterpretation, we show that DLP fixpoint semantics can be viewed as an\nabstraction of our semantics for LO. We prove that the resulting abstraction is\ncorrect and complete for an interesting class of LO programs encoding Petri\nNets."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0102030v1", 
    "other_authors": "Patricia M. Hill, Roberto Bagnara, Enea Zaffanella", 
    "title": "Soundness, Idempotence and Commutativity of Set-Sharing", 
    "arxiv-id": "cs/0102030v1", 
    "author": "Enea Zaffanella", 
    "publish": "2001-02-27T14:54:34Z", 
    "summary": "It is important that practical data-flow analyzers are backed by reliably\nproven theoretical results. Abstract interpretation provides a sound\nmathematical framework and necessary generic properties for an abstract domain\nto be well-defined and sound with respect to the concrete semantics. In logic\nprogramming, the abstract domain Sharing is a standard choice for sharing\nanalysis for both practical work and further theoretical study. In spite of\nthis, we found that there were no satisfactory proofs for the key properties of\ncommutativity and idempotence that are essential for Sharing to be well-defined\nand that published statements of the soundness of Sharing assume the\noccurs-check. This paper provides a generalization of the abstraction function\nfor Sharing that can be applied to any language, with or without the\noccurs-check. Results for soundness, idempotence and commutativity for abstract\nunification using this abstraction function are proven."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0105011v1", 
    "other_authors": "Frederic Goualard", 
    "title": "Component Programming and Interoperability in Constraint Solver Design", 
    "arxiv-id": "cs/0105011v1", 
    "author": "Frederic Goualard", 
    "publish": "2001-05-07T12:56:11Z", 
    "summary": "Prolog was once the main host for implementing constraint solvers.\n  It seems that it is no longer so. To be useful, constraint solvers have to be\nintegrable into industrial applications written in imperative or\nobject-oriented languages; to be efficient, they have to interact with other\nsolvers. To meet these requirements, many solvers are now implemented in the\nform of extensible object-oriented libraries. Following Pfister and Szyperski,\nwe argue that ``objects are not enough,'' and we propose to design solvers as\ncomponent-oriented libraries. We illustrate our approach by the description of\nthe architecture of a prototype, and we assess its strong points and\nweaknesses."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109002v1", 
    "other_authors": "Oltea Mihaela Herescu, Catuscia Palamidessi", 
    "title": "Probabilistic asynchronous pi-calculus", 
    "arxiv-id": "cs/0109002v1", 
    "author": "Catuscia Palamidessi", 
    "publish": "2001-09-03T04:10:41Z", 
    "summary": "We propose an extension of the asynchronous pi-calculus with a notion of\nrandom choice. We define an operational semantics which distinguishes between\nprobabilistic choice, made internally by the process, and nondeterministic\nchoice, made externally by an adversary scheduler. This distinction will allow\nus to reason about the probabilistic correctness of algorithms under certain\nschedulers. We show that in this language we can solve the electoral problem,\nwhich was proved not possible in the asynchronous $\\pi$-calculus. Finally, we\nshow an implementation of the probabilistic asynchronous pi-calculus in a\nJava-like language."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109003v1", 
    "other_authors": "Oltea Mihaela Herescu, Catuscia Palamidessi", 
    "title": "On the generalized dining philosophers problem", 
    "arxiv-id": "cs/0109003v1", 
    "author": "Catuscia Palamidessi", 
    "publish": "2001-09-03T05:37:48Z", 
    "summary": "We consider a generalization of the dining philosophers problem to arbitrary\nconnection topologies. We focus on symmetric, fully distributed systems, and we\naddress the problem of guaranteeing progress and lockout-freedom, even in\npresence of adversary schedulers, by using randomized algorithms. We show that\nthe well-known algorithms of Lehmann and Rabin do not work in the generalized\ncase, and we propose an alternative algorithm based on the idea of letting the\nphilosophers assign a random priority to their adjacent forks."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109024v1", 
    "other_authors": "Emmanuel Beffara, Olivier Bournez, Hassen Kacem, Claude Kirchner", 
    "title": "Verification of Timed Automata Using Rewrite Rules and Strategies", 
    "arxiv-id": "cs/0109024v1", 
    "author": "Claude Kirchner", 
    "publish": "2001-09-17T15:43:17Z", 
    "summary": "ELAN is a powerful language and environment for specifying and prototyping\ndeduction systems in a language based on rewrite rules controlled by\nstrategies. Timed automata is a class of continuous real-time models of\nreactive systems for which efficient model-checking algorithms have been\ndevised. In this paper, we show that these algorithms can very easily be\nprototyped in the ELAN system. This paper argues through this example that\nrewriting based systems relying on rules and strategies are a good framework to\nprototype, study and test rather efficiently symbolic model-checking\nalgorithms, i.e. algorithms which involve combination of graph exploration\nrules, deduction rules, constraint solving techniques and decision procedures."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109033v1", 
    "other_authors": "Francois Fages", 
    "title": "CLP versus LS on Log-based Reconciliation Problems", 
    "arxiv-id": "cs/0109033v1", 
    "author": "Francois Fages", 
    "publish": "2001-09-18T19:16:30Z", 
    "summary": "Nomadic applications create replicas of shared objects that evolve\nindependently while they are disconnected. When reconnecting, the system has to\nreconcile the divergent replicas. In the log-based approach to reconciliation,\nsuch as in the IceCube system, the input is a common initial state and logs of\nactions that were performed on each replica. The output is a consistent global\nschedule that maximises the number of accepted actions. The reconciler merges\nthe logs according to the schedule, and replays the operations in the merged\nlog against the initial state, yielding to a reconciled common final state.\n  In this paper, we show the NP-completeness of the log-based reconciliation\nproblem and present two programs for solving it. Firstly, a constraint logic\nprogram (CLP) that uses integer constraints for expressing precedence\nconstraints, boolean constraints for expressing dependencies between actions,\nand some heuristics for guiding the search. Secondly, a stochastic local search\nmethod with Tabu heuristic (LS), that computes solutions in an incremental\nfashion but does not prove optimality. One difficulty in the LS modeling lies\nin the handling of both boolean variables and integer variables, and in the\nhandling of the objective function which differs from a max-CSP problem.\nPreliminary evaluation results indicate better performance for the CLP program\nwhich, on somewhat realistic benchmarks, finds nearly optimal solutions up to a\nthousands of actions and proves optimality up to a hundreds of actions."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109060v1", 
    "other_authors": "Antonio J. Fernandez, Patricia M. Hill", 
    "title": "Branching: the Essence of Constraint Solving", 
    "arxiv-id": "cs/0109060v1", 
    "author": "Patricia M. Hill", 
    "publish": "2001-09-24T11:33:39Z", 
    "summary": "This paper focuses on the branching process for solving any constraint\nsatisfaction problem (CSP). A parametrised schema is proposed that (with\nsuitable instantiations of the parameters) can solve CSP's on both finite and\ninfinite domains. The paper presents a formal specification of the schema and a\nstatement of a number of interesting properties that, subject to certain\nconditions, are satisfied by any instances of the schema.\n  It is also shown that the operational procedures of many constraint systems\nincluding cooperative systems) satisfy these conditions.\n  Moreover, the schema is also used to solve the same CSP in different ways by\nmeans of different instantiations of its parameters."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0109066v1", 
    "other_authors": "Tomasz Szczygiel", 
    "title": "CLP Approaches to 2D Angle Placements", 
    "arxiv-id": "cs/0109066v1", 
    "author": "Tomasz Szczygiel", 
    "publish": "2001-09-24T15:39:57Z", 
    "summary": "The paper presents two CLP approaches to 2D angle placements, implemented in\nCHIP v.5.3. The first is based on the classical (rectangular) cumulative global\nconstraint, the second on the new trapezoidal cumulative global constraint.\nBoth approaches are applied to a specific presented."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0110007v1", 
    "other_authors": "Carlos Castro, Sebastian Manzano", 
    "title": "Variable and Value Ordering When Solving Balanced Academic Curriculum   Problems", 
    "arxiv-id": "cs/0110007v1", 
    "author": "Sebastian Manzano", 
    "publish": "2001-10-02T14:10:58Z", 
    "summary": "In this paper we present the use of Constraint Programming for solving\nbalanced academic curriculum problems. We discuss the important role that\nheuristics play when solving a problem using a constraint-based approach. We\nalso show how constraint solving techniques allow to very efficiently solve\ncombinatorial optimization problems that are too hard for integer programming\ntechniques."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0110012v1", 
    "other_authors": "Krzysztof R. Apt, Roman Bartak, Eric Monfroy, Francesca Rossi, Sebastian Brand", 
    "title": "Proceedings of the 6th Annual Workshop of the ERCIM Working Group on   Constraints", 
    "arxiv-id": "cs/0110012v1", 
    "author": "Sebastian Brand", 
    "publish": "2001-10-03T09:08:25Z", 
    "summary": "Homepage of the workshop proceedings, with links to all individually archived\npapers"
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0110037v1", 
    "other_authors": "Nancy Mazur, Peter Ross, Gerda Janssens, Maurice Bruynooghe", 
    "title": "Practical Aspects for a Working Compile Time Garbage Collection System   for Mercury", 
    "arxiv-id": "cs/0110037v1", 
    "author": "Maurice Bruynooghe", 
    "publish": "2001-10-17T16:20:33Z", 
    "summary": "Compile-time garbage collection (CTGC) is still a very uncommon feature\nwithin compilers. In previous work we have developed a compile-time structure\nreuse system for Mercury, a logic programming language. This system indicates\nwhich datastructures can safely be reused at run-time. As preliminary\nexperiments were promising, we have continued this work and have now a working\nand well performing near-to-ship CTGC-system built into the Melbourne Mercury\nCompiler (MMC).\n  In this paper we present the multiple design decisions leading to this\nsystem, we report the results of using CTGC for a set of benchmarks, including\na real-world program, and finally we discuss further possible improvements.\nBenchmarks show substantial memory savings and a noticeable reduction in\nexecution time."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0201012v1", 
    "other_authors": "Jacob M. Howe, Andy King", 
    "title": "Efficient Groundness Analysis in Prolog", 
    "arxiv-id": "cs/0201012v1", 
    "author": "Andy King", 
    "publish": "2002-01-16T12:02:33Z", 
    "summary": "Boolean functions can be used to express the groundness of, and trace\ngrounding dependencies between, program variables in (constraint) logic\nprograms. In this paper, a variety of issues pertaining to the efficient Prolog\nimplementation of groundness analysis are investigated, focusing on the domain\nof definite Boolean functions, Def. The systematic design of the representation\nof an abstract domain is discussed in relation to its impact on the algorithmic\ncomplexity of the domain operations; the most frequently called operations\nshould be the most lightweight. This methodology is applied to Def, resulting\nin a new representation, together with new algorithms for its domain operations\nutilising previously unexploited properties of Def -- for instance,\nquadratic-time entailment checking. The iteration strategy driving the analysis\nis also discussed and a simple, but very effective, optimisation of induced\nmagic is described. The analysis can be implemented straightforwardly in Prolog\nand the use of a non-ground representation results in an efficient, scalable\ntool which does not require widening to be invoked, even on the largest\nbenchmarks. An extensive experimental evaluation is given"
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0201029v1", 
    "other_authors": "James H. Andrews", 
    "title": "The Witness Properties and the Semantics of the Prolog Cut", 
    "arxiv-id": "cs/0201029v1", 
    "author": "James H. Andrews", 
    "publish": "2002-01-31T17:47:45Z", 
    "summary": "The semantics of the Prolog ``cut'' construct is explored in the context of\nsome desirable properties of logic programming systems, referred to as the\nwitness properties. The witness properties concern the operational consistency\nof responses to queries. A generalization of Prolog with negation as failure\nand cut is described, and shown not to have the witness properties. A\nrestriction of the system is then described, which preserves the choice and\nfirst-solution behaviour of cut but allows the system to have the witness\nproperties.\n  The notion of cut in the restricted system is more restricted than the Prolog\nhard cut, but retains the useful first-solution behaviour of hard cut, not\nretained by other proposed cuts such as the ``soft cut''. It is argued that the\nrestricted system achieves a good compromise between the power and utility of\nthe Prolog cut and the need for internal consistency in logic programming\nsystems. The restricted system is given an abstract semantics, which depends on\nthe witness properties; this semantics suggests that the restricted system has\na deeper connection to logic than simply permitting some computations which are\nlogical.\n  Parts of this paper appeared previously in a different form in the\nProceedings of the 1995 International Logic Programming Symposium."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0202010v1", 
    "other_authors": "W. Drabent, J. Maluszynski, P. Pietrzak", 
    "title": "Using parametric set constraints for locating errors in CLP programs", 
    "arxiv-id": "cs/0202010v1", 
    "author": "P. Pietrzak", 
    "publish": "2002-02-11T11:50:37Z", 
    "summary": "This paper introduces a framework of parametric descriptive directional types\nfor constraint logic programming (CLP). It proposes a method for locating type\nerrors in CLP programs and presents a prototype debugging tool. The main\ntechnique used is checking correctness of programs w.r.t. type specifications.\nThe approach is based on a generalization of known methods for proving\ncorrectness of logic programs to the case of parametric specifications.\nSet-constraint techniques are used for formulating and checking verification\nconditions for (parametric) polymorphic type specifications. The specifications\nare expressed in a parametric extension of the formalism of term grammars. The\nsoundness of the method is proved and the prototype debugging tool supporting\nthe proposed approach is illustrated on examples.\n  The paper is a substantial extension of the previous work by the same authors\nconcerning monomorphic directional types."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0203001v1", 
    "other_authors": "Ralf Laemmel", 
    "title": "Towards Generic Refactoring", 
    "arxiv-id": "cs/0203001v1", 
    "author": "Ralf Laemmel", 
    "publish": "2002-03-01T11:58:56Z", 
    "summary": "We study program refactoring while considering the language or even the\nprogramming paradigm as a parameter. We use typed functional programs, namely\nHaskell programs, as the specification medium for a corresponding refactoring\nframework. In order to detach ourselves from language syntax, our\nspecifications adhere to the following style. (I) As for primitive algorithms\nfor program analysis and transformation, we employ generic function combinators\nsupporting generic traversal and polymorphic functions refined by ad-hoc cases.\n(II) As for the language abstractions involved in refactorings, we design a\ndedicated multi-parameter class. This class can be instantiated for\nabstractions as present in various languages, e.g., Java, Prolog or Haskell."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0203022v1", 
    "other_authors": "Jacob M. Howe, Andy King", 
    "title": "Three Optimisations for Sharing", 
    "arxiv-id": "cs/0203022v1", 
    "author": "Andy King", 
    "publish": "2002-03-18T13:22:30Z", 
    "summary": "In order to improve precision and efficiency sharing analysis should track\nboth freeness and linearity. The abstract unification algorithms for these\ncombined domains are suboptimal, hence there is scope for improving precision.\nThis paper proposes three optimisations for tracing sharing in combination with\nfreeness and linearity. A novel connection between equations and sharing\nabstractions is used to establish correctness of these optimisations even in\nthe presence of rational trees. A method for pruning intermediate sharing\nabstractions to improve efficiency is also proposed. The optimisations are\nlightweight and therefore some, if not all, of these optimisations will be of\ninterest to the implementor."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0204013v2", 
    "other_authors": "Ralf Laemmel", 
    "title": "The Sketch of a Polymorphic Symphony", 
    "arxiv-id": "cs/0204013v2", 
    "author": "Ralf Laemmel", 
    "publish": "2002-04-08T14:12:21Z", 
    "summary": "In previous work, we have introduced functional strategies, that is,\nfirst-class generic functions that can traverse into terms of any type while\nmixing uniform and type-specific behaviour. In the present paper, we give a\ndetailed description of one particular Haskell-based model of functional\nstrategies. This model is characterised as follows. Firstly, we employ\nfirst-class polymorphism as a form of second-order polymorphism as for the mere\ntypes of functional strategies. Secondly, we use an encoding scheme of run-time\ntype case for mixing uniform and type-specific behaviour. Thirdly, we base all\ntraversal on a fundamental combinator for folding over constructor\napplications.\n  Using this model, we capture common strategic traversal schemes in a highly\nparameterised style. We study two original forms of parameterisation. Firstly,\nwe design parameters for the specific control-flow, data-flow and traversal\ncharacteristics of more concrete traversal schemes. Secondly, we use\noverloading to postpone commitment to a specific type scheme of traversal. The\nresulting portfolio of traversal schemes can be regarded as a challenging\nbenchmark for setups for typed generic programming.\n  The way we develop the model and the suite of traversal schemes, it becomes\nclear that parameterised + typed strategic programming is best viewed as a\npotent combination of certain bits of parametric, intensional, polytypic, and\nad-hoc polymorphism."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0204015v1", 
    "other_authors": "Ralf Laemmel, Joost Visser", 
    "title": "Design Patterns for Functional Strategic Programming", 
    "arxiv-id": "cs/0204015v1", 
    "author": "Joost Visser", 
    "publish": "2002-04-09T12:44:43Z", 
    "summary": "In previous work, we introduced the fundamentals and a supporting combinator\nlibrary for \\emph{strategic programming}. This an idiom for generic programming\nbased on the notion of a \\emph{functional strategy}: a first-class generic\nfunction that cannot only be applied to terms of any type, but which also\nallows generic traversal into subterms and can be customized with type-specific\nbehaviour.\n  This paper seeks to provide practicing functional programmers with pragmatic\nguidance in crafting their own strategic programs. We present the fundamentals\nand the support from a user's perspective, and we initiate a catalogue of\n\\emph{strategy design patterns}. These design patterns aim at consolidating\nstrategic programming expertise in accessible form."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0204018v3", 
    "other_authors": "Jan Kort, Ralf Laemmel", 
    "title": "A Framework for Datatype Transformation", 
    "arxiv-id": "cs/0204018v3", 
    "author": "Ralf Laemmel", 
    "publish": "2002-04-09T18:32:40Z", 
    "summary": "We study one dimension in program evolution, namely the evolution of the\ndatatype declarations in a program. To this end, a suite of basic\ntransformation operators is designed. We cover structure-preserving\nrefactorings, but also structure-extending and -reducing adaptations. Both the\nobject programs that are subject to datatype transformations, and the meta\nprograms that encode datatype transformations are functional programs."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0205018v2", 
    "other_authors": "Ralf Laemmel", 
    "title": "Typed Generic Traversal With Term Rewriting Strategies", 
    "arxiv-id": "cs/0205018v2", 
    "author": "Ralf Laemmel", 
    "publish": "2002-05-14T10:18:42Z", 
    "summary": "A typed model of strategic term rewriting is developed. The key innovation is\nthat generic traversal is covered. To this end, we define a typed rewriting\ncalculus S'_{gamma}. The calculus employs a many-sorted type system extended by\ndesignated generic strategy types gamma. We consider two generic strategy\ntypes, namely the types of type-preserving and type-unifying strategies.\nS'_{gamma} offers traversal combinators to construct traversals or schemes\nthereof from many-sorted and generic strategies. The traversal combinators\nmodel different forms of one-step traversal, that is, they process the\nimmediate subterms of a given term without anticipating any scheme of recursion\ninto terms. To inhabit generic types, we need to add a fundamental combinator\nto lift a many-sorted strategy $s$ to a generic type gamma. This step is called\nstrategy extension. The semantics of the corresponding combinator states that s\nis only applied if the type of the term at hand fits, otherwise the extended\nstrategy fails. This approach dictates that the semantics of strategy\napplication must be type-dependent to a certain extent. Typed strategic term\nrewriting with coverage of generic term traversal is a simple but expressive\nmodel of generic programming. It has applications in program transformation and\nprogram analysis."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0208029v1", 
    "other_authors": "Peter Van Roy, Per Brand, Denys Duchier, Seif Haridi, Martin Henz, Christian Schulte", 
    "title": "Logic programming in the context of multiparadigm programming: the Oz   experience", 
    "arxiv-id": "cs/0208029v1", 
    "author": "Christian Schulte", 
    "publish": "2002-08-20T11:12:58Z", 
    "summary": "Oz is a multiparadigm language that supports logic programming as one of its\nmajor paradigms. A multiparadigm language is designed to support different\nprogramming paradigms (logic, functional, constraint, object-oriented,\nsequential, concurrent, etc.) with equal ease. This article has two goals: to\ngive a tutorial of logic programming in Oz and to show how logic programming\nfits naturally into the wider context of multiparadigm programming. Our\nexperience shows that there are two classes of problems, which we call\nalgorithmic and search problems, for which logic programming can help formulate\npractical solutions. Algorithmic problems have known efficient algorithms.\nSearch problems do not have known efficient algorithms but can be solved with\nsearch. The Oz support for logic programming targets these two problem classes\nspecifically, using the concepts needed for each. This is in contrast to the\nProlog approach, which targets both classes with one set of concepts, which\nresults in less than optimal support for each class. To explain the essential\ndifference between algorithmic and search programs, we define the Oz execution\nmodel. This model subsumes both concurrent logic programming\n(committed-choice-style) and search-based logic programming (Prolog-style).\nInstead of Horn clause syntax, Oz has a simple, fully compositional,\nhigher-order syntax that accommodates the abilities of the language. We\nconclude with lessons learned from this work, a brief history of Oz, and many\nentry points into the Oz literature."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0212003v1", 
    "other_authors": "Anindya Banerjee, David A. Naumann", 
    "title": "Ownership Confinement Ensures Representation Independence for   Object-Oriented Programs", 
    "arxiv-id": "cs/0212003v1", 
    "author": "David A. Naumann", 
    "publish": "2002-12-04T23:11:22Z", 
    "summary": "Dedicated to the memory of Edsger W.Dijkstra.\n  Representation independence or relational parametricity formally\ncharacterizes the encapsulation provided by language constructs for data\nabstraction and justifies reasoning by simulation. Representation independence\nhas been shown for a variety of languages and constructs but not for shared\nreferences to mutable state; indeed it fails in general for such languages.\nThis paper formulates representation independence for classes, in an\nimperative, object-oriented language with pointers, subclassing and dynamic\ndispatch, class oriented visibility control, recursive types and methods, and a\nsimple form of module. An instance of a class is considered to implement an\nabstraction using private fields and so-called representation objects.\nEncapsulation of representation objects is expressed by a restriction, called\nconfinement, on aliasing. Representation independence is proved for programs\nsatisfying the confinement condition. A static analysis is given for\nconfinement that accepts common designs such as the observer and factory\npatterns. The formalization takes into account not only the usual interface\nbetween a client and a class that provides an abstraction but also the\ninterface (often called ``protected'') between the class and its subclasses."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0212048v1", 
    "other_authors": "Ralf Laemmel, Joost Visser", 
    "title": "Strategic polymorphism requires just two combinators!", 
    "arxiv-id": "cs/0212048v1", 
    "author": "Joost Visser", 
    "publish": "2002-12-19T13:54:26Z", 
    "summary": "In previous work, we introduced the notion of functional strategies:\nfirst-class generic functions that can traverse terms of any type while mixing\nuniform and type-specific behaviour. Functional strategies transpose the notion\nof term rewriting strategies (with coverage of traversal) to the functional\nprogramming paradigm. Meanwhile, a number of Haskell-based models and\ncombinator suites were proposed to support generic programming with functional\nstrategies.\n  In the present paper, we provide a compact and matured reconstruction of\nfunctional strategies. We capture strategic polymorphism by just two primitive\ncombinators. This is done without commitment to a specific functional language.\nWe analyse the design space for implementational models of functional\nstrategies. For completeness, we also provide an operational reference model\nfor implementing functional strategies (in Haskell). We demonstrate the\ngenerality of our approach by reconstructing representative fragments of the\nStrafunski library for functional strategies."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0301003v1", 
    "other_authors": "Alexandros Eleftheriadis, Danny Hong", 
    "title": "Flavor: A Language for Media Representation", 
    "arxiv-id": "cs/0301003v1", 
    "author": "Danny Hong", 
    "publish": "2003-01-07T07:53:20Z", 
    "summary": "Flavor (Formal Language for Audio-Visual Object Representation) has been\ncreated as a language for describing coded multimedia bitstreams in a formal\nway so that the code for reading and writing bitstreams can be automatically\ngenerated. It is an extension of C++ and Java, in which the typing system\nincorporates bitstream representation semantics. This allows describing in a\nsingle place both the in-memory representation of data as well as their\nbitstream-level (compressed) representation. Flavor also comes with a\ntranslator that automatically generates standard C++ or Java code from the\nFlavor source code so that direct access to compressed multimedia information\nby application developers can be achieved with essentially zero programming.\nFlavor has gone through many enhancements and this paper fully describes the\nlatest version of the language and the translator. The software has been made\ninto an open source project as of Version 4.1, and the latest downloadable\nFlavor package is available at http://flavor.sourceforge.net."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0308007v1", 
    "other_authors": "Ricardo Rocha, Fernando Silva, Vitor Santos Costa", 
    "title": "On Applying Or-Parallelism and Tabling to Logic Programs", 
    "arxiv-id": "cs/0308007v1", 
    "author": "Vitor Santos Costa", 
    "publish": "2003-08-04T18:59:42Z", 
    "summary": "The past years have seen widening efforts at increasing Prolog's\ndeclarativeness and expressiveness. Tabling has proved to be a viable technique\nto efficiently overcome SLD's susceptibility to infinite loops and redundant\nsubcomputations. Our research demonstrates that implicit or-parallelism is a\nnatural fit for logic programs with tabling. To substantiate this belief, we\nhave designed and implemented an or-parallel tabling engine -- OPTYap -- and we\nused a shared-memory parallel machine to evaluate its performance. To the best\nof our knowledge, OPTYap is the first implementation of a parallel tabling\nengine for logic programming systems. OPTYap builds on Yap's efficient\nsequential Prolog engine. Its execution model is based on the SLG-WAM for\ntabling, and on the environment copying for or-parallelism.\n  Preliminary results indicate that the mechanisms proposed to parallelize\nsearch in the context of SLD resolution can indeed be effectively and naturally\ngeneralized to parallelize tabled computations, and that the resulting systems\ncan achieve good performance on shared-memory parallel machines. More\nimportantly, it emphasizes our belief that through applying or-parallelism and\ntabling to logic programs the range of applications for Logic Programming can\nbe increased."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0309028v1", 
    "other_authors": "Fred Mesnard, Roberto Bagnara", 
    "title": "cTI: A constraint-based termination inference tool for ISO-Prolog", 
    "arxiv-id": "cs/0309028v1", 
    "author": "Roberto Bagnara", 
    "publish": "2003-09-16T07:43:50Z", 
    "summary": "We present cTI, the first system for universal left-termination inference of\nlogic programs. Termination inference generalizes termination analysis and\nchecking. Traditionally, a termination analyzer tries to prove that a given\nclass of queries terminates. This class must be provided to the system, for\ninstance by means of user annotations. Moreover, the analysis must be redone\nevery time the class of queries of interest is updated. Termination inference,\nin contrast, requires neither user annotations nor recomputation. In this\napproach, terminating classes for all predicates are inferred at once. We\ndescribe the architecture of cTI and report an extensive experimental\nevaluation of the system covering many classical examples from the logic\nprogramming termination literature and several Prolog programs of respectable\nsize and complexity."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0311002v1", 
    "other_authors": "Florence Benoy, Andy King, Fred Mesnard", 
    "title": "Computing Convex Hulls with a Linear Solver", 
    "arxiv-id": "cs/0311002v1", 
    "author": "Fred Mesnard", 
    "publish": "2003-11-04T12:43:54Z", 
    "summary": "A programming tactic involving polyhedra is reported that has been widely\napplied in the polyhedral analysis of (constraint) logic programs. The method\nenables the computations of convex hulls that are required for polyhedral\nanalysis to be coded with linear constraint solving machinery that is available\nin many Prolog systems.\n  To appear in Theory and Practice of Logic Programming (TPLP)"
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0311016v1", 
    "other_authors": "Erwan Jahier, Mireille Ducass'e", 
    "title": "Generic and Efficient Program Monitoring by trace analysis", 
    "arxiv-id": "cs/0311016v1", 
    "author": "Mireille Ducass'e", 
    "publish": "2003-11-14T10:23:11Z", 
    "summary": "Program execution monitoring consists of checking whole executions for given\nproperties in order to collect global run-time information.\n  Monitoring is very useful to maintain programs. However, application\ndevelopers face the following dilemma: either they use existing tools which\nnever exactly fit their needs, or they invest a lot of effort to implement\nmonitoring code.\n  In this article we argue that, when an event-oriented tracer exists, the\ncompiler developers can enable the application developers to easily code their\nown, relevant, monitors which will run efficiently.\n  We propose a high-level operator, called foldt, which operates on execution\ntraces. One of the key advantages of our approach is that it allows a clean\nseparation of concerns; the definition of monitors is neither intertwined in\nthe user source code nor in the language compiler.\n  We give a number of applications of the foldt operator to compute monitors\nfor Mercury program executions: execution profiles, graphical abstract views,\nand two test coverage measurements. Each example is implemented by a few simple\nlines of Mercury.\n  Detailed measurements show acceptable performance of the basic mechanism of\nfoldt for executions of several millions of execution events."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0311023v1", 
    "other_authors": "Peter J. Stuckey, Martin Sulzmann, Jeremy Wazny", 
    "title": "The Chameleon Type Debugger (Tool Demonstration)", 
    "arxiv-id": "cs/0311023v1", 
    "author": "Jeremy Wazny", 
    "publish": "2003-11-17T23:06:18Z", 
    "summary": "In this tool demonstration, we give an overview of the Chameleon type\ndebugger. The type debugger's primary use is to identify locations within a\nsource program which are involved in a type error. By further examining these\n(potentially) problematic program locations, users gain a better understanding\nof their program and are able to work towards the actual mistake which was the\ncause of the type error. The debugger is interactive, allowing the user to\nprovide additional information to narrow down the search space. One of the\nnovel aspects of the debugger is the ability to explain erroneous-looking\ntypes. In the event that an unexpected type is inferred, the debugger can\nhighlight program locations which contributed to that result. Furthermore, due\nto the flexible constraint-based foundation that the debugger is built upon, it\ncan naturally handle advanced type system features such as Haskell's type\nclasses and functional dependencies."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0311032v1", 
    "other_authors": "Oleg Mazonka, Daniel B. Cristofani", 
    "title": "A Very Short Self-Interpreter", 
    "arxiv-id": "cs/0311032v1", 
    "author": "Daniel B. Cristofani", 
    "publish": "2003-11-21T06:18:19Z", 
    "summary": "In this paper we would like to present a very short (possibly the shortest)\nself-interpreter, based on a simplistic Turing-complete imperative language.\nThis interpreter explicitly processes the statements of the language, which\nmeans the interpreter constitutes a description of the language inside that\nsame language. The paper does not require any specific knowledge; however,\nexperience in programming and a vivid imagination are beneficial."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0312023v1", 
    "other_authors": "Samir Genaim, Michael Codish", 
    "title": "Inferring Termination Conditions for Logic Programs using Backwards   Analysis", 
    "arxiv-id": "cs/0312023v1", 
    "author": "Michael Codish", 
    "publish": "2003-12-12T16:14:20Z", 
    "summary": "This paper focuses on the inference of modes for which a logic program is\nguaranteed to terminate. This generalises traditional termination analysis\nwhere an analyser tries to verify termination for a specified mode. Our\ncontribution is a methodology in which components of traditional termination\nanalysis are combined with backwards analysis to obtain an analyser for\ntermination inference. We identify a condition on the components of the\nanalyser which guarantees that termination inference will infer all modes which\ncan be checked to terminate. The application of this methodology to enhance a\ntraditional termination analyser to perform also termination inference is\ndemonstrated."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0312027v1", 
    "other_authors": "Henk Vandecasteele, Gerda Janssens", 
    "title": "An Open Ended Tree", 
    "arxiv-id": "cs/0312027v1", 
    "author": "Gerda Janssens", 
    "publish": "2003-12-15T13:43:37Z", 
    "summary": "An open ended list is a well known data structure in Prolog programs. It is\nfrequently used to represent a value changing over time, while this value is\nreferred to from several places in the data structure of the application. A\nweak point in this technique is that the time complexity is linear in the\nnumber of updates to the value represented by the open ended list. In this\nprogramming pearl we present a variant of the open ended list, namely an open\nended tree, with an update and access time complexity logarithmic in the number\nof updates to the value."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0401021v1", 
    "other_authors": "Patricia M. Hill, Enea Zaffanella, Roberto Bagnara", 
    "title": "A correct, precise and efficient integration of set-sharing, freeness   and linearity for the analysis of finite and rational tree languages", 
    "arxiv-id": "cs/0401021v1", 
    "author": "Roberto Bagnara", 
    "publish": "2004-01-26T10:14:21Z", 
    "summary": "It is well-known that freeness and linearity information positively interact\nwith aliasing information, allowing both the precision and the efficiency of\nthe sharing analysis of logic programs to be improved. In this paper we present\na novel combination of set-sharing with freeness and linearity information,\nwhich is characterized by an improved abstract unification operator. We provide\na new abstraction function and prove the correctness of the analysis for both\nthe finite tree and the rational tree cases. Moreover, we show that the same\nnotion of redundant information as identified in (Bagnara et al. 2002;\nZaffanella et al. 2002) also applies to this abstract domain combination: this\nallows for the implementation of an abstract unification operator running in\npolynomial time and achieving the same precision on all the considered\nobservable properties."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0401022v1", 
    "other_authors": "Roberto Bagnara, Enea Zaffanella, Patricia M. Hill", 
    "title": "Enhanced sharing analysis techniques: a comprehensive evaluation", 
    "arxiv-id": "cs/0401022v1", 
    "author": "Patricia M. Hill", 
    "publish": "2004-01-26T10:35:43Z", 
    "summary": "Sharing, an abstract domain developed by D. Jacobs and A. Langen for the\nanalysis of logic programs, derives useful aliasing information. It is\nwell-known that a commonly used core of techniques, such as the integration of\nSharing with freeness and linearity information, can significantly improve the\nprecision of the analysis. However, a number of other proposals for refined\ndomain combinations have been circulating for years. One feature that is common\nto these proposals is that they do not seem to have undergone a thorough\nexperimental evaluation even with respect to the expected precision gains. In\nthis paper we experimentally evaluate: helping Sharing with the definitely\nground variables found using Pos, the domain of positive Boolean formulas; the\nincorporation of explicit structural information; a full implementation of the\nreduced product of Sharing and Pos; the issue of reordering the bindings in the\ncomputation of the abstract mgu; an original proposal for the addition of a new\nmode recording the set of variables that are deemed to be ground or free; a\nrefined way of using linearity to improve the analysis; the recovery of hidden\ninformation in the combination of Sharing with freeness information. Finally,\nwe discuss the issue of whether tracking compoundness allows the computation of\nmore sharing information."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0401024v1", 
    "other_authors": "Duraid Madina, Russell K. Standish", 
    "title": "A system for reflection in C++", 
    "arxiv-id": "cs/0401024v1", 
    "author": "Russell K. Standish", 
    "publish": "2004-01-27T03:29:16Z", 
    "summary": "Object-oriented programming languages such as Java and Objective C have\nbecome popular for implementing agent-based and other object-based simulations\nsince objects in those languages can {\\em reflect} (i.e. make runtime queries\nof an object's structure). This allows, for example, a fairly trivial {\\em\nserialisation} routine (conversion of an object into a binary representation\nthat can be stored or passed over a network) to be written. However C++ does\nnot offer this ability, as type information is thrown away at compile time. Yet\nC++ is often a preferred development environment, whether for performance\nreasons or for its expressive features such as operator overloading.\n  In this paper, we present the {\\em Classdesc} system which brings many of the\nbenefits of object reflection to C++."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0402043v1", 
    "other_authors": "Evgueniy Vitchev", 
    "title": "The UPLNC Compiler: Design and Implementation", 
    "arxiv-id": "cs/0402043v1", 
    "author": "Evgueniy Vitchev", 
    "publish": "2004-02-18T06:30:12Z", 
    "summary": "The implementation of the compiler of the UPLNC language is presented with a\nfull source code listing."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0402058v1", 
    "other_authors": "Jacques Cohen", 
    "title": "A Tribute to Alain Colmerauer", 
    "arxiv-id": "cs/0402058v1", 
    "author": "Jacques Cohen", 
    "publish": "2004-02-25T19:00:31Z", 
    "summary": "The paper describes the contributions of Alain Colmerauer to the areas of\nlogic programs (LP) and constraint logic programs (CLP)."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0403011v1", 
    "other_authors": "Maria Alpuente, Michael Hanus, Salvador Lucas, German Vidal", 
    "title": "Specialization of Functional Logic Programs Based on Needed Narrowing", 
    "arxiv-id": "cs/0403011v1", 
    "author": "German Vidal", 
    "publish": "2004-03-09T15:43:00Z", 
    "summary": "Many functional logic languages are based on narrowing, a unification-based\ngoal-solving mechanism which subsumes the reduction mechanism of functional\nlanguages and the resolution principle of logic languages. Needed narrowing is\nan optimal evaluation strategy which constitutes the basis of modern\n(narrowing-based) lazy functional logic languages. In this work, we present the\nfundamentals of partial evaluation in such languages. We provide correctness\nresults for partial evaluation based on needed narrowing and show that the nice\nproperties of this strategy are essential for the specialization process. In\nparticular, the structure of the original program is preserved by partial\nevaluation and, thus, the same evaluation strategy can be applied for the\nexecution of specialized programs. This is in contrast to other partial\nevaluation schemes for lazy functional logic programs which may change the\nprogram structure in a negative way. Recent proposals for the partial\nevaluation of declarative multi-paradigm programs use (some form of) needed\nnarrowing to perform computations at partial evaluation time. Therefore, our\nresults constitute the basis for the correctness of such partial evaluators."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0403034v3", 
    "other_authors": "Matthew Fluet, Riccardo Pucella", 
    "title": "Phantom Types and Subtyping", 
    "arxiv-id": "cs/0403034v3", 
    "author": "Riccardo Pucella", 
    "publish": "2004-03-23T06:59:38Z", 
    "summary": "We investigate a technique from the literature, called the phantom-types\ntechnique, that uses parametric polymorphism, type constraints, and unification\nof polymorphic types to model a subtyping hierarchy. Hindley-Milner type\nsystems, such as the one found in Standard ML, can be used to enforce the\nsubtyping relation, at least for first-order values. We show that this\ntechnique can be used to encode any finite subtyping hierarchy (including\nhierarchies arising from multiple interface inheritance). We formally\ndemonstrate the suitability of the phantom-types technique for capturing\nfirst-order subtyping by exhibiting a type-preserving translation from a simple\ncalculus with bounded polymorphism to a calculus embodying the type system of\nSML."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0404020v1", 
    "other_authors": "Gopalan Nadathur", 
    "title": "A treatment of higher-order features in logic programming", 
    "arxiv-id": "cs/0404020v1", 
    "author": "Gopalan Nadathur", 
    "publish": "2004-04-07T17:26:09Z", 
    "summary": "The logic programming paradigm provides the basis for a new intensional view\nof higher-order notions. This view is realized primarily by employing the terms\nof a typed lambda calculus as representational devices and by using a richer\nform of unification for probing their structures. These additions have\nimportant meta-programming applications but they also pose non-trivial\nimplementation problems. One issue concerns the machine representation of\nlambda terms suitable to their intended use: an adequate encoding must\nfacilitate comparison operations over terms in addition to supporting the usual\nreduction computation. Another aspect relates to the treatment of a unification\noperation that has a branching character and that sometimes calls for the\ndelaying of the solution of unification problems. A final issue concerns the\nexecution of goals whose structures become apparent only in the course of\ncomputation. These various problems are exposed in this paper and solutions to\nthem are described. A satisfactory representation for lambda terms is developed\nby exploiting the nameless notation of de Bruijn as well as explicit encodings\nof substitutions. Special mechanisms are molded into the structure of\ntraditional Prolog implementations to support branching in unification and\ncarrying of unification problems over other computation steps; a premium is\nplaced in this context on exploiting determinism and on emulating usual\nfirst-order behaviour. An extended compilation model is presented that treats\nhigher-order unification and also handles dynamically emergent goals. The ideas\ndescribed here have been employed in the Teyjus implementation of the Lambda\nProlog language, a fact that is used to obtain a preliminary assessment of\ntheir efficacy."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0404050v1", 
    "other_authors": "Puri Arenas-Sanchez, Mario Rodriguez-Artalejo", 
    "title": "A General Framework For Lazy Functional Logic Programming With Algebraic   Polymorphic Types", 
    "arxiv-id": "cs/0404050v1", 
    "author": "Mario Rodriguez-Artalejo", 
    "publish": "2004-04-24T14:02:48Z", 
    "summary": "We propose a general framework for first-order functional logic programming,\nsupporting lazy functions, non-determinism and polymorphic datatypes whose data\nconstructors obey a set C of equational axioms. On top of a given C, we specify\na program as a set R of C-based conditional rewriting rules for defined\nfunctions. We argue that equational logic does not supply the proper semantics\nfor such programs. Therefore, we present an alternative logic which includes\nC-based rewriting calculi and a notion of model. We get soundness and\ncompleteness for C-based rewriting w.r.t. models, existence of free models for\nall programs, and type preservation results. As operational semantics, we\ndevelop a sound and complete procedure for goal solving, which is based on the\ncombination of lazy narrowing with unification modulo C. Our framework is quite\nexpressive for many purposes, such as solving action and change problems, or\nrealizing the GAMMA computation model."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0404052v1", 
    "other_authors": "Keith L. Clark, Peter J. Robinson, Richard Hagen", 
    "title": "Multi-Threading And Message Communication In Qu-Prolog", 
    "arxiv-id": "cs/0404052v1", 
    "author": "Richard Hagen", 
    "publish": "2004-04-25T03:49:44Z", 
    "summary": "This paper presents the multi-threading and internet message communication\ncapabilities of Qu-Prolog. Message addresses are symbolic and the\ncommunications package provides high-level support that completely hides\ndetails of IP addresses and port numbers as well as the underlying TCP/IP\ntransport layer. The combination of the multi-threads and the high level\ninter-thread message communications provide simple, powerful support for\nimplementing internet distributed intelligent applications."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0404053v1", 
    "other_authors": "Javier Leach, Susana Nieva, Mario Rodriguez-Artalejo", 
    "title": "Constraint Logic Programming with Hereditary Harrop Formula", 
    "arxiv-id": "cs/0404053v1", 
    "author": "Mario Rodriguez-Artalejo", 
    "publish": "2004-04-26T13:22:43Z", 
    "summary": "Constraint Logic Programming (CLP) and Hereditary Harrop formulas (HH) are\ntwo well known ways to enhance the expressivity of Horn clauses. In this paper,\nwe present a novel combination of these two approaches. We show how to enrich\nthe syntax and proof theory of HH with the help of a given constraint system,\nin such a way that the key property of HH as a logic programming language\n(namely, the existence of uniform proofs) is preserved. We also present a\nprocedure for goal solving, showing its soundness and completeness for\ncomputing answer constraints. As a consequence of this result, we obtain a new\nstrong completeness theorem for CLP that avoids the need to build disjunctions\nof computed answers, as well as a more abstract formulation of a known\ncompleteness theorem for HH."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0404055v2", 
    "other_authors": "Roberto Bagnara, Roberta Gori, Patricia M. Hill, Enea Zaffanella", 
    "title": "Finite-Tree Analysis for Constraint Logic-Based Languages: The Complete   Unabridged Version", 
    "arxiv-id": "cs/0404055v2", 
    "author": "Enea Zaffanella", 
    "publish": "2004-04-26T20:44:49Z", 
    "summary": "Logic languages based on the theory of rational, possibly infinite, trees\nhave much appeal in that rational trees allow for faster unification (due to\nthe safe omission of the occurs-check) and increased expressivity (cyclic terms\ncan provide very efficient representations of grammars and other useful\nobjects). Unfortunately, the use of infinite rational trees has problems. For\ninstance, many of the built-in and library predicates are ill-defined for such\ntrees and need to be supplemented by run-time checks whose cost may be\nsignificant. Moreover, some widely-used program analysis and manipulation\ntechniques are correct only for those parts of programs working over finite\ntrees. It is thus important to obtain, automatically, a knowledge of the\nprogram variables (the finite variables) that, at the program points of\ninterest, will always be bound to finite terms. For these reasons, we propose\nhere a new data-flow analysis, based on abstract interpretation, that captures\nsuch information."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0405075v1", 
    "other_authors": "Xiaochu Qi", 
    "title": "Reduction Strategies in Lambda Term Normalization and their Effects on   Heap Usage", 
    "arxiv-id": "cs/0405075v1", 
    "author": "Xiaochu Qi", 
    "publish": "2004-05-22T07:21:02Z", 
    "summary": "Higher-order representations of objects such as programs, proofs, formulas\nand types have become important to many symbolic computation tasks. Systems\nthat support such representations usually depend on the implementation of an\nintensional view of the terms of some variant of the typed lambda-calculus.\nVarious notations have been proposed for lambda-terms to explicitly treat\nsubstitutions as basis for realizing such implementations. There are, however,\nseveral choices in the actual reduction strategies. The most common strategy\nutilizes such notations only implicitly via an incremental use of environments.\nThis approach does not allow the smaller substitution steps to be intermingled\nwith other operations of interest on lambda-terms. However, a naive strategy\nexplicitly using such notations can also be costly: each use of the\nsubstitution propagation rules causes the creation of a new structure on the\nheap that is often discarded in the immediately following step. There is thus a\ntradeoff between these two approaches. This thesis describes the actual\nrealization of the two approaches, discusses their tradeoffs based on this and,\nfinally, offers an amalgamated approach that utilizes recursion in rewrite rule\napplication but also suspends substitution operations where necessary."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ISCC.2013.6754992", 
    "link": "http://arxiv.org/pdf/cs/0405079v1", 
    "other_authors": "Riccardo Pucella", 
    "title": "Higher-Order Concurrent Win32 Programming", 
    "arxiv-id": "cs/0405079v1", 
    "author": "Riccardo Pucella", 
    "publish": "2004-05-23T18:56:57Z", 
    "summary": "We present a concurrent framework for Win32 programming based on Concurrent\nML, a concurrent language with higher-order functions, static typing,\nlightweight threads and synchronous communication channels. The key points of\nthe framework are the move from an event loop model to a threaded model for the\nprocessing of window messages, and the decoupling of controls notifications\nfrom the system messages. This last point allows us to derive a general way of\nwriting controls that leads to easy composition, and can accommodate ActiveX\nControls in a transparent way."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405080v1", 
    "other_authors": "Riccardo Pucella", 
    "title": "Reactive Programming in Standard ML", 
    "arxiv-id": "cs/0405080v1", 
    "author": "Riccardo Pucella", 
    "publish": "2004-05-23T19:09:11Z", 
    "summary": "Reactive systems are systems that maintain an ongoing interaction with their\nenvironment, activated by receiving input events from the environment and\nproducing output events in response. Modern programming languages designed to\nprogram such systems use a paradigm based on the notions of instants and\nactivations. We describe a library for Standard ML that provides basic\nprimitives for programming reactive systems. The library is a low-level system\nupon which more sophisticated reactive behaviors can be built, which provides a\nconvenient framework for prototyping extensions to existing reactive languages."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405082v1", 
    "other_authors": "Riccardo Pucella, Erik Meijer, Dino Oliva", 
    "title": "Aspects de la Programmation d'Applications Win32 avec un Langage   Fonctionnel", 
    "arxiv-id": "cs/0405082v1", 
    "author": "Dino Oliva", 
    "publish": "2004-05-23T19:35:49Z", 
    "summary": "A useful programming language needs to support writing programs that take\nadvantage of services and communication mechanisms supplied by the operating\nsystem. We examine the problem of programming native Win32 applications under\nWindows with Standard ML. We introduce an framework based on the IDL interface\nlanguage et a minimal foreign-functions interface to explore the Win32 API et\nCOM in the context of Standard ML."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405083v1", 
    "other_authors": "Riccardo Pucella", 
    "title": "The Design of a COM-Oriented Module System", 
    "arxiv-id": "cs/0405083v1", 
    "author": "Riccardo Pucella", 
    "publish": "2004-05-23T19:51:01Z", 
    "summary": "We present in this paper the preliminary design of a module system based on a\nnotion of components such as they are found in COM. This module system is\ninspired from that of Standard ML, and features first-class instances of\ncomponents, first-class interfaces, and interface-polymorphic functions, as\nwell as allowing components to be both imported from the environment and\nexported to the environment using simple mechanisms. The module system\nautomates the memory management of interfaces and hides the IUnknown interface\nand QueryInterface mechanisms from the programmer, favoring instead a\nhigher-level approach to handling interfaces."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405084v1", 
    "other_authors": "Kathleen Fisher, Riccardo Pucella, John Reppy", 
    "title": "A Framework for Interoperability", 
    "arxiv-id": "cs/0405084v1", 
    "author": "John Reppy", 
    "publish": "2004-05-23T20:28:59Z", 
    "summary": "Practical implementations of high-level languages must provide access to\nlibraries and system services that have APIs specified in a low-level language\n(usually C). An important characteristic of such mechanisms is the\nforeign-interface policy that defines how to bridge the semantic gap between\nthe high-level language and C. For example, IDL-based tools generate code to\nmarshal data into and out of the high-level representation according to user\nannotations. The design space of foreign-interface policies is large and there\nare pros and cons to each approach. Rather than commit to a particular policy,\nwe choose to focus on the problem of supporting a gamut of interoperability\npolicies. In this paper, we describe a framework for language interoperability\nthat is expressive enough to support very efficient implementations of a wide\nrange of different foreign-interface policies. We describe two tools that\nimplement substantially different policies on top of our framework and present\nbenchmarks that demonstrate their efficiency."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405085v1", 
    "other_authors": "Riccardo Pucella, Prakash Panangaden", 
    "title": "On the Expressive Power of First-Order Boolean Functions in PCF", 
    "arxiv-id": "cs/0405085v1", 
    "author": "Prakash Panangaden", 
    "publish": "2004-05-24T05:16:33Z", 
    "summary": "Recent results of Bucciarelli show that the semilattice of degrees of\nparallelism of first-order boolean functions in PCF has both infinite chains\nand infinite antichains. By considering a simple subclass of Sieber's\nsequentiality relations, we identify levels in the semilattice and derive\ninexpressibility results concerning functions on different levels. This allows\nus to further explore the structure of the semilattice of degrees of\nparallelism: we identify semilattices characterized by simple level properties,\nand show the existence of new infinite hierarchies which are in a certain sense\nnatural with respect to the levels."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405088v1", 
    "other_authors": "Paul Tarau, Veronica Dahl", 
    "title": "High-Level Networking With Mobile Code And First Order AND-Continuations", 
    "arxiv-id": "cs/0405088v1", 
    "author": "Veronica Dahl", 
    "publish": "2004-05-24T12:41:07Z", 
    "summary": "We describe a scheme for moving living code between a set of distributed\nprocesses coordinated with unification based Linda operations, and its\napplication to building a comprehensive Logic programming based Internet\nprogramming framework. Mobile threads are implemented by capturing first order\ncontinuations in a compact data structure sent over the network. Code is\nfetched lazily from its original base turned into a server as the continuation\nexecutes at the remote site. Our code migration techniques, in combination with\na dynamic recompilation scheme, ensure that heavily used code moves up smoothly\non a speed hierarchy while volatile dynamic code is kept in a quickly updatable\nform. Among the examples, we describe how to build programmable client and\nserver components (Web servers, in particular) and mobile agents."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405091v1", 
    "other_authors": "Yves Caseau, Francois-Xavier Josset, Francois Laburthe", 
    "title": "CLAIRE: Combining Sets, Search And Rules To Better Express Algorithms", 
    "arxiv-id": "cs/0405091v1", 
    "author": "Francois Laburthe", 
    "publish": "2004-05-24T17:31:12Z", 
    "summary": "This paper presents a programming language which includes paradigms that are\nusually associated with declarative languages, such as sets, rules and search,\ninto an imperative (functional) language. Although these paradigms are\nseparately well known and are available under various programming environments,\nthe originality of the CLAIRE language comes from the tight integration, which\nyields interesting run-time performances, and from the richness of this\ncombination, which yields new ways in which to express complex algorithmic\npatterns with few elegant lines. To achieve the opposite goals of a high\nabstraction level (conciseness and readability) and run-time performance\n(CLAIRE is used as a C++ preprocessor), we have developed two kinds of\ncompiler: first, a pattern pre-processor handles iterations over both concrete\nand abstract sets (data types and program fragments), in a completely\nuser-extensible manner; secondly, an inference compiler transforms a set of\nlogical rules into a set of functions (demons that are used through procedural\nattachment)."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405092v1", 
    "other_authors": "Yves Caseau, Glenn Silverstein, Francois Laburthe", 
    "title": "Learning Hybrid Algorithms for Vehicle Routing Problems", 
    "arxiv-id": "cs/0405092v1", 
    "author": "Francois Laburthe", 
    "publish": "2004-05-24T17:41:50Z", 
    "summary": "This paper presents a generic technique for improving hybrid algorithms\nthrough the discovery of and tuning of meta-heuristics. The idea is to\nrepresent a family of push/pull heuristics that are based upon inserting and\nremoving tasks in a current solution, with an algebra. We then let a learning\nalgorithm search for the best possible algebraic term, which represents a\nhybrid algorithm for a given set of problems and an optimization criterion. In\na previous paper, we described this algebra in detail and provided a set of\npreliminary results demonstrating the utility of this approach, using vehicle\nrouting with time windows (VRPTW) as a domain example. In this paper we expand\nupon our results providing a more robust experimental framework and learning\nalgorithms, and report on some new results using the standard Solomon\nbenchmarks. In particular, we show that our learning algorithm is able to\nachieve results similar to the best-published algorithms using only a fraction\nof the CPU time. We also show that the automatic tuning of the best hybrid\ncombination of such techniques yields a better solution than hand tuning, with\nconsiderably less effort."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405100v1", 
    "other_authors": "Francois Fages, Emmanuel Coquery", 
    "title": "Typing constraint logic programs", 
    "arxiv-id": "cs/0405100v1", 
    "author": "Emmanuel Coquery", 
    "publish": "2004-05-27T02:39:01Z", 
    "summary": "We present a prescriptive type system with parametric polymorphism and\nsubtyping for constraint logic programs. The aim of this type system is to\ndetect programming errors statically. It introduces a type discipline for\nconstraint logic programs and modules, while maintaining the capabilities of\nperforming the usual coercions between constraint domains, and of typing\nmeta-programming predicates, thanks to the flexibility of subtyping. The\nproperty of subject reduction expresses the consistency of a prescriptive type\nsystem w.r.t. the execution model: if a program is \"well-typed\", then all\nderivations starting from a \"well-typed\" goal are again \"well-typed\". That\nproperty is proved w.r.t. the abstract execution model of constraint\nprogramming which proceeds by accumulation of constraints only, and w.r.t. an\nenriched execution model with type constraints for substitutions. We describe\nour implementation of the system for type checking and type inference. We\nreport our experimental results on type checking ISO-Prolog, the (constraint)\nlibraries of Sicstus Prolog and other Prolog programs."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405101v1", 
    "other_authors": "Samir Genaim, Michael Codish, Jacob M. Howe", 
    "title": "Worst-Case Groundness Analysis Using Definite Boolean Functions", 
    "arxiv-id": "cs/0405101v1", 
    "author": "Jacob M. Howe", 
    "publish": "2004-05-27T02:56:34Z", 
    "summary": "This note illustrates theoretical worst-case scenarios for groundness\nanalyses obtained through abstract interpretation over the abstract domains of\ndefinite (Def) and positive (Pos) Boolean functions. For Def, an example is\ngiven for which any Def-based abstract interpretation for groundness analysis\nfollows a chain which is exponential in the number of argument positions as\nwell as in the number of clauses but sub-exponential in the size of the\nprogram. For Pos, we strengthen a previous result by illustrating an example\nfor which any Pos-based abstract interpretation for groundness analysis follows\na chain which is exponential in the size of the program. It remains an open\nproblem to determine if the worst case for Def is really as bad as that for\nPos."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0405102v1", 
    "other_authors": "Francisco Javier Lopez-Fraguas, Jaime Sanchez-Hernandez", 
    "title": "A Proof Theoretic Approach to Failure in Functional Logic Programming", 
    "arxiv-id": "cs/0405102v1", 
    "author": "Jaime Sanchez-Hernandez", 
    "publish": "2004-05-27T03:08:30Z", 
    "summary": "How to extract negative information from programs is an important issue in\nlogic programming. Here we address the problem for functional logic programs,\nfrom a proof-theoretic perspective. The starting point of our work is CRWL\n(Constructor based ReWriting Logic), a well established theoretical framework\nfor functional logic programming, whose fundamental notion is that of\nnon-strict non-deterministic function. We present a proof calculus, CRWLF,\nwhich is able to deduce negative information from CRWL-programs. In particular,\nCRWLF is able to prove finite failure of reduction within CRWL."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0406012v1", 
    "other_authors": "Seng Wai Loke, Andrew Davison", 
    "title": "Secure Prolog-Based Mobile Code", 
    "arxiv-id": "cs/0406012v1", 
    "author": "Andrew Davison", 
    "publish": "2004-06-07T07:01:53Z", 
    "summary": "LogicWeb mobile code consists of Prolog-like rules embedded in Web pages,\nthereby adding logic programming behaviour to those pages. Since LogicWeb\nprograms are downloaded from foreign hosts and executed locally, there is a\nneed to protect the client from buggy or malicious code. A security model is\ncrucial for making LogicWeb mobile code safe to execute. This paper presents\nsuch a model, which supports programs of varying trust levels by using\ndifferent resource access policies. The implementation of the model derives\nfrom an extended operational semantics for the LogicWeb language, which\nprovides a precise meaning of safety."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0406014v1", 
    "other_authors": "Richard A. O'Keefe", 
    "title": "O(1) Reversible Tree Navigation Without Cycles", 
    "arxiv-id": "cs/0406014v1", 
    "author": "Richard A. O'Keefe", 
    "publish": "2004-06-07T14:04:48Z", 
    "summary": "Imperative programmers often use cyclically linked trees in order to achieve\nO(1) navigation time to neighbours. Some logic programmers believe that cyclic\nterms are necessary to achieve the same in logic-based languages. An old but\nlittle-known technique provides O(1) time and space navigation without cyclic\nlinks, in the form of reversible predicates. A small modification provides O(1)\namortised time and space editing."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0407043v1", 
    "other_authors": "Willem Jan van Hoeve", 
    "title": "A Hyper-Arc Consistency Algorithm for the Soft Alldifferent Constraint", 
    "arxiv-id": "cs/0407043v1", 
    "author": "Willem Jan van Hoeve", 
    "publish": "2004-07-16T14:44:21Z", 
    "summary": "This paper presents an algorithm that achieves hyper-arc consistency for the\nsoft alldifferent constraint. To this end, we prove and exploit the equivalence\nwith a minimum-cost flow problem. Consistency of the constraint can be checked\nin O(nm) time, and hyper-arc consistency is achieved in O(m) time, where n is\nthe number of variables involved and m is the sum of the cardinalities of the\ndomains. It improves a previous method that did not ensure hyper-arc\nconsistency."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0408025v1", 
    "other_authors": "Christian Holzbaur, Maria Garcia de la Banda, Peter J. Stuckey, Gregory J. Duck", 
    "title": "Optimizing compilation of constraint handling rules in HAL", 
    "arxiv-id": "cs/0408025v1", 
    "author": "Gregory J. Duck", 
    "publish": "2004-08-10T07:55:35Z", 
    "summary": "In this paper we discuss the optimizing compilation of Constraint Handling\nRules (CHRs). CHRs are a multi-headed committed choice constraint language,\ncommonly applied for writing incremental constraint solvers. CHRs are usually\nimplemented as a language extension that compiles to the underlying language.\nIn this paper we show how we can use different kinds of information in the\ncompilation of CHRs in order to obtain access efficiency, and a better\ntranslation of the CHR rules into the underlying language, which in this case\nis HAL. The kinds of information used include the types, modes, determinism,\nfunctional dependencies and symmetries of the CHR constraints. We also show how\nto analyze CHR programs to determine this information about functional\ndependencies, symmetries and other kinds of information supporting\noptimizations."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0409038v1", 
    "other_authors": "Maria Garcia de la Banda, Warwick Harvey, Kim Marriott, Peter J. Stuckey, Bart Demoen", 
    "title": "Checking modes of HAL programs", 
    "arxiv-id": "cs/0409038v1", 
    "author": "Bart Demoen", 
    "publish": "2004-09-21T11:48:47Z", 
    "summary": "Recent constraint logic programming (CLP) languages, such as HAL and Mercury,\nrequire type, mode and determinism declarations for predicates. This\ninformation allows the generation of efficient target code and the detection of\nmany errors at compile-time. Unfortunately, mode checking in such languages is\ndifficult. One of the main reasons is that, for each predicate mode\ndeclaration, the compiler is required to appropriately re-order literals in the\npredicate's definition. The task is further complicated by the need to handle\ncomplex instantiations (which interact with type declarations and higher-order\npredicates) and automatic initialization of solver variables. Here we define\nmode checking for strongly typed CLP languages which require reordering of\nclause body literals. In addition, we show how to handle a simple case of\npolymorphic modes by using the corresponding polymorphic types."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0412012v1", 
    "other_authors": "Catherine Oriat", 
    "title": "Jartege: a Tool for Random Generation of Unit Tests for Java Classes", 
    "arxiv-id": "cs/0412012v1", 
    "author": "Catherine Oriat", 
    "publish": "2004-12-03T12:19:16Z", 
    "summary": "This report presents Jartege, a tool which allows random generation of unit\ntests for Java classes specified in JML. JML (Java Modeling Language) is a\nspecification language for Java which allows one to write invariants for\nclasses, and pre- and postconditions for operations. As in the JML-JUnit tool,\nwe use JML specifications on the one hand to eliminate irrelevant test cases,\nand on the other hand as a test oracle. Jartege randomly generates test cases,\nwhich consist of a sequence of constructor and method calls for the classes\nunder test. The random aspect of the tool can be parameterized by associating\nweights to classes and operations, and by controlling the number of instances\nwhich are created for each class under test. The practical use of Jartege is\nillustrated by a small case study."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICCL.1998.674156", 
    "link": "http://arxiv.org/pdf/cs/0412043v1", 
    "other_authors": "Roberto Bagnara, Patricia M. Hill, Elena Mazzi, Enea Zaffanella", 
    "title": "Widening Operators for Weakly-Relational Numeric Abstractions (Extended   Abstract)", 
    "arxiv-id": "cs/0412043v1", 
    "author": "Enea Zaffanella", 
    "publish": "2004-12-10T15:52:29Z", 
    "summary": "We discuss the divergence problems recently identified in some extrapolation\noperators for weakly-relational numeric domains. We identify the cause of the\ndivergences and point out that resorting to more concrete, syntactic domains\ncan be avoided by researching suitable algorithms for the elimination of\nredundant constraints in the chosen representation."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0503067v5", 
    "other_authors": "Alan Jeffrey, Julian Rathke", 
    "title": "Contextual equivalence for higher-order pi-calculus revisited", 
    "arxiv-id": "cs/0503067v5", 
    "author": "Julian Rathke", 
    "publish": "2005-03-24T13:03:15Z", 
    "summary": "The higher-order pi-calculus is an extension of the pi-calculus to allow\ncommunication of abstractions of processes rather than names alone. It has been\nstudied intensively by Sangiorgi in his thesis where a characterisation of a\ncontextual equivalence for higher-order pi-calculus is provided using labelled\ntransition systems and normal bisimulations. Unfortunately the proof technique\nused there requires a restriction of the language to only allow finite types.\nWe revisit this calculus and offer an alternative presentation of the labelled\ntransition system and a novel proof technique which allows us to provide a\nfully abstract characterisation of contextual equivalence using labelled\ntransitions and bisimulations for higher-order pi-calculus with recursive types\nalso."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0504008v10", 
    "other_authors": "Raju Renjit. G", 
    "title": "Super Object Oriented Programming", 
    "arxiv-id": "cs/0504008v10", 
    "author": "Raju Renjit. G", 
    "publish": "2005-04-04T06:19:43Z", 
    "summary": "This submission has been withdrawn at the request of the author."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0504025v15", 
    "other_authors": "Raju Renjit. G", 
    "title": "Incorporating LINQ, State Diagrams Templating and Package Extension Into   Java", 
    "arxiv-id": "cs/0504025v15", 
    "author": "Raju Renjit. G", 
    "publish": "2005-04-07T13:21:15Z", 
    "summary": "This submission has been withdrawn at the request of the author."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0506005v1", 
    "other_authors": "Neng-Fa Zhou", 
    "title": "Programming Finite-Domain Constraint Propagators in Action Rules", 
    "arxiv-id": "cs/0506005v1", 
    "author": "Neng-Fa Zhou", 
    "publish": "2005-06-02T03:49:52Z", 
    "summary": "In this paper, we propose a new language, called AR ({\\it Action Rules}), and\ndescribe how various propagators for finite-domain constraints can be\nimplemented in it. An action rule specifies a pattern for agents, an action\nthat the agents can carry out, and an event pattern for events that can\nactivate the agents. AR combines the goal-oriented execution model of logic\nprogramming with the event-driven execution model. This hybrid execution model\nfacilitates programming constraint propagators. A propagator for a constraint\nis an agent that maintains the consistency of the constraint and is activated\nby the updates of the domain variables in the constraint. AR has a much\nstronger descriptive power than {\\it indexicals}, the language widely used in\nthe current finite-domain constraint systems, and is flexible for implementing\nnot only interval-consistency but also arc-consistency algorithms. As examples,\nwe present a weak arc-consistency propagator for the {\\tt all\\_distinct}\nconstraint and a hybrid algorithm for n-ary linear equality constraints.\nB-Prolog has been extended to accommodate action rules. Benchmarking shows that\nB-Prolog as a CLP(FD) system significantly outperforms other CLP(FD) systems."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0506035v1", 
    "other_authors": "Jerome Collin, Michel Dagenais", 
    "title": "Fast Recompilation of Object Oriented Modules", 
    "arxiv-id": "cs/0506035v1", 
    "author": "Michel Dagenais", 
    "publish": "2005-06-10T15:28:00Z", 
    "summary": "Once a program file is modified, the recompilation time should be minimized,\nwithout sacrificing execution speed or high level object oriented features. The\nrecompilation time is often a problem for the large graphical interactive\ndistributed applications tackled by modern OO languages. A compilation server\nand fast code generator were developed and integrated with the SRC Modula-3\ncompiler and Linux ELF dynamic linker. The resulting compilation and\nrecompilation speedups are impressive. The impact of different language\nfeatures, processor speed, and application size are discussed."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0508106v1", 
    "other_authors": "Etienne Payet, Fred Mesnard", 
    "title": "An Improved Non-Termination Criterion for Binary Constraint Logic   Programs", 
    "arxiv-id": "cs/0508106v1", 
    "author": "Fred Mesnard", 
    "publish": "2005-08-24T12:18:11Z", 
    "summary": "On one hand, termination analysis of logic programs is now a fairly\nestablished research topic within the logic programming community. On the other\nhand, non-termination analysis seems to remain a much less attractive subject.\nIf we divide this line of research into two kinds of approaches: dynamic versus\nstatic analysis, this paper belongs to the latter. It proposes a criterion for\ndetecting non-terminating atomic queries with respect to binary CLP clauses,\nwhich strictly generalizes our previous works on this subject. We give a\ngeneric operational definition and a logical form of this criterion. Then we\nshow that the logical form is correct and complete with respect to the\noperational definition."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0509027v1", 
    "other_authors": "Oleg Kiselyov, Ralf Laemmel", 
    "title": "Haskell's overlooked object system", 
    "arxiv-id": "cs/0509027v1", 
    "author": "Ralf Laemmel", 
    "publish": "2005-09-10T12:35:20Z", 
    "summary": "Haskell provides type-class-bounded and parametric polymorphism as opposed to\nsubtype polymorphism of object-oriented languages such as Java and OCaml. It is\na contentious question whether Haskell 98 without extensions, or with common\nextensions, or with new extensions can fully support conventional\nobject-oriented programming with encapsulation, mutable state, inheritance,\noverriding, statically checked implicit and explicit subtyping, and so on. We\nsystematically substantiate that Haskell 98, with some common extensions,\nsupports all the conventional OO features plus more advanced ones, including\nfirst-class lexically scoped classes, implicitly polymorphic classes, flexible\nmultiple inheritance, safe downcasts and safe co-variant arguments. Haskell\nindeed can support width and depth, structural and nominal subtyping. We\naddress the particular challenge to preserve Haskell's type inference even for\nobjects and object-operating functions. The OO features are introduced in\nHaskell as the OOHaskell library. OOHaskell lends itself as a sandbox for typed\nOO language design."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0509057v1", 
    "other_authors": "Todd L. Veldhuizen", 
    "title": "Language embeddings that preserve staging and safety", 
    "arxiv-id": "cs/0509057v1", 
    "author": "Todd L. Veldhuizen", 
    "publish": "2005-09-19T15:30:10Z", 
    "summary": "We study embeddings of programming languages into one another that preserve\nwhat reductions take place at compile-time, i.e., staging. A certain condition\n-- what we call a `Turing complete kernel' -- is sufficient for a language to\nbe stage-universal in the sense that any language may be embedded in it while\npreserving staging. A similar line of reasoning yields the notion of\nsafety-preserving embeddings, and a useful characterization of\nsafety-universality. Languages universal with respect to staging and safety are\ngood candidates for realizing domain-specific embedded languages (DSELs) and\n`active libraries' that provide domain-specific optimizations and safety\nchecks."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0510074v1", 
    "other_authors": "Matthew Fluet, Riccardo Pucella", 
    "title": "Practical Datatype Specializations with Phantom Types and Recursion   Schemes", 
    "arxiv-id": "cs/0510074v1", 
    "author": "Riccardo Pucella", 
    "publish": "2005-10-24T16:27:00Z", 
    "summary": "Datatype specialization is a form of subtyping that captures program\ninvariants on data structures that are expressed using the convenient and\nintuitive datatype notation. Of particular interest are structural invariants\nsuch as well-formedness. We investigate the use of phantom types for describing\ndatatype specializations. We show that it is possible to express\nstatically-checked specializations within the type system of Standard ML. We\nalso show that this can be done in a way that does not lose useful programming\nfacilities such as pattern matching in case expressions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0511092v1", 
    "other_authors": "Roberto Amadio", 
    "title": "The SL synchronous language, revisited", 
    "arxiv-id": "cs/0511092v1", 
    "author": "Roberto Amadio", 
    "publish": "2005-11-28T07:20:43Z", 
    "summary": "We revisit the SL synchronous programming model introduced by Boussinot and\nDe Simone (IEEE, Trans. on Soft. Eng., 1996). We discuss an alternative design\nof the model including thread spawning and recursive definitions and we explore\nsome basic properties of the revised model: determinism, reactivity, CPS\ntranslation to a tail recursive form, computational expressivity, and a\ncompositional notion of program equivalence."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0512026v1", 
    "other_authors": "I. Josopait", 
    "title": "Checking C++ Programs for Dimensional Consistency", 
    "arxiv-id": "cs/0512026v1", 
    "author": "I. Josopait", 
    "publish": "2005-12-07T15:51:26Z", 
    "summary": "I will present my implementation 'n-units' of physical units into C++\nprograms. It allows the compiler to check for dimensional consistency."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0512057v1", 
    "other_authors": "Roberto Amadio, Silvano Dal Zilio", 
    "title": "Resource Control for Synchronous Cooperative Threads", 
    "arxiv-id": "cs/0512057v1", 
    "author": "Silvano Dal Zilio", 
    "publish": "2005-12-14T13:41:07Z", 
    "summary": "We develop new methods to statically bound the resources needed for the\nexecution of systems of concurrent, interactive threads. Our study is concerned\nwith a \\emph{synchronous} model of interaction based on cooperative threads\nwhose execution proceeds in synchronous rounds called instants. Our\ncontribution is a system of compositional static analyses to guarantee that\neach instant terminates and to bound the size of the values computed by the\nsystem as a function of the size of its parameters at the beginning of the\ninstant. Our method generalises an approach designed for first-order functional\nlanguages that relies on a combination of standard termination techniques for\nterm rewriting systems and an analysis of the size of the computed values based\non the notion of quasi-interpretation. We show that these two methods can be\ncombined to obtain an explicit polynomial bound on the resources needed for the\nexecution of the system during an instant. As a second contribution, we\nintroduce a virtual machine and a related bytecode thus producing a precise\ndescription of the resources needed for the execution of a system. In this\ncontext, we present a suitable control flow analysis that allows to formulte\nthe static analyses for resource control at byte code level."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0512058v1", 
    "other_authors": "Roberto Amadio, Gerard Boudol, Ilaria Castellani, Frederic Boussinot", 
    "title": "Reactive concurrent programming revisited", 
    "arxiv-id": "cs/0512058v1", 
    "author": "Frederic Boussinot", 
    "publish": "2005-12-14T13:42:17Z", 
    "summary": "In this note we revisit the so-called reactive programming style, which\nevolves from the synchronous programming model of the Esterel language by\nweakening the assumption that the absence of an event can be detected\ninstantaneously. We review some research directions that have been explored\nsince the emergence of the reactive model ten years ago. We shall also outline\nsome questions that remain to be investigated."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0512065v1", 
    "other_authors": "Todd L. Veldhuizen", 
    "title": "Tradeoffs in Metaprogramming", 
    "arxiv-id": "cs/0512065v1", 
    "author": "Todd L. Veldhuizen", 
    "publish": "2005-12-15T21:22:23Z", 
    "summary": "The design of metaprogramming languages requires appreciation of the\ntradeoffs that exist between important language characteristics such as safety\nproperties, expressive power, and succinctness. Unfortunately, such tradeoffs\nare little understood, a situation we try to correct by embarking on a study of\nmetaprogramming language tradeoffs using tools from computability theory.\nSafety properties of metaprograms are in general undecidable; for example, the\nproperty that a metaprogram always halts and produces a type-correct instance\nis $\\Pi^0_2$-complete. Although such safety properties are undecidable, they\nmay sometimes be captured by a restricted language, a notion we adapt from\ncomplexity theory. We give some sufficient conditions and negative results on\nwhen languages capturing properties can exist: there can be no languages\ncapturing total correctness for metaprograms, and no `functional' safety\nproperties above $\\Sigma^0_3$ can be captured. We prove that translating a\nmetaprogram from a general-purpose to a restricted metaprogramming language\ncapturing a property is tantamount to proving that property for the\nmetaprogram. Surprisingly, when one shifts perspective from programming to\nmetaprogramming, the corresponding safety questions do not become substantially\nharder -- there is no `jump' of Turing degree for typical safety properties."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0601003v1", 
    "other_authors": "Ruben Vandeginste, Bart Demoen", 
    "title": "Incremental copying garbage collection for WAM-based Prolog systems", 
    "arxiv-id": "cs/0601003v1", 
    "author": "Bart Demoen", 
    "publish": "2006-01-02T20:58:56Z", 
    "summary": "The design and implementation of an incremental copying heap garbage\ncollector for WAM-based Prolog systems is presented. Its heap layout consists\nof a number of equal-sized blocks. Other changes to the standard WAM allow\nthese blocks to be garbage collected independently. The independent collection\nof heap blocks forms the basis of an incremental collecting algorithm which\nemploys copying without marking (contrary to the more frequently used mark&copy\nor mark&slide algorithms in the context of Prolog). Compared to standard\nsemi-space copying collectors, this approach to heap garbage collection lowers\nin many cases the memory usage and reduces pause times. The algorithm also\nallows for a wide variety of garbage collection policies including generational\nones. The algorithm is implemented and evaluated in the context of hProlog."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0601019v2", 
    "other_authors": "Antoine Reilles", 
    "title": "Canonical Abstract Syntax Trees", 
    "arxiv-id": "cs/0601019v2", 
    "author": "Antoine Reilles", 
    "publish": "2006-01-06T19:50:29Z", 
    "summary": "This paper presents Gom, a language for describing abstract syntax trees and\ngenerating a Java implementation for those trees. Gom includes features\nallowing the user to specify and modify the interface of the data structure.\nThese features provide in particular the capability to maintain the internal\nrepresentation of data in canonical form with respect to a rewrite system. This\nexplicitly guarantees that the client program only manipulates normal forms for\nthis rewrite system, a feature which is only implicitly used in many\nimplementations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0601039v1", 
    "other_authors": "Maria Alpuente, Santiago Escobar, Salvador Lucas", 
    "title": "Removing Redundant Arguments Automatically", 
    "arxiv-id": "cs/0601039v1", 
    "author": "Salvador Lucas", 
    "publish": "2006-01-10T13:13:54Z", 
    "summary": "The application of automatic transformation processes during the formal\ndevelopment and optimization of programs can introduce encumbrances in the\ngenerated code that programmers usually (or presumably) do not write. An\nexample is the introduction of redundant arguments in the functions defined in\nthe program. Redundancy of a parameter means that replacing it by any\nexpression does not change the result. In this work, we provide methods for the\nanalysis and elimination of redundant arguments in term rewriting systems as a\nmodel for the programs that can be written in more sophisticated languages. On\nthe basis of the uselessness of redundant arguments, we also propose an erasure\nprocedure which may avoid wasteful computations while still preserving the\nsemantics (under ascertained conditions). A prototype implementation of these\nmethods has been undertaken, which demonstrates the practicality of our\napproach."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0601071v1", 
    "other_authors": "Antonio J. Fernandez, Teresa Hortala-Gonzalez, Fernando Saenz-Perez, Rafael del Vado-Virseda", 
    "title": "Constraint Functional Logic Programming over Finite Domains", 
    "arxiv-id": "cs/0601071v1", 
    "author": "Rafael del Vado-Virseda", 
    "publish": "2006-01-16T11:45:02Z", 
    "summary": "In this paper, we present our proposal to Constraint Functional Logic\nProgramming over Finite Domains (CFLP(FD)) with a lazy functional logic\nprogramming language which seamlessly embodies finite domain (FD) constraints.\nThis proposal increases the expressiveness and power of constraint logic\nprogramming over finite domains (CLP(FD)) by combining functional and\nrelational notation, curried expressions, higher-order functions, patterns,\npartial applications, non-determinism, lazy evaluation, logical variables,\ntypes, domain variables, constraint composition, and finite domain constraints.\n  We describe the syntax of the language, its type discipline, and its\ndeclarative and operational semantics. We also describe TOY(FD), an\nimplementation for CFLPFD(FD), and a comparison of our approach with respect to\nCLP(FD) from a programming point of view, showing the new features we\nintroduce. And, finally, we show a performance analysis which demonstrates that\nour implementation is competitive with respect to existing CLP(FD) systems and\nthat clearly outperforms the closer approach to CFLP(FD)."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0602098v2", 
    "other_authors": "M. H. van Emden", 
    "title": "Compositional Semantics for the Procedural Interpretation of Logic", 
    "arxiv-id": "cs/0602098v2", 
    "author": "M. H. van Emden", 
    "publish": "2006-02-28T19:28:31Z", 
    "summary": "Semantics of logic programs has been given by proof theory, model theory and\nby fixpoint of the immediate-consequence operator. If clausal logic is a\nprogramming language, then it should also have a compositional semantics.\nCompositional semantics for programming languages follows the abstract syntax\nof programs, composing the meaning of a unit by a mathematical operation on the\nmeanings of its constituent units. The procedural interpretation of logic has\nonly yielded an incomplete abstract syntax for logic programs. We complete it\nand use the result as basis of a compositional semantics. We present for\ncomparison Tarski's algebraization of first-order predicate logic, which is in\nsubstance the compositional semantics for his choice of syntax. We characterize\nour semantics by equivalence with the immediate-consequence operator."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0602099v1", 
    "other_authors": "H. Ibrahim, M. H. van Emden", 
    "title": "Towards Applicative Relational Programming", 
    "arxiv-id": "cs/0602099v1", 
    "author": "M. H. van Emden", 
    "publish": "2006-02-28T19:53:59Z", 
    "summary": "Functional programming comes in two flavours: one where ``functions are\nfirst-class citizens'' (we call this applicative) and one which is based on\nequations (we call this declarative). In relational programming clauses play\nthe role of equations. Hence Prolog is declarative. The purpose of this paper\nis to provide in relational programming a mathematical basis for the relational\nanalog of applicative functional programming. We use the cylindric semantics of\nfirst-order logic due to Tarski and provide a new notation for the required\ncylinders that we call tables. We define the Table/Relation Algebra with\noperators sufficient to translate Horn clauses into algebraic form. We\nestablish basic mathematical properties of these operators. We show how\nrelations can be first-class citizens, and devise mechanisms for modularity,\nfor local scoping of predicates, and for exporting/importing relations between\nprograms."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0603079v1", 
    "other_authors": "Maurizio Gabbrielli, Maria Chiara Meo", 
    "title": "A compositional Semantics for CHR", 
    "arxiv-id": "cs/0603079v1", 
    "author": "Maria Chiara Meo", 
    "publish": "2006-03-20T14:17:14Z", 
    "summary": "Constraint Handling Rules (CHR) are a committed-choice declarative language\nwhich has been designed for writing constraint solvers. A CHR program consists\nof multi-headed guarded rules which allow one to rewrite constraints into\nsimpler ones until a solved form is reached.\n  CHR has received a considerable attention, both from the practical and from\nthe theoretical side. Nevertheless, due the use of multi-headed clauses, there\nare several aspects of the CHR semantics which have not been clarified yet. In\nparticular, no compositional semantics for CHR has been defined so far.\n  In this paper we introduce a fix-point semantics which characterizes the\ninput/output behavior of a CHR program and which is and-compositional, that is,\nwhich allows to retrieve the semantics of a conjunctive query from the\nsemantics of its components. Such a semantics can be used as a basis to define\nincremental and modular analysis and verification tools."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0603100v1", 
    "other_authors": "Alin Suciu, Kalman Pusztai", 
    "title": "Efficient Compression of Prolog Programs", 
    "arxiv-id": "cs/0603100v1", 
    "author": "Kalman Pusztai", 
    "publish": "2006-03-26T20:27:40Z", 
    "summary": "We propose a special-purpose class of compression algorithms for efficient\ncompression of Prolog programs. It is a dictionary-based compression method,\nspecially designed for the compression of Prolog code, and therefore we name it\nPCA (Prolog Compression Algorithm). According to the experimental results this\nmethod provides better compression than state-of-the-art general-purpose\ncompression algorithms. Since the algorithm works with Prolog syntactic\nentities (e.g. atoms, terms, etc.) the implementation of a Prolog prototype is\nstraightforward and very easy to use in any Prolog application that needs\ncompression. Although the algorithm is designed for Prolog programs, the idea\ncan be easily applied for the compression of programs written in other (logic)\nlanguages."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0605104v2", 
    "other_authors": "Blake Hegerle", 
    "title": "Parsing Transformative LR(1) Languages", 
    "arxiv-id": "cs/0605104v2", 
    "author": "Blake Hegerle", 
    "publish": "2006-05-24T19:09:39Z", 
    "summary": "We consider, as a means of making programming languages more flexible and\npowerful, a parsing algorithm in which the parser may freely modify the grammar\nwhile parsing. We are particularly interested in a modification of the\ncanonical LR(1) parsing algorithm in which, after the reduction of certain\nproductions, we examine the source sentence seen so far to determine the\ngrammar to use to continue parsing. A naive modification of the canonical LR(1)\nparsing algorithm along these lines cannot be guaranteed to halt; as a result,\nwe develop a test which examines the grammar as it changes, stopping the parse\nif the grammar changes in a way that would invalidate earlier assumptions made\nby the parser. With this test in hand, we can develop our parsing algorithm and\nprove that it is correct. That being done, we turn to earlier, related work;\nthe idea of programming languages which can be extended to include new\nsyntactic constructs has existed almost as long as the idea of high-level\nprogramming languages. Early efforts to construct such a programming language\nwere hampered by an immature theory of formal languages. More recent efforts to\nconstruct transformative languages relied either on an inefficient chain of\nsource-to-source translators; or they have a defect, present in our naive\nparsing algorithm, in that they cannot be known to halt. The present algorithm\ndoes not have these undesirable properties, and as such, it should prove a\nuseful foundation for a new kind of programming language."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0607101v2", 
    "other_authors": "Patricia M. Hill, Fausto Spoto", 
    "title": "Deriving Escape Analysis by Abstract Interpretation: Proofs of results", 
    "arxiv-id": "cs/0607101v2", 
    "author": "Fausto Spoto", 
    "publish": "2006-07-24T09:24:07Z", 
    "summary": "Escape analysis of object-oriented languages approximates the set of objects\nwhich do not escape from a given context. If we take a method as context, the\nnon-escaping objects can be allocated on its activation stack; if we take a\nthread, Java synchronisation locks on such objects are not needed. In this\npaper, we formalise a basic escape domain e as an abstract interpretation of\nconcrete states, which we then refine into an abstract domain er which is more\nconcrete than e and, hence, leads to a more precise escape analysis than e. We\nprovide optimality results for both e and er, in the form of Galois insertions\nfrom the concrete to the abstract domains and of optimal abstract operations.\nThe Galois insertion property is obtained by restricting the abstract domains\nto those elements which do not contain garbage, by using an abstract garbage\ncollector. Our implementation of er is hence an implementation of a formally\ncorrect escape analyser, able to detect the stack allocatable creation points\nof Java (bytecode) applications.\n  This report contains the proofs of results of a paper with the same title and\nauthors and to be published in the Journal \"Higher-Order Symbolic Computation\"."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0609092v1", 
    "other_authors": "P. Emelyanov", 
    "title": "Analysis of Equality Relationships for Imperative Programs", 
    "arxiv-id": "cs/0609092v1", 
    "author": "P. Emelyanov", 
    "publish": "2006-09-16T12:22:15Z", 
    "summary": "In this article, we discuss a flow--sensitive analysis of equality\nrelationships for imperative programs. We describe its semantic domains,\ngeneral purpose operations over abstract computational states (term evaluation\nand identification, semantic completion, widening operator, etc.) and semantic\ntransformers corresponding to program constructs. We summarize our experiences\nfrom the last few years concerning this analysis and give attention to\napplications of analysis of automatically generated code. Among other\nillustrating examples, we consider a program for which the analysis diverges\nwithout a widening operator and results of analyzing residual programs produced\nby some automatic partial evaluator. An example of analysis of a program\ngenerated by this evaluator is given."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0610164v2", 
    "other_authors": "Bageshri Karkare, Uday Khedker", 
    "title": "Complexity of Data Flow Analysis for Non-Separable Frameworks", 
    "arxiv-id": "cs/0610164v2", 
    "author": "Uday Khedker", 
    "publish": "2006-10-30T08:39:04Z", 
    "summary": "The complexity of round robin method of intraprocedural data flow analysis is\nmeasured in number of iterations over the control flow graph. Existing\ncomplexity bounds realistically explain the complexity of only Bit-vector\nframeworks which are separable. In this paper we define the complexity bounds\nfor non-separable frameworks by quantifying the interdependences among the data\nflow information of program entities using an Entity Dependence Graph."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0612063v1", 
    "other_authors": "Lunjin Lu", 
    "title": "Improving Precision of Type Analysis Using Non-Discriminative Union", 
    "arxiv-id": "cs/0612063v1", 
    "author": "Lunjin Lu", 
    "publish": "2006-12-12T03:09:44Z", 
    "summary": "This paper presents a new type analysis for logic programs. The analysis is\nperformed with a priori type definitions; and type expressions are formed from\na fixed alphabet of type constructors. Non-discriminative union is used to join\ntype information from different sources without loss of precision. An operation\nthat is performed repeatedly during an analysis is to detect if a fixpoint has\nbeen reached. This is reduced to checking the emptiness of types. Due to the\nuse of non-discriminative union, the fundamental problem of checking the\nemptiness of types is more complex in the proposed type analysis than in other\ntype analyses with a priori type definitions. The experimental results,\nhowever, show that use of tabling reduces the effect to a small fraction of\nanalysis time on a set of benchmarks.\n  Keywords: Type analysis, Non-discriminative union, Abstract interpretation,\nTabling"
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701106v1", 
    "other_authors": "Pierre Deransart", 
    "title": "On using Tracer Driver for External Dynamic Process Observation", 
    "arxiv-id": "cs/0701106v1", 
    "author": "Pierre Deransart", 
    "publish": "2007-01-17T13:43:50Z", 
    "summary": "One is interested here in the observation of dynamic processes starting from\nthe traces which they leave or those that one makes them produce. It is\nconsidered here that it should be possible to make several observations\nsimultaneously, using a large variety of independently developed analyzers. For\nthis purpose, we introduce the original notion of ``full trace'' to capture the\nidea that a process can be instrumented in such a way that it may broadcast all\ninformation which could ever be requested by any kind of observer. Each\nanalyzer can then find in the full trace the data elements which it needs. This\napproach uses what has been called a \"tracer driver\" which completes the tracer\nand drives it to answer the requests of the analyzers. A tracer driver allows\nto restrict the flow of information and makes this approach tractable. On the\nother side, the potential size of a full trace seems to make the idea of full\ntrace unrealistic. In this work we explore the consequences of this notion in\nterm of potential efficiency, by analyzing the respective workloads between the\n(full) tracer and many different analyzers, all being likely run in true\nparallel environments. To illustrate this study, we use the example of the\nobservation of the resolution of constraints systems (proof-tree, search-tree\nand propagation) using sophisticated visualization tools, as developed in the\nproject OADymPPaC (2001-2004). The processes considered here are computer\nprograms, but we believe the approach can be extended to many other kinds of\nprocesses."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701107v1", 
    "other_authors": "Hani Girgis, Bharat Jayaraman", 
    "title": "JavaTA: A Logic-based Debugger for Java", 
    "arxiv-id": "cs/0701107v1", 
    "author": "Bharat Jayaraman", 
    "publish": "2007-01-17T13:48:49Z", 
    "summary": "This paper presents a logic based approach to debugging Java programs. In\ncontrast with traditional debugging we propose a debugging methodology for Java\nprograms using logical queries on individual execution states and also over the\nhistory of execution. These queries were arrived at by a systematic study of\nerrors in object-oriented programs in our earlier research. We represent the\nsalient events during the execution of a Java program by a logic database, and\nimplement the queries as logic programs. Such an approach allows us to answer a\nnumber of useful and interesting queries about a Java program, such as the\ncalling sequence that results in a certain outcome, the state of an object at a\nparticular execution point, etc. Our system also provides the ability to\ncompose new queries during a debugging session. We believe that logic\nprogramming offers a significant contribution to the art of object-oriented\nprograms debugging."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701108v1", 
    "other_authors": "Edison Mera, Pedro Lopez-Garcia, German Puebla, Manuel Carro, Manuel Hermenegildo", 
    "title": "Towards Execution Time Estimation for Logic Programs via Static Analysis   and Profiling", 
    "arxiv-id": "cs/0701108v1", 
    "author": "Manuel Hermenegildo", 
    "publish": "2007-01-17T14:22:17Z", 
    "summary": "Effective static analyses have been proposed which infer bounds on the number\nof resolutions or reductions. These have the advantage of being independent\nfrom the platform on which the programs are executed and have been shown to be\nuseful in a number of applications, such as granularity control in parallel\nexecution. On the other hand, in distributed computation scenarios where\nplatforms with different capabilities come into play, it is necessary to\nexpress costs in metrics that include the characteristics of the platform. In\nparticular, it is specially interesting to be able to infer upper and lower\nbounds on actual execution times. With this objective in mind, we propose an\napproach which combines compile-time analysis for cost bounds with a one-time\nprofiling of the platform in order to determine the values of certain\nparameters for a given platform. These parameters calibrate a cost model which,\nfrom then on, is able to compute statically time bound functions for procedures\nand to predict with a significant degree of accuracy the execution times of\nsuch procedures in the given platform. The approach has been implemented and\nintegrated in the CiaoPP system."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701109v1", 
    "other_authors": "Siddharth Chitnis, Madhu Yennamani, Gopal Gupta", 
    "title": "ExSched: Solving Constraint Satisfaction Problems with the Spreadsheet   Paradigm", 
    "arxiv-id": "cs/0701109v1", 
    "author": "Gopal Gupta", 
    "publish": "2007-01-17T14:26:40Z", 
    "summary": "We report on the development of a general tool called ExSched, implemented as\na plug-in for Microsoft Excel, for solving a class of constraint satisfaction\nproblems. The traditional spreadsheet paradigm is based on attaching arithmetic\nexpressions to individual cells and then evaluating them. The ExSched interface\ngeneralizes the spreadsheet paradigm to allow finite domain constraints to be\nattached to the individual cells that are then solved to get a solution. This\nextension provides a user-friendly interface for solving constraint\nsatisfaction problems that can be modeled as 2D tables, such as scheduling\nproblems, timetabling problems, product configuration, etc. ExSched can be\nregarded as a spreadsheet interface to CLP(FD) that hides the syntactic and\nsemantic complexity of CLP(FD) and enables novice users to solve many\nscheduling and timetabling problems interactively."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701110v1", 
    "other_authors": "Kim Henriksen, John Gallagher", 
    "title": "A Web-based Tool Combining Different Type Analyses", 
    "arxiv-id": "cs/0701110v1", 
    "author": "John Gallagher", 
    "publish": "2007-01-17T14:35:56Z", 
    "summary": "There are various kinds of type analysis of logic programs. These include for\nexample inference of types that describe an over-approximation of the success\nset of a program, inference of well-typings, and abstractions based on given\ntypes. Analyses can be descriptive or prescriptive or a mixture of both, and\nthey can be goal-dependent or goal-independent. We describe a prototype tool\nthat can be accessed from a web browser, allowing various type analyses to be\nrun. The first goal of the tool is to allow the analysis results to be examined\nconveniently by clicking on points in the original program clauses, and to\nhighlight ill-typed program constructs, empty types or other type anomalies.\nSecondly the tool allows combination of the various styles of analysis. For\nexample, a descriptive regular type can be automatically inferred for a given\nprogram, and then that type can be used to generate the minimal \"domain model\"\nof the program with respect to the corresponding pre-interpretation, which can\ngive more precise information than the original descriptive type."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701111v1", 
    "other_authors": "Elvira Albert, Puri Arenas, German Puebla", 
    "title": "Some Issues on Incremental Abstraction-Carrying Code", 
    "arxiv-id": "cs/0701111v1", 
    "author": "German Puebla", 
    "publish": "2007-01-17T14:44:30Z", 
    "summary": "Abstraction-Carrying Code (ACC) has recently been proposed as a framework for\nproof-carrying code (PCC) in which the code supplier provides a program\ntogether with an abstraction (or abstract model of the program) whose validity\nentails compliance with a predefined safety policy. The abstraction thus plays\nthe role of safety certificate and its generation (and validation) is carried\nout automatically by a fixed-point analyzer. Existing approaches for PCC are\ndeveloped under the assumption that the consumer reads and validates the entire\nprogram w.r.t. the full certificate at once, in a non incremental way. In this\nabstract, we overview the main issues on incremental ACC. In particular, in the\ncontext of logic programming, we discuss both the generation of incremental\ncertificates and the design of an incremental checking algorithm for untrusted\nupdates of a (trusted) program, i.e., when a producer provides a modified\nversion of a previously validated program. By update, we refer to any arbitrary\nchange on a program, i.e., the extension of the program with new predicates,\nthe deletion of existing predicates and the replacement of existing predicates\nby new versions for them. We also discuss how each kind of update affects the\nincremental extension in terms of accuracy and correctness."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701147v1", 
    "other_authors": "Michael Hanus", 
    "title": "A Generic Analysis Environment for Curry Programs", 
    "arxiv-id": "cs/0701147v1", 
    "author": "Michael Hanus", 
    "publish": "2007-01-24T06:55:34Z", 
    "summary": "We present CurryBrowser, a generic analysis environment for the declarative\nmulti-paradigm language Curry. CurryBrowser supports browsing through the\nprogram code of an application written in Curry, i.e., the main module and all\ndirectly or indirectly imported modules. Each module can be shown in different\nformats (e.g., source code, interface, intermediate code) and, inside each\nmodule, various properties of functions defined in this module can be analyzed.\nIn order to support the integration of various program analyses, CurryBrowser\nhas a generic interface to connect local and global analyses implemented in\nCurry. CurryBrowser is completely implemented in Curry using libraries for GUI\nprogramming and meta-programming."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701148v1", 
    "other_authors": "Wim Vanhoof, Susana Munoz-Hernandez", 
    "title": "Proceedings of the 16th Workshop in Logic-based Methods in Programming   Environments (WLPE2006)", 
    "arxiv-id": "cs/0701148v1", 
    "author": "Susana Munoz-Hernandez", 
    "publish": "2007-01-24T07:03:17Z", 
    "summary": "This volume contains the papers presented at WLPE'06: the 16th Workshop on\nLogic-based Methods in Programming Environments held on August 16, 2006 in the\nSeattle Sheraton Hotel and Towers, Seattle, Washington (USA). It was organised\nas a satellite workshop of ICLP'06, the 22th International Conference on Logic\nProgramming."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0701176v1", 
    "other_authors": "Alain Frisch, Haruo Hosoya", 
    "title": "Towards Practical Typechecking for Macro Tree Transducers", 
    "arxiv-id": "cs/0701176v1", 
    "author": "Haruo Hosoya", 
    "publish": "2007-01-26T15:16:31Z", 
    "summary": "Macro tree transducers (mtt) are an important model that both covers many\nuseful XML transformations and allows decidable exact typechecking. This paper\nreports our first step toward an implementation of mtt typechecker that has a\npractical efficiency. Our approach is to represent an input type obtained from\na backward inference as an alternating tree automaton, in a style similar to\nTozawa's XSLT0 typechecking. In this approach, typechecking reduces to checking\nemptiness of an alternating tree automaton. We propose several optimizations\n(Cartesian factorization, state partitioning) on the backward inference process\nin order to produce much smaller alternating tree automata than the naive\nalgorithm, and we present our efficient algorithm for checking emptiness of\nalternating tree automata, where we exploit the explicit representation of\nalternation for local optimizations. Our preliminary experiments confirm that\nour algorithm has a practical performance that can typecheck simple\ntransformations with respect to the full XHTML in a reasonable time."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703073v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "A New Numerical Abstract Domain Based on Difference-Bound Matrices", 
    "arxiv-id": "cs/0703073v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T05:44:24Z", 
    "summary": "This paper presents a new numerical abstract domain for static analysis by\nabstract interpretation. This domain allows us to represent invariants of the\nform (x-y<=c) and (+/-x<=c), where x and y are variables values and c is an\ninteger or real constant. Abstract elements are represented by Difference-Bound\nMatrices, widely used by model-checkers, but we had to design new operators to\nmeet the needs of abstract interpretation. The result is a complete lattice of\ninfinite height featuring widening, narrowing and common transfer functions. We\nfocus on giving an efficient O(n2) representation and graph-based O(n3)\nalgorithms - where n is the number of variables|and claim that this domain\nalways performs more precisely than the well-known interval domain. To\nillustrate the precision/cost tradeoff of this domain, we have implemented\nsimple abstract interpreters for toy imperative and parallel languages which\nallowed us to prove some non-trivial algorithms correct."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703074v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "Field-Sensitive Value Analysis of Embedded C Programs with Union Types   and Pointer Arithmetics", 
    "arxiv-id": "cs/0703074v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T05:46:39Z", 
    "summary": "We propose a memory abstraction able to lift existing numerical static\nanalyses to C programs containing union types, pointer casts, and arbitrary\npointer arithmetics. Our framework is that of a combined points-to and\ndata-value analysis. We abstract the contents of compound variables in a\nfield-sensitive way, whether these fields contain numeric or pointer values,\nand use stock numerical abstract domains to find an overapproximation of all\npossible memory states--with the ability to discover relationships between\nvariables. A main novelty of our approach is the dynamic mapping scheme we use\nto associate a flat collection of abstract cells of scalar type to the set of\naccessed memory locations, while taking care of byte-level aliases - i.e., C\nvariables with incompatible types allocated in overlapping memory locations. We\ndo not rely on static type information which can be misleading in C programs as\nit does not account for all the uses a memory zone may be put to. Our work was\nincorporated within the Astr\\'{e}e static analyzer that checks for the absence\nof run-time-errors in embedded, safety-critical, numerical-intensive software.\nIt replaces the former memory domain limited to well-typed, union-free,\npointer-cast free data-structures. Early results demonstrate that this\nabstraction allows analyzing a larger class of C programs, without much cost\noverhead."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703075v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "A Few Graph-Based Relational Numerical Abstract Domains", 
    "arxiv-id": "cs/0703075v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T05:56:51Z", 
    "summary": "This article presents the systematic design of a class of relational\nnumerical abstract domains from non-relational ones. Constructed domains\nrepresent sets of invariants of the form (vj - vi in C), where vj and vi are\ntwo variables, and C lives in an abstraction of P(Z), P(Q), or P(R). We will\ncall this family of domains weakly relational domains. The underlying concept\nallowing this construction is an extension of potential graphs and\nshortest-path closure algorithms in exotic-like algebras. Example constructions\nare given in order to retrieve well-known domains as well as new ones. Such\ndomains can then be used in the Abstract Interpretation framework in order to\ndesign various static analyses. Amajor benfit of this construction is its\nmodularity, allowing to quickly implement new abstract domains from existing\nones."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703076v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "Symbolic Methods to Enhance the Precision of Numerical Abstract Domains", 
    "arxiv-id": "cs/0703076v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T06:05:40Z", 
    "summary": "We present lightweight and generic symbolic methods to improve the precison\nof numerical static analyses based on Abstract Interpretation. The main idea is\nto simplify numerical expressions before they are fed to abstract transfer\nfunctions. An important novelty is that these simplifications are performed\non-the-fly, using information gathered dynamically by the analyzer. A first\nmethod, called \"linearization,\" allows abstracting arbitrary expressions into\naffine forms with interval coefficients while simplifying them. A second\nmethod, called \"symbolic constant propagation,\" enhances the simplification\nfeature of the linearization by propagating assigned expressions in a symbolic\nway. Combined together, these methods increase the relationality level of\nnumerical abstract domains and make them more robust against program\ntransformations. We show how they can be integrated within the classical\ninterval, octagon and polyhedron domains. These methods have been incorporated\nwithin the Astr\\'{e}e static analyzer that checks for the absence of run-time\nerrors in embedded critical avionics software. We present an experimental proof\nof their usefulness."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703077v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "Relational Abstract Domains for the Detection of Floating-Point Run-Time   Errors", 
    "arxiv-id": "cs/0703077v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T06:07:02Z", 
    "summary": "We present a new idea to adapt relational abstract domains to the analysis of\nIEEE 754-compliant floating-point numbers in order to statically detect,\nthrough abstract Interpretation-based static analyses, potential floating-point\nrun-time exceptions such as overflows or invalid operations. In order to take\nthe non-linearity of rounding into account, expressions are modeled as linear\nforms with interval coefficients. We show how to extend already existing\nnumerical abstract domains, such as the octagon abstract domain, to efficiently\nabstract transfer functions based on interval linear forms. We discuss specific\nfixpoint stabilization techniques and give some experimental results."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703084v2", 
    "other_authors": "Antoine Min\u00e9", 
    "title": "The Octagon Abstract Domain", 
    "arxiv-id": "cs/0703084v2", 
    "author": "Antoine Min\u00e9", 
    "publish": "2007-03-15T18:16:32Z", 
    "summary": "This article presents a new numerical abstract domain for static analysis by\nabstract interpretation. It extends a former numerical abstract domain based on\nDifference-Bound Matrices and allows us to represent invariants of the form\n(+/-x+/-y<=c), where x and y are program variables and c is a real constant. We\nfocus on giving an efficient representation based on Difference-Bound Matrices\n- O(n2) memory cost, where n is the number of variables - and graph-based\nalgorithms for all common abstract operators - O(n3) time cost. This includes a\nnormal form algorithm to test equivalence of representation and a widening\noperator to compute least fixpoint approximations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/cs/0703155v1", 
    "other_authors": "Amey Karkare, Uday Khedker, Amitabha Sanyal", 
    "title": "Liveness of Heap Data for Functional Programs", 
    "arxiv-id": "cs/0703155v1", 
    "author": "Amitabha Sanyal", 
    "publish": "2007-03-30T16:06:27Z", 
    "summary": "Functional programming languages use garbage collection for heap memory\nmanagement. Ideally, garbage collectors should reclaim all objects that are\ndead at the time of garbage collection. An object is dead at an execution\ninstant if it is not used in future. Garbage collectors collect only those dead\nobjects that are not reachable from any program variable. This is because they\nare not able to distinguish between reachable objects that are dead and\nreachable objects that are live.\n  In this paper, we describe a static analysis to discover reachable dead\nobjects in programs written in first-order, eager functional programming\nlanguages. The results of this technique can be used to make reachable dead\nobjects unreachable, thereby allowing garbage collectors to reclaim more dead\nobjects."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0704.1373v1", 
    "other_authors": "Burgy Laurent, Laurent R\u00e9veill\u00e8re, Julia Lawall, Gilles Muller", 
    "title": "A Language-Based Approach for Improving the Robustness of Network   Application Protocol Implementations", 
    "arxiv-id": "0704.1373v1", 
    "author": "Gilles Muller", 
    "publish": "2007-04-11T08:35:32Z", 
    "summary": "The secure and robust functioning of a network relies on the defect-free\nimplementation of network applications. As network protocols have become\nincreasingly complex, however, hand-writing network message processing code has\nbecome increasingly error-prone. In this paper, we present a domain-specific\nlanguage, Zebu, for describing protocol message formats and related processing\nconstraints. From a Zebu specification, a compiler automatically generates\nstubs to be used by an application to parse network messages. Zebu is easy to\nuse, as it builds on notations used in RFCs to describe protocol grammars. Zebu\nis also efficient, as the memory usage is tailored to application needs and\nmessage fragments can be specified to be processed on demand. Finally,\nZebu-based applications are robust, as the Zebu compiler automatically checks\nspecification consistency and generates parsing stubs that include validation\nof the message structure. Using a mutation analysis in the context of SIP and\nRTSP, we show that Zebu significantly improves application robustness."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0705.1452v1", 
    "other_authors": "Gr\u00e9goire Henry, Michel Mauny, Emmanuel Chailloux", 
    "title": "Typer la d\u00e9-s\u00e9rialisation sans s\u00e9rialiser les types", 
    "arxiv-id": "0705.1452v1", 
    "author": "Emmanuel Chailloux", 
    "publish": "2007-05-10T12:19:51Z", 
    "summary": "In this paper, we propose a way of assigning static type information to\nunmarshalling functions and we describe a verification technique for\nunmarshalled data that preserves the execution safety provided by static type\nchecking. This technique, whose correctness is proven, relies on singleton\ntypes whose values are transmitted to unmarshalling routines at runtime, and on\nan efficient checking algorithm able to deal with sharing and cycles."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0705.1458v1", 
    "other_authors": "Emmanuel Chailloux, Gr\u00e9goire Henry, Rapha\u00ebl Montelatici", 
    "title": "Mixing the Objective Caml and C# Programming Models in the .Net   Framework", 
    "arxiv-id": "0705.1458v1", 
    "author": "Rapha\u00ebl Montelatici", 
    "publish": "2007-05-10T12:31:17Z", 
    "summary": "We present a new code generator, called O'Jacare.net, to inter-operate\nbetween C# and Objective Caml through their object models. O'Jacare.net defines\na basic IDL (Interface Definition Language) that describes classes and\ninterfaces in order to communicate between Objective Caml and C#. O'Jacare.net\ngenerates all needed wrapper classes and takes advantage of static type\nchecking in both worlds. Although the IDL intersects these two object models,\nO'Jacare.net allows to combine features from both."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0705.2126v1", 
    "other_authors": "Francois De Ferriere", 
    "title": "Improvements to the Psi-SSA representation", 
    "arxiv-id": "0705.2126v1", 
    "author": "Francois De Ferriere", 
    "publish": "2007-05-15T12:06:32Z", 
    "summary": "Modern compiler implementations use the Static Single Assignment\nrepresentation as a way to efficiently implement optimizing algorithms. However\nthis representation is not well adapted to architectures with a predicated\ninstruction set. The Psi-SSA representation extends the SSA representation such\nthat standard SSA algorithms can be easily adapted to an architecture with a\nfully predicated instruction set. A new pseudo operation, the Psi operation, is\nintroduced to merge several conditional definitions into a unique definition."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0705.2145v2", 
    "other_authors": "Paul Feautrier", 
    "title": "Elementary transformation analysis for Array-OL", 
    "arxiv-id": "0705.2145v2", 
    "author": "Paul Feautrier", 
    "publish": "2007-05-15T13:44:35Z", 
    "summary": "Array-OL is a high-level specification language dedicated to the definition\nof intensive signal processing applications. Several tools exist for\nimplementing an Array-OL specification as a data parallel program. While\nArray-OL can be used directly, it is often convenient to be able to deduce part\nof the specification from a sequential version of the application. This paper\nproposes such an analysis and examines its feasibility and its limits."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0705.3468v1", 
    "other_authors": "Neng-Fa Zhou, Taisuke Sato, Yi-Dong Shen", 
    "title": "Linear Tabling Strategies and Optimizations", 
    "arxiv-id": "0705.3468v1", 
    "author": "Yi-Dong Shen", 
    "publish": "2007-05-23T20:52:42Z", 
    "summary": "Recently, the iterative approach named linear tabling has received\nconsiderable attention because of its simplicity, ease of implementation, and\ngood space efficiency. Linear tabling is a framework from which different\nmethods can be derived based on the strategies used in handling looping\nsubgoals. One decision concerns when answers are consumed and returned. This\npaper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies,\nand compares them both qualitatively and quantitatively. The results indicate\nthat, while the lazy strategy has good locality and is well suited for finding\nall solutions, the eager strategy is comparable in speed with the lazy strategy\nand is well suited for programs with cuts. Linear tabling relies on depth-first\niterative deepening rather than suspension to compute fixpoints. Each cluster\nof inter-dependent subgoals as represented by a top-most looping subgoal is\niteratively evaluated until no subgoal in it can produce any new answers. Naive\nre-evaluation of all looping subgoals, albeit simple, may be computationally\nunacceptable. In this paper, we also introduce semi-naive optimization, an\neffective technique employed in bottom-up evaluation of logic programs to avoid\nredundant joins of answers, into linear tabling. We give the conditions for the\ntechnique to be safe (i.e. sound and complete) and propose an optimization\ntechnique called {\\it early answer promotion} to enhance its effectiveness.\nBenchmarking in B-Prolog demonstrates that with this optimization linear\ntabling compares favorably well in speed with the state-of-the-art\nimplementation of SLG."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0706.2073v1", 
    "other_authors": "Samuel Thibault, Fran\u00e7ois Broquedis, Brice Goglin, Raymond Namyst, Pierre-Andr\u00e9 Wacrenier", 
    "title": "An Efficient OpenMP Runtime System for Hierarchical Arch", 
    "arxiv-id": "0706.2073v1", 
    "author": "Pierre-Andr\u00e9 Wacrenier", 
    "publish": "2007-06-14T09:43:23Z", 
    "summary": "Exploiting the full computational power of always deeper hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. The emergence of multi-core\nchips and NUMA machines makes it important to minimize the number of remote\nmemory accesses, to favor cache affinities, and to guarantee fast completion of\nsynchronization steps. By using the BubbleSched platform as a threading backend\nfor the GOMP OpenMP compiler, we are able to easily transpose affinities of\nthread teams into scheduling hints using abstractions called bubbles. We then\npropose a scheduling strategy suited to nested OpenMP parallelism. The\nresulting preliminary performance evaluations show an important improvement of\nthe speedup on a typical NAS OpenMP benchmark application."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0707.1059v1", 
    "other_authors": "Jan A. Bergstra, Alban Ponse", 
    "title": "Projection semantics for rigid loops", 
    "arxiv-id": "0707.1059v1", 
    "author": "Alban Ponse", 
    "publish": "2007-07-06T23:58:45Z", 
    "summary": "A rigid loop is a for-loop with a counter not accessible to the loop body or\nany other part of a program. Special instructions for rigid loops are\nintroduced on top of the syntax of the program algebra PGA. Two different\nsemantic projections are provided and proven equivalent. One of these is taken\nto have definitional status on the basis of two criteria: `normative semantic\nadequacy' and `indicative algorithmic adequacy'."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0707.3807v1", 
    "other_authors": "Catherine Recanati", 
    "title": "How to be correct, lazy and efficient ?", 
    "arxiv-id": "0707.3807v1", 
    "author": "Catherine Recanati", 
    "publish": "2007-07-25T19:33:07Z", 
    "summary": "This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented\nat the Research Laboratory of Paris XI University (Laboratoire de Recherche en\nInformatique, Orsay). Lambdix was devised in the course of an investigation\ninto the relationship between the semantics of programming languages and their\nimplementation; it was used to demonstrate that in the Lisp domain, semantic\ncorrectness is consistent with efficiency, contrary to what has often been\nclaimed. The first part of the paper is an overview of well-known semantic\ndifficulties encountered by Lisp as well as an informal presentation of\nLambdix; it is shown that the difficulties which Lisp encouters do not arise in\nLambdix. The second part is about efficiency in implementation models. It\nexplains why Lambdix is better suited for lazy evaluation than previous models.\nThe section ends by giving comparative execution time tables."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0707.4389v1", 
    "other_authors": "Andrew W. Appel, Sandrine Blazy", 
    "title": "Separation Logic for Small-step Cminor", 
    "arxiv-id": "0707.4389v1", 
    "author": "Sandrine Blazy", 
    "publish": "2007-07-30T12:09:16Z", 
    "summary": "Cminor is a mid-level imperative programming language; there are\nproved-correct optimizing compilers from C to Cminor and from Cminor to machine\nlanguage. We have redesigned Cminor so that it is suitable for Hoare Logic\nreasoning and we have designed a Separation Logic for Cminor. In this paper, we\ngive a small-step semantics (instead of the big-step of the proved-correct\ncompiler) that is motivated by the need to support future concurrent\nextensions. We detail a machine-checked proof of soundness of our Separation\nLogic. This is the first large-scale machine-checked proof of a Separation\nLogic w.r.t. a small-step semantics. The work presented in this paper has been\ncarried out in the Coq proof assistant. It is a first step towards an\nenvironment in which concurrent Cminor programs can be verified using\nSeparation Logic and also compiled by a proved-correct compiler with formal\nend-to-end correctness guarantees."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0710.4640v1", 
    "other_authors": "Ilya Issenin, Nikil Dutt", 
    "title": "FORAY-GEN: Automatic Generation of Affine Functions for Memory   Optimizations", 
    "arxiv-id": "0710.4640v1", 
    "author": "Nikil Dutt", 
    "publish": "2007-10-25T08:11:20Z", 
    "summary": "In today's embedded applications a significant portion of energy is spent in\nthe memory subsystem. Several approaches have been proposed to minimize this\nenergy, including the use of scratch pad memories, with many based on static\nanalysis of a program. However, often it is not possible to perform static\nanalysis and optimization of a program's memory access behavior unless the\nprogram is specifically written for this purpose. In this paper we introduce\nthe FORAY model of a program that permits aggressive analysis of the\napplication's memory behavior that further enables such optimizations since it\nconsists of 'for' loops and array accesses which are easily analyzable. We\npresent FORAY-GEN: an automated profile-based approach for extraction of the\nFORAY model from the original program. We also demonstrate how FORAY-GEN\nenhances applicability of other memory subsystem optimization approaches,\nresulting in an average of two times increase in the number of memory\nreferences that can be analyzed by existing static approaches."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0710.4683v1", 
    "other_authors": "Stephen A. Edwards", 
    "title": "The Challenges of Hardware Synthesis from C-Like Languages", 
    "arxiv-id": "0710.4683v1", 
    "author": "Stephen A. Edwards", 
    "publish": "2007-10-25T09:07:39Z", 
    "summary": "MANY TECHNIQUES for synthesizing digital hardware from C-like languages have\nbeen proposed, but none have emerged as successful as Verilog or VHDL for\nregister-transfer-level design. This paper looks at two of the fundamental\nchallenges: concurrency and timing control."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0710.4702v1", 
    "other_authors": "Nastaran Baradaran, Pedro C. Diniz", 
    "title": "A Register Allocation Algorithm in the Presence of Scalar Replacement   for Fine-Grain Configurable Architectures", 
    "arxiv-id": "0710.4702v1", 
    "author": "Pedro C. Diniz", 
    "publish": "2007-10-25T09:27:20Z", 
    "summary": "The aggressive application of scalar replacement to array references\nsubstantially reduces the number of memory operations at the expense of a\npossibly very large number of registers. In this paper we describe a register\nallocation algorithm that assigns registers to scalar replaced array references\nalong the critical paths of a computation, in many cases exploiting the\nopportunity for concurrent memory accesses. Experimental results, for a set of\nimage/signal processing code kernels, reveal that the proposed algorithm leads\nto a substantial reduction of the number of execution cycles for the\ncorresponding hardware implementation on a contemporary\nField-Programmable-Gate-Array (FPGA) when compared to other greedy allocation\nalgorithms, in some cases, using even fewer number of registers."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0710.4807v1", 
    "other_authors": "G. Chen, M. Kandemir, M. Karakoy", 
    "title": "A Constraint Network Based Approach to Memory Layout Optimization", 
    "arxiv-id": "0710.4807v1", 
    "author": "M. Karakoy", 
    "publish": "2007-10-25T11:59:01Z", 
    "summary": "While loop restructuring based code optimization for array intensive\napplications has been successful in the past, it has several problems such as\nthe requirement of checking dependences (legality issues) and transformation of\nall of the array references within the loop body indiscriminately (while some\nof the references can benefit from the transformation, others may not). As a\nresult, data transformations, i.e., transformations that modify memory layout\nof array data instead of loop structure have been proposed. One of the problems\nassociated with data transformations is the difficulty of selecting a memory\nlayout for an array that is acceptable to the entire program (not just to a\nsingle loop). In this paper, we formulate the problem of determining the memory\nlayouts of arrays as a constraint network, and explore several methods of\nsolution in a systematic way. Our experiments provide strong support in favor\nof employing constraint processing, and point out future research directions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0711.0829v2", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Instruction sequences with indirect jumps", 
    "arxiv-id": "0711.0829v2", 
    "author": "C. A. Middelburg", 
    "publish": "2007-11-06T10:26:03Z", 
    "summary": "We study sequential programs that are instruction sequences with direct and\nindirect jump instructions. The intuition is that indirect jump instructions\nare jump instructions where the position of the instruction to jump to is the\ncontent of some memory cell. We consider several kinds of indirect jump\ninstructions. For each kind, we define the meaning of programs with indirect\njump instructions of that kind by means of a translation into programs without\nindirect jump instructions. For each kind, the intended behaviour of a program\nwith indirect jump instructions of that kind under execution is the behaviour\nof the translated program under execution on interaction with some memory\ndevice."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-1(1:4)2005", 
    "link": "http://arxiv.org/pdf/0711.4071v1", 
    "other_authors": "Pierre Deransart, Mireille Ducass\u00e9, G\u00e9rard Ferrand", 
    "title": "Observational semantics of the Prolog Resolution Box Model", 
    "arxiv-id": "0711.4071v1", 
    "author": "G\u00e9rard Ferrand", 
    "publish": "2007-11-26T18:03:07Z", 
    "summary": "This paper specifies an observational semantics and gives an original\npresentation of the Byrd box model. The approach accounts for the semantics of\nProlog tracers independently of a particular Prolog implementation. Prolog\ntraces are, in general, considered as rather obscure and difficult to use. The\nproposed formal presentation of its trace constitutes a simple and pedagogical\napproach for teaching Prolog or for implementing Prolog tracers. It is a form\nof declarative specification for the tracers. The trace model introduced here\nis only one example to illustrate general problems relating to tracers and\nobserving processes. Observing processes know, from observed processes, only\ntheir traces. The issue is then to be able to reconstitute, by the sole\nanalysis of the trace, part of the behaviour of the observed process, and if\npossible, without any loss of information. As a matter of fact, our approach\nhighlights qualities of the Prolog resolution box model which made its success,\nbut also its insufficiencies."
},{
    "category": "cs.PL", 
    "doi": "10.3233/FI-2009-165", 
    "link": "http://arxiv.org/pdf/0711.4217v4", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Instruction sequences with dynamically instantiated instructions", 
    "arxiv-id": "0711.4217v4", 
    "author": "C. A. Middelburg", 
    "publish": "2007-11-27T10:24:34Z", 
    "summary": "We study sequential programs that are instruction sequences with dynamically\ninstantiated instructions. We define the meaning of such programs in two\ndifferent ways. In either case, we give a translation by which each program\nwith dynamically instantiated instructions is turned into a program without\nthem that exhibits on execution the same behaviour by interaction with some\nservice. The complexity of the translations differ considerably, whereas the\nservices concerned are equally simple. However, the service concerned in the\ncase of the simpler translation is far more powerful than the service concerned\nin the other case."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jal.2008.07.001", 
    "link": "http://arxiv.org/pdf/0712.1658v1", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Program algebra with a jump-shift instruction", 
    "arxiv-id": "0712.1658v1", 
    "author": "C. A. Middelburg", 
    "publish": "2007-12-11T08:53:49Z", 
    "summary": "We study sequential programs that are instruction sequences with jump-shift\ninstructions in the setting of PGA (ProGram Algebra). Jump-shift instructions\npreceding a jump instruction increase the position to jump to. The jump-shift\ninstruction is not found in programming practice. Its merit is that the\nexpressive power of PGA extended with the jump-shift instruction, is not\nreduced if the reach of jump instructions is bounded. This is used to show that\nthere exists a finite-state execution mechanism that by making use of a counter\ncan produce each finite-state thread from some program that is a finite or\nperiodic infinite sequence of instructions from a finite set."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jal.2008.07.001", 
    "link": "http://arxiv.org/pdf/0712.3830v1", 
    "other_authors": "Tom Schrijvers, Bart Demoen, David S. Warren", 
    "title": "TCHR: a framework for tabled CLP", 
    "arxiv-id": "0712.3830v1", 
    "author": "David S. Warren", 
    "publish": "2007-12-26T15:28:16Z", 
    "summary": "Tabled Constraint Logic Programming is a powerful execution mechanism for\ndealing with Constraint Logic Programming without worrying about fixpoint\ncomputation. Various applications, e.g in the fields of program analysis and\nmodel checking, have been proposed. Unfortunately, a high-level system for\ndeveloping new applications is lacking, and programmers are forced to resort to\ncomplicated ad hoc solutions.\n  This papers presents TCHR, a high-level framework for tabled Constraint Logic\nProgramming. It integrates in a light-weight manner Constraint Handling Rules\n(CHR), a high-level language for constraint solvers, with tabled Logic\nProgramming. The framework is easily instantiated with new application-specific\nconstraint domains. Various high-level operations can be instantiated to\ncontrol performance. In particular, we propose a novel, generalized technique\nfor compacting answer sets."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jal.2008.07.001", 
    "link": "http://arxiv.org/pdf/0801.0133v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "An Approach to Programming Based on Concepts", 
    "arxiv-id": "0801.0133v1", 
    "author": "Alexandr Savinov", 
    "publish": "2007-12-30T14:43:27Z", 
    "summary": "In this paper we describe a new approach to programming which generalizes\nobject-oriented programming. It is based on using a new programming construct,\ncalled concept, which generalizes classes. Concept is defined as a pair of two\nclasses: one reference class and one object class. Each concept has a parent\nconcept which is specified using inclusion relation generalizing inheritance.\nWe describe several important mechanisms such as reference resolution, context\nstack, dual methods and life-cycle management, inheritance and polymorphism.\nThis approach to programming is positioned as a new programming paradigm and\ntherefore we formulate its main principles and rules."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jal.2008.07.001", 
    "link": "http://arxiv.org/pdf/0801.0135v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Concepts and their Use for Modelling Objects and References in   Programming Languages", 
    "arxiv-id": "0801.0135v1", 
    "author": "Alexandr Savinov", 
    "publish": "2007-12-30T14:50:01Z", 
    "summary": "In the paper a new programming construct, called concept, is introduced.\nConcept is pair of two classes: a reference class and an object class.\nInstances of the reference classes are passed-by-value and are intended to\nrepresent objects. Instances of the object class are passed-by-reference. An\napproach to programming where concepts are used instead of classes is called\nconcept-oriented programming (CoP). In CoP objects are represented and accessed\nindirectly by means of references. The structure of concepts describes a\nhierarchical space with a virtual address system. The paper describes this new\napproach to programming including such mechanisms as reference resolution,\ncomplex references, method interception, dual methods, life-cycle management\ninheritance and polymorphism."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jal.2008.07.001", 
    "link": "http://arxiv.org/pdf/0801.0136v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Indirect Object Representation and Access by Means of Concepts", 
    "arxiv-id": "0801.0136v1", 
    "author": "Alexandr Savinov", 
    "publish": "2007-12-30T14:56:05Z", 
    "summary": "The paper describes a mechanism for indirect object representation and access\n(ORA) in programming languages. The mechanism is based on using a new\nprogramming construct which is referred to as concept. Concept consists of one\nobject class and one reference class both having their fields and methods. The\nobject class is the conventional class as defined in OOP with instances passed\nby reference. Instances of the reference class are passed by value and are\nintended to represent objects. The reference classes are used to describe how\nobjects have to be represented and accessed by providing custom format for\ntheir identifiers and custom access procedures. Such an approach to programming\nwhere concepts are used instead of classes is referred to as concept-oriented\nprogramming. It generalizes OOP and its main advantage is that it allows the\nprogrammer to describe not only the functionality of target objects but also\nintermediate functions which are executed behind the scenes as an object is\nbeing accessed."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-4(1:3)2008", 
    "link": "http://arxiv.org/pdf/0801.0882v2", 
    "other_authors": "Neil D. Jones, Nina Bohr", 
    "title": "Call-by-value Termination in the Untyped lambda-calculus", 
    "arxiv-id": "0801.0882v2", 
    "author": "Nina Bohr", 
    "publish": "2008-01-06T19:01:02Z", 
    "summary": "A fully-automated algorithm is developed able to show that evaluation of a\ngiven untyped lambda-expression will terminate under CBV (call-by-value). The\n``size-change principle'' from first-order programs is extended to arbitrary\nuntyped lambda-expressions in two steps. The first step suffices to show CBV\ntermination of a single, stand-alone lambda;-expression. The second suffices to\nshow CBV termination of any member of a regular set of lambda-expressions,\ndefined by a tree grammar. (A simple example is a minimum function, when\napplied to arbitrary Church numerals.) The algorithm is sound and proven so in\nthis paper. The Halting Problem's undecidability implies that any sound\nalgorithm is necessarily incomplete: some lambda-expressions may in fact\nterminate under CBV evaluation, but not be recognised as terminating.\n  The intensional power of the termination algorithm is reasonably high. It\ncertifies as terminating many interesting and useful general recursive\nalgorithms including programs with mutual recursion and parameter exchanges,\nand Colson's ``minimum'' algorithm. Further, our type-free approach allows use\nof the Y combinator, and so can identify as terminating a substantial subset of\nPCF."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-4(1:3)2008", 
    "link": "http://arxiv.org/pdf/0801.1219v1", 
    "other_authors": "Andrey Breslav", 
    "title": "DSL development based on target meta-models. Using AST transformations   for automating semantic analysis in a textual DSL framework", 
    "arxiv-id": "0801.1219v1", 
    "author": "Andrey Breslav", 
    "publish": "2008-01-08T12:28:18Z", 
    "summary": "This paper describes an approach to creating textual syntax for Do-\nmain-Specific Languages (DSL). We consider target meta-model to be the main\nartifact and hence to be developed first. The key idea is to represent analysis\nof textual syntax as a sequence of transformations. This is made by explicit\nopera- tions on abstract syntax trees (ATS), for which a simple language is\nproposed. Text-to-model transformation is divided into two parts: text-to-AST\n(developed by openArchitectureWare [1]) and AST-to-model (proposed by this\npaper). Our approach simplifies semantic analysis and helps to generate as much\nas possi- ble."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-4(1:3)2008", 
    "link": "http://arxiv.org/pdf/0801.2226v1", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Programming an interpreter using molecular dynamics", 
    "arxiv-id": "0801.2226v1", 
    "author": "C. A. Middelburg", 
    "publish": "2008-01-15T07:56:12Z", 
    "summary": "PGA (ProGram Algebra) is an algebra of programs which concerns programs in\ntheir simplest form: sequences of instructions. Molecular dynamics is a simple\nmodel of computation developed in the setting of PGA, which bears on the use of\ndynamic data structures in programming. We consider the programming of an\ninterpreter for a program notation that is close to existing assembly languages\nusing PGA with the primitives of molecular dynamics as basic instructions. It\nhappens that, although primarily meant for explaining programming language\nfeatures relating to the use of dynamic data structures, the collection of\nprimitives of molecular dynamics in itself is suited to our programming wants."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-4(1:3)2008", 
    "link": "http://arxiv.org/pdf/0802.1578v3", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Thread extraction for polyadic instruction sequences", 
    "arxiv-id": "0802.1578v3", 
    "author": "C. A. Middelburg", 
    "publish": "2008-02-12T07:49:27Z", 
    "summary": "In this paper, we study the phenomenon that instruction sequences are split\ninto fragments which somehow produce a joint behaviour. In order to bring this\nphenomenon better into the picture, we formalize a simple mechanism by which\nseveral instruction sequence fragments can produce a joint behaviour. We also\nshow that, even in the case of this simple mechanism, it is a non-trivial\nmatter to explain by means of a translation into a single instruction sequence\nwhat takes place on execution of a collection of instruction sequence\nfragments."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.knosys.2011.04.004", 
    "link": "http://arxiv.org/pdf/0802.3492v2", 
    "other_authors": "Marko A. Rodriguez", 
    "title": "The RDF Virtual Machine", 
    "arxiv-id": "0802.3492v2", 
    "author": "Marko A. Rodriguez", 
    "publish": "2008-02-24T05:28:52Z", 
    "summary": "The Resource Description Framework (RDF) is a semantic network data model\nthat is used to create machine-understandable descriptions of the world and is\nthe basis of the Semantic Web. This article discusses the application of RDF to\nthe representation of computer software and virtual computing machines. The\nSemantic Web is posited as not only a web of data, but also as a web of\nprograms and processes."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.knosys.2011.04.004", 
    "link": "http://arxiv.org/pdf/0804.0970v1", 
    "other_authors": "Marie-Claude Gaudel, Pascale Le Gall", 
    "title": "Testing data types implementations from algebraic specifications", 
    "arxiv-id": "0804.0970v1", 
    "author": "Pascale Le Gall", 
    "publish": "2008-04-07T06:35:44Z", 
    "summary": "Algebraic specifications of data types provide a natural basis for testing\ndata types implementations. In this framework, the conformance relation is\nbased on the satisfaction of axioms. This makes it possible to formally state\nthe fundamental concepts of testing: exhaustive test set, testability\nhypotheses, oracle. Various criteria for selecting finite test sets have been\nproposed. They depend on the form of the axioms, and on the possibilities of\nobservation of the implementation under test. This last point is related to the\nwell-known oracle problem. As the main interest of algebraic specifications is\ndata type abstraction, testing a concrete implementation raises the issue of\nthe gap between the abstract description and the concrete representation. The\nobservational semantics of algebraic specifications bring solutions on the\nbasis of the so-called observable contexts. After a description of testing\nmethods based on algebraic specifications, the chapter gives a brief\npresentation of some tools and case studies, and presents some applications to\nother formal methods involving datatypes."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.knosys.2011.04.004", 
    "link": "http://arxiv.org/pdf/0804.1118v1", 
    "other_authors": "Donald A. Sofge", 
    "title": "A Survey of Quantum Programming Languages: History, Methods, and Tools", 
    "arxiv-id": "0804.1118v1", 
    "author": "Donald A. Sofge", 
    "publish": "2008-04-07T19:48:31Z", 
    "summary": "Quantum computer programming is emerging as a new subject domain from\nmultidisciplinary research in quantum computing, computer science, mathematics\n(especially quantum logic, lambda calculi, and linear logic), and engineering\nattempts to build the first non-trivial quantum computer. This paper briefly\nsurveys the history, methods, and proposed tools for programming quantum\ncomputers circa late 2007. It is intended to provide an extensive but\nnon-exhaustive look at work leading up to the current state-of-the-art in\nquantum computer programming. Further, it is an attempt to analyze the needed\nprogramming tools for quantum programmers, to use this analysis to predict the\ndirection in which the field is moving, and to make recommendations for further\ndevelopment of quantum programming language tools."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.knosys.2011.04.004", 
    "link": "http://arxiv.org/pdf/0808.0586v1", 
    "other_authors": "Xavier Leroy, Herv\u00e9 Grall", 
    "title": "Coinductive big-step operational semantics", 
    "arxiv-id": "0808.0586v1", 
    "author": "Herv\u00e9 Grall", 
    "publish": "2008-08-05T14:47:32Z", 
    "summary": "Using a call-by-value functional language as an example, this article\nillustrates the use of coinductive definitions and proofs in big-step\noperational semantics, enabling it to describe diverging evaluations in\naddition to terminating evaluations. We formalize the connections between the\ncoinductive big-step semantics and the standard small-step semantics, proving\nthat both semantics are equivalent. We then study the use of coinductive\nbig-step semantics in proofs of type soundness and proofs of semantic\npreservation for compilers. A methodological originality of this paper is that\nall results have been proved using the Coq proof assistant. We explain the\nproof-theoretic presentation of coinductive definitions and proofs offered by\nCoq, and show that it facilitates the discovery and the presentation of the\nresults."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.knosys.2011.04.004", 
    "link": "http://arxiv.org/pdf/0810.0753v1", 
    "other_authors": "Stefano Soffia", 
    "title": "Definition and Implementation of a Points-To Analysis for C-like   Languages", 
    "arxiv-id": "0810.0753v1", 
    "author": "Stefano Soffia", 
    "publish": "2008-10-04T10:34:09Z", 
    "summary": "The points-to problem is the problem of determining the possible run-time\ntargets of pointer variables and is usually considered part of the more general\naliasing problem, which consists in establishing whether and when different\nexpressions can refer to the same memory address. Aliasing information is\nessential to every tool that needs to reason about the semantics of programs.\nHowever, due to well-known undecidability results, for all interesting\nlanguages that admit aliasing, the exact solution of nontrivial aliasing\nproblems is not generally computable. This work focuses on approximated\nsolutions to this problem by presenting a store-based, flow-sensitive points-to\nanalysis, for applications in the field of automated software verification. In\ncontrast to software testing procedures, which heuristically check the program\nagainst a finite set of executions, the methods considered in this work are\nstatic analyses, where the computed results are valid for all the possible\nexecutions of the analyzed program. We present a simplified programming\nlanguage and its execution model; then an approximated execution model is\ndeveloped using the ideas of abstract interpretation theory. Finally, the\nsoundness of the approximation is formally proved. The aim of developing a\nrealistic points-to analysis is pursued by presenting some extensions to the\ninitial simplified model and discussing the correctness of their formulation.\nThis work contains original contributions to the issue of points-to analysis,\nas it provides a formulation of a filter operation on the points-to abstract\ndomain and a formal proof of the soundness of the defined abstract operations:\nthese, as far as we now, are lacking from the previous literature."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s00224-010-9301-8", 
    "link": "http://arxiv.org/pdf/0810.1106v3", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "On the expressiveness of single-pass instruction sequences", 
    "arxiv-id": "0810.1106v3", 
    "author": "C. A. Middelburg", 
    "publish": "2008-10-07T06:51:53Z", 
    "summary": "We perceive programs as single-pass instruction sequences. A single-pass\ninstruction sequence under execution is considered to produce a behaviour to be\ncontrolled by some execution environment. Threads as considered in basic thread\nalgebra model such behaviours. We show that all regular threads, i.e. threads\nthat can only be in a finite number of states, can be produced by single-pass\ninstruction sequences without jump instructions if use can be made of Boolean\nregisters. We also show that, in the case where goto instructions are used\ninstead of jump instructions, a bound to the number of labels restricts the\nexpressiveness."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s00224-010-9301-8", 
    "link": "http://arxiv.org/pdf/0810.1151v2", 
    "other_authors": "Jan A. Bergstra, Alban Ponse", 
    "title": "Periodic Single-Pass Instruction Sequences", 
    "arxiv-id": "0810.1151v2", 
    "author": "Alban Ponse", 
    "publish": "2008-10-07T13:55:21Z", 
    "summary": "A program is a finite piece of data that produces a (possibly infinite)\nsequence of primitive instructions. From scratch we develop a linear notation\nfor sequential, imperative programs, using a familiar class of primitive\ninstructions and so-called repeat instructions, a particular type of control\ninstructions. The resulting mathematical structure is a semigroup. We relate\nthis set of programs to program algebra (PGA) and show that a particular\nsubsemigroup is a carrier for PGA by providing axioms for single-pass\ncongruence, structural congruence, and thread extraction. This subsemigroup\ncharacterizes periodic single-pass instruction sequences and provides a direct\nbasis for PGA's toolset."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s00224-010-9301-8", 
    "link": "http://arxiv.org/pdf/0810.5575v1", 
    "other_authors": "R. Nuriyev", 
    "title": "Detection of parallel steps in programs with arrays", 
    "arxiv-id": "0810.5575v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-10-30T20:48:49Z", 
    "summary": "The problem of detecting of information and logically independent (DILD)\nsteps in programs is a key for equivalent program transformations. Here we are\nconsidering the problem of independence of loop iterations, the concentration\nof massive data processing and hence the most challenge construction for\nparallelizing. We introduced a separated form of loops when loop's body is a\nsequence of procedures each of them are used array's elements selected in a\nprevious procedure. We prove that any loop may be algorithmically represented\nin this form and number of such procedures is invariant. We show that for this\nform of loop the steps connections are determined with some integer equations\nand hence the independence problem is algorithmically unsolvable if index\nexpressions are more complex than cubical. We suggest a modification of index\nsemantics that made connection equations trivial and loops iterations can be\nexecuted in parallel."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s00224-010-9301-8", 
    "link": "http://arxiv.org/pdf/0812.4973v1", 
    "other_authors": "Neil G. Dickson", 
    "title": "A Simple, Linear-Time Algorithm for x86 Jump Encoding", 
    "arxiv-id": "0812.4973v1", 
    "author": "Neil G. Dickson", 
    "publish": "2008-12-29T21:07:52Z", 
    "summary": "The problem of space-optimal jump encoding in the x86 instruction set, also\nknown as branch displacement optimization, is described, and a linear-time\nalgorithm is given that uses no complicated data structures, no recursion, and\nno randomization. The only assumption is that there are no array declarations\nwhose size depends on the negative of the size of a section of code (Hyde\n2006), which is reasonable for real code."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10817-009-9148-3", 
    "link": "http://arxiv.org/pdf/0901.3619v1", 
    "other_authors": "Sandrine Blazy, Xavier Leroy", 
    "title": "Mechanized semantics for the Clight subset of the C language", 
    "arxiv-id": "0901.3619v1", 
    "author": "Xavier Leroy", 
    "publish": "2009-01-23T08:40:31Z", 
    "summary": "This article presents the formal semantics of a large subset of the C\nlanguage called Clight. Clight includes pointer arithmetic, \"struct\" and\n\"union\" types, C loops and structured \"switch\" statements. Clight is the source\nlanguage of the CompCert verified compiler. The formal semantics of Clight is a\nbig-step operational semantics that observes both terminating and diverging\nexecutions and produces traces of input/output events. The formal semantics of\nClight is mechanized using the Coq proof assistant. In addition to the\nsemantics of Clight, this article describes its integration in the CompCert\nverified compiler and several ways by which the semantics was validated."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10817-009-9148-3", 
    "link": "http://arxiv.org/pdf/0901.3906v1", 
    "other_authors": "Pablo Chico de Guzman, Manuel Carro, Manuel V. Hermenegildo", 
    "title": "A Program Transformation for Continuation Call-Based Tabled Execution", 
    "arxiv-id": "0901.3906v1", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2009-01-25T15:40:48Z", 
    "summary": "The advantages of tabled evaluation regarding program termination and\nreduction of complexity are well known --as are the significant implementation,\nportability, and maintenance efforts that some proposals (especially those\nbased on suspension) require. This implementation effort is reduced by program\ntransformation-based continuation call techniques, at some efficiency cost.\nHowever, the traditional formulation of this proposal by Ramesh and Cheng\nlimits the interleaving of tabled and non-tabled predicates and thus cannot be\nused as-is for arbitrary programs. In this paper we present a complete\ntranslation for the continuation call technique which, using the runtime\nsupport needed for the traditional proposal, solves these problems and makes it\npossible to execute arbitrary tabled programs. We present performance results\nwhich show that CCall offers a useful tradeoff that can be competitive with\nstate-of-the-art implementations."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-009-9042-z", 
    "link": "http://arxiv.org/pdf/0902.1257v1", 
    "other_authors": "Tom Hirschowitz, Xavier Leroy, J. B. Wells", 
    "title": "Compilation of extended recursion in call-by-value functional languages", 
    "arxiv-id": "0902.1257v1", 
    "author": "J. B. Wells", 
    "publish": "2009-02-07T18:00:30Z", 
    "summary": "This paper formalizes and proves correct a compilation scheme for\nmutually-recursive definitions in call-by-value functional languages. This\nscheme supports a wider range of recursive definitions than previous methods.\nWe formalize our technique as a translation scheme to a lambda-calculus\nfeaturing in-place update of memory blocks, and prove the translation to be\ncorrect."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-009-9042-z", 
    "link": "http://arxiv.org/pdf/0905.0737v1", 
    "other_authors": "Ignacio Vega-Paez, Jose Angel Ortega, Georgina G. Pulido", 
    "title": "REC language is a live on IBM1130 simulator, EL lenguaje REC esta vivo   en el simulador de la IBM 1130", 
    "arxiv-id": "0905.0737v1", 
    "author": "Georgina G. Pulido", 
    "publish": "2009-05-06T04:21:15Z", 
    "summary": "REC (Regular Expression Compiler) is a concise programming language\ndevelopment in mayor Mexican Universities at end of 60s which allows students\nto write programs without knowledge of the complicated syntax of languages like\nFORTRAN and ALGOL. The language is recursive and contains only four elements\nfor control. This paper describes use of the interpreter of REC written in\nFORTRAN on IBM1130 Simulator from -Computer History Simulation- Project."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-009-9042-z", 
    "link": "http://arxiv.org/pdf/0905.3432v1", 
    "other_authors": "Francisco Heron de Carvalho-Junior, Rafael Dueire Lins", 
    "title": "A Type System for Parallel Components", 
    "arxiv-id": "0905.3432v1", 
    "author": "Rafael Dueire Lins", 
    "publish": "2009-05-21T03:35:52Z", 
    "summary": "The # component model was proposed to improve the practice of parallel\nprogramming. This paper introduces a type system for # programming systems,\naiming to lift the abstraction and safety of programming for parallel computing\narchitectures by introducing a notion of abstract component based on universal\nand existential bounded quantification. Issues about the implementation of such\ntype system in HPE, a # programming system, are also discussed."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-009-9042-z", 
    "link": "http://arxiv.org/pdf/0906.3083v2", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Instruction sequence notations with probabilistic instructions", 
    "arxiv-id": "0906.3083v2", 
    "author": "C. A. Middelburg", 
    "publish": "2009-06-17T07:03:17Z", 
    "summary": "This paper concerns instruction sequences that contain probabilistic\ninstructions, i.e. instructions that are themselves probabilistic by nature. We\npropose several kinds of probabilistic instructions, provide an informal\noperational meaning for each of them, and discuss related work. On purpose, we\nrefrain from providing an ad hoc formal meaning for the proposed kinds of\ninstructions. We also discuss the approach of projection semantics, which was\nintroduced in earlier work on instruction sequences, in the light of\nprobabilistic instruction sequences."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SERA.2010.23", 
    "link": "http://arxiv.org/pdf/0906.3911v1", 
    "other_authors": "Serguei A. Mokhov, Joey Paquet", 
    "title": "Using the General Intensional Programming System (GIPSY) for Evaluation   of Higher-Order Intensional Logic (HOIL) Expressions", 
    "arxiv-id": "0906.3911v1", 
    "author": "Joey Paquet", 
    "publish": "2009-06-22T03:05:48Z", 
    "summary": "The General Intensional Programming System (GIPSY) has been built around the\nLucid family of intensional programming languages that rely on the higher-order\nintensional logic (HOIL) to provide context-oriented multidimensional reasoning\nof intensional expressions. HOIL combines functional programming with various\nintensional logics to allow explicit context expressions to be evaluated as\nfirst-class values that can be passed as parameters to functions and return as\nresults with an appropriate set of operators defined on contexts. GIPSY's\nframeworks are implemented in Java as a collection of replaceable components\nfor the compilers of various Lucid dialects and the demand-driven eductive\nevaluation engine that can run distributively. GIPSY provides support for\nhybrid programming models that couple intensional and imperative languages for\na variety of needs. Explicit context expressions limit the scope of evaluation\nof math expressions (effectively a Lucid program is a mathematics or physics\nexpression constrained by the context) in tensor physics, regular math in\nmultiple dimensions, etc., and for cyberforensic reasoning as one of the\nuse-cases of interest. Thus, GIPSY is a support testbed for HOIL-based\nlanguages some of which enable such reasoning, as in formal cyberforensic case\nanalysis with event reconstruction. In this paper we discuss the GIPSY\narchitecture, its evaluation engine and example use-cases."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SERA.2010.23", 
    "link": "http://arxiv.org/pdf/0906.4474v2", 
    "other_authors": "Jon Sneyers, Peter Van Weert, Tom Schrijvers, Leslie De Koninck", 
    "title": "As time goes by: Constraint Handling Rules - A survey of CHR research   from 1998 to 2007", 
    "arxiv-id": "0906.4474v2", 
    "author": "Leslie De Koninck", 
    "publish": "2009-06-24T13:54:41Z", 
    "summary": "Constraint Handling Rules (CHR) is a high-level programming language based on\nmulti-headed multiset rewrite rules. Originally designed for writing\nuser-defined constraint solvers, it is now recognized as an elegant general\npurpose language. CHR-related research has surged during the decade following\nthe previous survey by Fruehwirth. Covering more than 180 publications, this\nnew survey provides an overview of recent results in a wide range of research\nareas, from semantics and analysis to systems, extensions and applications."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.entcs.2009.06.005", 
    "link": "http://arxiv.org/pdf/0906.5446v1", 
    "other_authors": "Daniel Hirschkoff, Aur\u00e9lien Pardon, Tom Hirschowitz, Samuel Hym, Damien Pous", 
    "title": "Encapsulation and Dynamic Modularity in the Pi-Calculus", 
    "arxiv-id": "0906.5446v1", 
    "author": "Damien Pous", 
    "publish": "2009-06-30T09:16:03Z", 
    "summary": "We describe a process calculus featuring high level constructs for\ncomponent-oriented programming in a distributed setting. We propose an\nextension of the higher-order pi-calculus intended to capture several important\nmechanisms related to component-based programming, such as dynamic update,\nreconfiguration and code migration. In this paper, we are primarily concerned\nwith the possibility to build a distributed implementation of our calculus.\nAccordingly, we define a low-level calculus, that describes how the high-level\nconstructs are implemented, as well as details of the data structures\nmanipulated at runtime. We also discuss current and future directions of\nresearch in relation to our analysis of component-based programming."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0907.2059v2", 
    "other_authors": "Sylvain Lebresne", 
    "title": "A Type System For Call-By-Name Exceptions", 
    "arxiv-id": "0907.2059v2", 
    "author": "Sylvain Lebresne", 
    "publish": "2009-07-12T18:30:31Z", 
    "summary": "We present an extension of System F with call-by-name exceptions. The type\nsystem is enriched with two syntactic constructs: a union type for programs\nwhose execution may raise an exception at top level, and a corruption type for\nprograms that may raise an exception in any evaluation context (not necessarily\nat top level). We present the syntax and reduction rules of the system, as well\nas its typing and subtyping rules. We then study its properties, such as\nconfluence. Finally, we construct a realizability model using orthogonality\ntechniques, from which we deduce that well-typed programs are weakly\nnormalizing and that the ones who have the type of natural numbers really\ncompute a natural number, without raising exceptions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0907.2173v2", 
    "other_authors": "Oleg Mazonka", 
    "title": "Bit Copying - The Ultimate Computational Simplicity", 
    "arxiv-id": "0907.2173v2", 
    "author": "Oleg Mazonka", 
    "publish": "2009-07-13T14:15:01Z", 
    "summary": "A computational abstract machine based on two operations: referencing and bit\ncopying is presented. These operations are sufficient for carrying out any\ncomputation. They can be used as the primitives for a Turing-complete\nprogramming language. The interesting point is that the computation can be done\nwithout logic operations such as AND or OR. The compiler and emulator of this\nlanguage with sample programs are available on the Internet."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0907.4640v1", 
    "other_authors": "Keiko Nakata, Masahito Hasegawa", 
    "title": "Small-step and big-step semantics for call-by-need", 
    "arxiv-id": "0907.4640v1", 
    "author": "Masahito Hasegawa", 
    "publish": "2009-07-27T17:19:28Z", 
    "summary": "We present natural semantics for acyclic as well as cyclic call-by-need\nlambda calculi, which are proved equivalent to the reduction semantics given by\nAriola and Felleisen. The natural semantics are big-step and use global heaps,\nwhere evaluation is suspended and memorized. The reduction semantics are\nsmall-step and evaluation is suspended and memorized locally in let-bindings.\nThus two styles of formalization describe the call-by-need strategy from\ndifferent angles.\n  The natural semantics for the acyclic calculus is revised from the previous\npresentation by Maraist et al. and its adequacy is ascribed to its\ncorrespondence with the reduction semantics, which has been proved equivalent\nto call-by-name by Ariola and Felleisen. The natural semantics for the cyclic\ncalculus is inspired by that of Launchbury and Sestoft and we state its\nadequacy using a denotational semantics in the style of Launchbury; adequacy of\nthe reduction semantics for the cyclic calculus is in turn ascribed to its\ncorrespondence with the natural semantics."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0907.5290v2", 
    "other_authors": "Alex Shkotin", 
    "title": "Program structure", 
    "arxiv-id": "0907.5290v2", 
    "author": "Alex Shkotin", 
    "publish": "2009-07-30T09:33:53Z", 
    "summary": "A program is usually represented as a word chain. It is exactly a word chain\nthat appears as the lexical analyzer output and is parsed. The work shows that\na program can be syntactically represented as an oriented word tree, that is a\nsyntactic program tree, program words being located both in tree nodes and on\ntree arrows. The basic property of a tree is that arrows starting from each\nnode are marked by different words (including an empty word). Semantics can\nthen be directly specified on such tree using either requirements or additional\nlinks, and adding instructions to some tree nodes enables program execution\nspecification."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0908.3650v1", 
    "other_authors": "Keiko Nakata", 
    "title": "Lazy mixin modules and disciplined effects", 
    "arxiv-id": "0908.3650v1", 
    "author": "Keiko Nakata", 
    "publish": "2009-08-25T17:20:47Z", 
    "summary": "Programming languages are expected to support programmer's effort to\nstructure program code. The ML module system, object systems and mixins are\ngood examples of language constructs promoting modular programming. Among the\nthree, mixins can be thought of as a generalization of the two others in the\nsense that mixins can incorporate features of ML modules and objects with a set\nof primitive operators with clean semantics. Much work has been devoted to\nbuild mixin-based module systems for practical programming languages. In\nrespect of the operational semantics, previous work notably investigated mixin\ncalculi in call-by-name and call-by-value evaluation settings. In this paper we\nexamine a mixin calculus in a call-by-need, or lazy, evaluation setting. We\ndemonstrate how lazy mixins can be interesting in practice with a series of\nexamples, and formalize the operational semantics by adapting Ancona and\nZucca's concise formalization of call-by-name mixins. We then extend the\nsemantics with constraints to control the evaluation order of components of\nmixins in several ways. The main motivation for considering the constraints is\nto produce side effects in a more explicit order than in a purely lazy,\ndemand-driven setting. We explore the design space of possibly interesting\nconstraints and consider two examples in detail."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0910.2654v1", 
    "other_authors": "Wonseok Chae", 
    "title": "Type Safe Extensible Programming", 
    "arxiv-id": "0910.2654v1", 
    "author": "Wonseok Chae", 
    "publish": "2009-10-14T16:08:29Z", 
    "summary": "Software products evolve over time. Sometimes they evolve by adding new\nfeatures, and sometimes by either fixing bugs or replacing outdated\nimplementations with new ones. When software engineers fail to anticipate such\nevolution during development, they will eventually be forced to re-architect or\nre-build from scratch. Therefore, it has been common practice to prepare for\nchanges so that software products are extensible over their lifetimes. However,\nmaking software extensible is challenging because it is difficult to anticipate\nsuccessive changes and to provide adequate abstraction mechanisms over\npotential changes. Such extensibility mechanisms, furthermore, should not\ncompromise any existing functionality during extension. Software engineers\nwould benefit from a tool that provides a way to add extensions in a reliable\nway. It is natural to expect programming languages to serve this role.\nExtensible programming is one effort to address these issues.\n  In this thesis, we present type safe extensible programming using the MLPolyR\nlanguage. MLPolyR is an ML-like functional language whose type system provides\ntype-safe extensibility mechanisms at several levels. After presenting the\nlanguage, we will show how these extensibility mechanisms can be put to good\nuse in the context of product line engineering. Product line engineering is an\nemerging software engineering paradigm that aims to manage variations, which\noriginate from successive changes in software."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0910.3321v1", 
    "other_authors": "Ian Mackie, Jorge Sousa Pinto, Miguel Vilaca", 
    "title": "Iterators, Recursors and Interaction Nets", 
    "arxiv-id": "0910.3321v1", 
    "author": "Miguel Vilaca", 
    "publish": "2009-10-17T18:52:53Z", 
    "summary": "We propose a method for encoding iterators (and recursion operators in\ngeneral) using interaction nets (INs). There are two main applications for\nthis: the method can be used to obtain a visual nota- tion for functional\nprograms; and it can be used to extend the existing translations of the\nlambda-calculus into INs to languages with recursive types."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0910.5833v1", 
    "other_authors": "Jean-Loup Carre, Charles Hymans", 
    "title": "From Single-thread to Multithreaded: An Efficient Static Analysis   Algorithm", 
    "arxiv-id": "0910.5833v1", 
    "author": "Charles Hymans", 
    "publish": "2009-10-30T10:27:23Z", 
    "summary": "A great variety of static analyses that compute safety properties of\nsingle-thread programs have now been developed. This paper presents a\nsystematic method to extend a class of such static analyses, so that they\nhandle programs with multiple POSIX-style threads. Starting from a pragmatic\noperational semantics, we build a denotational semantics that expresses\nreasoning a la assume-guarantee. The final algorithm is then derived by\nabstract interpretation. It analyses each thread in turn, propagating\ninterferences between threads, in addition to other semantic information. The\ncombinatorial explosion, ensued from the explicit consideration of all\ninterleavings, is thus avoided. The worst case complexity is only increased by\na factor n compared to the single-thread case, where n is the number of\ninstructions in the program. We have implemented prototype tools, demonstrating\nthe practicality of the approach."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0911.2899v3", 
    "other_authors": "Michael A. Covington, Roberto Bagnara, Richard A. O'Keefe, Jan Wielemaker, Simon Price", 
    "title": "Coding Guidelines for Prolog", 
    "arxiv-id": "0911.2899v3", 
    "author": "Simon Price", 
    "publish": "2009-11-15T18:21:41Z", 
    "summary": "Coding standards and good practices are fundamental to a disciplined approach\nto software projects, whatever programming languages they employ. Prolog\nprogramming can benefit from such an approach, perhaps more than programming in\nother languages. Despite this, no widely accepted standards and practices seem\nto have emerged up to now. The present paper is a first step towards filling\nthis void: it provides immediate guidelines for code layout, naming\nconventions, documentation, proper use of Prolog features, program development,\ndebugging and testing. Presented with each guideline is its rationale and,\nwhere sensible options exist, illustrations of the relative pros and cons for\neach alternative. A coding standard should always be selected on a per-project\nbasis, based on a host of issues pertinent to any given programming project;\nfor this reason the paper goes beyond the mere provision of normative\nguidelines by discussing key factors and important criteria that should be\ntaken into account when deciding on a fully-fledged coding standard for the\nproject."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0911.5203v1", 
    "other_authors": "Xiaochu Qi", 
    "title": "An Implementation of the Language Lambda Prolog Organized around   Higher-Order Pattern Unification", 
    "arxiv-id": "0911.5203v1", 
    "author": "Xiaochu Qi", 
    "publish": "2009-11-27T06:18:15Z", 
    "summary": "This thesis concerns the implementation of Lambda Prolog, a higher-order\nlogic programming language that supports the lambda-tree syntax approach to\nrepresenting and manipulating formal syntactic objects. Lambda Prolog achieves\nits functionality by extending a Prolog-like language by using typed lambda\nterms as data structures that it then manipulates via higher-order unification\nand some new program-level abstraction mechanisms. These additional features\nraise new implementation questions that must be adequately addressed for Lambda\nProlog to be an effective programming tool. We consider these questions here,\nproviding eventually a virtual machine and compilation based realization. A key\nidea is the orientation of the computation model of Lambda Prolog around a\nrestricted version of higher-order unification with nice algorithmic properties\nand appearing to encompass most interesting applications. Our virtual machine\nembeds a treatment of this form of unification within the structure of the\nWarren Abstract Machine that is used in traditional Prolog implementations.\nAlong the way, we treat various auxiliary issues such as the low-level\nrepresentation of lambda terms, the implementation of reduction on such terms\nand the optimized processing of types in computation. We also develop an actual\nimplementation of Lambda Prolog called Teyjus Version 2. A characteristic of\nthis system is that it realizes an emulator for the virtual machine in the C\nlanguage a compiler in the OCaml language. We present a treatment of the\nsoftware issues that arise from this kind of mixing of languages within one\nsystem and we discuss issues relevant to the portability of our virtual machine\nemulator across arbitrary architectures. Finally, we assess the the efficacy of\nour various design ideas through experiments carried out using the system."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0912.2861v1", 
    "other_authors": "Artur Ventura", 
    "title": "JSC : A JavaScript Object System", 
    "arxiv-id": "0912.2861v1", 
    "author": "Artur Ventura", 
    "publish": "2009-12-15T12:26:02Z", 
    "summary": "The JSC language is a superset of JavaScript designed to ease the development\nof large web applications. This language extends JavaScripts own object system\nby isolating code in a class declaration, simplifying multiple inheritance and\nusing method implementation agreements."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-5(4:1)2009", 
    "link": "http://arxiv.org/pdf/0912.3583v1", 
    "other_authors": "Martin Levesque", 
    "title": "A Metamodel of Unit Testing for Object-Oriented Programming Languages", 
    "arxiv-id": "0912.3583v1", 
    "author": "Martin Levesque", 
    "publish": "2009-12-18T04:35:24Z", 
    "summary": "A unit test is a method for verifying the accuracy and the proper functioning\nof a portion of a program. This work consists to study the relation and the\napproaches to test Object-Oriented Programming (OOP) programs and to propose a\nmetamodel that enables the programmer to write the tests while writing the\nsource code to be tested by exploiting the key features of OOP programming\nlanguages such as inheritance, polymorphism, etc."
},{
    "category": "cs.PL", 
    "doi": "10.1016/S1571-0661(04)80676-5", 
    "link": "http://arxiv.org/pdf/0912.4877v1", 
    "other_authors": "Julien Cohen", 
    "title": "Typing rule-based transformations over topological collections", 
    "arxiv-id": "0912.4877v1", 
    "author": "Julien Cohen", 
    "publish": "2009-12-24T15:27:18Z", 
    "summary": "Pattern-matching programming is an example of a rule-based programming style\ndeveloped in functional languages. This programming style is intensively used\nin dialects of ML but is restricted to algebraic data-types. This restriction\nlimits the field of application. However, as shown by Giavitto and Michel at\nRULE'02, case-based function definitions can be extended to more general data\nstructures called topological collections. We show in this paper that this\nextension retains the benefits of the typed discipline of the functional\nlanguages. More precisely, we show that topological collections and the\nrule-based definition of functions associated with them fit in a polytypic\nextension of mini-ML where type inference is still possible."
},{
    "category": "cs.PL", 
    "doi": "10.1016/S1571-0661(04)80676-5", 
    "link": "http://arxiv.org/pdf/0912.4878v1", 
    "other_authors": "Julien Cohen", 
    "title": "Typage fort et typage souple des collections topologiques et des   transformations", 
    "arxiv-id": "0912.4878v1", 
    "author": "Julien Cohen", 
    "publish": "2009-12-24T15:27:50Z", 
    "summary": "Topological collections allow to consider uniformly many data structures in\nprogramming languages and are handled by functions defined by pattern matching\ncalled transformations. We present two type systems for languages with\ntopological collections and transformations. The first one is a strong type\nsystem \\`a la Hindley/Milner which can be entirely typed at compile time. The\nsecond one is a mixed static and dynamic type system allowing to handle\nheterogeneous collections, that is collections which contain values with\ndifferent types. In the two cases, automatic type inference is possible."
},{
    "category": "cs.PL", 
    "doi": "10.1016/S1571-0661(04)80676-5", 
    "link": "http://arxiv.org/pdf/1001.2188v1", 
    "other_authors": "Pierre Deransart, Rafael Oliveira", 
    "title": "Towards a Generic Framework to Generate Explanatory Traces of Constraint   Solving and Rule-Based Reasoning", 
    "arxiv-id": "1001.2188v1", 
    "author": "Rafael Oliveira", 
    "publish": "2010-01-13T14:54:54Z", 
    "summary": "In this report, we show how to use the Simple Fluent Calculus (SFC) to\nspecify generic tracers, i.e. tracers which produce a generic trace. A generic\ntrace is a trace which can be produced by different implementations of a\nsoftware component and used independently from the traced component. This\napproach is used to define a method for extending a java based CHRor platform\ncalled CHROME (Constraint Handling Rule Online Model-driven Engine) with an\nextensible generic tracer. The method includes a tracer specification in SFC, a\nmethodology to extend it, and the way to integrate it with CHROME, resulting in\nthe platform CHROME-REF (for Reasoning Explanation Facilities), which is a\nconstraint solving and rule based reasoning engine with explanatory traces."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.17.8", 
    "link": "http://arxiv.org/pdf/1002.0942v2", 
    "other_authors": "Vasco T. Vasconcelos, Francisco Martins, Tiago Cogumbreiro", 
    "title": "Type Inference for Deadlock Detection in a Multithreaded Polymorphic   Typed Assembly Language", 
    "arxiv-id": "1002.0942v2", 
    "author": "Tiago Cogumbreiro", 
    "publish": "2010-02-04T09:53:02Z", 
    "summary": "We previously developed a polymorphic type system and a type checker for a\nmultithreaded lock-based polymorphic typed assembly language (MIL) that ensures\nthat well-typed programs do not encounter race conditions. This paper extends\nsuch work by taking into consideration deadlocks. The extended type system\nverifies that locks are acquired in the proper order. Towards this end we\nrequire a language with annotations that specify the locking order. Rather than\nasking the programmer (or the compiler's backend) to specifically annotate each\nnewly introduced lock, we present an algorithm to infer the annotations. The\nresult is a type checker whose input language is non-decorated as before, but\nthat further checks that programs are exempt from deadlocks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.20.9", 
    "link": "http://arxiv.org/pdf/1003.0554v1", 
    "other_authors": "L. Besnard, T. Gautier, J. Ouy, J. -P. Talpin, J. -P. Bodeveix, A. Cortier, M. Pantel, M. Strecker, G. Garcia, A. Rugina, J. Buisson, F. Dagnat", 
    "title": "Polychronous Interpretation of Synoptic, a Domain Specific Modeling   Language for Embedded Flight-Software", 
    "arxiv-id": "1003.0554v1", 
    "author": "F. Dagnat", 
    "publish": "2010-03-02T10:49:04Z", 
    "summary": "The SPaCIFY project, which aims at bringing advances in MDE to the satellite\nflight software industry, advocates a top-down approach built on a\ndomain-specific modeling language named Synoptic. In line with previous\napproaches to real-time modeling such as Statecharts and Simulink, Synoptic\nfeatures hierarchical decomposition of application and control modules in\nsynchronous block diagrams and state machines. Its semantics is described in\nthe polychronous model of computation, which is that of the synchronous\nlanguage Signal."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.20.9", 
    "link": "http://arxiv.org/pdf/1003.1572v1", 
    "other_authors": "Stephan Schroevers", 
    "title": "Expressiveness and Extensions of an Instruction Sequence Semigroup", 
    "arxiv-id": "1003.1572v1", 
    "author": "Stephan Schroevers", 
    "publish": "2010-03-08T08:56:17Z", 
    "summary": "PGA, short for ProGram Algebra, describes sequential programs as finite or\ninfinite (repeating) sequences of instructions. The semigroup C of finite\ninstruction sequences was introduced as an equally expressive alternative to\nPGA. PGA instructions are executed from left to right; most C instructions come\nin a left-to-right as well as a right-to-left flavor. This thesis builds on C\nby introducing an alternative semigroup Cg which employs label and goto\ninstructions instead of relative jump instructions as control structures. Cg\ncan be translated to C and vice versa (and is thus equally expressive). It is\nshown that restricting the instruction sets of C and Cg to contain only\nfinitely many distinct jump, goto or label instructions in either or both\ndirections reduces their expressiveness. Instruction sets with an infinite\nnumber of these instructions in both directions (not necessarily all such\ninstructions) do not suffer a loss of expressiveness."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.20.9", 
    "link": "http://arxiv.org/pdf/1003.2547v1", 
    "other_authors": "Laurent Deniau", 
    "title": "The C Object System: Using C as a High-Level Object-Oriented Language", 
    "arxiv-id": "1003.2547v1", 
    "author": "Laurent Deniau", 
    "publish": "2010-03-12T14:31:23Z", 
    "summary": "The C Object System (Cos) is a small C library which implements high-level\nconcepts available in Clos, Objc and other object-oriented programming\nlanguages: uniform object model (class, meta-class and property-metaclass),\ngeneric functions, multi-methods, delegation, properties, exceptions, contracts\nand closures. Cos relies on the programmable capabilities of the C programming\nlanguage to extend its syntax and to implement the aforementioned concepts as\nfirst-class objects. Cos aims at satisfying several general principles like\nsimplicity, extensibility, reusability, efficiency and portability which are\nrarely met in a single programming language. Its design is tuned to provide\nefficient and portable implementation of message multi-dispatch and message\nmulti-forwarding which are the heart of code extensibility and reusability.\nWith COS features in hand, software should become as flexible and extensible as\nwith scripting languages and as efficient and portable as expected with C\nprogramming. Likewise, Cos concepts should significantly simplify adaptive and\naspect-oriented programming as well as distributed and service-oriented\ncomputing"
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.20.4", 
    "link": "http://arxiv.org/pdf/1003.2871v1", 
    "other_authors": "Julien Forget, Fr\u00e9d\u00e9ric Boniol, David Lesens, Claire Pagetti", 
    "title": "Implementing Multi-Periodic Critical Systems: from Design to Code   Generation", 
    "arxiv-id": "1003.2871v1", 
    "author": "Claire Pagetti", 
    "publish": "2010-03-15T09:59:16Z", 
    "summary": "This article presents a complete scheme for the development of Critical\nEmbedded Systems with Multiple Real-Time Constraints. The system is programmed\nwith a language that extends the synchronous approach with high-level real-time\nprimitives. It enables to assemble in a modular and hierarchical manner several\nlocally mono-periodic synchronous systems into a globally multi-periodic\nsynchronous system. It also allows to specify flow latency constraints. A\nprogram is translated into a set of real-time tasks. The generated code (\\C\\\ncode) can be executed on a simple real-time platform with a dynamic-priority\nscheduler (EDF). The compilation process (each algorithm of the process, not\nthe compiler itself) is formally proved correct, meaning that the generated\ncode respects the real-time semantics of the original program (respect of\nperiods, deadlines, release dates and precedences) as well as its functional\nsemantics (respect of variable consumption)."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:1)2010", 
    "link": "http://arxiv.org/pdf/1003.5197v3", 
    "other_authors": "Ronald Garcia, Andrew Lumsdaine, Amr Sabry", 
    "title": "Lazy Evaluation and Delimited Control", 
    "arxiv-id": "1003.5197v3", 
    "author": "Amr Sabry", 
    "publish": "2010-03-26T18:23:35Z", 
    "summary": "The call-by-need lambda calculus provides an equational framework for\nreasoning syntactically about lazy evaluation. This paper examines its\noperational characteristics. By a series of reasoning steps, we systematically\nunpack the standard-order reduction relation of the calculus and discover a\nnovel abstract machine definition which, like the calculus, goes \"under\nlambdas.\" We prove that machine evaluation is equivalent to standard-order\nevaluation. Unlike traditional abstract machines, delimited control plays a\nsignificant role in the machine's behavior. In particular, the machine replaces\nthe manipulation of a heap using store-based effects with disciplined\nmanagement of the evaluation stack using control-based effects. In short, state\nis replaced with control. To further articulate this observation, we present a\nsimulation of call-by-need in a call-by-value language using delimited control\noperations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:1)2010", 
    "link": "http://arxiv.org/pdf/1004.3241v1", 
    "other_authors": "James Cheney", 
    "title": "Causality and the semantics of provenance", 
    "arxiv-id": "1004.3241v1", 
    "author": "James Cheney", 
    "publish": "2010-04-19T16:14:13Z", 
    "summary": "Provenance, or information about the sources, derivation, custody or history\nof data, has been studied recently in a number of contexts, including\ndatabases, scientific workflows and the Semantic Web. Many provenance\nmechanisms have been developed, motivated by informal notions such as\ninfluence, dependence, explanation and causality. However, there has been\nlittle study of whether these mechanisms formally satisfy appropriate policies\nor even how to formalize relevant motivating concepts such as causality. We\ncontend that mathematical models of these concepts are needed to justify and\ncompare provenance techniques. In this paper we review a theory of causality\nbased on structural models that has been developed in artificial intelligence,\nand describe work in progress on a causal semantics for provenance graphs."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1004.3808v3", 
    "other_authors": "Chris Hawblitzel, Erez Petrank", 
    "title": "Automated Verification of Practical Garbage Collectors", 
    "arxiv-id": "1004.3808v3", 
    "author": "Erez Petrank", 
    "publish": "2010-04-21T21:14:34Z", 
    "summary": "Garbage collectors are notoriously hard to verify, due to their low-level\ninteraction with the underlying system and the general difficulty in reasoning\nabout reachability in graphs. Several papers have presented verified\ncollectors, but either the proofs were hand-written or the collectors were too\nsimplistic to use on practical applications. In this work, we present two\nmechanically verified garbage collectors, both practical enough to use for\nreal-world C# benchmarks. The collectors and their associated allocators\nconsist of x86 assembly language instructions and macro instructions, annotated\nwith preconditions, postconditions, invariants, and assertions. We used the\nBoogie verification generator and the Z3 automated theorem prover to verify\nthis assembly language code mechanically. We provide measurements comparing the\nperformance of the verified collector with that of the standard Bartok\ncollectors on off-the-shelf C# benchmarks, demonstrating their competitiveness."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1006.2816v1", 
    "other_authors": "Santosh Kumar Pani, Priya Arundhati", 
    "title": "An approach to find dynamic slice for C++ Program", 
    "arxiv-id": "1006.2816v1", 
    "author": "Priya Arundhati", 
    "publish": "2010-06-14T19:42:24Z", 
    "summary": "Object-oriented programming has been considered a most promising method in\nprogram development and maintenance. An important feature of object-oriented\nprograms (OOPs) is their reusability which can be achieved through the\ninheritance of classes or reusable components.Dynamic program slicing is an\neffective technique for narrowing the errors to the relevant parts of a program\nwhen debugging. Given a slicing criterion, the dynamic slice contains only\nthose statements that actually affect the variables in the slicing criterion.\nThis paper proposes a method to dynamically slice object-oriented (00) programs\nbased on dependence analysis. It uses the Control Dependency Graph for object\nprogram and other static information to reduce the information to be traced\nduring program execution. In this paper we present a method to find the dynamic\nSlice of object oriented programs where we are finding the slices for object\nand in case of function overloading."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1006.3039v2", 
    "other_authors": "Edmund S. L. Lam, Martin Sulzmann", 
    "title": "Concurrent Goal-Based Execution of Constraint Handling Rules", 
    "arxiv-id": "1006.3039v2", 
    "author": "Martin Sulzmann", 
    "publish": "2010-06-15T17:44:15Z", 
    "summary": "(To appear in Theory and Practice of Logic Programming (TPLP)) We introduce a\nsystematic, concurrent execution scheme for Constraint Handling Rules (CHR)\nbased on a previously proposed sequential goal-based CHR semantics. We\nestablish strong correspondence results to the abstract CHR semantics, thus\nguaranteeing that any answer in the concurrent, goal-based CHR semantics is\nreproducible in the abstract CHR semantics. Our work provides the foundation to\nobtain efficient, parallel CHR execution schemes."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1009.0305v1", 
    "other_authors": "Sleiman Rabah, Jiang Li, Mingzhi Liu, Yuanwei Lai", 
    "title": "Comparative Studies of 10 Programming Languages within 10 Diverse   Criteria -- a Team 7 COMP6411-S10 Term Report", 
    "arxiv-id": "1009.0305v1", 
    "author": "Yuanwei Lai", 
    "publish": "2010-09-01T23:53:15Z", 
    "summary": "There are many programming languages in the world today.Each language has\ntheir advantage and disavantage. In this paper, we will discuss ten programming\nlanguages: C++, C#, Java, Groovy, JavaScript, PHP, Schalar, Scheme, Haskell and\nAspectJ. We summarize and compare these ten languages on ten different\ncriterion. For example, Default more secure programming practices, Web\napplications development, OO-based abstraction and etc. At the end, we will\ngive our conclusion that which languages are suitable and which are not for\nusing in some cases. We will also provide evidence and our analysis on why some\nlanguage are better than other or have advantages over the other on some\ncriterion."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1009.0817v1", 
    "other_authors": "Jan Olaf Blech, Anton Hattendorf, Jia Huang", 
    "title": "Towards a Property Preserving Transformation from IEC 61131-3 to BIP", 
    "arxiv-id": "1009.0817v1", 
    "author": "Jia Huang", 
    "publish": "2010-09-04T09:39:53Z", 
    "summary": "We report on a transformation from Sequential Function Charts of the IEC\n61131-3 standard to BIP. Our presentation features a description of formal\nsyntax and semantics representation of the involved languages and\ntransformation rules. Furthermore, we present a formalism for describing\ninvariants of IEC 61131-3 systems and establish a notion of invariant\npreservation between the two languages. For a subset of our transformation\nrules we sketch a proof showing invariant preservation during the\ntransformation of IEC 61131-3 to BIP and vice versa."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:6)2010", 
    "link": "http://arxiv.org/pdf/1009.1560v1", 
    "other_authors": "Christopher Earl, Matthew Might, David Van Horn", 
    "title": "Stack-Summarizing Control-Flow Analysis of Higher-Order Programs", 
    "arxiv-id": "1009.1560v1", 
    "author": "David Van Horn", 
    "publish": "2010-09-08T15:14:13Z", 
    "summary": "Two sinks drain precision from higher-order flow analyses: (1) merging of\nargument values upon procedure call and (2) merging of return values upon\nprocedure return. To combat the loss of precision, these two sinks have been\naddressed independently. In the case of procedure calls, abstract garbage\ncollection reduces argument merging; while in the case of procedure returns,\ncontext-free approaches eliminate return value merging. It is natural to expect\na combined analysis could enjoy the mutually beneficial interaction between the\ntwo approaches. The central contribution of this work is a direct product of\nabstract garbage collection with context-free analysis. The central challenge\nto overcome is the conflict between the core constraint of a pushdown system\nand the needs of garbage collection: a pushdown system can only see the top of\nthe stack, yet garbage collection needs to see the entire stack during a\ncollection. To make the direct product computable, we develop \"stack\nsummaries,\" a method for tracking stack properties at each control state in a\npushdown analysis of higher-order programs."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.2405v2", 
    "other_authors": "Mart\u00edn Abadi, Gordon D. Plotkin", 
    "title": "A Model of Cooperative Threads", 
    "arxiv-id": "1009.2405v2", 
    "author": "Gordon D. Plotkin", 
    "publish": "2010-09-13T14:28:30Z", 
    "summary": "We develop a model of concurrent imperative programming with threads. We\nfocus on a small imperative language with cooperative threads which execute\nwithout interruption until they terminate or explicitly yield control. We\ndefine and study a trace-based denotational semantics for this language; this\nsemantics is fully abstract but mathematically elementary. We also give an\nequational theory for the computational effects that underlie the language,\nincluding thread spawning. We then analyze threads in terms of the free algebra\nmonad for this theory."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.2900v1", 
    "other_authors": "Hariolf Betz, Thom W. Fr\u00fchwirth", 
    "title": "Linear-Logic Based Analysis of Constraint Handling Rules with   Disjunction", 
    "arxiv-id": "1009.2900v1", 
    "author": "Thom W. Fr\u00fchwirth", 
    "publish": "2010-09-15T11:43:13Z", 
    "summary": "Constraint Handling Rules (CHR) is a declarative committed-choice programming\nlanguage with a strong relationship to linear logic. Its generalization CHR\nwith Disjunction (CHRv) is a multi-paradigm declarative programming language\nthat allows the embedding of horn programs. We analyse the assets and the\nlimitations of the classical declarative semantics of CHR before we motivate\nand develop a linear-logic declarative semantics for CHR and CHRv. We show how\nto apply the linear-logic semantics to decide program properties and to prove\noperational equivalence of CHRv programs across the boundaries of language\nparadigms."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.3174v1", 
    "other_authors": "Stephen Chang, David Van Horn, Matthias Felleisen", 
    "title": "Evaluating Call-By-Need on the Control Stack", 
    "arxiv-id": "1009.3174v1", 
    "author": "Matthias Felleisen", 
    "publish": "2010-09-16T13:38:10Z", 
    "summary": "Ariola and Felleisen's call-by-need {\\lambda}-calculus replaces a variable\noccurrence with its value at the last possible moment. To support this gradual\nnotion of substitution, function applications-once established-are never\ndischarged. In this paper we show how to translate this notion of reduction\ninto an abstract machine that resolves variable references via the control\nstack. In particular, the machine uses the static address of a variable\noccurrence to extract its current value from the dynamic control stack."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.3773v1", 
    "other_authors": "Paulo Moura", 
    "title": "Towards a Study of Meta-Predicate Semantics", 
    "arxiv-id": "1009.3773v1", 
    "author": "Paulo Moura", 
    "publish": "2010-09-20T11:09:27Z", 
    "summary": "We describe and compare design choices for meta-predicate semantics, as found\nin representative Prolog module systems and in Logtalk. We look at the\nconsequences of these design choices from a pragmatic perspective, discussing\nexplicit qualification semantics, computational reflection support,\nexpressiveness of meta-predicate declarations, safety of meta-predicate\ndefinitions, portability of meta-predicate definitions, and meta-predicate\nperformance. Our aim is to provide useful insight for debating meta-predicate\nsemantics and portability issues based on actual implementations and common\nusage patterns."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.3796v1", 
    "other_authors": "Jan Wielemaker, V\u00edtor Santos Costa", 
    "title": "Portability of Prolog programs: theory and case-studies", 
    "arxiv-id": "1009.3796v1", 
    "author": "V\u00edtor Santos Costa", 
    "publish": "2010-09-20T12:38:58Z", 
    "summary": "(Non-)portability of Prolog programs is widely considered as an important\nfactor in the lack of acceptance of the language. Since 1995, the core of the\nlanguage is covered by the ISO standard 13211-1. Since 2007, YAP and SWI-Prolog\nhave established a basic compatibility framework. This article describes and\nevaluates this framework. The aim of the framework is running the same code on\nboth systems rather than migrating an application. We show that today, the\nportability within the family of Edinburgh/Quintus derived Prolog\nimplementations is good enough to allow for maintaining portable real-world\napplications."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.3800v1", 
    "other_authors": "Vasco Pedro, Salvador Abreu", 
    "title": "Distributed Work Stealing for Constraint Solving", 
    "arxiv-id": "1009.3800v1", 
    "author": "Salvador Abreu", 
    "publish": "2010-09-20T12:52:40Z", 
    "summary": "With the dissemination of affordable parallel and distributed hardware,\nparallel and distributed constraint solving has lately been the focus of some\nattention. To effectually apply the power of distributed computational systems,\nthere must be an effective sharing of the work involved in the search for a\nsolution to a Constraint Satisfaction Problem (CSP) between all the\nparticipating agents, and it must happen dynamically, since it is hard to\npredict the effort associated with the exploration of some part of the search\nspace. We describe and provide an initial experimental assessment of an\nimplementation of a work stealing-based approach to distributed CSP solving."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.3806v1", 
    "other_authors": "Paulo Andr\u00e9, Salvador Abreu", 
    "title": "Casting of the WAM as an EAM", 
    "arxiv-id": "1009.3806v1", 
    "author": "Salvador Abreu", 
    "publish": "2010-09-20T13:04:13Z", 
    "summary": "Logic programming provides a very high-level view of programming, which comes\nat the cost of some execution efficiency. Improving performance of logic\nprograms is thus one of the holy grails of Prolog system implementations and a\nwide range of approaches have historically been taken towards this goal.\nDesigning computational models that both exploit the available parallelism in a\ngiven application and that try hard to reduce the explored search space has\nbeen an ongoing line of research for many years. These goals in particular have\nmotivated the design of several computational models, one of which is the\nExtended Andorra Model (EAM). In this paper, we present a preliminary\nspecification and implementation of the EAM with Implicit Control, the WAM2EAM,\nwhich supplies regular WAM instructions with an EAM-centered interpretation."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1009.4020v1", 
    "other_authors": "German Vidal, Neng-Fa Zhou", 
    "title": "Proceedings of CICLOPS-WLPE 2010", 
    "arxiv-id": "1009.4020v1", 
    "author": "Neng-Fa Zhou", 
    "publish": "2010-09-21T08:44:49Z", 
    "summary": "Online proceedings of the Joint Workshop on Implementation of Constraint\nLogic Programming Systems and Logic-based Methods in Programming Environments\n(CICLOPS-WLPE 2010), Edinburgh, Scotland, U.K., July 15, 2010."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1010.1697v1", 
    "other_authors": "Roberto M. Amadio, Nicolas Ayache, Yann R\u00e9gis-Gianas, Ronan Saillard", 
    "title": "Certifying cost annotations in compilers", 
    "arxiv-id": "1010.1697v1", 
    "author": "Ronan Saillard", 
    "publish": "2010-10-08T14:13:09Z", 
    "summary": "We discuss the problem of building a compiler which can lift in a provably\ncorrect way pieces of information on the execution cost of the object code to\ncost annotations on the source code. To this end, we need a clear and flexible\npicture of: (i) the meaning of cost annotations, (ii) the method to prove them\nsound and precise, and (iii) the way such proofs can be composed. We propose a\nso-called labelling approach to these three questions. As a first step, we\nexamine its application to a toy compiler. This formal study suggests that the\nlabelling approach has good compositionality and scalability properties. In\norder to provide further evidence for this claim, we report our successful\nexperience in implementing and testing the labelling approach on top of a\nprototype compiler written in OCAML for (a large fragment of) the C language."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1010.2196v2", 
    "other_authors": "T. Glek, J. Hubicka", 
    "title": "Optimizing real world applications with GCC Link Time Optimization", 
    "arxiv-id": "1010.2196v2", 
    "author": "J. Hubicka", 
    "publish": "2010-10-11T19:30:25Z", 
    "summary": "GCC has a new infrastructure to support a link time optimization (LTO). The\ninfrastructure is designed to allow linking of large applications using a\nspecial mode (WHOPR) which support parallelization of the compilation process.\nIn this paper we present overview of the design and implementation of WHOPR and\npresent test results of its behavior when optimizing large applications. We\ngive numbers on compile time, memory usage and code quality comparisons to the\nclassical file by file based optimization model. In particular we focus on\nFirefox web browser. We show main problems seen only when compiling a large\napplication, such as startup time and code size growth."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:2)2010", 
    "link": "http://arxiv.org/pdf/1010.2850v1", 
    "other_authors": "Jan A. Bergstra", 
    "title": "Steering Fragments of Instruction Sequences", 
    "arxiv-id": "1010.2850v1", 
    "author": "Jan A. Bergstra", 
    "publish": "2010-10-14T08:14:10Z", 
    "summary": "A steering fragment of an instruction sequence consists of a sequence of\nsteering instructions. These are decision points involving the check of a\npropositional statement in sequential logic. The question is addressed why\ncomposed propositional statements occur in steering fragments given the fact\nthat a straightforward transformation allows their elimination. A survey is\nprovided of constraints that may be implicitly assumed when composed\npropositional statements occur in a meaningful instruction sequence."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:8)2010", 
    "link": "http://arxiv.org/pdf/1010.3806v3", 
    "other_authors": "Takeshi Tsukada, Atsushi Igarashi", 
    "title": "A Logical Foundation for Environment Classifiers", 
    "arxiv-id": "1010.3806v3", 
    "author": "Atsushi Igarashi", 
    "publish": "2010-10-19T06:22:16Z", 
    "summary": "Taha and Nielsen have developed a multi-stage calculus {\\lambda}{\\alpha} with\na sound type system using the notion of environment classifiers. They are\nspecial identifiers, with which code fragments and variable declarations are\nannotated, and their scoping mechanism is used to ensure statically that\ncertain code fragments are closed and safely runnable. In this paper, we\ninvestigate the Curry-Howard isomorphism for environment classifiers by\ndeveloping a typed {\\lambda}-calculus {\\lambda}|>. It corresponds to\nmulti-modal logic that allows quantification by transition variables---a\ncounterpart of classifiers---which range over (possibly empty) sequences of\nlabeled transitions between possible worlds. This interpretation will reduce\nthe \"run\" construct---which has a special typing rule in\n{\\lambda}{\\alpha}---and embedding of closed code into other code fragments of\ndifferent stages---which would be only realized by the cross-stage persistence\noperator in {\\lambda}{\\alpha}---to merely a special case of classifier\napplication. {\\lambda}|> enjoys not only basic properties including subject\nreduction, confluence, and strong normalization but also an important property\nas a multi-stage calculus: time-ordered normalization of full reduction. Then,\nwe develop a big-step evaluation semantics for an ML-like language based on\n{\\lambda}|> with its type system and prove that the evaluation of a well-typed\n{\\lambda}|> program is properly staged. We also identify a fragment of the\nlanguage, where erasure evaluation is possible. Finally, we show that the proof\nsystem augmented with a classical axiom is sound and complete with respect to a\nKripke semantics of the logic."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:8)2010", 
    "link": "http://arxiv.org/pdf/1010.4533v1", 
    "other_authors": "Elvira Albert, Puri Arenas, Germ\u00e1n Puebla, Manuel Hermenegildo", 
    "title": "Certificate size reduction in Abstraction-Carrying Code", 
    "arxiv-id": "1010.4533v1", 
    "author": "Manuel Hermenegildo", 
    "publish": "2010-10-13T10:46:13Z", 
    "summary": "Carrying Code (ACC) has recently been proposed as a framework for mobile code\nsafety in which the code supplier provides a program together with an\nabstraction (or abstract model of the program) whose validity entails\ncompliance with a predefined safety policy. The advantage of providing a\n(fixpoint) abstraction to the code consumer is that its validity is checked in\na single pass (i.e., one iteration) of an abstract interpretation-based\nchecker. A main challenge to make ACC useful in practice is to reduce the size\nof certificates as much as possible while at the same time not increasing\nchecking time. The intuitive idea is to only include in the certificate\ninformation that the checker is unable to reproduce without iterating. We\nintroduce the notion of reduced certificate which characterizes the subset of\nthe abstraction which a checker needs in order to validate (and re-construct)\nthe full certificate in a single pass. Based on this notion, we instrument a\ngeneric analysis algorithm with the necessary extensions in order to identify\nthe information relevant to the checker. Interestingly, the fact that the\nreduced certificate omits (parts of) the abstraction has implications in the\ndesign of the checker. We provide the sufficient conditions which allow us to\nensure that 1) if the checker succeeds in validating the certificate, then the\ncertificate is valid for the program (correctness) and 2) the checker will\nsucceed for any reduced certificate which is valid (completeness). Our approach\nhas been implemented and benchmarked within the ciaopp system. The experimental\nresults show that our proposal is able to greatly reduce the size of\ncertificates in practice.To appear in Theory and Practice of Logic Programming\n(TPLP)."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(4:8)2010", 
    "link": "http://arxiv.org/pdf/1010.5023v1", 
    "other_authors": "Matthew Might, David Darais", 
    "title": "Yacc is dead", 
    "arxiv-id": "1010.5023v1", 
    "author": "David Darais", 
    "publish": "2010-10-24T23:12:28Z", 
    "summary": "We present two novel approaches to parsing context-free languages. The first\napproach is based on an extension of Brzozowski's derivative from regular\nexpressions to context-free grammars. The second approach is based on a\ngeneralization of the derivative to parser combinators. The payoff of these\ntechniques is a small (less than 250 lines of code), easy-to-implement parsing\nlibrary capable of parsing arbitrary context-free grammars into lazy parse\nforests. Implementations for both Scala and Haskell are provided. Preliminary\nexperiments with S-Expressions parsed millions of tokens per second, which\nsuggests this technique is efficient enough for use in practice."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.38.7", 
    "link": "http://arxiv.org/pdf/1010.5569v1", 
    "other_authors": "Ivan Lanese", 
    "title": "Static vs Dynamic SAGAs", 
    "arxiv-id": "1010.5569v1", 
    "author": "Ivan Lanese", 
    "publish": "2010-10-27T05:04:27Z", 
    "summary": "SAGAs calculi (or simply SAGAs) have been proposed by Bruni et al. as a model\nfor long-running transactions. The approach therein can be considered static,\nwhile a dynamic approach has been proposed by Lanese and Zavattaro. In this\npaper we first extend both static SAGAs (in the centralized interruption\npolicy) and dynamic SAGAs to deal with nesting, then we compare the two\napproaches."
},{
    "category": "cs.PL", 
    "doi": "10.3233/978-1-60750-100-8-195", 
    "link": "http://arxiv.org/pdf/1010.5582v1", 
    "other_authors": "Xavier Leroy", 
    "title": "Mechanized semantics", 
    "arxiv-id": "1010.5582v1", 
    "author": "Xavier Leroy", 
    "publish": "2010-10-27T06:12:50Z", 
    "summary": "The goal of this lecture is to show how modern theorem provers---in this\ncase, the Coq proof assistant---can be used to mechanize the specification of\nprogramming languages and their semantics, and to reason over individual\nprograms and over generic program transformations, as typically found in\ncompilers. The topics covered include: operational semantics (small-step,\nbig-step, definitional interpreters); a simple form of denotational semantics;\naxiomatic semantics and Hoare logic; generation of verification conditions,\nwith application to program proof; compilation to virtual machine code and its\nproof of correctness; an example of an optimizing program transformation (dead\ncode elimination) and its proof of correctness."
},{
    "category": "cs.PL", 
    "doi": "10.3233/978-1-60750-100-8-195", 
    "link": "http://arxiv.org/pdf/1011.1783v3", 
    "other_authors": "Benedikt Meurer", 
    "title": "OCamlJIT 2.0 - Faster Objective Caml", 
    "arxiv-id": "1011.1783v3", 
    "author": "Benedikt Meurer", 
    "publish": "2010-11-08T12:16:37Z", 
    "summary": "This paper presents the current state of an ongoing research project to\nimprove the performance of the OCaml byte-code interpreter using Just-In-Time\nnative code generation. Our JIT engine OCamlJIT2 currently runs on x86-64\nprocessors, mimicing precisely the behavior of the OCaml virtual machine. Its\ndesign and implementation is described, and performance measures are given."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-012-9084-5", 
    "link": "http://arxiv.org/pdf/1011.4558v3", 
    "other_authors": "Gabriel Kerneis, Juliusz Chroboczek", 
    "title": "Continuation-Passing C: compiling threads to events through   continuations", 
    "arxiv-id": "1011.4558v3", 
    "author": "Juliusz Chroboczek", 
    "publish": "2010-11-20T07:35:14Z", 
    "summary": "In this paper, we introduce Continuation Passing C (CPC), a programming\nlanguage for concurrent systems in which native and cooperative threads are\nunified and presented to the programmer as a single abstraction. The CPC\ncompiler uses a compilation technique, based on the CPS transform, that yields\nefficient code and an extremely lightweight representation for contexts. We\nprovide a proof of the correctness of our compilation scheme. We show in\nparticular that lambda-lifting, a common compilation technique for functional\nlanguages, is also correct in an imperative language like C, under some\nconditions enforced by the CPC compiler. The current CPC compiler is mature\nenough to write substantial programs such as Hekate, a highly concurrent\nBitTorrent seeder. Our benchmark results show that CPC is as efficient, while\nusing significantly less space, as the most efficient thread libraries\navailable."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-012-9084-5", 
    "link": "http://arxiv.org/pdf/1011.5332v1", 
    "other_authors": "Jan Wielemaker, Tom Schrijvers, Markus Triska, Torbj\u00f6rn Lager", 
    "title": "SWI-Prolog", 
    "arxiv-id": "1011.5332v1", 
    "author": "Torbj\u00f6rn Lager", 
    "publish": "2010-11-24T10:28:56Z", 
    "summary": "SWI-Prolog is neither a commercial Prolog system nor a purely academic\nenterprise, but increasingly a community project. The core system has been\nshaped to its current form while being used as a tool for building research\nprototypes, primarily for \\textit{knowledge-intensive} and \\textit{interactive}\nsystems. Community contributions have added several interfaces and the\nconstraint (CLP) libraries. Commercial involvement has created the initial\ngarbage collector, added several interfaces and two development tools: PlDoc (a\nliterate programming documentation system) and PlUnit (a unit testing\nenvironment).\n  In this article we present SWI-Prolog as an integrating tool, supporting a\nwide range of ideas developed in the Prolog community and acting as glue\nbetween \\textit{foreign} resources. This article itself is the glue between\ntechnical articles on SWI-Prolog, providing context and experience in applying\nthem over a longer period."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10990-012-9084-5", 
    "link": "http://arxiv.org/pdf/1011.5640v1", 
    "other_authors": "Mats Carlsson, Per Mildner", 
    "title": "SICStus Prolog -- the first 25 years", 
    "arxiv-id": "1011.5640v1", 
    "author": "Per Mildner", 
    "publish": "2010-11-25T15:11:42Z", 
    "summary": "SICStus Prolog has evolved for nearly 25 years. This is an appropriate point\nin time for revisiting the main language and design decisions, and try to\ndistill some lessons. SICStus Prolog was conceived in a context of multiple,\nconflicting Prolog dialect camps and a fledgling standardization effort. We\nreflect on the impact of this effort and role model implementations on our\ndevelopment. After summarizing the development history, we give a guided tour\nof the system anatomy, exposing some designs that were not published before. We\ngive an overview of our new interactive development environment, and describe a\nsample of key applications. Finally, we try to identify key good and not so\ngood design decisions."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.infsof.2012.08.013", 
    "link": "http://arxiv.org/pdf/1011.6047v1", 
    "other_authors": "Sebastian Nanz, Faraz Torshizi, Michela Pedroni, Bertrand Meyer", 
    "title": "A Comparative Study of the Usability of Two Object-oriented Concurrent   Programming Languages", 
    "arxiv-id": "1011.6047v1", 
    "author": "Bertrand Meyer", 
    "publish": "2010-11-28T14:49:14Z", 
    "summary": "Concurrency has been rapidly gaining importance in general-purpose computing,\ncaused by the recent turn towards multicore processing architectures. As a\nresult, an increasing number of developers have to learn to write concurrent\nprograms, a task that is known to be hard even for the expert. Language\ndesigners are therefore working on languages that promise to make concurrent\nprogramming \"easier\" than using traditional thread libraries. However, the\nclaim that a new language is more usable than another cannot be supported by\npurely theoretical considerations, but calls for empirical studies. In this\npaper, we present the design of a study to compare concurrent programming\nlanguages with respect to comprehending and debugging existing programs and\nwriting correct new programs. A critical challenge for such a study is avoiding\nthe bias that might be introduced during the training phase and when\ninterpreting participants' solutions. We address these issues by the use of\nself-study material and an evaluation scheme that exposes any subjective\ndecisions of the corrector, or eliminates them altogether. We apply our design\nto a comparison of two object-oriented languages for concurrency, multithreaded\nJava and SCOOP (Simple Concurrent Object-Oriented Programming), in an academic\nsetting. We obtain results in favor of SCOOP even though the study participants\nhad previous training in Java Threads."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.45.5", 
    "link": "http://arxiv.org/pdf/1101.4428v1", 
    "other_authors": "Joshua Dunfield", 
    "title": "Untangling Typechecking of Intersections and Unions", 
    "arxiv-id": "1101.4428v1", 
    "author": "Joshua Dunfield", 
    "publish": "2011-01-24T01:39:46Z", 
    "summary": "Intersection and union types denote conjunctions and disjunctions of\nproperties. Using bidirectional typechecking, intersection types are relatively\nstraightforward, but union types present challenges. For union types, we can\ncase-analyze a subterm of union type when it appears in evaluation position\n(replacing the subterm with a variable, and checking that term twice under\nappropriate assumptions). This technique preserves soundness in a call-by-value\nsemantics.\n  Sadly, there are so many choices of subterms that a direct implementation is\nnot practical. But carefully transforming programs into let-normal form\ndrastically reduces the number of choices. The key results are soundness and\ncompleteness: a typing derivation (in the system with too many subterm choices)\nexists for a program if and only if a derivation exists for the let-normalized\nprogram."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.45.7", 
    "link": "http://arxiv.org/pdf/1101.4430v1", 
    "other_authors": "Vilhelm Sj\u00f6berg, Aaron Stump", 
    "title": "Equality, Quasi-Implicit Products, and Large Eliminations", 
    "arxiv-id": "1101.4430v1", 
    "author": "Aaron Stump", 
    "publish": "2011-01-24T01:40:00Z", 
    "summary": "This paper presents a type theory with a form of equality reflection:\nprovable equalities can be used to coerce the type of a term. Coercions and\nother annotations, including implicit arguments, are dropped during reduction\nof terms. We develop the metatheory for an undecidable version of the system\nwith unannotated terms. We then devise a decidable system with annotated terms,\njustified in terms of the unannotated system. Finally, we show how the approach\ncan be extended to account for large eliminations, using what we call\nquasi-implicit products."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.46.2", 
    "link": "http://arxiv.org/pdf/1101.4732v1", 
    "other_authors": "Maria Grazia Buscemi, Hern\u00e1n Melgratti", 
    "title": "Contracts for Abstract Processes in Service Composition", 
    "arxiv-id": "1101.4732v1", 
    "author": "Hern\u00e1n Melgratti", 
    "publish": "2011-01-25T06:57:41Z", 
    "summary": "Contracts are a well-established approach for describing and analyzing\nbehavioral aspects of web service compositions. The theory of contracts comes\nequipped with a notion of compatibility between clients and servers that\nensures that every possible interaction between compatible clients and servers\nwill complete successfully. It is generally agreed that real applications often\nrequire the ability of exposing just partial descriptions of their behaviors,\nwhich are usually known as abstract processes. We propose a formal\ncharacterization of abstraction as an extension of the usual symbolic\nbisimulation and we recover the notion of abstraction in the context of\ncontracts."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.46.2", 
    "link": "http://arxiv.org/pdf/1101.5569v1", 
    "other_authors": "Piotr J. Puczynski", 
    "title": "T2Script Programming Language", 
    "arxiv-id": "1101.5569v1", 
    "author": "Piotr J. Puczynski", 
    "publish": "2011-01-28T16:42:00Z", 
    "summary": "Event-driven programming is used in many fields of modern Computer Science.\nIn event-driven programming languages user interacts with a program by\ntriggering the events. We propose a new approach that we denote command-event\ndriven programming in which the user interacts with a program by means of\nevents and commands. We describe a new programming language, T2Script, which is\nbased on command-event driven paradigm. T2Script has been already implemented\nand used in one of industrial products. We describe the rationale, basic\nconcepts and advanced programming techniques of new T2Script language. We\nevaluate the new language and show what advantages and limitations it has."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1101.6029v2", 
    "other_authors": "Ricardo Lopes, V\u00edtor Santos Costa, Fernando Silva", 
    "title": "A Design and Implementation of the Extended Andorra Model", 
    "arxiv-id": "1101.6029v2", 
    "author": "Fernando Silva", 
    "publish": "2011-01-31T17:15:09Z", 
    "summary": "Logic programming provides a high-level view of programming, giving\nimplementers a vast latitude into what techniques to explore to achieve the\nbest performance for logic programs. Towards obtaining maximum performance, one\nof the holy grails of logic programming has been to design computational models\nthat could be executed efficiently and that would allow both for a reduction of\nthe search space and for exploiting all the available parallelism in the\napplication. These goals have motivated the design of the Extended Andorra\nModel, a model where goals that do not constrain non-deterministic goals can\nexecute first.\n  In this work we present and evaluate the Basic design for Extended Andorra\nModel (BEAM), a system that builds upon David H. D. Warren's original EAM with\nImplicit Control. We provide a complete description and implementation of the\nBEAM System as a set of rewrite and control rules. We present the major data\nstructures and execution algorithms that are required for efficient execution,\nand evaluate system performance.\n  A detailed performance study of our system is included. Our results show that\nthe system achieves acceptable base performance, and that a number of\napplications benefit from the advanced search inherent to the EAM."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.0951v1", 
    "other_authors": "Gabriel Kerneis, Juliusz Chroboczek", 
    "title": "CPC: programming with a massive number of lightweight threads", 
    "arxiv-id": "1102.0951v1", 
    "author": "Juliusz Chroboczek", 
    "publish": "2011-02-04T15:57:48Z", 
    "summary": "Threads are a convenient and modular abstraction for writing concurrent\nprograms, but often fairly expensive. The standard alternative to threads,\nevent-loop programming, allows much lighter units of concurrency, but leads to\ncode that is difficult to write and even harder to understand. Continuation\nPassing C (CPC) is a translator that converts a program written in threaded\nstyle into a program written with events and native system threads, at the\nprogrammer's choice. Together with two undergraduate students, we taught\nourselves how to program in CPC by writing Hekate, a massively concurrent\nnetwork server designed to efficiently handle tens of thousands of\nsimultaneously connected peers. In this paper, we describe a number of\nprogramming idioms that we learnt while writing Hekate; while some of these\nidioms are specific to CPC, many should be applicable to other programming\nsystems with sufficiently cheap threads."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.1178v1", 
    "other_authors": "Paul Tarau", 
    "title": "The BinProlog Experience: Architecture and Implementation Choices for   Continuation Passing Prolog and First-Class Logic Engines", 
    "arxiv-id": "1102.1178v1", 
    "author": "Paul Tarau", 
    "publish": "2011-02-06T17:43:54Z", 
    "summary": "We describe the BinProlog system's compilation technology, runtime system and\nits extensions supporting first-class Logic Engines while providing a short\nhistory of its development, details of some of its newer re-implementations as\nwell as an overview of the most important architectural choices involved in\ntheir design.\n  With focus on its differences with conventional WAM implementations, we\nexplain key details of BinProlog's compilation technique, which replaces the\nWAM with a simplified continuation passing runtime system (the \"BinWAM\"), based\non a mapping of full Prolog to binary logic programs. This is followed by a\ndescription of a term compression technique using a \"tag-on-data\"\nrepresentation.\n  Later derivatives, the Java-based Jinni Prolog compiler and the recently\ndeveloped Lean Prolog system refine the BinProlog architecture with first-class\nLogic Engines, made generic through the use of an Interactor interface. An\noverview of their applications with focus on the ability to express at source\nlevel a wide variety of Prolog built-ins and extensions, covers these newer\ndevelopments."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.2003v2", 
    "other_authors": "Paul Stansifer, Mitchell Wand", 
    "title": "Parsing Reflective Grammars", 
    "arxiv-id": "1102.2003v2", 
    "author": "Mitchell Wand", 
    "publish": "2011-02-09T22:44:26Z", 
    "summary": "Existing technology can parse arbitrary context-free grammars, but only a\nsingle, static grammar per input. In order to support more powerful\nsyntax-extension systems, we propose reflective grammars, which can modify\ntheir own syntax during parsing. We demonstrate and prove the correctness of an\nalgorithm for parsing reflective grammars. The algorithm is based on Earley's\nalgorithm, and we prove that it performs asymptotically no worse than Earley's\nalgorithm on ordinary context-free grammars."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.2262v1", 
    "other_authors": "Siim Karus, Harald Gall", 
    "title": "A Study of Language Usage Evolution in Open Source Software", 
    "arxiv-id": "1102.2262v1", 
    "author": "Harald Gall", 
    "publish": "2011-02-11T00:09:27Z", 
    "summary": "The use of programming languages such as Java and C in Open Source Software\n(OSS) has been well studied. However, many other popular languages such as XSL\nor XML have received minor attention. In this paper, we discuss some trends in\nOSS development that we observed when considering multiple programming language\nevolution of OSS. Based on the revision data of 22 OSS projects, we tracked the\nevolution of language usage and other artefacts such as documentation files,\nbinaries and graphics files. In these systems several different languages and\nartefact types including C/C++, Java, XML, XSL, Makefile, Groovy, HTML, Shell\nscripts, CSS, Graphics files, JavaScript, JSP, Ruby, Phyton, XQuery,\nOpenDocument files, PHP, etc. have been used. We found that the amount of code\nwritten in different languages differs substantially. Some of our findings can\nbe summarized as follows: (1) JavaScript and CSS files most often co-evolve\nwith XSL; (2) Most Java developers but only every second C/C++ developer work\nwith XML; (3) and more generally, we observed a significant increase of usage\nof XML and XSL during recent years and found that Java or C are hardly ever the\nonly language used by a developer. In fact, a developer works with more than 5\ndifferent artefact types (or 4 different languages) in a project on average."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.2330v1", 
    "other_authors": "Alastair Donaldson, Alexander Kaiser, Daniel Kroening, Thomas Wahl", 
    "title": "Symmetry-Aware Predicate Abstraction for Shared-Variable Concurrent   Programs (Extended Technical Report)", 
    "arxiv-id": "1102.2330v1", 
    "author": "Thomas Wahl", 
    "publish": "2011-02-11T11:31:23Z", 
    "summary": "Predicate abstraction is a key enabling technology for applying finite-state\nmodel checkers to programs written in mainstream languages. It has been used\nvery successfully for debugging sequential system-level C code. Although model\nchecking was originally designed for analyzing concurrent systems, there is\nlittle evidence of fruitful applications of predicate abstraction to\nshared-variable concurrent software. The goal of this paper is to close this\ngap. We have developed a symmetry-aware predicate abstraction strategy: it\ntakes into account the replicated structure of C programs that consist of many\nthreads executing the same procedure, and generates a Boolean program template\nwhose multi-threaded execution soundly overapproximates the concurrent C\nprogram. State explosion during model checking parallel instantiations of this\ntemplate can now be absorbed by exploiting symmetry. We have implemented our\nmethod in the SATABS predicate abstraction framework, and demonstrate its\nsuperior performance over alternative approaches on a large range of\nsynchronization programs."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000068", 
    "link": "http://arxiv.org/pdf/1102.2339v1", 
    "other_authors": "Roberto Amadio", 
    "title": "A decompilation of the pi-calculus and its application to termination", 
    "arxiv-id": "1102.2339v1", 
    "author": "Roberto Amadio", 
    "publish": "2011-02-11T12:53:14Z", 
    "summary": "We study the correspondence between a concurrent lambda-calculus in\nadministrative, continuation passing style and a pi-calculus and we derive a\ntermination result for the latter."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.48.9", 
    "link": "http://arxiv.org/pdf/1102.2656v1", 
    "other_authors": "Jan Rochel, Clemens Grabmayer", 
    "title": "Repetitive Reduction Patterns in Lambda Calculus with letrec (Work in   Progress)", 
    "arxiv-id": "1102.2656v1", 
    "author": "Clemens Grabmayer", 
    "publish": "2011-02-14T01:09:58Z", 
    "summary": "For the lambda-calculus with letrec we develop an optimisation, which is\nbased on the contraction of a certain class of 'future' (also: virtual)\nredexes.\n  In the implementation of functional programming languages it is common\npractice to perform beta-reductions at compile time whenever possible in order\nto produce code that requires fewer reductions at run time. This is, however,\nin principle limited to redexes and created redexes that are 'visible' (in the\nsense that they can be contracted without the need for unsharing), and cannot\ngenerally be extended to redexes that are concealed by sharing constructs such\nas letrec. In the case of recursion, concealed redexes become visible only\nafter unwindings during evaluation, and then have to be contracted time and\nagain.\n  We observe that in some cases such redexes exhibit a certain form of\nrepetitive behaviour at run time. We describe an analysis for identifying\nbinders that give rise to such repetitive reduction patterns, and eliminate\nthem by a sort of predictive contraction. Thereby these binders are lifted out\nof recursive positions or eliminated altogether, as a result alleviating the\namount of beta-reductions required for each recursive iteration.\n  Both our analysis and simplification are suitable to be integrated into\nexisting compilers for functional programming languages as an additional\noptimisation phase. With this work we hope to contribute to increasing the\nefficiency of executing programs written in such languages."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1102.3676v6", 
    "other_authors": "Dimitrios Vardoulakis, Olin Shivers", 
    "title": "CFA2: a Context-Free Approach to Control-Flow Analysis", 
    "arxiv-id": "1102.3676v6", 
    "author": "Olin Shivers", 
    "publish": "2011-02-17T19:59:51Z", 
    "summary": "In a functional language, the dominant control-flow mechanism is function\ncall and return. Most higher-order flow analyses, including k-CFA, do not\nhandle call and return well: they remember only a bounded number of pending\ncalls because they approximate programs with control-flow graphs. Call/return\nmismatch introduces precision-degrading spurious control-flow paths and\nincreases the analysis time. We describe CFA2, the first flow analysis with\nprecise call/return matching in the presence of higher-order functions and tail\ncalls. We formulate CFA2 as an abstract interpretation of programs in\ncontinuation-passing style and describe a sound and complete summarization\nalgorithm for our abstract semantics. A preliminary evaluation shows that CFA2\ngives more accurate data-flow information than 0CFA and 1CFA."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1102.3896v1", 
    "other_authors": "V\u00edtor Santos Costa, Lu\u00eds Damas, Ricardo Rocha", 
    "title": "The YAP Prolog System", 
    "arxiv-id": "1102.3896v1", 
    "author": "Ricardo Rocha", 
    "publish": "2011-02-18T19:57:18Z", 
    "summary": "Yet Another Prolog (YAP) is a Prolog system originally developed in the\nmid-eighties and that has been under almost constant development since then.\nThis paper presents the general structure and design of the YAP system,\nfocusing on three important contributions to the Logic Programming community.\nFirst, it describes the main techniques used in YAP to achieve an efficient\nProlog engine. Second, most Logic Programming systems have a rather limited\nindexing algorithm. YAP contributes to this area by providing a dynamic\nindexing mechanism, or just-in-time indexer (JITI). Third, a important\ncontribution of the YAP system has been the integration of both or-parallelism\nand tabling in a single Logic Programming system."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1102.4971v2", 
    "other_authors": "Antoine Madet, Roberto M. Amadio", 
    "title": "Elementary affine $lambda$-calculus with multithreading and side effects", 
    "arxiv-id": "1102.4971v2", 
    "author": "Roberto M. Amadio", 
    "publish": "2011-02-24T12:28:36Z", 
    "summary": "Linear logic provides a framework to control the complexity of higher-order\nfunctional programs. We present an extension of this framework to programs with\nmultithreading and side effects focusing on the case of elementary time. Our\nmain contributions are as follows. First, we provide a new combinatorial proof\nof termination in elementary time for the functional case. Second, we develop\nan extension of the approach to a call-by-value $lambda$-calculus with\nmultithreading and side effects. Third, we introduce an elementary affine type\nsystem that guarantees the standard subject reduction and progress properties.\nFinally, we illustrate the programming of iterative functions with side effects\nin the presented formalism."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1102.5497v1", 
    "other_authors": "M. V. Hermenegildo, F. Bueno, M. Carro, P. L\u00f3pez-Garc\u00eda, E. Mera, J. F. Morales, G. Puebla", 
    "title": "An overview of Ciao and its design philosophy", 
    "arxiv-id": "1102.5497v1", 
    "author": "G. Puebla", 
    "publish": "2011-02-27T12:30:12Z", 
    "summary": "We provide an overall description of the Ciao multiparadigm programming\nsystem emphasizing some of the novel aspects and motivations behind its design\nand implementation. An important aspect of Ciao is that, in addition to\nsupporting logic programming (and, in particular, Prolog), it provides the\nprogrammer with a large number of useful features from different programming\nparadigms and styles, and that the use of each of these features (including\nthose of Prolog) can be turned on and off at will for each program module.\nThus, a given module may be using, e.g., higher order functions and\nconstraints, while another module may be using assignment, predicates, Prolog\nmeta-programming, and concurrency. Furthermore, the language is designed to be\nextensible in a simple and modular way. Another important aspect of Ciao is its\nprogramming environment, which provides a powerful preprocessor (with an\nassociated assertion language) capable of statically finding non-trivial bugs,\nverifying that programs comply with specifications, and performing many types\nof optimizations (including automatic parallelization). Such optimizations\nproduce code that is highly competitive with other dynamic languages or, with\nthe (experimental) optimizing compiler, even that of static languages, all\nwhile retaining the flexibility and interactive development of a dynamic\nlanguage. This compilation architecture supports modularity and separate\ncompilation throughout. The environment also includes a powerful\nauto-documenter and a unit testing framework, both closely integrated with the\nassertion system. The paper provides an informal overview of the language and\nprogram development environment. It aims at illustrating the design philosophy\nrather than at being exhaustive, which would be impossible in a single journal\npaper, pointing instead to previous Ciao literature."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1103.0812v1", 
    "other_authors": "Neng-Fa Zhou", 
    "title": "The Language Features and Architecture of B-Prolog", 
    "arxiv-id": "1103.0812v1", 
    "author": "Neng-Fa Zhou", 
    "publish": "2011-03-04T02:14:29Z", 
    "summary": "B-Prolog is a high-performance implementation of the standard Prolog language\nwith several extensions including matching clauses, action rules for event\nhandling, finite-domain constraint solving, arrays and hash tables, declarative\nloop constructs, and tabling. The B-Prolog system is based on the TOAM\narchitecture which differs from the WAM mainly in that (1) arguments are passed\nold-fashionedly through the stack, (2) only one frame is used for each\npredicate call, and (3) instructions are provided for encoding matching trees.\nThe most recent architecture, called TOAM Jr., departs further from the WAM in\nthat it employs no registers for arguments or temporary variables, and provides\nvariable-size instructions for encoding predicate calls. This paper gives an\noverview of the language features and a detailed description of the TOAM Jr.\narchitecture, including architectural support for action rules and tabling."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1103.1362v4", 
    "other_authors": "Sam Tobin-Hochstadt, David Van Horn", 
    "title": "Higher-Order Symbolic Execution via Contracts", 
    "arxiv-id": "1103.1362v4", 
    "author": "David Van Horn", 
    "publish": "2011-03-07T20:06:23Z", 
    "summary": "We present a new approach to automated reasoning about higher-order programs\nby extending symbolic execution to use behavioral contracts as symbolic values,\nenabling symbolic approximation of higher-order behavior.\n  Our approach is based on the idea of an abstract reduction semantics that\ngives an operational semantics to programs with both concrete and symbolic\ncomponents. Symbolic components are approximated by their contract and our\nsemantics gives an operational interpretation of contracts-as-values. The\nresult is a executable semantics that soundly predicts program behavior,\nincluding contract failures, for all possible instantiations of symbolic\ncomponents. We show that our approach scales to an expressive language of\ncontracts including arbitrary programs embedded as predicates, dependent\nfunction contracts, and recursive contracts. Supporting this feature-rich\nlanguage of specifications leads to powerful symbolic reasoning using existing\nprogram assertions.\n  We then apply our approach to produce a verifier for contract correctness of\ncomponents, including a sound and computable approximation to our semantics\nthat facilitates fully automated contract verification. Our implementation is\ncapable of verifying contracts expressed in existing programs, and of\njustifying valuable contract-elimination optimizations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(2:3)2011", 
    "link": "http://arxiv.org/pdf/1103.2841v2", 
    "other_authors": "Russell O'Connor", 
    "title": "Functor is to Lens as Applicative is to Biplate: Introducing Multiplate", 
    "arxiv-id": "1103.2841v2", 
    "author": "Russell O'Connor", 
    "publish": "2011-03-15T04:03:54Z", 
    "summary": "This paper gives two new categorical characterisations of lenses: one as a\ncoalgebra of the store comonad, and the other as a monoidal natural\ntransformation on a category of a certain class of coalgebras. The store\ncomonad of the first characterisation can be generalized to a Cartesian store\ncomonad, and the coalgebras of this Cartesian store comonad turn out to be\nexactly the Biplates of the Uniplate generic programming library. On the other\nhand, the monoidal natural transformations on functors can be generalized to\nwork on a category of more specific coalgebras. This generalization turns out\nto be the type of compos from the Compos generic programming library. A\ntheorem, originally conjectured by van Laarhoven, proves that these two\ngeneralizations are isomorphic, thus the core data types of the Uniplate and\nCompos libraries supporting generic program on single recursive types are the\nsame. Both the Uniplate and Compos libraries generalize this core functionality\nto support mutually recursive types in different ways. This paper proposes a\nthird extension to support mutually recursive data types that is as powerful as\nCompos and as easy to use as Uniplate. This proposal, called Multiplate, only\nrequires rank 3 polymorphism in addition to the normal type class mechanism of\nHaskell."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1103.4133v2", 
    "other_authors": "Michael Hanus, Sven Koschnicke", 
    "title": "An ER-based Framework for Declarative Web Programming", 
    "arxiv-id": "1103.4133v2", 
    "author": "Sven Koschnicke", 
    "publish": "2011-03-21T20:01:02Z", 
    "summary": "We describe a framework to support the implementation of web-based systems\nintended to manipulate data stored in relational databases. Since the\nconceptual model of a relational database is often specified as an\nentity-relationship (ER) model, we propose to use the ER model to generate a\ncomplete implementation in the declarative programming language Curry. This\nimplementation contains operations to create and manipulate entities of the\ndata model, supports authentication, authorization, session handling, and the\ncomposition of individual operations to user processes. Furthermore, the\nimplementation ensures the consistency of the database w.r.t. the data\ndependencies specified in the ER model, i.e., updates initiated by the user\ncannot lead to an inconsistent state of the database. In order to generate a\nhigh-level declarative implementation that can be easily adapted to individual\ncustomer requirements, the framework exploits previous works on declarative\ndatabase programming and web user interface construction in Curry."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1103.5055v2", 
    "other_authors": "Ravi Chugh, Patrick M. Rondon, Ranjit Jhala", 
    "title": "Nested Refinements for Dynamic Languages", 
    "arxiv-id": "1103.5055v2", 
    "author": "Ranjit Jhala", 
    "publish": "2011-03-25T18:19:55Z", 
    "summary": "Programs written in dynamic languages make heavy use of features --- run-time\ntype tests, value-indexed dictionaries, polymorphism, and higher-order\nfunctions --- that are beyond the reach of type systems that employ either\npurely syntactic or purely semantic reasoning. We present a core calculus,\nSystem D, that merges these two modes of reasoning into a single powerful\nmechanism of nested refinement types wherein the typing relation is itself a\npredicate in the refinement logic. System D coordinates SMT-based logical\nimplication and syntactic subtyping to automatically typecheck sophisticated\ndynamic language programs. By coupling nested refinements with McCarthy's\ntheory of finite maps, System D can precisely reason about the interaction of\nhigher-order functions, polymorphism, and dictionaries. The addition of type\npredicates to the refinement logic creates a circularity that leads to unique\ntechnical challenges in the metatheory, which we solve with a novel\nstratification approach that we use to prove the soundness of System D."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1103.5167v2", 
    "other_authors": "Matthew Might, David Van Horn", 
    "title": "A family of abstract interpretations for static analysis of concurrent   higher-order programs", 
    "arxiv-id": "1103.5167v2", 
    "author": "David Van Horn", 
    "publish": "2011-03-26T21:39:58Z", 
    "summary": "We develop a framework for computing two foundational analyses for concurrent\nhigher-order programs: (control-)flow analysis (CFA) and may-happen-in-parallel\nanalysis (MHP). We pay special attention to the unique challenges posed by the\nunrestricted mixture of first-class continuations and dynamically spawned\nthreads. To set the stage, we formulate a concrete model of concurrent\nhigher-order programs: the P(CEK*)S machine. We find that the systematic\nabstract interpretation of this machine is capable of computing both flow and\nMHP analyses. Yet, a closer examination finds that the precision for MHP is\npoor. As a remedy, we adapt a shape analytic technique-singleton abstraction-to\ndynamically spawned threads (as opposed to objects in the heap). We then show\nthat if MHP analysis is not of interest, we can substantially accelerate the\ncomputation of flow analysis alone by collapsing thread interleavings with a\nsecond layer of abstraction."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1105.0069v2", 
    "other_authors": "Guido Salvaneschi, Carlo Ghezzi, Matteo Pradella", 
    "title": "Context-Oriented Programming: A Programming Paradigm for Autonomic   Systems", 
    "arxiv-id": "1105.0069v2", 
    "author": "Matteo Pradella", 
    "publish": "2011-04-30T10:23:27Z", 
    "summary": "Dynamic software adaptability is one of the central features leveraged by\nautonomic computing. However, developing software that changes its behavior at\nrun time adapting to the operational conditions is a challenging task. Several\napproaches have been proposed in the literature to attack this problem at\ndifferent and complementary abstraction levels: software architecture,\nmiddleware, and programming level. We focus on the support that ad-hoc\nprogramming language constructs may provide to support dynamically adaptive\nbehaviors. We introduce context-oriented programming languages and we present a\nframework that positions the supported paradigm in the MAPE-K autonomic loop.\nWe discuss the advantages of using context-oriented programming languages\ninstead of other mainstream approaches based on dynamic aspect oriented\nprogramming languages and present a case study that shows how the proposed\nprogramming style naturally fits dynamic adaptation requirements. Finally, we\ndiscuss some known problems and outline a number of open research challenges."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1105.0106v1", 
    "other_authors": "Sam Tobin-Hochstadt, David Van Horn", 
    "title": "Semantic Solutions to Program Analysis Problems", 
    "arxiv-id": "1105.0106v1", 
    "author": "David Van Horn", 
    "publish": "2011-04-30T18:26:49Z", 
    "summary": "Problems in program analysis can be solved by developing novel program\nsemantics and deriving abstractions conventionally. For over thirty years,\nhigher-order program analysis has been sold as a hard problem. Its solutions\nhave required ingenuity and complex models of approximation. We claim that this\ndifficulty is due to premature focus on abstraction and propose a new approach\nthat emphasizes semantics. Its simplicity enables new analyses that are beyond\nthe current state of the art."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1105.1743v1", 
    "other_authors": "David Van Horn, Matthew Might", 
    "title": "Abstracting Abstract Machines: A Systematic Approach to Higher-Order   Program Analysis", 
    "arxiv-id": "1105.1743v1", 
    "author": "Matthew Might", 
    "publish": "2011-05-09T17:57:26Z", 
    "summary": "Predictive models are fundamental to engineering reliable software systems.\nHowever, designing conservative, computable approximations for the behavior of\nprograms (static analyses) remains a difficult and error-prone process for\nmodern high-level programming languages. What analysis designers need is a\nprincipled method for navigating the gap between semantics and analytic models:\nanalysis designers need a method that tames the interaction of complex\nlanguages features such as higher-order functions, recursion, exceptions,\ncontinuations, objects and dynamic allocation.\n  We contribute a systematic approach to program analysis that yields novel and\ntransparently sound static analyses. Our approach relies on existing\nderivational techniques to transform high-level language semantics into\nlow-level deterministic state-transition systems (with potentially infinite\nstate spaces). We then perform a series of simple machine refactorings to\nobtain a sound, computable approximation, which takes the form of a\nnon-deterministic state-transition systems with finite state spaces. The\napproach scales up uniformly to enable program analysis of realistic language\nfeatures, including higher-order functions, tail calls, conditionals, side\neffects, exceptions, first-class continuations, and even garbage collection."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1105.1985v3", 
    "other_authors": "Benedikt Meurer", 
    "title": "A Step-indexed Semantic Model of Types for the Call-by-Name Lambda   Calculus", 
    "arxiv-id": "1105.1985v3", 
    "author": "Benedikt Meurer", 
    "publish": "2011-05-10T15:59:09Z", 
    "summary": "Step-indexed semantic models of types were proposed as an alternative to\npurely syntactic safety proofs using subject-reduction. Building upon the work\nby Appel and others, we introduce a generalized step-indexed model for the\ncall-by-name lambda calculus. We also show how to prove type safety of general\nrecursion in our call-by-name model."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000385", 
    "link": "http://arxiv.org/pdf/1105.2554v1", 
    "other_authors": "Sven Auhagen, Lars Bergstrom, Matthew Fluet, John Reppy", 
    "title": "Garbage Collection for Multicore NUMA Machines", 
    "arxiv-id": "1105.2554v1", 
    "author": "John Reppy", 
    "publish": "2011-05-12T19:41:34Z", 
    "summary": "Modern high-end machines feature multiple processor packages, each of which\ncontains multiple independent cores and integrated memory controllers connected\ndirectly to dedicated physical RAM. These packages are connected via a shared\nbus, creating a system with a heterogeneous memory hierarchy. Since this shared\nbus has less bandwidth than the sum of the links to memory, aggregate memory\nbandwidth is higher when parallel threads all access memory local to their\nprocessor package than when they access memory attached to a remote package.\nThis bandwidth limitation has traditionally limited the scalability of modern\nfunctional language implementations, which seldom scale well past 8 cores, even\non small benchmarks.\n  This work presents a garbage collector integrated with our strict, parallel\nfunctional language implementation, Manticore, and shows that it scales\neffectively on both a 48-core AMD Opteron machine and a 32-core Intel Xeon\nmachine."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000263", 
    "link": "http://arxiv.org/pdf/1108.0190v1", 
    "other_authors": "Sergio Antoy", 
    "title": "On the Correctness of Pull-Tabbing", 
    "arxiv-id": "1108.0190v1", 
    "author": "Sergio Antoy", 
    "publish": "2011-07-31T18:01:14Z", 
    "summary": "Pull-tabbing is an evaluation approach for functional logic computations,\nbased on a graph transformation recently proposed, which avoids making\nirrevocable non-deterministic choices that would jeopardize the completeness of\ncomputations. In contrast to other approaches with this property, it does not\nrequire an upfront cloning of a possibly large portion of the choice's context.\nWe formally define the pull-tab transformation, characterize the class of\nprograms for which the transformation is intended, extend the computations in\nthese programs to include the transformation, and prove the correctness of the\nextended computations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000123", 
    "link": "http://arxiv.org/pdf/1108.0329v1", 
    "other_authors": "R\u00e9my Haemmerl\u00e9", 
    "title": "Observational equivalences for linear logic CC languages", 
    "arxiv-id": "1108.0329v1", 
    "author": "R\u00e9my Haemmerl\u00e9", 
    "publish": "2011-08-01T14:56:56Z", 
    "summary": "Linear logic Concurrent Constraint programming (LCC) is an extension of\nconcurrent constraint programming (CC) where the constraint system is based on\nGirard's linear logic instead of the classical logic. In this paper we address\nthe problem of program equivalence for this programming framework. For this\npurpose, we present a structural operational semantics for LCC based on a label\ntransition system and investigate different notions of observational\nequivalences inspired by the state of art of process algebras. Then, we\ndemonstrate that the asynchronous \\pi-calculus can be viewed as simple\nsyntactical restrictions of LCC. Finally we show LCC observational equivalences\ncan be transposed straightforwardly to classical Concurrent Constraint\nlanguages and Constraint Handling Rules, and investigate the resulting\nequivalences."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.0468v1", 
    "other_authors": "Sung-Shik T. Q. Jongmans, Farhad Arbab", 
    "title": "Correlating Formal Semantic Models of Reo Connectors: Connector Coloring   and Constraint Automata", 
    "arxiv-id": "1108.0468v1", 
    "author": "Farhad Arbab", 
    "publish": "2011-08-02T02:27:34Z", 
    "summary": "Over the past decades, coordination languages have emerged for the\nspecification and implementation of interaction protocols for communicating\nsoftware components. This class of languages includes Reo, a platform for\ncompositional construction of connectors. In recent years, various formalisms\nfor describing the behavior of Reo connectors have come to existence, each of\nthem serving its own purpose. Naturally, questions about how these models\nrelate to each other arise. From a theoretical point of view, answers to these\nquestions provide us with better insight into the fundamentals of Reo, while\nfrom a more practical perspective, these answers broaden the applicability of\nReo's development tools. In this paper, we address one of these questions: we\ninvestigate the equivalence between coloring models and constraint automata,\nthe two most dominant and practically relevant semantic models of Reo. More\nspecifically, we define operators that transform one model to the other (and\nvice versa), prove their correctness, and show that they distribute over\ncomposition. To ensure that the transformation operators map one-to-one\n(instead of many-to-one), we extend coloring models with data constraints.\nThough primarily a theoretical contribution, we sketch some potential\napplications of our results: the broadening of the applicability of existing\ntools for connector verification and animation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.2683v1", 
    "other_authors": "Hamid A. Toussi, Ahmed Khademzadeh", 
    "title": "Improving bit-vector representation of points-to sets using class   hierarchy", 
    "arxiv-id": "1108.2683v1", 
    "author": "Ahmed Khademzadeh", 
    "publish": "2011-08-12T18:24:16Z", 
    "summary": "Points-to analysis is the problem of approximating run-time values of\npointers statically or at compile-time. Points-to sets are used to store the\napproximated values of pointers during points-to analysis. Memory usage and\nrunning time limit the ability of points-to analysis to analyze large programs.\n  To our knowledge, works which have implemented a bit-vector representation of\npoints-to sets so far, allocates bits for each pointer without considering\npointer's type. By considering the type, we are able to allocate bits only for\na subset of all abstract objects which are of compatible type with the\npointer's type and as a consequence improve the memory usage and running time.\nTo achieve this goal, we number abstract objects in a way that all the abstract\nobjects of a type and all of its sub-types be consecutive in order.\n  Our most efficient implementation uses about 2.5 times less memory than\nhybrid points-to set (default points-to set in Spark) and also improves the\nanalysis time for sufficiently large programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.3265v1", 
    "other_authors": "Matthew A. Hammer, Georg Neis, Yan Chen, Umut A. Acar", 
    "title": "Self-Adjusting Stack Machines", 
    "arxiv-id": "1108.3265v1", 
    "author": "Umut A. Acar", 
    "publish": "2011-08-16T15:19:42Z", 
    "summary": "Self-adjusting computation offers a language-based approach to writing\nprograms that automatically respond to dynamically changing data. Recent work\nmade significant progress in developing sound semantics and associated\nimplementations of self-adjusting computation for high-level, functional\nlanguages. These techniques, however, do not address issues that arise for\nlow-level languages, i.e., stack-based imperative languages that lack strong\ntype systems and automatic memory management.\n  In this paper, we describe techniques for self-adjusting computation which\nare suitable for low-level languages. Necessarily, we take a different approach\nthan previous work: instead of starting with a high-level language with\nadditional primitives to support self-adjusting computation, we start with a\nlow-level intermediate language, whose semantics is given by a stack-based\nabstract machine. We prove that this semantics is sound: it always updates\ncomputations in a way that is consistent with full reevaluation. We give a\ncompiler and runtime system for the intermediate language used by our abstract\nmachine. We present an empirical evaluation that shows that our approach is\nefficient in practice, and performs favorably compared to prior proposals."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.4547v1", 
    "other_authors": "William Harrison", 
    "title": "Language Support for Declarative Future Commitments", 
    "arxiv-id": "1108.4547v1", 
    "author": "William Harrison", 
    "publish": "2011-08-23T10:44:28Z", 
    "summary": "Sequential programming and work-flow programming are two useful, but\nradically different, ways of describing computational processing. Of the two,\nit is sequential programming that we teach all programmers and support by\nprogramming languages, whether in procedural, objectoriented, or functional\nparadigms. We teach workflow as a secondary style of problem decomposition for\nuse in special situations, like distributed or networked processing. Both\nstyles offer complementary advantages, but the fact that they employ radically\ndifferent models for ownership of continuations interferes with our ability to\nintegrate them in a way that allows them to be taught and used in a single\nprogramming language. This paper describes a programming language construct,\ndeclarative future commitments, that permit better integration of the two."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.4706v1", 
    "other_authors": "Stephen Chang, John Clements, Eli Barzilay, Matthias Felleisen", 
    "title": "Stepping Lazy Programs", 
    "arxiv-id": "1108.4706v1", 
    "author": "Matthias Felleisen", 
    "publish": "2011-08-23T21:37:59Z", 
    "summary": "Debugging lazy functional programs poses serious challenges. In support of\nthe \"stop, examine, and resume\" debugging style of imperative languages, some\ndebugging tools abandon lazy evaluation. Other debuggers preserve laziness but\npresent it in a way that may confuse programmers because the focus of\nevaluation jumps around in a seemingly random manner.\n  In this paper, we introduce a supplemental tool, the algebraic program\nstepper. An algebraic stepper shows computation as a mathematical calculation.\nAlgebraic stepping could be particularly useful for novice programmers or\nprogrammers new to lazy programming. Mathematically speaking, an algebraic\nstepper renders computation as the standard rewriting sequence of a lazy\nlambda-calculus. Our novel lazy semantics introduces lazy evaluation as a form\nof parallel program rewriting. It represents a compromise between Launchbury's\nstore-based semantics and a simple, axiomatic description of lazy computation\nas sharing-via-parameters. Finally, we prove that the stepper's run-time\nmachinery correctly reconstructs the standard rewriting sequence."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.4816v1", 
    "other_authors": "William Harrison, Tim Walsh, Paul Biggar", 
    "title": "Some Measurements of Nullable and Non-Nullable Parameter Declarations in   Relation to Software Malleability", 
    "arxiv-id": "1108.4816v1", 
    "author": "Paul Biggar", 
    "publish": "2011-08-24T11:41:00Z", 
    "summary": "The usual advantages put forward for including nullability declarations in\nthe type systems of programming languages are that they improve program\nreliability or performance. But there is another, entirely different, reason\nfor doing so. In the right context, this information enables the software\nartifacts we produce, the objects and methods, to exhibit much greater\nmalleability. For declaratively typed languages, we can obtain greater software\nmalleability by extending the model of method call so that assurance of a\nmethod's availability can be provided by any non-nullable parameter, not simply\nthe target parameter, and by allowing the method's implementation to reside in\nclasses or objects other than the target..\n  This paper examines the question of whether this hypothetical improvement in\nsoftware malleability is consistent with existing programming practice by\nexamining the question of the extent to which methods in existing software have\nmultiplicities of non-nullable parameters. The circumstance occurs frequently\nenough to provide an important reason to introduce declarations of nullability\ninto programming languages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.5434v1", 
    "other_authors": "Onofrio Febbraro, Nicola Leone, Kristian Reale, Francesco Ricca", 
    "title": "Unit Testing in ASPIDE", 
    "arxiv-id": "1108.5434v1", 
    "author": "Francesco Ricca", 
    "publish": "2011-08-27T09:36:35Z", 
    "summary": "Answer Set Programming (ASP) is a declarative logic programming formalism,\nwhich is employed nowadays in both academic and industrial real-world\napplications. Although some tools for supporting the development of ASP\nprograms have been proposed in the last few years, the crucial task of testing\nASP programs received less attention, and is an Achilles' heel of the available\nprogramming environments.\n  In this paper we present a language for specifying and running unit tests on\nASP programs. The testing language has been implemented in ASPIDE, a\ncomprehensive IDE for ASP, which supports the entire life-cycle of ASP\ndevelopment with a collection of user-friendly graphical tools for program\ncomposition, testing, debugging, profiling, solver execution configuration, and\noutput-handling."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1108.5609v1", 
    "other_authors": "Bernd Bra\u00dfel, Michael Hanus, Bj\u00f6rn Peem\u00f6ller, Fabian Reck", 
    "title": "Implementing Equational Constraints in a Functional Language", 
    "arxiv-id": "1108.5609v1", 
    "author": "Fabian Reck", 
    "publish": "2011-08-29T15:38:31Z", 
    "summary": "KiCS2 is a new system to compile functional logic programs of the source\nlanguage Curry into purely functional Haskell programs. The implementation is\nbased on the idea to represent the search space as a data structure and logic\nvariables as operations that generate their values. This has the advantage that\none can apply various, and in particular, complete search strategies to compute\nsolutions. However, the generation of all values for logic variables might be\ninefficient for applications that exploit constraints on partially known\nvalues. To overcome this drawback, we propose new techniques to implement\nequational constraints in this framework. In particular, we show how\nunification modulo function evaluation and functional patterns can be added\nwithout sacrificing the efficiency of the kernel implementation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1110.2350v2", 
    "other_authors": "Roberto M. Amadio, Yann Regis-Gianas", 
    "title": "Certifying and reasoning about cost annotations of functional programs", 
    "arxiv-id": "1110.2350v2", 
    "author": "Yann Regis-Gianas", 
    "publish": "2011-10-11T12:35:00Z", 
    "summary": "We present a so-called labelling method to insert cost annotations in a\nhigher-order functional program, to certify their correctness with respect to a\nstandard compilation chain to assembly code including safe memory management,\nand to reason on them in a higher-order Hoare logic."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.59.8", 
    "link": "http://arxiv.org/pdf/1110.3470v1", 
    "other_authors": "David Lievens, Bill Harrison", 
    "title": "Symmetric Encapsulated Multi-Methods", 
    "arxiv-id": "1110.3470v1", 
    "author": "Bill Harrison", 
    "publish": "2011-10-16T10:52:53Z", 
    "summary": "In object systems, classes take the role of modules, and interfaces consist\nof methods. Because methods are encapsulated in objects, interfaces in object\nsystems do not allow abstracting over \\emph{where} methods are implemented.\nThis implies that any change to the implementation structure may cause a\nrippling effect. Sometimes this unduly restricts the scope of software\nevolution, in particular for methods with multiple parameters where there is no\nclear owner. We propose a simple scheme where symmetric methods may be defined\nin the classes of any of their parameters. This allows client code to be\noblivious of what class contains a method implementation, and therefore immune\nagainst it changing. When combined with multiple dynamic dispatch, this scheme\nallows for modular extensibility where a method defined in one class is\noverridden by a method defined in a class that is not its subtype. In this\npaper, we illustrate the scheme by extending a core calculus of class-based\nlanguages with these symmetric encapsulated multi-methods, and prove the result\nsound."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.2", 
    "link": "http://arxiv.org/pdf/1110.4157v1", 
    "other_authors": "Joana Campos, Vasco T. Vasconcelos", 
    "title": "Channels as Objects in Concurrent Object-Oriented Programming", 
    "arxiv-id": "1110.4157v1", 
    "author": "Vasco T. Vasconcelos", 
    "publish": "2011-10-19T02:18:07Z", 
    "summary": "There is often a sort of a protocol associated to each class, stating when\nand how certain methods should be called. Given that this protocol is, if at\nall, described in the documentation accompanying the class, current mainstream\nobject-oriented languages cannot provide for the verification of client code\nadherence against the sought class behaviour. We have defined a class-based\nconcurrent object-oriented language that formalises such protocols in the form\nof usage types. Usage types are attached to class definitions, allowing for the\nspecification of (1) the available methods, (2) the tests clients must perform\non the result of methods, and (3) the object status - linear or shared - all of\nwhich depend on the object's state. Our work extends the recent approach on\nmodular session types by eliminating channel operations, and defining the\nmethod call as the single communication primitive in both sequential and\nconcurrent settings. In contrast to previous works, we define a single category\nfor objects, instead of distinct categories for linear and for shared objects,\nand let linear objects evolve into shared ones. We introduce a standard sync\nqualifier to prevent thread interference in certain operations on shared\nobjects. We formalise the language syntax, the operational semantics, and a\ntype system that enforces by static typing that methods are called only when\navailable, and by a single client if so specified in the usage type. We\nillustrate the language via a complete example."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.6", 
    "link": "http://arxiv.org/pdf/1110.4163v1", 
    "other_authors": "Keigo Imai, Shoji Yuen, Kiyoshi Agusa", 
    "title": "Session Type Inference in Haskell", 
    "arxiv-id": "1110.4163v1", 
    "author": "Kiyoshi Agusa", 
    "publish": "2011-10-19T02:38:21Z", 
    "summary": "We present an inference system for a version of the Pi-calculus in Haskell\nfor the session type proposed by Honda et al. The session type is very useful\nin checking if the communications are well-behaved. The full session type\nimplementation in Haskell was first presented by Pucella and Tov, which is\n'semi-automatic' in that the manual operations for the type representation was\nnecessary. We give an automatic type inference for the session type by using a\nmore abstract representation for the session type based on the 'de Bruijn\nlevels'. We show an example of the session type inference for a simple SMTP\nclient."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1110.4165v1", 
    "other_authors": "Francisco Martins, Vasco T. Vasconcelos, Tiago Cogumbreiro", 
    "title": "Types for X10 Clocks", 
    "arxiv-id": "1110.4165v1", 
    "author": "Tiago Cogumbreiro", 
    "publish": "2011-10-19T02:38:38Z", 
    "summary": "X10 is a modern language built from the ground up to handle future parallel\nsystems, from multicore machines to cluster configurations. We take a closer\nlook at a pair of synchronisation mechanisms: finish and clocks. The former\nwaits for the termination of parallel computations, the latter allow multiple\nconcurrent activities to wait for each other at certain points in time. In\norder to better understand these concepts we study a type system for a stripped\ndown version of X10. The main result assures that well typed programs do not\nrun into the errors identified in the X10 language reference, namely the\nClockUseException. The study will open, we hope, doors to a more flexible\nutilisation of clocks in the X10 language."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1110.4802v1", 
    "other_authors": "O. Cugnon de Sevricourt, V. Tariel", 
    "title": "Cameleon language Part 1: Processor", 
    "arxiv-id": "1110.4802v1", 
    "author": "V. Tariel", 
    "publish": "2011-09-27T15:28:38Z", 
    "summary": "Emergence is the way complex systems arise out of a multiplicity of\nrelatively simple interactions between primitives. Since programming problems\nbecome more and more complexes and transverses, our vision is that application\ndevelopment should be process at two scales: micro- and macro-programming where\nat the micro-level the paradigm is step-by-step and at macro-level the paradigm\nis emergence. For micro-programming, which focuses on how things happen,\npopular languages, Java, C++, Python, are imperative writing languages where\nthe code is a sequence of sentences executed by the computer. For\nmacro-programming, which focuses on how things connect, popular languages,\nlabVIEW, Blender, Simulink, are graphical data flow languages such that the\nprogram is a composition of operators, a unit-process consuming input data and\nproducing output data, and connectors, a data-flow between an output data and\nan input data of two operators. However, despite their fruitful applications,\nthese macro-languages are not transversal since different data-structures of\nnative data-structures cannot be integrated in their framework easily. Cameleon\nlanguage is a graphical data flow language following a two-scale paradigm. It\nallows an easy up-scale that is the integration of any library writing in C++\nin the data flow language. Cameleon language aims to democratize\nmacro-programming by an intuitive interaction between the human and the\ncomputer where building an application based on a data-process and a GUI is a\nsimple task to learn and to do. Cameleon language allows conditional execution\nand repetition to solve complex macro-problems. In this paper we introduce a\nnew model based on the extension of the petri net model for the description of\nhow the Cameleon language executes a composition."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.1751v1", 
    "other_authors": "Michael E. Cotterell, John A. Miller, Tom Horton", 
    "title": "Unicode in Domain-Specific Programming Languages for Modeling &   Simulation: ScalaTion as a Case Study", 
    "arxiv-id": "1112.1751v1", 
    "author": "Tom Horton", 
    "publish": "2011-12-08T02:17:18Z", 
    "summary": "As recent programming languages provide improved conciseness and flexibility\nof syntax, the development of embedded or internal Domain-Specific Languages\nhas increased. The field of Modeling and Simulation has had a long history of\ninnovation in programming languages (e.g. Simula-67, GPSS). Much effort has\ngone into the development of Simulation Programming Languages.\n  The ScalaTion project is working to develop an embedded or internal\nDomain-Specific Language for Modeling and Simulation which could streamline\nlanguage innovation in this domain. One of its goals is to make the code\nconcise, readable, and in a form familiar to experts in the domain. In some\ncases the code looks very similar to textbook formulas. To enhance readability\nby domain experts, a version of ScalaTion is provided that heavily utilizes\nUnicode.\n  This paper discusses the development of the ScalaTion DSL and the underlying\nfeatures of Scala that make this possible. It then provides an overview of\nScalaTion highlighting some uses of Unicode. Statistical analysis capabilities\nneeded for Modeling and Simulation are presented in some detail. The notation\ndeveloped is clear and concise which should lead to improved usability and\nextendibility."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3779v1", 
    "other_authors": "Flavio Cruz, Ricardo Rocha", 
    "title": "Single Time-Stamped Tries for Retroactive Call Subsumption", 
    "arxiv-id": "1112.3779v1", 
    "author": "Ricardo Rocha", 
    "publish": "2011-12-16T12:24:02Z", 
    "summary": "Tabling is an evaluation strategy for Prolog programs that works by storing\nanswers in a table space and then by using them in similar subgoals. Some\ntabling engines use call by subsumption, where it is determined that a subgoal\nwill consume answers from a more general subgoal in order to reduce the search\nspace and increase efficiency. We designed an extension, named Retroactive Call\nSubsumption (RCS), that implements call by subsumption independently of the\ncall order, thus allowing a more general subgoal to force previous called\nsubgoals to become answer consumers. For this extension, we propose a new table\nspace design, the Single Time Stamped Trie (STST), that is organized to make\nanswer sharing across subsumed/subsuming subgoals simple and efficient. In this\npaper, we present the new STST table space design and we discuss the main\nmodifications made to the original Time Stamped Tries approach to\nnon-retroactive call by subsumption. In experimental results, with programs\nthat stress some deficiencies of the new STST design, some overheads may be\nobserved, however the results achieved with more realistic programs greatly\noffset these overheads."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3780v1", 
    "other_authors": "Jo\u00e3o Raimundo, Ricardo Rocha", 
    "title": "Global Trie for Subterms", 
    "arxiv-id": "1112.3780v1", 
    "author": "Ricardo Rocha", 
    "publish": "2011-12-16T12:24:12Z", 
    "summary": "A critical component in the implementation of an efficient tabling system is\nthe design of the table space. The most popular and successful data structure\nfor representing tables is based on a two-level trie data structure, where one\ntrie level stores the tabled subgoal calls and the other stores the computed\nanswers. The Global Trie (GT) is an alternative table space organization\ndesigned with the intent to reduce the tables's memory usage, namely by storing\nterms in a global trie, thus preventing repeated representations of the same\nterm in different trie data structures. In this paper, we propose an extension\nto the GT organization, named Global Trie for Subterms (GT-ST), where compound\nsubterms in term arguments are represented as unique entries in the GT.\nExperiments results using the YapTab tabling system show that GT-ST support has\npotential to achieve significant reductions on memory usage, for programs with\nincreasing compound subterms in term arguments, without compromising the\nexecution time for other programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3782v1", 
    "other_authors": "Paul Tarau", 
    "title": "Computing with Hereditarily Finite Sequences", 
    "arxiv-id": "1112.3782v1", 
    "author": "Paul Tarau", 
    "publish": "2011-12-16T12:24:21Z", 
    "summary": "e use Prolog as a flexible meta-language to provide executable specifications\nof some fundamental mathematical objects and their transformations. In the\nprocess, isomorphisms are unraveled between natural numbers and combinatorial\nobjects (rooted ordered trees representing hereditarily finite sequences and\nrooted ordered binary trees representing G\\\"odel's System {\\bf T} types).\n  This paper focuses on an application that can be seen as an unexpected\n\"paradigm shift\": we provide recursive definitions showing that the resulting\nrepresentations are directly usable to perform symbolically arbitrary-length\ninteger computations.\n  Besides the theoretically interesting fact of \"breaking the\narithmetic/symbolic barrier\", the arithmetic operations performed with symbolic\nobjects like trees or types turn out to be genuinely efficient -- we derive\nimplementations with asymptotic performance comparable to ordinary bitstring\nimplementations of arbitrary-length integer arithmetic.\n  The source code of the paper, organized as a literate Prolog program, is\navailable at \\url{http://logic.cse.unt.edu/tarau/research/2011/pPAR.pl}"
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3783v1", 
    "other_authors": "Paulo Moura, Artur Miguel Dias", 
    "title": "L-FLAT: Logtalk Toolkit for Formal Languages and Automata Theory", 
    "arxiv-id": "1112.3783v1", 
    "author": "Artur Miguel Dias", 
    "publish": "2011-12-16T12:24:31Z", 
    "summary": "We describe L-FLAT, a Logtalk Toolkit for teaching Formal Languages and\nAutomata Theory. L-FLAT supports the definition of \\textsl{alphabets}, the\ndefinition of \\textsl{orders} over alphabet symbols, the partial definition of\n\\textsl{languages} using unit tests, and the definition of \\textsl{mechanisms},\nwhich implement language generators or language recognizers. Supported\nmechanisms include \\textsl{predicates}, \\textsl{regular expressions},\n\\textsl{finite automata}, \\textsl{context-free grammars}, \\textsl{Turing\nmachines}, and \\textsl{push-down automata}. L-FLAT entities are implemented\nusing the object-oriented features of Logtalk, providing a highly portable and\neasily extendable framework. The use of L-FLAT in educational environments is\nenhanced by supporting Mooshak, a web application that features automatic\ngrading of submitted programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3784v1", 
    "other_authors": "J\u00e1nos Csorba, Zsolt Zombori, P\u00e9ter Szeredi", 
    "title": "Using Constraint Handling Rules to Provide Static Type Analysis for the   Q Functional Language", 
    "arxiv-id": "1112.3784v1", 
    "author": "P\u00e9ter Szeredi", 
    "publish": "2011-12-16T12:25:12Z", 
    "summary": "We describe an application of Prolog: a type checking tool for the Q\nfunctional language. Q is a terse vector processing language, a descendant of\nAPL, which is getting more and more popular, especially in financial\napplications. Q is a dynamically typed language, much like Prolog. Extending Q\nwith static typing improves both the readability of programs and programmer\nproductivity, as type errors are discovered by the tool at compile time, rather\nthan through debugging the program execution.\n  The type checker uses constraints that are handled by Prolog Constraint\nHandling Rules. During the analysis, we determine the possible type values for\neach program expression and detect inconsistencies. As most built-in function\nnames of Q are overloaded, i.e. their meaning depends on the argument types, a\nquite complex system of constraints had to be implemented."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3785v1", 
    "other_authors": "Theofrastos Mantadelis, Gerda Janssens", 
    "title": "Nesting Probabilistic Inference", 
    "arxiv-id": "1112.3785v1", 
    "author": "Gerda Janssens", 
    "publish": "2011-12-16T12:25:28Z", 
    "summary": "When doing inference in ProbLog, a probabilistic extension of Prolog, we\nextend SLD resolution with some additional bookkeeping. This additional\ninformation is used to compute the probabilistic results for a probabilistic\nquery. In Prolog's SLD, goals are nested very naturally. In ProbLog's SLD,\nnesting probabilistic queries interferes with the probabilistic bookkeeping. In\norder to support nested probabilistic inference we propose the notion of a\nparametrised ProbLog engine. Nesting becomes possible by suspending and\nresuming instances of ProbLog engines. With our approach we realise several\nextensions of ProbLog such as meta-calls, negation, and answers of\nprobabilistic goals."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3786v2", 
    "other_authors": "Timon Van Overveldt, Bart Demoen", 
    "title": "High-Level Multi-Threading in hProlog", 
    "arxiv-id": "1112.3786v2", 
    "author": "Bart Demoen", 
    "publish": "2011-12-16T12:25:42Z", 
    "summary": "A new high-level interface to multi-threading in Prolog, implemented in\nhProlog, is described. Modern CPUs often contain multiple cores and through\nhigh-level multi-threading a programmer can leverage this power without having\nto worry about low-level details. Two common types of high-level explicit\nparallelism are discussed: independent and-parallelism and competitive\nor-parallelism. A new type of explicit parallelism, pipeline parallelism, is\nproposed. This new type can be used in certain cases where independent\nand-parallelism and competitive or-parallelism cannot be used."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3787v1", 
    "other_authors": "Dario Campagna, Beata Sarna-Starosta, Tom Schrijvers", 
    "title": "Approximating Constraint Propagation in Datalog", 
    "arxiv-id": "1112.3787v1", 
    "author": "Tom Schrijvers", 
    "publish": "2011-12-16T12:26:59Z", 
    "summary": "We present a technique exploiting Datalog with aggregates to improve the\nperformance of programs with arithmetic (in)equalities. Our approach employs a\nsource-to-source program transformation which approximates the propagation\ntechnique from Constraint Programming. The experimental evaluation of the\napproach shows good run time speed-ups on a range of non-recursive as well as\nrecursive programs. Furthermore, our technique improves upon the previously\nreported in the literature constraint magic set transformation approach."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3788v1", 
    "other_authors": "Paul Tarau", 
    "title": "Bijective Term Encodings", 
    "arxiv-id": "1112.3788v1", 
    "author": "Paul Tarau", 
    "publish": "2011-12-16T12:27:09Z", 
    "summary": "We encode/decode Prolog terms as unique natural numbers. Our encodings have\nthe following properties: a) are bijective b) natural numbers always decode to\nsyntactically valid terms c) they work in low polynomial time in the bitsize of\nthe representations d) the bitsize of our encodings is within constant factor\nof the syntactic representation of the input.\n  We describe encodings of term algebras with finite signature as well as\nalgorithms that separate the \"structure\" of a term, a natural number encoding\nof a list of balanced parenthesis, from its \"content\", a list of atomic terms\nand Prolog variables. The paper is organized as a literate Prolog program\navailable from \\url{http://logic.cse.unt.edu/tarau/research/2011/bijenc.pl}."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3789v1", 
    "other_authors": "Abdulla Alqaddoumi, Enrico Pontelli", 
    "title": "An Implementation of Bubbling", 
    "arxiv-id": "1112.3789v1", 
    "author": "Enrico Pontelli", 
    "publish": "2011-12-16T12:27:19Z", 
    "summary": "Non-determinism is of great importance in functional logic programming. It\nprovides expressiveness and efficiency to functional logic computations. In\nthis paper we describe an implementation of the multi-paradigm functional logic\nlanguage Curry. The evaluation strategy employed by the implementation is based\non definitional trees and needed narrowing for deterministic operations, while\nnon-deterministic operations will depend on the graph transformation, bubbling.\nBubbling preserves the completeness of non-deterministic operations and avoids\nunnecessary large-scale reconstruction of expressions done by other approaches."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.3833v1", 
    "other_authors": "Alasdair Armstrong, Simon Foster, Georg Struth", 
    "title": "Dependently Typed Programming based on Automated Theorem Proving", 
    "arxiv-id": "1112.3833v1", 
    "author": "Georg Struth", 
    "publish": "2011-12-16T14:48:42Z", 
    "summary": "Mella is a minimalistic dependently typed programming language and\ninteractive theorem prover implemented in Haskell. Its main purpose is to\ninvestigate the effective integration of automated theorem provers in a pure\nand simple setting. Such integrations are essential for supporting program\ndevelopment in dependently typed languages. We integrate the equational theorem\nprover Waldmeister and test it on more than 800 proof goals from the TPTP\nlibrary. In contrast to previous approaches, the reconstruction of Waldmeister\nproofs within Mella is quite robust and does not generate a significant\noverhead to proof search. Mella thus yields a template for integrating more\nexpressive theorem provers in more sophisticated languages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.4106v3", 
    "other_authors": "Ravi Chugh, David Herman, Ranjit Jhala", 
    "title": "Dependent Types for JavaScript", 
    "arxiv-id": "1112.4106v3", 
    "author": "Ranjit Jhala", 
    "publish": "2011-12-18T01:42:11Z", 
    "summary": "We present Dependent JavaScript (DJS), a statically-typed dialect of the\nimperative, object-oriented, dynamic language. DJS supports the particularly\nchallenging features such as run-time type-tests, higher-order functions,\nextensible objects, prototype inheritance, and arrays through a combination of\nnested refinement types, strong updates to the heap, and heap unrolling to\nprecisely track prototype hierarchies. With our implementation of DJS, we\ndemonstrate that the type system is expressive enough to reason about a variety\nof tricky idioms found in small examples drawn from several sources, including\nthe popular book JavaScript: The Good Parts and the SunSpider benchmark suite."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.4993v1", 
    "other_authors": "Salvador Abreu, Vitor Santos Costa", 
    "title": "Online Proceedings of the 11th International Colloquium on   Implementation of Constraint LOgic Programming Systems (CICLOPS 2011),   Lexington, KY, U.S.A., July 10, 2011", 
    "arxiv-id": "1112.4993v1", 
    "author": "Vitor Santos Costa", 
    "publish": "2011-12-21T11:31:42Z", 
    "summary": "These are the revised versions of the papers presented at CICLOPS 2011, a\nworkshop colocated with ICLP 2011."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.5000v1", 
    "other_authors": "Uday P. Khedker, Alan Mycroft, Prashant Singh Rawat", 
    "title": "Lazy Pointer Analysis", 
    "arxiv-id": "1112.5000v1", 
    "author": "Prashant Singh Rawat", 
    "publish": "2011-12-21T12:01:37Z", 
    "summary": "Flow- and context-sensitive pointer analysis is generally considered too\nexpensive for large programs; most tools relax one or both of the requirements\nfor scalability. We formulate a flow- and context-sensitive points-to analysis\nthat is lazy in the following sense: points-to information is computed only for\nlive pointers and its propagation is sparse (restricted to live ranges of\nrespective pointers). Further, our analysis (a) uses strong liveness,\neffectively including dead code elimination; (b) afterwards calculates\nmust-points-to information from may-points-to information instead of using a\nmutual fixed-point; and (c) uses value-based termination of call strings during\ninterprocedural analysis (which reduces the number of call strings\nsignificantly).\n  A naive implementation of our analysis within GCC-4.6.0 gave analysis time\nand size of points-to measurements for SPEC2006. Using liveness reduced the\namount of points-to information by an order of magnitude with no loss of\nprecision. For all programs under 30kLoC we found that the results were much\nmore precise than gcc's analysis. What comes as a pleasant surprise however, is\nthe fact that below this cross-over point, our naive linked-list implementation\nis faster than a flow- and context-insensitive analysis which is primarily used\nfor efficiency. We speculate that lazy flow- and context-sensitive analyses may\nbe not only more precise, but also more efficient, than current approaches."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1112.6096v1", 
    "other_authors": "Massimo Morara, Jacopo Mauro, Maurizio Gabbrielli", 
    "title": "Solving XCSP problems by using Gecode", 
    "arxiv-id": "1112.6096v1", 
    "author": "Maurizio Gabbrielli", 
    "publish": "2011-12-28T10:49:18Z", 
    "summary": "Gecode is one of the most efficient libraries that can be used for constraint\nsolving. However, using it requires dealing with C++ programming details. On\nthe other hand several formats for representing constraint networks have been\nproposed. Among them, XCSP has been proposed as a format based on XML which\nallows us to represent constraints defined either extensionally or\nintensionally, permits global constraints and has been the standard format of\nthe international competition of constraint satisfaction problems solvers. In\nthis paper we present a plug-in for solving problems specified in XCSP by\nexploiting the Gecode solver. This is done by dynamically translating\nconstraints into Gecode library calls, thus avoiding the need to interact with\nC++."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.0023v1", 
    "other_authors": "Jeremy G. Siek, Michael M. Vitousek, Jonathan D. Turner", 
    "title": "Effects for Funargs", 
    "arxiv-id": "1201.0023v1", 
    "author": "Jonathan D. Turner", 
    "publish": "2011-12-29T21:22:31Z", 
    "summary": "Stack allocation and first-class functions don't naturally mix together. In\nthis paper we show that a type and effect system can be the detergent that\nhelps these features form a nice emulsion. Our interest in this problem comes\nfrom our work on the Chapel language, but this problem is also relevant to\nlambda expressions in C++ and blocks in Objective C. The difficulty in mixing\nfirst-class functions and stack allocation is a tension between safety,\nefficiency, and simplicity. To preserve safety, one must worry about functions\noutliving the variables they reference: the classic upward funarg problem.\nThere are systems which regain safety but lose programmer-predictable\nefficiency, and ones that provide both safety and efficiency, but give up\nsimplicity by exposing regions to the programmer. In this paper we present a\nsimple design that combines a type and effect system, for safety, with\nfunction-local storage, for control over efficiency."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.0024v1", 
    "other_authors": "Erik Silkensen, Jeremy G. Siek", 
    "title": "Well-typed Islands Parse Faster", 
    "arxiv-id": "1201.0024v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2011-12-29T21:29:59Z", 
    "summary": "This paper addresses the problem of specifying and parsing the syntax of\ndomain-specific languages (DSLs) in a modular, user-friendly way. That is, we\nwant to enable the design of composable DSLs that combine the natural syntax of\nexternal DSLs with the easy implementation of internal DSLs. The challenge in\nparsing composable DSLs is that the composition of several (individually\nunambiguous) languages is likely to contain ambiguities. In this paper, we\npresent the design of a system that uses a type-oriented variant of island\nparsing to efficiently parse the syntax of composable DSLs. In particular, we\nshow how type-oriented island parsing is constant time with respect to the\nnumber of DSLs imported. We also show how to use our tool to implement DSLs on\ntop of a host language such as Typed Racket."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.0027v1", 
    "other_authors": "Jeremy G. Siek", 
    "title": "The C++0x \"Concepts\" Effort", 
    "arxiv-id": "1201.0027v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2011-12-29T21:38:55Z", 
    "summary": "C++0x is the working title for the revision of the ISO standard of the C++\nprogramming language that was originally planned for release in 2009 but that\nwas delayed to 2011. The largest language extension in C++0x was \"concepts\",\nthat is, a collection of features for constraining template parameters. In\nSeptember of 2008, the C++ standards committee voted the concepts extension\ninto C++0x, but then in July of 2009, the committee voted the concepts\nextension back out of C++0x.\n  This article is my account of the technical challenges and debates within the\n\"concepts\" effort in the years 2003 to 2009. To provide some background, the\narticle also describes the design space for constrained parametric\npolymorphism, or what is colloquially know as constrained generics. While this\narticle is meant to be generally accessible, the writing is aimed toward\nreaders with background in functional programming and programming language\ntheory. This article grew out of a lecture at the Spring School on Generic and\nIndexed Programming at the University of Oxford, March 2010."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.0874v1", 
    "other_authors": "Dariusz Biernacki, Serguei Lenglet", 
    "title": "Applicative Bisimulations for Delimited-Control Operators", 
    "arxiv-id": "1201.0874v1", 
    "author": "Serguei Lenglet", 
    "publish": "2012-01-04T11:22:08Z", 
    "summary": "We develop a behavioral theory for the untyped call-by-value lambda calculus\nextended with the delimited-control operators shift and reset. For this\ncalculus, we discuss the possible observable behaviors and we define an\napplicative bisimilarity that characterizes contextual equivalence. We then\ncompare the applicative bisimilarity and the CPS equivalence, a relation on\nterms often used in studies of control operators. In the process, we illustrate\nhow bisimilarity can be used to prove equivalence of terms with\ndelimited-control effects."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.1101v1", 
    "other_authors": "Sergue\u00ef Lenglet, J. B. Wells", 
    "title": "Expansion for Universal Quantifiers", 
    "arxiv-id": "1201.1101v1", 
    "author": "J. B. Wells", 
    "publish": "2012-01-05T10:11:52Z", 
    "summary": "Expansion is an operation on typings (i.e., pairs of typing environments and\nresult types) defined originally in type systems for the lambda-calculus with\nintersection types in order to obtain principal (i.e., most informative,\nstrongest) typings. In a type inference scenario, expansion allows postponing\nchoices for whether and how to use non-syntax-driven typing rules (e.g.,\nintersection introduction) until enough information has been gathered to make\nthe right decision. Furthermore, these choices can be equivalent to inserting\nuses of such typing rules at deeply nested positions in a typing derivation,\nwithout needing to actually inspect or modify (or even have) the typing\nderivation. Expansion has in recent years become simpler due to the use of\nexpansion variables (e.g., in System E).\n  This paper extends expansion and expansion variables to systems with\nforall-quantifiers. We present System Fs, an extension of System F with\nexpansion, and prove its main properties. This system turns type inference into\na constraint solving problem; this could be helpful to design a modular type\ninference algorithm for System F types in the future."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.2312v1", 
    "other_authors": "B. Seetha Lakshmi, C. D. Balapriya, R. Soniya", 
    "title": "Actor Garbage Collection in Distributed Systems using Graph   Transformation", 
    "arxiv-id": "1201.2312v1", 
    "author": "R. Soniya", 
    "publish": "2012-01-10T08:24:07Z", 
    "summary": "A lot of research work has been done in the area of Garbage collection for\nboth uniprocessor and distributed systems. Actors are associated with activity\n(thread) and hence usual garbage collection algorithms cannot be applied for\nthem. Hence a separate algorithm should be used to collect them. If we\ntransform the active reference graph into a graph which captures all the\nfeatures of actors and looks like passive reference graph then any passive\nreference graph algorithm can be applied for it. But the cost of transformation\nand optimization are the core issues. An attempt has been made to walk through\nthese issues."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.3907v1", 
    "other_authors": "Stephen Chang, Matthias Felleisen", 
    "title": "The Call-by-need Lambda Calculus, Revisited", 
    "arxiv-id": "1201.3907v1", 
    "author": "Matthias Felleisen", 
    "publish": "2012-01-18T20:44:35Z", 
    "summary": "The existing call-by-need lambda calculi describe lazy evaluation via\nequational logics. A programmer can use these logics to safely ascertain\nwhether one term is behaviorally equivalent to another or to determine the\nvalue of a lazy program. However, neither of the existing calculi models\nevaluation in a way that matches lazy implementations.\n  Both calculi suffer from the same two problems. First, the calculi never\ndiscard function calls, even after they are completely resolved. Second, the\ncalculi include re-association axioms even though these axioms are merely\nadministrative steps with no counterpart in any implementation.\n  In this paper, we present an alternative axiomatization of lazy evaluation\nusing a single axiom. It eliminates both the function call retention problem\nand the extraneous re-association axioms. Our axiom uses a grammar of contexts\nto describe the exact notion of a needed computation. Like its predecessors,\nour new calculus satisfies consistency and standardization properties and is\nthus suitable for reasoning about behavioral equivalence. In addition, we\nestablish a correspondence between our semantics and Launchbury's natural\nsemantics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.4715v2", 
    "other_authors": "Ji\u0159\u00ed Slab\u00fd, Jan Strej\u010dek, Marek Trt\u00edk", 
    "title": "Compact Symbolic Execution", 
    "arxiv-id": "1201.4715v2", 
    "author": "Marek Trt\u00edk", 
    "publish": "2012-01-23T13:50:55Z", 
    "summary": "We present a generalisation of King's symbolic execution technique called\ncompact symbolic execution. It proceeds in two steps. First, we analyse cyclic\npaths in the control flow graph of a given program, independently from the rest\nof the program. Our goal is to compute a so called template for each such a\ncyclic path. A template is a declarative parametric description of all possible\nprogram states, which may leave the analysed cyclic path after any number of\niterations along it. In the second step, we execute the program symbolically\nwith the templates in hand. The result is a compact symbolic execution tree. A\ncompact tree always carry the same information in all its leaves as the\ncorresponding classic symbolic execution tree. Nevertheless, a compact tree is\ntypically substantially smaller than the corresponding classic tree. There are\neven programs for which compact symbolic execution trees are finite while\nclassic symbolic execution trees are infinite."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.69.8", 
    "link": "http://arxiv.org/pdf/1201.4719v1", 
    "other_authors": "Ji\u0159\u00ed Slab\u00fd, Jan Strej\u010dek, Marek Trt\u00edk", 
    "title": "On Synergy of Metal, Slicing, and Symbolic Execution", 
    "arxiv-id": "1201.4719v1", 
    "author": "Marek Trt\u00edk", 
    "publish": "2012-01-23T14:06:32Z", 
    "summary": "We introduce a novel technique for finding real errors in programs. The\ntechnique is based on a synergy of three well-known methods: metacompilation,\nslicing, and symbolic execution. More precisely, we instrument a given program\nwith a code that tracks runs of state machines representing various kinds of\nerrors. Next we slice the program to reduce its size without affecting runs of\nstate machines. And then we symbolically execute the sliced program. Depending\non the kind of symbolic execution, the technique can be applied as a\nstand-alone bug finding technique, or to weed out some false positives from an\noutput of another bug-finding tool. We provide several examples demonstrating\nthe practical applicability of our technique."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000069", 
    "link": "http://arxiv.org/pdf/1201.4801v2", 
    "other_authors": "Dagand Pierre-Evariste, McBride Conor", 
    "title": "Transporting Functions across Ornaments", 
    "arxiv-id": "1201.4801v2", 
    "author": "McBride Conor", 
    "publish": "2012-01-23T18:27:52Z", 
    "summary": "Programming with dependent types is a blessing and a curse. It is a blessing\nto be able to bake invariants into the definition of data-types: we can finally\nwrite correct-by-construction software. However, this extreme accuracy is also\na curse: a data-type is the combination of a structuring medium together with a\nspecial purpose logic. These domain-specific logics hamper any effort of code\nreuse among similarly structured data.\n  In this paper, we exorcise our data-types by adapting the notion of ornament\nto our universe of inductive families. We then show how code reuse can be\nachieved by ornamenting functions. Using these functional ornament, we capture\nthe relationship between functions such as the addition of natural numbers and\nthe concatenation of lists. With this knowledge, we demonstrate how the\nimplementation of the former informs the implementation of the latter: the user\ncan ask the definition of addition to be lifted to lists and she will only be\nasked the details necessary to carry on adding lists rather than numbers.\n  Our presentation is formalised in a type theory with a universe of data-types\nand all our constructions have been implemented as generic programs, requiring\nno extension to the type theory."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000069", 
    "link": "http://arxiv.org/pdf/1201.6057v1", 
    "other_authors": "Ralf Laemmel, Simon Thompson, Markus Kaiser", 
    "title": "Programming errors in traversal programs over structured data", 
    "arxiv-id": "1201.6057v1", 
    "author": "Markus Kaiser", 
    "publish": "2012-01-29T16:50:15Z", 
    "summary": "Traversal strategies \\'a la Stratego (also \\'a la Strafunski and 'Scrap Your\nBoilerplate') provide an exceptionally versatile and uniform means of querying\nand transforming deeply nested and heterogeneously structured data including\nterms in functional programming and rewriting, objects in OO programming, and\nXML documents in XML programming. However, the resulting traversal programs are\nprone to programming errors. We are specifically concerned with errors that go\nbeyond conservative type errors; examples we examine include divergent\ntraversals, prematurely terminated traversals, and traversals with dead code.\nBased on an inventory of possible programming errors we explore options of\nstatic typing and static analysis so that some categories of errors can be\navoided. This exploration generates suggestions for improvements to strategy\nlibraries as well as their underlying programming languages. Haskell is used\nfor illustrations and specifications with sufficient explanations to make the\npresentation comprehensible to the non-specialist. The overall ideas are\nlanguage-agnostic and they are summarized accordingly."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000069", 
    "link": "http://arxiv.org/pdf/1201.6188v1", 
    "other_authors": "Massimo Bartoletti, Emilio Tuosto, Roberto Zunino", 
    "title": "On the realizability of contracts in dishonest systems", 
    "arxiv-id": "1201.6188v1", 
    "author": "Roberto Zunino", 
    "publish": "2012-01-30T11:58:01Z", 
    "summary": "We develop a theory of contracting systems, where behavioural contracts may\nbe violated by dishonest participants after they have been agreed upon - unlike\nin traditional approaches based on behavioural types. We consider the contracts\nof \\cite{CastagnaPadovaniGesbert09toplas}, and we embed them in a calculus that\nallows distributed participants to advertise contracts, reach agreements, query\nthe fulfilment of contracts, and realise them (or choose not to).\n  Our contract theory makes explicit who is culpable at each step of a\ncomputation. A participant is honest in a given context S when she is not\nculpable in each possible interaction with S. Our main result is a sufficient\ncriterion for classifying a participant as honest in all possible contexts."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000069", 
    "link": "http://arxiv.org/pdf/1203.0681v1", 
    "other_authors": "Mohammed Fadle Abdulla", 
    "title": "Manual and Fast C Code Optimization", 
    "arxiv-id": "1203.0681v1", 
    "author": "Mohammed Fadle Abdulla", 
    "publish": "2012-03-03T20:42:37Z", 
    "summary": "Developing an application with high performance through the code optimization\nplaces a greater responsibility on the programmers. While most of the existing\ncompilers attempt to automatically optimize the program code, manual techniques\nremain the predominant method for performing optimization. Deciding where to\ntry to optimize code is difficult, especially for large complex applications.\nFor manual optimization, the programmers can use his experiences in writing the\ncode, and then he can use a software profiler in order to collect and analyze\nthe performance data from the code. In this work, we have gathered the most\nexperiences which can be applied to improve the style of writing programs in C\nlanguage as well as we present an implementation of the manual optimization of\nthe codes using the Intel VTune profiler. The paper includes two case studies\nto illustrate our optimization on the Heap Sort and Factorial functions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(1:24)2012", 
    "link": "http://arxiv.org/pdf/1203.0780v2", 
    "other_authors": "Giuseppe Castagna, Mariangiola Dezani-Ciancaglini, Luca Padovani", 
    "title": "On Global Types and Multi-Party Session", 
    "arxiv-id": "1203.0780v2", 
    "author": "Luca Padovani", 
    "publish": "2012-03-04T22:05:29Z", 
    "summary": "Global types are formal specifications that describe communication protocols\nin terms of their global interactions. We present a new, streamlined language\nof global types equipped with a trace-based semantics and whose features and\nrestrictions are semantically justified. The multi-party sessions obtained\nprojecting our global types enjoy a liveness property in addition to the\ntraditional progress and are shown to be sound and complete with respect to the\nset of traces of the originating global type. Our notion of completeness is\nless demanding than the classical ones, allowing a multi-party session to leave\nout redundant traces from an underspecified global type. In addition to the\ntechnical content, we discuss some limitations of our language of global types\nand provide an extensive comparison with related specification languages\nadopted in different communities."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(1:24)2012", 
    "link": "http://arxiv.org/pdf/1203.1392v1", 
    "other_authors": "Quan Phan, Gerda Janssens, Zoltan Somogyi", 
    "title": "Region-based memory management for Mercury programs", 
    "arxiv-id": "1203.1392v1", 
    "author": "Zoltan Somogyi", 
    "publish": "2012-03-07T07:28:08Z", 
    "summary": "Region-based memory management (RBMM) is a form of compile time memory\nmanagement, well-known from the functional programming world. In this paper we\ndescribe our work on implementing RBMM for the logic programming language\nMercury. One interesting point about Mercury is that it is designed with strong\ntype, mode, and determinism systems. These systems not only provide Mercury\nprogrammers with several direct software engineering benefits, such as\nself-documenting code and clear program logic, but also give language\nimplementors a large amount of information that is useful for program analyses.\nIn this work, we make use of this information to develop program analyses that\ndetermine the distribution of data into regions and transform Mercury programs\nby inserting into them the necessary region operations. We prove the\ncorrectness of our program analyses and transformation. To execute the\nannotated programs, we have implemented runtime support that tackles the two\nmain challenges posed by backtracking. First, backtracking can require regions\nremoved during forward execution to be \"resurrected\"; and second, any memory\nallocated during a computation that has been backtracked over must be recovered\npromptly and without waiting for the regions involved to come to the end of\ntheir life. We describe in detail our solution of both these problems. We study\nin detail how our RBMM system performs on a selection of benchmark programs,\nincluding some well-known difficult cases for RBMM. Even with these difficult\ncases, our RBMM-enabled Mercury system obtains clearly faster runtimes for 15\nout of 18 benchmarks compared to the base Mercury system with its Boehm runtime\ngarbage collector, with an average runtime speedup of 24%, and an average\nreduction in memory requirements of 95%. In fact, our system achieves optimal\nmemory consumption in some programs."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2014.02.001", 
    "link": "http://arxiv.org/pdf/1203.1539v1", 
    "other_authors": "Andrej Bauer, Matija Pretnar", 
    "title": "Programming with Algebraic Effects and Handlers", 
    "arxiv-id": "1203.1539v1", 
    "author": "Matija Pretnar", 
    "publish": "2012-03-07T17:05:09Z", 
    "summary": "Eff is a programming language based on the algebraic approach to\ncomputational effects, in which effects are viewed as algebraic operations and\neffect handlers as homomorphisms from free algebras. Eff supports first-class\neffects and handlers through which we may easily define new computational\neffects, seamlessly combine existing ones, and handle them in novel ways. We\ngive a denotational semantics of eff and discuss a prototype implementation\nbased on it. Through examples we demonstrate how the standard effects are\ntreated in eff, and how eff supports programming techniques that use various\nforms of delimited continuations, such as backtracking, breadth-first search,\nselection functionals, cooperative multi-threading, and others."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2014.02.001", 
    "link": "http://arxiv.org/pdf/1203.1986v1", 
    "other_authors": "Neal Glew, Leaf Petersen", 
    "title": "Type-Preserving Flow Analysis and Interprocedural Unboxing (Extended   Version)", 
    "arxiv-id": "1203.1986v1", 
    "author": "Leaf Petersen", 
    "publish": "2012-03-09T04:19:19Z", 
    "summary": "Interprocedural flow analysis can be used to eliminate otherwise unnecessary\nheap allocated objects (unboxing), and in previous work we have shown how to do\nso while maintaining correctness with respect to the garbage collector. In this\npaper, we extend the notion of flow analysis to incorporate types, enabling\nanalysis and optimization of typed programs. We apply this typed analysis to\nspecify a type preserving interprocedural unboxing optimization, and prove that\nthe optimization preserves both type and GC safety along with program\nsemantics. We also show that the unboxing optimization can be applied\nindependently to separately compiled program modules, and prove via a\ncontextual equivalence result that unboxing a module in isolation preserves\nprogram semantics."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2014.02.001", 
    "link": "http://arxiv.org/pdf/1203.2296v2", 
    "other_authors": "M. H. van Emden", 
    "title": "Discovering Algorithms with Matrix Code", 
    "arxiv-id": "1203.2296v2", 
    "author": "M. H. van Emden", 
    "publish": "2012-03-11T00:12:56Z", 
    "summary": "In first-year programming courses it is often difficult to show students how\nan algorithm can be discovered. In this paper we present a program format that\nsupports the development from specification to code in small and obvious steps;\nthat is, a discovery process. The format, called Matrix Code, can be\ninterpreted as a proof according to the Floyd-Hoare program verification\nmethod. The process consists of expressing the specification of a function body\nas an initial code matrix and then growing the matrix by adding rows and\ncolumns until the completed matrix is translated in a routine fashion to\ncompilable code. As worked example we develop a Java program that generates the\ntable of the first N prime numbers."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200004X", 
    "link": "http://arxiv.org/pdf/1203.2431v1", 
    "other_authors": "Adri\u00e1n Riesco, Juan Rodr\u00edguez-Hortal\u00e1", 
    "title": "Singular and Plural Functions for Functional Logic Programming", 
    "arxiv-id": "1203.2431v1", 
    "author": "Juan Rodr\u00edguez-Hortal\u00e1", 
    "publish": "2012-03-12T09:23:07Z", 
    "summary": "Functional logic programming (FLP) languages use non-terminating and\nnon-confluent constructor systems (CS's) as programs in order to define\nnon-strict non-determi-nistic functions. Two semantic alternatives have been\nusually considered for parameter passing with this kind of functions: call-time\nchoice and run-time choice. While the former is the standard choice of modern\nFLP languages, the latter lacks some properties---mainly\ncompositionality---that have prevented its use in practical FLP systems.\nTraditionally it has been considered that call-time choice induces a singular\ndenotational semantics, while run-time choice induces a plural semantics. We\nhave discovered that this latter identification is wrong when pattern matching\nis involved, and thus we propose two novel compositional plural semantics for\nCS's that are different from run-time choice.\n  We study the basic properties of our plural semantics---compositionality,\npolarity, monotonicity for substitutions, and a restricted form of the bubbling\nproperty for constructor systems---and the relation between them and to\nprevious proposals, concluding that these semantics form a hierarchy in the\nsense of set inclusion of the set of computed values. We have also identified a\nclass of programs characterized by a syntactic criterion for which the proposed\nplural semantics behave the same, and a program transformation that can be used\nto simulate one of them by term rewriting. At the practical level, we study how\nto use the expressive capabilities of these semantics for improving the\ndeclarative flavour of programs. We also propose a language which combines\ncall-time choice and our plural semantics, that we have implemented in Maude.\nThe resulting interpreter is employed to test several significant examples\nshowing the capabilities of the combined semantics.\n  To appear in Theory and Practice of Logic Programming (TPLP)"
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200004X", 
    "link": "http://arxiv.org/pdf/1203.4499v1", 
    "other_authors": "Bruno C. d. S. Oliveira, Tom Schrijvers, Wontae Choi, Wonchan Lee, Kwangkeun Yi", 
    "title": "Extended Report: The Implicit Calculus", 
    "arxiv-id": "1203.4499v1", 
    "author": "Kwangkeun Yi", 
    "publish": "2012-03-20T16:44:27Z", 
    "summary": "Generic programming (GP) is an increasingly important trend in programming\nlanguages. Well-known GP mechanisms, such as type classes and the C++0x\nconcepts proposal, usually combine two features: 1) a special type of\ninterfaces; and 2) implicit instantiation of implementations of those\ninterfaces.\n  Scala implicits are a GP language mechanism, inspired by type classes, that\nbreak with the tradition of coupling implicit instantiation with a special type\nof interface. Instead, implicits provide only implicit instantiation, which is\ngeneralized to work for any types. This turns out to be quite powerful and\nuseful to address many limitations that show up in other GP mechanisms.\n  This paper synthesizes the key ideas of implicits formally in a minimal and\ngeneral core calculus called the implicit calculus, and it shows how to build\nsource languages supporting implicit instantiation on top of it. A novelty of\nthe calculus is its support for partial resolution and higher-order rules (a\nfeature that has been proposed before, but was never formalized or\nimplemented). Ultimately, the implicit calculus provides a formal model of\nimplicits, which can be used by language designers to study and inform\nimplementations of similar mechanisms in their own languages."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200004X", 
    "link": "http://arxiv.org/pdf/1203.5303v1", 
    "other_authors": "Florian Zuleger, Sumit Gulwani, Moritz Sinn, Helmut Veith", 
    "title": "Bound Analysis of Imperative Programs with the Size-change Abstraction   (extended version)", 
    "arxiv-id": "1203.5303v1", 
    "author": "Helmut Veith", 
    "publish": "2012-03-23T17:22:06Z", 
    "summary": "The size-change abstraction (SCA) is an important program abstraction for\ntermination analysis, which has been successfully implemented in many tools for\nfunctional and logic programs. In this paper, we demonstrate that SCA is also a\nhighly effective abstract domain for the bound analysis of imperative programs.\n  We have implemented a bound analysis tool based on SCA for imperative\nprograms. We abstract programs in a pathwise and context dependent manner,\nwhich enables our tool to analyze real-world programs effectively. Our work\nshows that SCA captures many of the essential ideas of previous termination and\nbound analysis and goes beyond in a conceptually simpler framework."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200004X", 
    "link": "http://arxiv.org/pdf/1302.2692v1", 
    "other_authors": "Shuying Liang, Matthew Might, Thomas Gilray, David Van Horn", 
    "title": "Pushdown Exception-Flow Analysis of Object-Oriented Programs", 
    "arxiv-id": "1302.2692v1", 
    "author": "David Van Horn", 
    "publish": "2013-02-12T03:29:55Z", 
    "summary": "Statically reasoning in the presence of and about exceptions is challenging:\nexceptions worsen the well-known mutual recursion between data-flow and\ncontrol-flow analysis. The recent development of pushdown control-flow analysis\nfor the {\\lambda}-calculus hints at a way to improve analysis of exceptions: a\npushdown stack can precisely match catches to throws in the same way it matches\nreturns to calls. This work generalizes pushdown control-flow analysis to\nobject-oriented programs and to exceptions. Pushdown analysis of exceptions\nimproves precision over the next best analysis, Bravenboer and Smaragdakis's\nDoop, by orders of magnitude. By then generalizing abstract garbage collection\nto object-oriented programs, we reduce analysis time by half over pure pushdown\nanalysis. We evaluate our implementation for Dalvik bytecode on standard\nbenchmarks as well as several Android applications."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200004X", 
    "link": "http://arxiv.org/pdf/1302.3178v1", 
    "other_authors": "Martin Lester, Luke Ong, Max Schaefer", 
    "title": "Information Flow Analysis for a Dynamically Typed Functional Language   with Staged Metaprogramming", 
    "arxiv-id": "1302.3178v1", 
    "author": "Max Schaefer", 
    "publish": "2013-02-13T18:09:05Z", 
    "summary": "Web applications written in JavaScript are regularly used for dealing with\nsensitive or personal data. Consequently, reasoning about their security\nproperties has become an important problem, which is made very difficult by the\nhighly dynamic nature of the language, particularly its support for runtime\ncode generation. As a first step towards dealing with this, we propose to\ninvestigate security analyses for languages with more principled forms of\ndynamic code generation. To this end, we present a static information flow\nanalysis for a dynamically typed functional language with prototype-based\ninheritance and staged metaprogramming. We prove its soundness, implement it\nand test it on various examples designed to show its relevance to proving\nsecurity properties, such as noninterference, in JavaScript. To our knowledge,\nthis is the first fully static information flow analysis for a language with\nstaged metaprogramming, and the first formal soundness proof of a CFA-based\ninformation flow analysis for a functional programming language."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.1", 
    "link": "http://arxiv.org/pdf/1302.6328v1", 
    "other_authors": "Yu David Liu", 
    "title": "Variant-Frequency Semantics for Green Futures", 
    "arxiv-id": "1302.6328v1", 
    "author": "Yu David Liu", 
    "publish": "2013-02-26T06:48:39Z", 
    "summary": "This paper describes an operational semantics for futures, with the primary\ntarget on energy efficiency. The work in progress is built around an insight\nthat different threads can coordinate by running at different \"paces,\" so that\nthe time for synchronization and the resulting wasteful energy consumption can\nbe reduced. We exploit several inherent characteristics of futures to determine\nhow the paces of involving threads can be coordinated. The semantics is\ninspired by recent advances in computer architectures, where the frequencies of\nCPU cores can be adjusted dynamically. The work is a first-step toward a\ndirection where variant frequencies are directly modeled as an essential\nsemantic feature in concurrent programming languages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.6", 
    "link": "http://arxiv.org/pdf/1302.6333v1", 
    "other_authors": "Sung-Shik T. Q. Jongmans, Farhad Arbab", 
    "title": "Modularizing and Specifying Protocols among Threads", 
    "arxiv-id": "1302.6333v1", 
    "author": "Farhad Arbab", 
    "publish": "2013-02-26T06:49:18Z", 
    "summary": "We identify three problems with current techniques for implementing protocols\namong threads, which complicate and impair the scalability of multicore\nsoftware development: implementing synchronization, implementing coordination,\nand modularizing protocols. To mend these deficiencies, we argue for the use of\ndomain-specific languages (DSL) based on existing models of concurrency. To\ndemonstrate the feasibility of this proposal, we explain how to use the model\nof concurrency Reo as a high-level protocol DSL, which offers appropriate\nabstractions and a natural separation of protocols and computations. We\ndescribe a Reo-to-Java compiler and illustrate its use through examples."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.6", 
    "link": "http://arxiv.org/pdf/1303.0427v1", 
    "other_authors": "Andrew P. Black", 
    "title": "Object-oriented programming: some history, and challenges for the next   fifty years", 
    "arxiv-id": "1303.0427v1", 
    "author": "Andrew P. Black", 
    "publish": "2013-03-02T20:38:33Z", 
    "summary": "Object-oriented programming is inextricably linked to the pioneering work of\nOle-Johan Dahl and Kristen Nygaard on the design of the Simula language, which\nstarted at the Norwegian Computing Centre in the Spring of 1961. However,\nobject-orientation, as we think of it today---fifty years later---is the result\nof a complex interplay of ideas, constraints and people. Dahl and Nygaard would\ncertainly recognise it as their progeny, but might also be amazed at how much\nit has grown up.\n  This article is based on a lecture given on 22nd August 2011, on the occasion\nof the scientific opening of the Ole-Johan Dahl hus at the University of Oslo.\nIt looks at the foundational ideas from Simula that stand behind\nobject-orientation, how those ideas have evolved to become the dominant\nprogramming paradigm, and what they have to offer as we approach the challenges\nof the next fifty years of informatics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.6", 
    "link": "http://arxiv.org/pdf/1303.0722v1", 
    "other_authors": "Iztok Fister Jr., Toma\u017e Kosar, Iztok Fister, Marjan Mernik", 
    "title": "EasyTime++: A case study of incremental domain-specific language   development", 
    "arxiv-id": "1303.0722v1", 
    "author": "Marjan Mernik", 
    "publish": "2013-03-04T15:11:38Z", 
    "summary": "EasyTime is a domain-specific language (DSL) for measuring time during sports\ncompetitions. A distinguishing feature of DSLs is that they are much more\namenable to change, and EasyTime is no exception in this regard. This paper\nintroduces two new EasyTime features: classifications of competitors into\ncategories, and the inclusion of competitions where the number of laps must be\ndynamically determined. It shows how such extensions can be incrementally added\ninto the base-language reusing most of the language specifications. Two case\nstudies are presented showing the suitability of this approach."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.6", 
    "link": "http://arxiv.org/pdf/1303.0908v1", 
    "other_authors": "Rajasekhara Babu, Krishnakumar V., George Abraham, Kiransinh Borasia", 
    "title": "KRAB Algorithm - A Revised Algorithm for Incremental Call Graph   Generation", 
    "arxiv-id": "1303.0908v1", 
    "author": "Kiransinh Borasia", 
    "publish": "2013-03-05T02:13:28Z", 
    "summary": "This paper is aimed to present the importance and implementation of an\nincremental call graph plugin. An algorithm is proposed for the call graph\nimplementation which has better overall performance than the algorithm that has\nbeen proposed previously. In addition to this, the algorithm has been\nempirically proved to have excellent performance on recursive codes. The\nalgorithm also readily checks for function skip and returns exceptions."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.109.6", 
    "link": "http://arxiv.org/pdf/1303.1880v2", 
    "other_authors": "Nabizath Saleena, Vineeth Paleri", 
    "title": "A Simple Algorithm for Global Value Numbering", 
    "arxiv-id": "1303.1880v2", 
    "author": "Vineeth Paleri", 
    "publish": "2013-03-08T04:02:33Z", 
    "summary": "Global Value Numbering(GVN) is a method for detecting redundant computations\nin programs. Here, we introduce the problem of Global Value Numbering in its\noriginal form, as conceived by Kildall(1973), and present an algorithm which is\na simpler variant of Kildall's. The algorithm uses the concept of value\nexpression - an abstraction of a set of expressions - enabling a representation\nof the equivalence information which is compact and simple to manipulate."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1303.2201v1", 
    "other_authors": "Emanuele D'Osualdo, Jonathan Kochems, C. -H. Luke Ong", 
    "title": "Automatic Verification of Erlang-Style Concurrency", 
    "arxiv-id": "1303.2201v1", 
    "author": "C. -H. Luke Ong", 
    "publish": "2013-03-09T12:10:01Z", 
    "summary": "This paper presents an approach to verify safety properties of Erlang-style,\nhigher-order concurrent programs automatically. Inspired by Core Erlang, we\nintroduce Lambda-Actor, a prototypical functional language with\npattern-matching algebraic data types, augmented with process creation and\nasynchronous message-passing primitives. We formalise an abstract model of\nLambda-Actor programs called Actor Communicating System (ACS) which has a\nnatural interpretation as a vector addition system, for which some verification\nproblems are decidable. We give a parametric abstract interpretation framework\nfor Lambda-Actor and use it to build a polytime computable, flow-based,\nabstract semantics of Lambda-Actor programs, which we then use to bootstrap the\nACS construction, thus deriving a more accurate abstract model of the input\nprogram. We have constructed Soter, a tool implementation of the verification\nmethod, thereby obtaining the first fully-automatic, infinite-state model\nchecker for a core fragment of Erlang. We find that in practice our abstraction\ntechnique is accurate enough to verify an interesting range of safety\nproperties. Though the ACS coverability problem is Expspace-complete, Soter can\nanalyse these verification problems surprisingly efficiently."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.1246v2", 
    "other_authors": "Keehang Kwon, Jeongyoon Seo, Daeseong Kang", 
    "title": "Bounded-Choice Statements for User Interaction in Imperative and   Object-Oriented Programming", 
    "arxiv-id": "1308.1246v2", 
    "author": "Daeseong Kang", 
    "publish": "2013-08-06T11:46:02Z", 
    "summary": "Adding versatile interactions to imperative programming -- C, Java and\nAndroid -- is an essential task. Unfortunately, existing languages provide only\nlimited constructs for user interaction. These constructs are usually in the\nform of $unbounded$ quantification. For example, existing languages can take\nthe keyboard input from the user only via the $read(x)/scan(x)$ construct. Note\nthat the value of $x$ is unbounded in the sense that $x$ can have any value.\nThis construct is thus not useful for applications with bounded inputs. To\nsupport bounded choices, we propose new bounded-choice statements for user\ninteration. Each input device (the keyboard, the mouse, the touch, $...$)\nnaturally requires a new bounded-choice statement. To make things simple,\nhowever, we focus on a bounded-choice statement for keyboard -- kchoose -- to\nallow for more controlled and more guided participation from the user. It is\nstraightforward to adjust our idea to other input devices. We illustrate our\nidea via Java(BI), an extension of the core Java with a new bounded-choice\nstatement for the keyboard."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.2055v3", 
    "other_authors": "R\u00e9my Haemmerl\u00e9, Jose Morales", 
    "title": "Proceedings of the 23rd Workshop on Logic-based methods in Programming   Environments (WLPE 2013)", 
    "arxiv-id": "1308.2055v3", 
    "author": "Jose Morales", 
    "publish": "2013-08-09T09:03:57Z", 
    "summary": "This volume contains the papers presented at the 23rd Workshop on Logic-based\nMethods in Programming Environments (WLPE 2013), which was held in Istanbul,\nTurkey, on August 24 & 25 2013 as a satellite event of the 29th International\nConference on Logic Programming, (ICLP 2013)."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3203v1", 
    "other_authors": "Dino Distefano, Jeremy Dubreil", 
    "title": "Detecting Data Races on OpenCL Kernels with Symbolic Execution", 
    "arxiv-id": "1308.3203v1", 
    "author": "Jeremy Dubreil", 
    "publish": "2013-08-14T18:30:37Z", 
    "summary": "We present an automatic analysis technique for checking data races on OpenCL\nkernels. Our method defines symbolic execution techniques based on separation\nlogic with suitable abstractions to automatically detect non-benign racy\nbehaviours on kernel"
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3937v1", 
    "other_authors": "Michael Codish, Yoav Fekete, Amit Metodi", 
    "title": "Compiling Finite Domain Constraints to SAT with BEE: the Director's Cut", 
    "arxiv-id": "1308.3937v1", 
    "author": "Amit Metodi", 
    "publish": "2013-08-19T07:15:16Z", 
    "summary": "BEE is a compiler which facilitates solving finite domain constraints by\nencoding them to CNF and applying an underlying SAT solver. In BEE constraints\nare modeled as Boolean functions which propagate information about equalities\nbetween Boolean literals. This information is then applied to simplify the CNF\nencoding of the constraints. We term this process equi-propagation. A key\nfactor is that considering only a small fragment of a constraint model at one\ntime enables to apply stronger, and even complete reasoning to detect\nequivalent literals in that fragment. Once detected, equivalences propagate to\nsimplify the entire constraint model and facilitate further reasoning on other\nfragments. BEE is described in several recent papers. In this paper, after a\nquick review of BEE, we elaborate on two undocumented details of the\nimplementation: the hybrid encoding of cardinality constraints and complete\nequi-propagation. We thendescribe on-going work aimed to extend BEE to consider\nbinary representation of numbers."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3938v1", 
    "other_authors": "Spyros Hadjichristodoulou, Donald E. Porter, David S. Warren", 
    "title": "Efficiently Retrieving Function Dependencies in the Linux Kernel Using   XSB", 
    "arxiv-id": "1308.3938v1", 
    "author": "David S. Warren", 
    "publish": "2013-08-19T07:15:23Z", 
    "summary": "In this paper we investigate XSB-Prolog as a static analysis engine for data\nrepresented by medium-sized graphs. We use XSB-Prolog to automatically identify\nfunction dependencies in the Linux Kernel---queries that are difficult to\nimplement efficiently in a commodity database and that developers often have to\nidentify manually. This project illustrates that Prolog systems are ideal for\nbuilding tools for use in other disciplines that require sophisticated\ninferences, because Prolog is both declarative and can efficiently implement\ncomplex problem specifications through tabling and indexing."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3939v1", 
    "other_authors": "Dragan Ivanovi\u0107", 
    "title": "Implementing Constraint Handling Rules as a Domain-Specific Language   Embedded in Java", 
    "arxiv-id": "1308.3939v1", 
    "author": "Dragan Ivanovi\u0107", 
    "publish": "2013-08-19T07:15:27Z", 
    "summary": "Programming languages and techniques based on logic and constraints, such as\nthe Constraint Handling Rules (CHR), can support many common programming tasks\nthat can be expressed in the form of a search for feasible or optimal\nsolutions. Developing new constraint solvers using CHR is especially\ninteresting in configuration management for large scale, distributed and\ndynamic cloud applications, where dynamic configuration and component selection\nis an integral part of the programming environment. Writing CHR-style\nconstraint solvers in a domain-specific language which is a subset of Java --\ninstead of using a separate language layer -- solves many integration,\ndevelopment cycle disruption, testing and debugging problems that discourage or\nmake difficult the adoption of the CHR-based approach in the mainstream\nprogramming environments. Besides, the prototype implementation exposes a\nwell-defined API that supports transactional store behavior, safe termination,\nand debugging via event notifications."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3940v1", 
    "other_authors": "Alejandro Serrano, Pedro L\u00f3pez-Garc\u00eda, Manuel Hermenegildo", 
    "title": "Towards an Abstract Domain for Resource Analysis of Logic Programs Using   Sized Types", 
    "arxiv-id": "1308.3940v1", 
    "author": "Manuel Hermenegildo", 
    "publish": "2013-08-19T07:15:30Z", 
    "summary": "We present a novel general resource analysis for logic programs based on\nsized types.Sized types are representations that incorporate structural (shape)\ninformation and allow expressing both lower and upper bounds on the size of a\nset of terms and their subterms at any position and depth. They also allow\nrelating the sizes of terms and subterms occurring at different argument\npositions in logic predicates. Using these sized types, the resource analysis\ncan infer both lower and upper bounds on the resources used by all the\nprocedures in a program as functions on input term (and subterm) sizes,\novercoming limitations of existing analyses and enhancing their precision. Our\nnew resource analysis has been developed within the abstract interpretation\nframework, as an extension of the sized types abstract domain, and has been\nintegrated into the Ciao preprocessor, CiaoPP. The abstract domain operations\nare integrated with the setting up and solving of recurrence equations for\nboth, inferring size and resource usage functions. We show that the analysis is\nan improvement over the previous resource analysis present in CiaoPP and\ncompares well in power to state of the art systems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.3941v1", 
    "other_authors": "Jan Wielemaker, Michael Hendricks", 
    "title": "Why It's Nice to be Quoted: Quasiquoting for Prolog", 
    "arxiv-id": "1308.3941v1", 
    "author": "Michael Hendricks", 
    "publish": "2013-08-19T07:15:38Z", 
    "summary": "Prolog's support for dynamic programming, meta programming and text\nprocessing using context free grammars make the language highly suitable for\ndefining domain specific languages (DSL) as well as analysing, refactoring or\ngenerating expression states in other (programming) languages. Well known DSLs\nare the DCG (Definite Clause Grammar) notation and constraint languages such as\nCHR. These extensions use Prolog operator declarations and the {...} notation\nto realise a good syntax. When external languages, such as HTML, SQL or\nJavaScript enter the picture, operators no longer satisfy for embedding\nsnippets of these languages into a Prolog source file. In addition, Prolog has\npoor support for quoting long text fragments.\n  Haskell introduced quasi quotationsto resolve this problem. In this paper we\n`ported' the Haskell mechanism for quasi quoting to Prolog. We show that this\ncan be done cleanly and that quasi quoting can solve the above mentioned\nproblems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.4452v3", 
    "other_authors": "Keehang Kwon", 
    "title": "A New Statement for Selection and Exception Handling in Imperative   Languages", 
    "arxiv-id": "1308.4452v3", 
    "author": "Keehang Kwon", 
    "publish": "2013-08-21T00:13:32Z", 
    "summary": "Diverse selection statements -- if-then-else, switch and try-catch -- are\ncommonly used in modern programming languages. To make things simple, we\npropose a unifying statement for selection. This statement is of the form\nschoose(G_1,...,G_n) where each $G_i$ is a statement. It has a a simple\nsemantics: sequentially choose the first successful statement $G_i$ and then\nproceeds with executing $G_i$. Examples will be provided for this statement."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.4815v2", 
    "other_authors": "Clinton F. Goss", 
    "title": "Machine Code Optimization - Improving Executable Object Code", 
    "arxiv-id": "1308.4815v2", 
    "author": "Clinton F. Goss", 
    "publish": "2013-08-22T10:14:43Z", 
    "summary": "This dissertation explores classes of compiler optimization techniques that\nare applicable late in the compilation process, after all executable code for a\nprogram has been linked. I concentrate on techniques which, for various\nreasons, cannot be applied earlier in the compilation process. In addition to a\ntheoretical treatment of this class of optimization techniques, this\ndissertation reports on an implementation of these techniques in a production\nenvironment. I describe the details of the implementation which allows these\ntechniques to be re-targeted easily and report on improvements gained when\noptimizing production software."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-38856-9_24", 
    "link": "http://arxiv.org/pdf/1308.6096v1", 
    "other_authors": "Robert B. K. Dewar, Martin Charles Golumbic, Clinton F. Goss", 
    "title": "Micro Spitbol", 
    "arxiv-id": "1308.6096v1", 
    "author": "Clinton F. Goss", 
    "publish": "2013-08-28T08:59:41Z", 
    "summary": "A compact version of MACRO SPITBOL, a compiler/ interpreter for a variant of\nSNOBOL4, has been developed for use on microcomputer systems. The techniques\nfor producing an implementation are largely automatic in order to preserve the\nintegrity and portability of the SPITBOL system. These techniques are discussed\nalong with a description of an initial implementation on a 65K byte\nminicomputer. An interesting theoretical problem which arises when using\nprocedures which compact the interpretive object code is also analyzed."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla", 
    "link": "http://arxiv.org/pdf/1402.0087v1", 
    "other_authors": "Mehran Alidoost Nia, Reza Ebrahimi Atani", 
    "title": "A novel datatype architecture support for programming languages", 
    "arxiv-id": "1402.0087v1", 
    "author": "Reza Ebrahimi Atani", 
    "publish": "2014-02-01T14:07:59Z", 
    "summary": "In programmers point of view, Datatypes in programming language level have a\nsimple description but inside hardware, huge machine codes are responsible to\ndescribe type features. Datatype architecture design is a novel approach to\nmatch programming features along with hardware design. In this paper a novel\nData type-Based Code Reducer (TYPELINE) architecture is proposed and\nimplemented according to significant data types (SDT) of programming languages.\nTYPELINE uses TEUs for processing various SDT operations. This architecture\ndesign leads to reducing the number of machine codes, and increases execution\nspeed, and also improves some parallelism level. This is because this\narchitecture supports some operation for the execution of Abstract Data Types\nin parallel. Also it ensures to maintain data type features and entire\napplication level specifications using the proposed type conversion unit. This\nframework includes compiler level identifying execution modes and memory\nmanagement unit for decreasing object read/write in heap memory by ISA support.\nThis energy-efficient architecture is completely compatible with object\noriented programming languages and in combination mode it can process complex\nC++ data structures with respect to parallel TYPELINE architecture support."
},{
    "category": "cs.PL", 
    "doi": "10.1109/ICIINFS.2009.5429845", 
    "link": "http://arxiv.org/pdf/1402.0671v1", 
    "other_authors": "Rajitha Navarathna, Swarnalatha Radhakrishnan, Roshan Ragel", 
    "title": "Loop Unrolling in Multi-pipeline ASIP Design", 
    "arxiv-id": "1402.0671v1", 
    "author": "Roshan Ragel", 
    "publish": "2014-02-04T09:36:34Z", 
    "summary": "Application Specific Instruction-set Processor (ASIP) is one of the popular\nprocessor design techniques for embedded systems which allows customizability\nin processor design without overly hindering design flexibility. Multi-pipeline\nASIPs were proposed to improve the performance of such systems by compromising\nbetween speed and processor area. One of the problems in the multi-pipeline\ndesign is the limited inherent instruction level parallelism (ILP) available in\napplications. The ILP of application programs can be improved via a compiler\noptimization technique known as loop unrolling. In this paper, we present how\nloop unrolling effects the performance of multi-pipeline ASIPs. The\nimprovements in performance average around 15% for a number of benchmark\napplications with the maximum improvement of around 30%. In addition, we\nanalyzed the variable of performance against loop unrolling factor, which is\nthe amount of unrolling we perform."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.1287v1", 
    "other_authors": "Brijender Kahanwal", 
    "title": "Towards High Performance Computing (Hpc) Through Parallel Programming   Paradigms and Their Principles", 
    "arxiv-id": "1402.1287v1", 
    "author": "Brijender Kahanwal", 
    "publish": "2014-02-06T09:34:01Z", 
    "summary": "Nowadays, we are to find out solutions to huge computing problems very\nrapidly. It brings the idea of parallel computing in which several machines or\nprocessors work cooperatively for computational tasks. In the past decades,\nthere are a lot of variations in perceiving the importance of parallelism in\ncomputing machines. And it is observed that the parallel computing is a\nsuperior solution to many of the computing limitations like speed and density;\nnon-recurring and high cost; and power consumption and heat dissipation etc.\nThe commercial multiprocessors have emerged with lower prices than the\nmainframe machines and supercomputers machines. In this article the high\nperformance computing (HPC) through parallel programming paradigms (PPPs) are\ndiscussed with their constructs and design approaches."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.1699v2", 
    "other_authors": "Mauro Jaskelioff, Russell O'Connor", 
    "title": "A Representation Theorem for Second-Order Functionals", 
    "arxiv-id": "1402.1699v2", 
    "author": "Russell O'Connor", 
    "publish": "2014-02-07T17:21:14Z", 
    "summary": "Representation theorems relate seemingly complex objects to concrete, more\ntractable ones.\n  In this paper, we take advantage of the abstraction power of category theory\nand provide a general representation theorem for a wide class of second-order\nfunctionals which are polymorphic over a class of functors. Types polymorphic\nover a class of functors are easily representable in languages such as Haskell,\nbut are difficult to analyse and reason about. The concrete representation\nprovided by the theorem is easier to analyse, but it might not be as convenient\nto implement. Therefore, depending on the task at hand, the change of\nrepresentation may prove valuable in one direction or the other.\n  We showcase the usefulness of the representation theorem with a range of\nexamples. Concretely, we show how the representation theorem can be used to\nshow that traversable functors are finitary containers, how parameterised\ncoalgebras relate to very well-behaved lenses, and how algebraic effects might\nbe implemented in a functional language."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.4043v3", 
    "other_authors": "Radha Jagadeesan, James Riely", 
    "title": "Between Linearizability and Quiescent Consistency: Quantitative   Quiescent Consistency", 
    "arxiv-id": "1402.4043v3", 
    "author": "James Riely", 
    "publish": "2014-02-17T15:58:49Z", 
    "summary": "Linearizability is the de facto correctness criterion for concurrent data\nstructures. Unfortunately, linearizability imposes a performance penalty which\nscales linearly in the number of contending threads. Quiescent consistency is\nan alternative criterion which guarantees that a concurrent data structure\nbehaves correctly when accessed sequentially. Yet quiescent consistency says\nvery little about executions that have any contention.\n  We define quantitative quiescent consistency (QQC), a relaxation of\nlinearizability where the degree of relaxation is proportional to the degree of\ncontention. When quiescent, no relaxation is allowed, and therefore QQC refines\nquiescent consistency, unlike other proposed relaxations of linearizability. We\nshow that high performance counters and stacks designed to satisfy quiescent\nconsistency continue to satisfy QQC. The precise assumptions under which QQC\nholds provides fresh insight on these structures. To demonstrate the robustness\nof QQC, we provide three natural characterizations and prove compositionality."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.4843v5", 
    "other_authors": "Aleksandar Perisic", 
    "title": "Exercise: +-1 bug and center of an array problem", 
    "arxiv-id": "1402.4843v5", 
    "author": "Aleksandar Perisic", 
    "publish": "2014-02-19T22:50:29Z", 
    "summary": "A problem that is constantly cropping up in designing even the simplest\nalgorithm or a program is dealing with +-1 bug when we calculate positions\nwithin an array, very noticeably while splitting it in half. This bug is often\nfound in buffer overflow type of bugs. While designing one complicated\nalgorithm, we needed various ways of splitting an array, and we found lack of\ngeneral guidance for this apparently minor problem. We present an exercise that\ntracks the cause of the problem and leads to the solution. This problem looks\ntrivial because it seems obvious or insignificant, however treating it without\noutmost precision can lead to subtle bugs, unbalanced solution, not transparent\nexpressions for various languages. Basically, the exercise is about dealing\nwith <= < as well as n/2, n/2-1, (n+1)/2, n-1 and similar expressions when they\nare rounded down to the nearest integer and used to define a range."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1007.0159v1", 
    "other_authors": "Adrian Kuhn, David Erni, Marcus Denker", 
    "title": "Empowering Collections with Swarm Behavior", 
    "arxiv-id": "1007.0159v1", 
    "author": "Marcus Denker", 
    "publish": "2010-07-01T13:14:39Z", 
    "summary": "Often, when modelling a system there are properties and operations that are\nrelated to a group of objects rather than to a single object. In this paper we\nextend Java with Swarm Behavior, a new composition operator that associates\nbehavior with a collection of instances. The lookup resolution of swarm\nbehavior is based on the element type of a collection and is thus orthogonal to\nthe collection hierarchy."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1007.2123v6", 
    "other_authors": "Joey Paquet, Serguei A. Mokhov", 
    "title": "Comparative Studies of Programming Languages; Course Lecture Notes", 
    "arxiv-id": "1007.2123v6", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-07-12T17:22:54Z", 
    "summary": "Lecture notes for the Comparative Studies of Programming Languages course,\nCOMP6411, taught at the Department of Computer Science and Software\nEngineering, Faculty of Engineering and Computer Science, Concordia University,\nMontreal, QC, Canada. These notes include a compiled book of primarily related\narticles from the Wikipedia, the Free Encyclopedia, as well as Comparative\nProgramming Languages book and other resources, including our own. The original\nnotes were compiled by Dr. Paquet."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1007.3023v2", 
    "other_authors": "Steven Obua", 
    "title": "Purely Functional Structured Programming", 
    "arxiv-id": "1007.3023v2", 
    "author": "Steven Obua", 
    "publish": "2010-07-18T17:29:53Z", 
    "summary": "The idea of functional programming has played a big role in shaping today's\nlandscape of mainstream programming languages. Another concept that dominates\nthe current programming style is Dijkstra's structured programming. Both\nconcepts have been successfully married, for example in the programming\nlanguage Scala. This paper proposes how the same can be achieved for structured\nprogramming and PURELY functional programming via the notion of LINEAR SCOPE.\nOne advantage of this proposal is that mainstream programmers can reap the\nbenefits of purely functional programming like easily exploitable parallelism\nwhile using familiar structured programming syntax and without knowing concepts\nlike monads. A second advantage is that professional purely functional\nprogrammers can often avoid hard to read functional code by using structured\nprogramming syntax that is often easier to parse mentally."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4104", 
    "link": "http://arxiv.org/pdf/1007.3133v1", 
    "other_authors": "Laurent Hubert, Thomas Jensen, Vincent Monfort, David Pichardie", 
    "title": "Enforcing Secure Object Initialization in Java", 
    "arxiv-id": "1007.3133v1", 
    "author": "David Pichardie", 
    "publish": "2010-07-19T12:40:38Z", 
    "summary": "Sun and the CERT recommend for secure Java development to not allow partially\ninitialized objects to be accessed. The CERT considers the severity of the\nrisks taken by not following this recommendation as high. The solution\ncurrently used to enforce object initialization is to implement a coding\npattern proposed by Sun, which is not formally checked. We propose a modular\ntype system to formally specify the initialization policy of libraries or\nprograms and a type checker to statically check at load time that all loaded\nclasses respect the policy. This allows to prove the absence of bugs which have\nallowed some famous privilege escalations in Java. Our experimental results\nshow that our safe default policy allows to prove 91% of classes of java.lang,\njava.security and javax.security safe without any annotation and by adding 57\nsimple annotations we proved all classes but four safe. The type system and its\nsoundness theorem have been formalized and machine checked using Coq."
},{
    "category": "cs.PL", 
    "doi": "10.1145/1512475.1512484", 
    "link": "http://arxiv.org/pdf/1007.3183v1", 
    "other_authors": "Laurent Hubert", 
    "title": "A Non-Null Annotation Inferencer for Java Bytecode", 
    "arxiv-id": "1007.3183v1", 
    "author": "Laurent Hubert", 
    "publish": "2010-07-19T15:28:00Z", 
    "summary": "We present a non-null annotations inferencer for the Java bytecode language.\nWe previously proposed an analysis to infer non-null annotations and proved it\nsoundness and completeness with respect to a state of the art type system. This\npaper proposes extensions to our former analysis in order to deal with the Java\nbytecode language. We have implemented both analyses and compared their\nbehaviour on several benchmarks. The results show a substantial improvement in\nthe precision and, despite being a whole-program analysis, production\napplications can be analyzed within minutes."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.entcs.2009.11.012", 
    "link": "http://arxiv.org/pdf/1007.3249v2", 
    "other_authors": "Laurent Hubert, David Pichardie", 
    "title": "Soundly Handling Static Fields: Issues, Semantics and Analysis", 
    "arxiv-id": "1007.3249v2", 
    "author": "David Pichardie", 
    "publish": "2010-07-19T19:45:02Z", 
    "summary": "Although in most cases class initialization works as expected, some static\nfields may be read before being initialized, despite being initialized in their\ncorresponding class initializer. We propose an analysis which compute, for each\nprogram point, the set of static fields that must have been initialized and\ndiscuss its soundness. We show that such an analysis can be directly applied to\nidentify the static fields that may be read before being initialized and to\nimprove the precision while preserving the soundness of a null-pointer\nanalysis."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-540-69611-7_8", 
    "link": "http://arxiv.org/pdf/1007.3250v1", 
    "other_authors": "Elvira Albert, Miguel G\u00f3mez-Zamalloa, Laurent Hubert, German Puebla", 
    "title": "Verification of Java Bytecode using Analysis and Transformation of Logic   Programs", 
    "arxiv-id": "1007.3250v1", 
    "author": "German Puebla", 
    "publish": "2010-07-19T19:46:43Z", 
    "summary": "State of the art analyzers in the Logic Programming (LP) paradigm are\nnowadays mature and sophisticated. They allow inferring a wide variety of\nglobal properties including termination, bounds on resource consumption, etc.\nThe aim of this work is to automatically transfer the power of such analysis\ntools for LP to the analysis and verification of Java bytecode (JVML). In order\nto achieve our goal, we rely on well-known techniques for meta-programming and\nprogram specialization. More precisely, we propose to partially evaluate a JVML\ninterpreter implemented in LP together with (an LP representation of) a JVML\nprogram and then analyze the residual program. Interestingly, at least for the\nexamples we have studied, our approach produces very simple LP representations\nof the original JVML programs. This can be seen as a decompilation from JVML to\nhigh-level LP source. By reasoning about such residual programs, we can\nautomatically prove in the CiaoPP system some non-trivial properties of JVML\nprograms such as termination, run-time error freeness and infer bounds on its\nresource consumption. We are not aware of any other system which is able to\nverify such advanced properties of Java bytecode."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18070-5_7", 
    "link": "http://arxiv.org/pdf/1007.3353v1", 
    "other_authors": "Laurent Hubert, Nicolas Barr\u00e9, Fr\u00e9d\u00e9ric Besson, Delphine Demange, Thomas Jensen, Vincent Monfort, David Pichardie, Tiphaine Turpin", 
    "title": "Sawja: Static Analysis Workshop for Java", 
    "arxiv-id": "1007.3353v1", 
    "author": "Tiphaine Turpin", 
    "publish": "2010-07-20T07:03:59Z", 
    "summary": "Static analysis is a powerful technique for automatic verification of\nprograms but raises major engineering challenges when developing a full-fledged\nanalyzer for a realistic language such as Java. This paper describes the Sawja\nlibrary: a static analysis framework fully compliant with Java 6 which provides\nOCaml modules for efficiently manipulating Java bytecode programs. We present\nthe main features of the library, including (i) efficient functional\ndata-structures for representing program with implicit sharing and lazy\nparsing, (ii) an intermediate stack-less representation, and (iii) fast\ncomputation and manipulation of complete programs."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18070-5_7", 
    "link": "http://arxiv.org/pdf/1007.3961v1", 
    "other_authors": "Pablo Chico de Guzman, Manuel Carro, David S. Warren", 
    "title": "Swapping Evaluation: A Memory-Scalable Solution for Answer-On-Demand   Tabling", 
    "arxiv-id": "1007.3961v1", 
    "author": "David S. Warren", 
    "publish": "2010-07-22T18:26:48Z", 
    "summary": "One of the differences among the various approaches to suspension-based\ntabled evaluation is the scheduling strategy. The two most popular strategies\nare local and batched evaluation.\n  The former collects all the solutions to a tabled predicate before making any\none of them available outside the tabled computation. The latter returns\nanswers one by one before computing them all, which in principle is better if\nonly one answer (or a subset of the answers) is desired.\n  Batched evaluation is closer to SLD evaluation in that it computes solutions\nlazily as they are demanded, but it may need arbitrarily more memory than local\nevaluation, which is able to reclaim memory sooner. Some programs which in\npractice can be executed under the local strategy quickly run out of memory\nunder batched evaluation. This has led to the general adoption of local\nevaluation at the expense of the more depth-first batched strategy.\n  In this paper we study the reasons for the high memory consumption of batched\nevaluation and propose a new scheduling strategy which we have termed swapping\nevaluation. Swapping evaluation also returns answers one by one before\ncompleting a tabled call, but its memory usage can be orders of magnitude less\nthan batched evaluation. An experimental implementation in the XSB system shows\nthat swapping evaluation is a feasible memory-scalable strategy that need not\ncompromise execution speed."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18070-5_7", 
    "link": "http://arxiv.org/pdf/1007.4268v1", 
    "other_authors": "Christopher Earl, Matthew Might, David Van Horn", 
    "title": "Pushdown Control-Flow Analysis of Higher-Order Programs", 
    "arxiv-id": "1007.4268v1", 
    "author": "David Van Horn", 
    "publish": "2010-07-24T13:28:46Z", 
    "summary": "Context-free approaches to static analysis gain precision over classical\napproaches by perfectly matching returns to call sites---a property that\neliminates spurious interprocedural paths. Vardoulakis and Shivers's recent\nformulation of CFA2 showed that it is possible (if expensive) to apply\ncontext-free methods to higher-order languages and gain the same boost in\nprecision achieved over first-order programs.\n  To this young body of work on context-free analysis of higher-order programs,\nwe contribute a pushdown control-flow analysis framework, which we derive as an\nabstract interpretation of a CESK machine with an unbounded stack. One\ninstantiation of this framework marks the first polyvariant pushdown analysis\nof higher-order programs; another marks the first polynomial-time analysis. In\nthe end, we arrive at a framework for control-flow analysis that can\nefficiently compute pushdown generalizations of classical control-flow\nanalyses."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18070-5_7", 
    "link": "http://arxiv.org/pdf/1007.4446v2", 
    "other_authors": "David Van Horn, Matthew Might", 
    "title": "Abstracting Abstract Machines", 
    "arxiv-id": "1007.4446v2", 
    "author": "Matthew Might", 
    "publish": "2010-07-26T13:05:10Z", 
    "summary": "We describe a derivational approach to abstract interpretation that yields\nnovel and transparently sound static analyses when applied to well-established\nabstract machines. To demonstrate the technique and support our claim, we\ntransform the CEK machine of Felleisen and Friedman, a lazy variant of\nKrivine's machine, and the stack-inspecting CM machine of Clements and\nFelleisen into abstract interpretations of themselves. The resulting analyses\nbound temporal ordering of program events; predict return-flow and\nstack-inspection behavior; and approximate the flow and evaluation of by-need\nparameters. For all of these machines, we find that a series of well-known\nconcrete machine refactorings, plus a technique we call store-allocated\ncontinuations, leads to machines that abstract into static analyses simply by\nbounding their stores. We demonstrate that the technique scales up uniformly to\nallow static analysis of realistic language features, including tail calls,\nconditionals, side effects, exceptions, first-class continuations, and even\ngarbage collection."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18070-5_7", 
    "link": "http://arxiv.org/pdf/1007.4958v2", 
    "other_authors": "Rajeev Alur, Pavol Cerny", 
    "title": "Algorithmic Verification of Single-Pass List Processing Programs", 
    "arxiv-id": "1007.4958v2", 
    "author": "Pavol Cerny", 
    "publish": "2010-07-28T13:20:17Z", 
    "summary": "We introduce streaming data string transducers that map input data strings to\noutput data strings in a single left-to-right pass in linear time. Data strings\nare (unbounded) sequences of data values, tagged with symbols from a finite\nset, over a potentially infinite data domain that supports only the operations\nof equality and ordering. The transducer uses a finite set of states, a finite\nset of variables ranging over the data domain, and a finite set of variables\nranging over data strings. At every step, it can make decisions based on the\nnext input symbol, updating its state, remembering the input data value in its\ndata variables, and updating data-string variables by concatenating data-string\nvariables and new symbols formed from data variables, while avoiding\nduplication. We establish that the problems of checking functional equivalence\nof two streaming transducers, and of checking whether a streaming transducer\nsatisfies pre/post verification conditions specified by streaming acceptors\nover input/output data-strings, are in PSPACE. We identify a class of\nimperative and a class of functional programs, manipulating lists of data\nitems, which can be effectively translated to streaming data-string\ntransducers. The imperative programs dynamically modify a singly-linked heap by\nchanging next-pointers of heap-nodes and by adding new nodes. The main\nrestriction specifies how the next-pointers can be used for traversal. We also\nidentify an expressively equivalent fragment of functional programs that\ntraverse a list using syntactically restricted recursive calls. Our results\nlead to algorithms for assertion checking and for checking functional\nequivalence of two programs, written possibly in different programming styles,\nfor commonly used routines such as insert, delete, and reverse."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1007.4986v1", 
    "other_authors": "Johannes Oetsch, J\u00f6rg P\u00fchrer, Hans Tompits", 
    "title": "Catching the Ouroboros: On Debugging Non-ground Answer-Set Programs", 
    "arxiv-id": "1007.4986v1", 
    "author": "Hans Tompits", 
    "publish": "2010-07-28T14:18:32Z", 
    "summary": "An important issue towards a broader acceptance of answer-set programming\n(ASP) is the deployment of tools which support the programmer during the coding\nphase. In particular, methods for debugging an answer-set program are\nrecognised as a crucial step in this regard. Initial work on debugging in ASP\nmainly focused on propositional programs, yet practical debuggers need to\nhandle programs with variables as well. In this paper, we discuss a debugging\ntechnique that is directly geared towards non-ground programs. Following\nprevious work, we address the central debugging question why some\ninterpretation is not an answer set. The explanations provided by our method\nare computed by means of a meta-programming technique, using a uniform encoding\nof a debugging request in terms of ASP itself. Our method also permits programs\ncontaining comparison predicates and integer arithmetics, thus covering a\nrelevant language class commonly supported by all state-of-the-art ASP solvers."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1104.1351v1", 
    "other_authors": "Guido Salvaneschi, Carlo Ghezzi, Matteo Pradella", 
    "title": "JavaCtx: Seamless Toolchain Integration for Context-Oriented Programming", 
    "arxiv-id": "1104.1351v1", 
    "author": "Matteo Pradella", 
    "publish": "2011-04-07T15:12:56Z", 
    "summary": "Context-oriented programming is an emerging paradigm addressing at the\nlanguage level the issue of dynamic software adaptation and modularization of\ncontext-specific concerns. In this paper we propose JavaCtx, a tool which\nemploys coding conventions to generate the context-aware semantics for Java\nprograms and subsequently weave it into the application. The contribution of\nJavaCtx is twofold: the design of a set of coding conventions which allow to\nwrite context-oriented software in plain Java and the concept of\ncontext-oriented semantics injection, which allows to introduce the\ncontext-aware semantics without a source-to-source compilations process which\ndisrupts the structure of the code. Both these points allow to seamless\nintegrate JavaCtx in the existing industrial-strength appliances and by far\nease the development of context-oriented software."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1104.2293v1", 
    "other_authors": "Camil Demetrescu, Irene Finocchi, Andrea Ribichini", 
    "title": "Reactive Imperative Programming with Dataflow Constraints", 
    "arxiv-id": "1104.2293v1", 
    "author": "Andrea Ribichini", 
    "publish": "2011-04-12T18:31:17Z", 
    "summary": "Dataflow languages provide natural support for specifying constraints between\nobjects in dynamic applications, where programs need to react efficiently to\nchanges of their environment. Researchers have long investigated how to take\nadvantage of dataflow constraints by embedding them into procedural languages.\nPrevious mixed imperative/dataflow systems, however, require syntactic\nextensions or libraries of ad hoc data types for binding the imperative program\nto the dataflow solver. In this paper we propose a novel approach that smoothly\ncombines the two paradigms without placing undue burden on the programmer. In\nour framework, programmers can define ordinary commands of the host imperative\nlanguage that enforce constraints between objects stored in \"reactive\" memory\nlocations. Reactive objects can be of any legal type in the host language,\nincluding primitive data types, pointers, arrays, and structures. Constraints\nare automatically re-executed every time their input memory locations change,\nletting a program behave like a spreadsheet where the values of some variables\ndepend upon the values of other variables. The constraint solving mechanism is\nhandled transparently by altering the semantics of elementary operations of the\nhost language for reading and modifying objects. We provide a formal semantics\nand describe a concrete embodiment of our technique into C/C++, showing how to\nimplement it efficiently in conventional platforms using off-the-shelf\ncompilers. We discuss relevant applications to reactive scenarios, including\nincremental computation, observer design pattern, and data structure repair.\nThe performance of our implementation is compared to ad hoc problem-specific\nchange propagation algorithms and to language-centric approaches such as\nself-adjusting computation and subject/observer communication mechanisms,\nshowing that the proposed approach is efficient in practice."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1104.3116v3", 
    "other_authors": "Eli Gottlieb", 
    "title": "Simple, Decidable Type Inference with Subtyping", 
    "arxiv-id": "1104.3116v3", 
    "author": "Eli Gottlieb", 
    "publish": "2011-04-15T17:27:30Z", 
    "summary": "We demonstrate a method to infer polymorphically principal and\nsubtyping-minimal types for an ML-like core language by assigning ranges within\na lattice to type variables. We demonstrate the termination and completeness of\nthis algorithm, and proceed to show that it solves a broad special-case of the\ngenerally-undecidable semi-unification problem. Our procedure requires no type\nannotations, leaves no subtyping constraints in the inferred types, and\nproduces no proof obligations. We demonstrate the practical utility of our\ntechnique by showing a type-preserving encoding of Featherweight Java into the\nexpression calculus over which we infer types."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1104.4306v1", 
    "other_authors": "Pavol Cerny, Krishnendu Chatterjee, Thomas Henzinger, Arjun Radhakrishna, Rohit Singh", 
    "title": "Quantitative Synthesis for Concurrent Programs", 
    "arxiv-id": "1104.4306v1", 
    "author": "Rohit Singh", 
    "publish": "2011-04-21T16:30:11Z", 
    "summary": "We present an algorithmic method for the quantitative, performance-aware\nsynthesis of concurrent programs. The input consists of a nondeterministic\npartial program and of a parametric performance model. The nondeterminism\nallows the programmer to omit which (if any) synchronization construct is used\nat a particular program location. The performance model, specified as a\nweighted automaton, can capture system architectures by assigning different\ncosts to actions such as locking, context switching, and memory and cache\naccesses. The quantitative synthesis problem is to automatically resolve the\nnondeterminism of the partial program so that both correctness is guaranteed\nand performance is optimal. As is standard for shared memory concurrency,\ncorrectness is formalized \"specification free\", in particular as race freedom\nor deadlock freedom. For worst-case (average-case) performance, we show that\nthe problem can be reduced to 2-player graph games (with probabilistic\ntransitions) with quantitative objectives. While we show, using game-theoretic\nmethods, that the synthesis problem is NEXP-complete, we present an algorithmic\nmethod and an implementation that works efficiently for concurrent programs and\nperformance models of practical interest. We have implemented a prototype tool\nand used it to synthesize finite-state concurrent programs that exhibit\ndifferent programming patterns, for several performance models representing\ndifferent architectures."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1204.0221v1", 
    "other_authors": "Youssef Bassil, Aziz Barbar", 
    "title": "MyProLang - My Programming Language: A Template-Driven Automatic Natural   Programming Language", 
    "arxiv-id": "1204.0221v1", 
    "author": "Aziz Barbar", 
    "publish": "2012-04-01T15:35:11Z", 
    "summary": "Modern computer programming languages are governed by complex syntactic\nrules. They are unlike natural languages; they require extensive manual work\nand a significant amount of learning and practicing for an individual to become\nskilled at and to write correct programs. Computer programming is a difficult,\ncomplicated, unfamiliar, non-automated, and a challenging discipline for\neveryone; especially, for students, new programmers and end-users. This paper\nproposes a new programming language and an environment for writing computer\napplications based on source-code generation. It is mainly a template-driven\nautomatic natural imperative programming language called MyProLang. It\nharnesses GUI templates to generate proprietary natural language source-code,\ninstead of having computer programmers write the code manually. MyProLang is a\nblend of five elements. A proprietary natural programming language with\nunsophisticated grammatical rules and expressive syntax; automation templates\nthat automate the generation of instructions and thereby minimizing the\nlearning and training time; an NLG engine to generate natural instructions; a\nsource-to-source compiler that analyzes, parses, and build executables; and an\nergonomic IDE that houses diverse functions whose role is to simplify the\nsoftware development process. MyProLang is expected to make programming open to\neveryone including students, programmers and end-users. In that sense, anyone\ncan start programming systematically, in an automated manner and in natural\nlanguage; without wasting time in learning how to formulate instructions and\narrange expressions, without putting up with unfamiliar structures and symbols,\nand without being annoyed by syntax errors. In the long run, this increases the\nproductivity, quality and time-to-market in software development."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1204.1147v1", 
    "other_authors": "Thomas Martin Gawlitza, Helmut Seidl", 
    "title": "Numerical Invariants through Convex Relaxation and Max-Strategy   Iteration", 
    "arxiv-id": "1204.1147v1", 
    "author": "Helmut Seidl", 
    "publish": "2012-04-05T08:33:16Z", 
    "summary": "In this article we develop a max-strategy improvement algorithm for computing\nleast fixpoints of operators on on the reals that are point-wise maxima of\nfinitely many monotone and order-concave operators. Computing the uniquely\ndetermined least fixpoint of such operators is a problem that occurs frequently\nin the context of numerical program/systems verification/analysis. As an\nexample for an application we discuss how our algorithm can be applied to\ncompute numerical invariants of programs by abstract interpretation based on\nquadratic templates."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068410000256", 
    "link": "http://arxiv.org/pdf/1204.1568v5", 
    "other_authors": "Georg Moser, Michael Schaper", 
    "title": "A Complexity Preserving Transformation from Jinja Bytecode to Rewrite   Systems", 
    "arxiv-id": "1204.1568v5", 
    "author": "Michael Schaper", 
    "publish": "2012-04-06T21:38:22Z", 
    "summary": "We revisit known transformations from Jinja bytecode to rewrite systems from\nthe viewpoint of runtime complexity. Suitably generalising the constructions\nproposed in the literature, we define an alternative representation of Jinja\nbytecode (JBC) executions as \"computation graphs\" from which we obtain a novel\nrepresentation of JBC executions as \"constrained rewrite systems\". We prove\nnon-termination and complexity preservation of the transformation. We restrict\nto well-formed JBC programs that only make use of non-recursive methods and\nexpect tree-shaped objects as input. Our approach allows for simplified\ncorrectness proofs and provides a framework for the combination of the\ncomputation graph method with standard techniques from static program analysis\nlike for example \"reachability analysis\"."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1204.4322v3", 
    "other_authors": "Thomas Jensen, Florent Kirchner, David Pichardie", 
    "title": "Secure the Clones", 
    "arxiv-id": "1204.4322v3", 
    "author": "David Pichardie", 
    "publish": "2012-04-19T11:49:03Z", 
    "summary": "Exchanging mutable data objects with untrusted code is a delicate matter\nbecause of the risk of creating a data space that is accessible by an attacker.\nConsequently, secure programming guidelines for Java stress the importance of\nusing defensive copying before accepting or handing out references to an\ninternal mutable object. However, implementation of a copy method (like\nclone()) is entirely left to the programmer. It may not provide a sufficiently\ndeep copy of an object and is subject to overriding by a malicious sub-class.\nCurrently no language-based mechanism supports secure object cloning. This\npaper proposes a type-based annotation system for defining modular copy\npolicies for class-based object-oriented programs. A copy policy specifies the\nmaximally allowed sharing between an object and its clone. We present a static\nenforcement mechanism that will guarantee that all classes fulfil their copy\npolicy, even in the presence of overriding of copy methods, and establish the\nsemantic correctness of the overall approach in Coq. The mechanism has been\nimplemented and experimentally evaluated on clone methods from several Java\nlibraries."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1204.5280v1", 
    "other_authors": "Armando Gon\u00e7alves Da Silva Junior, Pierre Deransart, Luis-Carlos Menezes, Marcos-Aur\u00e9lio Almeida Da Silva, Jacques Robin", 
    "title": "Towards a Generic Trace for Rule Based Constraint Reasoning", 
    "arxiv-id": "1204.5280v1", 
    "author": "Jacques Robin", 
    "publish": "2012-04-24T06:57:10Z", 
    "summary": "CHR is a very versatile programming language that allows programmers to\ndeclaratively specify constraint solvers. An important part of the development\nof such solvers is in their testing and debugging phases. Current CHR\nimplementations support those phases by offering tracing facilities with\nlimited information. In this report, we propose a new trace for CHR which\ncontains enough information to analyze any aspects of \\CHRv\\ execution at some\nuseful abstract level, common to several implementations. %a large family of\nrule based solvers. This approach is based on the idea of generic trace. Such a\ntrace is formally defined as an extension of the $\\omega_r^\\lor$ semantics of\nCHR. We show that it can be derived form the SWI Prolog CHR trace."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1207.1813v1", 
    "other_authors": "Christopher Earl, Ilya Sergey, Matthew Might, David Van Horn", 
    "title": "Introspective Pushdown Analysis of Higher-Order Programs", 
    "arxiv-id": "1207.1813v1", 
    "author": "David Van Horn", 
    "publish": "2012-07-07T16:56:04Z", 
    "summary": "In the static analysis of functional programs, pushdown flow analysis and\nabstract garbage collection skirt just inside the boundaries of soundness and\ndecidability. Alone, each method reduces analysis times and boosts precision by\norders of magnitude. This work illuminates and conquers the theoretical\nchallenges that stand in the way of combining the power of these techniques.\nThe challenge in marrying these techniques is not subtle: computing the\nreachable control states of a pushdown system relies on limiting access during\ntransition to the top of the stack; abstract garbage collection, on the other\nhand, needs full access to the entire stack to compute a root set, just as\nconcrete collection does. \\emph{Introspective} pushdown systems resolve this\nconflict. Introspective pushdown systems provide enough access to the stack to\nallow abstract garbage collection, but they remain restricted enough to compute\ncontrol-state reachability, thereby enabling the sound and precise product of\npushdown analysis and abstract garbage collection. Experiments reveal\nsynergistic interplay between the techniques, and the fusion demonstrates\n\"better-than-both-worlds\" precision."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1207.2017v1", 
    "other_authors": "Joachim Breitner", 
    "title": "dup -- Explicit un-sharing in Haskell", 
    "arxiv-id": "1207.2017v1", 
    "author": "Joachim Breitner", 
    "publish": "2012-07-09T12:05:25Z", 
    "summary": "We propose two operations to prevent sharing in Haskell that do not require\nmodifying the data generating code, demonstrate their use and usefulness, and\ncompare them to other approaches to preventing sharing. Our claims are\nsupported by a formal semantics and a prototype implementation."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1207.2544v2", 
    "other_authors": "Sandeep Bindal, Sorav Bansal, Akash Lal", 
    "title": "Variable and Thread Bounding for Systematic Testing of Multithreaded   Programs", 
    "arxiv-id": "1207.2544v2", 
    "author": "Akash Lal", 
    "publish": "2012-07-11T06:02:30Z", 
    "summary": "Previous approaches to systematic state-space exploration for testing\nmulti-threaded programs have proposed context-bounding and depth-bounding to be\neffective ranking algorithms for testing multithreaded programs. This paper\nproposes two new metrics to rank thread schedules for systematic state-space\nexploration. Our metrics are based on characterization of a concurrency bug\nusing v (the minimum number of distinct variables that need to be involved for\nthe bug to manifest) and t (the minimum number of distinct threads among which\nscheduling constraints are required to manifest the bug). Our algorithm is\nbased on the hypothesis that in practice, most concurrency bugs have low v\n(typically 1- 2) and low t (typically 2-4) characteristics. We iteratively\nexplore the search space of schedules in increasing orders of v and t. We show\nqualitatively and empirically that our algorithm finds common bugs in fewer\nnumber of execution runs, compared with previous approaches. We also show that\nusing v and t improves the lower bounds on the probability of finding bugs\nthrough randomized algorithms.\n  Systematic exploration of schedules requires instrumenting each variable\naccess made by a program, which can be very expensive and severely limits the\napplicability of this approach. Previous work [5, 19] has avoided this problem\nby interposing only on synchronization operations (and ignoring other variable\naccesses). We demonstrate that by using variable bounding (v) and a static\nimprecise alias analysis, we can interpose on all variable accesses (and not\njust synchronization operations) at 10-100x less overhead than previous\napproaches."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:5)2012", 
    "link": "http://arxiv.org/pdf/1207.6369v1", 
    "other_authors": "T. Gregorics", 
    "title": "Concept of the abstract program", 
    "arxiv-id": "1207.6369v1", 
    "author": "T. Gregorics", 
    "publish": "2012-07-26T18:56:59Z", 
    "summary": "The aim of this paper is to alter the abstract definition of the program of\nthe theoretical programming model which has been developed at Eotvos Lorand\nUniversity for many years in order to investigate methods that support\ndesigning correct programs. The motivation of this modification was that the\ndynamic properties of programs appear in the model. This new definition of the\nprogram gives a hand to extend the model with the concept of subprograms while\nthe earlier results of the original programming model are preserved."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841200035X", 
    "link": "http://arxiv.org/pdf/1207.6816v1", 
    "other_authors": "Lee Naish", 
    "title": "Transforming floundering into success", 
    "arxiv-id": "1207.6816v1", 
    "author": "Lee Naish", 
    "publish": "2012-07-30T01:11:12Z", 
    "summary": "We show how logic programs with \"delays\" can be transformed to programs\nwithout delays in a way which preserves information concerning floundering\n(also known as deadlock). This allows a declarative (model-theoretic),\nbottom-up or goal independent approach to be used for analysis and debugging of\nproperties related to floundering. We rely on some previously introduced\nrestrictions on delay primitives and a key observation which allows properties\nsuch as groundness to be analysed by approximating the (ground) success set.\nThis paper is to appear in Theory and Practice of Logic Programming (TPLP).\n  Keywords: Floundering, delays, coroutining, program analysis, abstract\ninterpretation, program transformation, declarative debugging"
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000288", 
    "link": "http://arxiv.org/pdf/1307.0679v1", 
    "other_authors": "Maurizio Gabbrielli, Maria Chiara Meo, Paolo Tacchella, Herbert Wiklicky", 
    "title": "Unfolding for CHR programs", 
    "arxiv-id": "1307.0679v1", 
    "author": "Herbert Wiklicky", 
    "publish": "2013-07-02T10:55:24Z", 
    "summary": "Program transformation is an appealing technique which allows to improve\nrun-time efficiency, space-consumption, and more generally to optimize a given\nprogram. Essentially, it consists of a sequence of syntactic program\nmanipulations which preserves some kind of semantic equivalence. Unfolding is\none of the basic operations which is used by most program transformation\nsystems and which consists in the replacement of a procedure call by its\ndefinition. While there is a large body of literature on transformation and\nunfolding of sequential programs, very few papers have addressed this issue for\nconcurrent languages.\n  This paper defines an unfolding system for CHR programs. We define an\nunfolding rule, show its correctness and discuss some conditions which can be\nused to delete an unfolded rule while preserving the program meaning. We also\nprove that, under some suitable conditions, confluence and termination are\npreserved by the above transformation.\n  To appear in Theory and Practice of Logic Programming (TPLP)"
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000288", 
    "link": "http://arxiv.org/pdf/1307.2328v1", 
    "other_authors": "Bertram Felgenhauer, Martin Avanzini, Christian Sternagel", 
    "title": "A Haskell Library for Term Rewriting", 
    "arxiv-id": "1307.2328v1", 
    "author": "Christian Sternagel", 
    "publish": "2013-07-09T03:45:41Z", 
    "summary": "We present a Haskell library for first-order term rewriting covering basic\noperations on positions, terms, contexts, substitutions and rewrite rules. This\neffort is motivated by the increasing number of term rewriting tools that are\nwritten in Haskell."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000288", 
    "link": "http://arxiv.org/pdf/1307.2473v1", 
    "other_authors": "Dan R. Ghica, Alex Smith", 
    "title": "From bounded affine types to automatic timing analysis", 
    "arxiv-id": "1307.2473v1", 
    "author": "Alex Smith", 
    "publish": "2013-07-09T14:29:24Z", 
    "summary": "Bounded linear types have proved to be useful for automated resource analysis\nand control in functional programming languages. In this paper we introduce an\naffine bounded linear typing discipline on a general notion of resource which\ncan be modeled in a semiring. For this type system we provide both a general\ntype-inference procedure, parameterized by the decision procedure of the\nsemiring equational theory, and a (coherent) categorical semantics. This is a\nvery useful type-theoretic and denotational framework for many applications to\nresource-sensitive compilation, and it represents a generalization of several\nexisting type systems. As a non-trivial instance, motivated by our ongoing work\non hardware compilation, we present a complex new application to calculating\nand controlling timing of execution in a (recursion-free) higher-order\nfunctional programming language with local store."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4474v1", 
    "other_authors": "Alessandra Di Pierro, Herbert Wiklicky", 
    "title": "Probabilistic data flow analysis: a linear equational approach", 
    "arxiv-id": "1307.4474v1", 
    "author": "Herbert Wiklicky", 
    "publish": "2013-07-17T01:42:29Z", 
    "summary": "Speculative optimisation relies on the estimation of the probabilities that\ncertain properties of the control flow are fulfilled. Concrete or estimated\nbranch probabilities can be used for searching and constructing advantageous\nspeculative and bookkeeping transformations.\n  We present a probabilistic extension of the classical equational approach to\ndata-flow analysis that can be used to this purpose. More precisely, we show\nhow the probabilistic information introduced in a control flow graph by branch\nprediction can be used to extract a system of linear equations from a program\nand present a method for calculating correct (numerical) solutions."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4585v1", 
    "other_authors": "Michal Terepeta, Hanne Riis Nielson, Flemming Nielson", 
    "title": "Pushdown Systems for Monotone Frameworks", 
    "arxiv-id": "1307.4585v1", 
    "author": "Flemming Nielson", 
    "publish": "2013-07-17T11:36:15Z", 
    "summary": "Monotone frameworks is one of the most successful frameworks for\nintraprocedural data flow analysis extending the traditional class of bitvector\nframeworks (like live variables and available expressions). Weighted pushdown\nsystems is similarly one of the most general frameworks for interprocedural\nanalysis of programs. However, it makes use of idempotent semirings to\nrepresent the sets of properties and unfortunately they do not admit analyses\nwhose transfer functions are not strict (e.g., classical bitvector frameworks).\nThis motivates the development of algorithms for backward and forward\nreachability of pushdown systems using sets of properties forming so-called\nflow algebras that weaken some of the assumptions of idempotent semirings. In\nparticular they do admit the bitvector frameworks, monotone frameworks, as well\nas idempotent semirings. We show that the algorithms are sound under mild\nassumptions on the flow algebras, mainly that the set of properties constitutes\na join semi-lattice, and complete provided that the transfer functions are\nsuitably distributive (but not necessarily strict)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4641v1", 
    "other_authors": "Danny Munera, Daniel Diaz, Salvador Abreu", 
    "title": "Experimenting with X10 for Parallel Constraint-Based Local Search", 
    "arxiv-id": "1307.4641v1", 
    "author": "Salvador Abreu", 
    "publish": "2013-07-17T14:13:50Z", 
    "summary": "In this study, we have investigated the adequacy of the PGAS parallel\nlanguage X10 to implement a Constraint-Based Local Search solver. We decided to\ncode in this language to benefit from the ease of use and architectural\nindependence from parallel resources which it offers. We present the\nimplementation strategy, in search of different sources of parallelism in the\ncontext of an implementation of the Adaptive Search algorithm. We extensively\ndiscuss the algorithm and its implementation. The performance evaluation on a\nrepresentative set of benchmarks shows close to linear speed-ups, in all the\nproblems treated."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4642v1", 
    "other_authors": "Paul Tarau", 
    "title": "A Prolog Specification of Giant Number Arithmetic", 
    "arxiv-id": "1307.4642v1", 
    "author": "Paul Tarau", 
    "publish": "2013-07-17T14:14:08Z", 
    "summary": "The tree based representation described in this paper, hereditarily binary\nnumbers, applies recursively a run-length compression mechanism that enables\ncomputations limited by the structural complexity of their operands rather than\nby their bitsizes. While within constant factors from their traditional\ncounterparts for their worst case behavior, our arithmetic operations open the\ndoors for interesting numerical computations, impossible with traditional\nnumber representations. We provide a complete specification of our algorithms\nin the form of a purely declarative Prolog program."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4644v1", 
    "other_authors": "David S. Warren", 
    "title": "Interning Ground Terms in XSB", 
    "arxiv-id": "1307.4644v1", 
    "author": "David S. Warren", 
    "publish": "2013-07-17T14:20:15Z", 
    "summary": "This paper presents an implementation of interning of ground terms in the XSB\nTabled Prolog system. This is related to the idea of hash-consing. I describe\nthe concept of interning atoms and discuss the issues around interning ground\nstructured terms, motivating why tabling Prolog systems may change the\ncost-benefit tradeoffs from those of traditional Prolog systems. I describe the\ndetails of the implementation of interning ground terms in the XSB Tabled\nProlog System and show some of its performance properties. This implementation\nachieves the effects of that of Zhou and Have but is tuned for XSB's\nrepresentations and is arguably simpler."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4648v1", 
    "other_authors": "Michael Hanus, Fabian Reck", 
    "title": "A Generic Analysis Server System for Functional Logic Programs", 
    "arxiv-id": "1307.4648v1", 
    "author": "Fabian Reck", 
    "publish": "2013-07-17T14:30:17Z", 
    "summary": "We present a system, called CASS, for the analysis of functional logic\nprograms. The system is generic so that various kinds of analyses (e.g.,\ngroundness, non-determinism, demanded arguments) can be easily integrated. In\norder to analyze larger applications consisting of dozens or hundreds of\nmodules, CASS supports a modular and incremental analysis of programs.\nMoreover, it can be used by different programming tools, like documentation\ngenerators, analysis environments, program optimizers, as well as Eclipse-based\ndevelopment environments. For this purpose, CASS can also be invoked as a\nserver system to get a language-independent access to its functionality. CASS\nis completely implemented in the functional logic language Curry as a\nmaster/worker architecture to exploit parallel or distributed execution\nenvironments."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4655v1", 
    "other_authors": "Igor St\u00e9phan", 
    "title": "Compilation for QCSP", 
    "arxiv-id": "1307.4655v1", 
    "author": "Igor St\u00e9phan", 
    "publish": "2013-07-17T14:42:55Z", 
    "summary": "We propose in this article a framework for compilation of quantified\nconstraint satisfaction problems (QCSP). We establish the semantics of this\nformalism by an interpretation to a QCSP. We specify an algorithm to compile a\nQCSP embedded into a search algorithm and based on the inductive semantics of\nQCSP. We introduce an optimality property and demonstrate the optimality of the\ninterpretation of the compiled QCSP."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.4827v1", 
    "other_authors": "Raphael 'kena' Poss", 
    "title": "Characterizing traits of coordination", 
    "arxiv-id": "1307.4827v1", 
    "author": "Raphael 'kena' Poss", 
    "publish": "2013-07-18T04:06:49Z", 
    "summary": "How can one recognize coordination languages and technologies? As this report\nshows, the common approach that contrasts coordination with computation is\nintellectually unsound: depending on the selected understanding of the word\n\"computation\", it either captures too many or too few programming languages.\nInstead, we argue for objective criteria that can be used to evaluate how well\nprogramming technologies offer coordination services. Of the various criteria\ncommonly used in this community, we are able to isolate three that are strongly\ncharacterizing: black-box componentization, which we had identified previously,\nbut also interface extensibility and customizability of run-time optimization\ngoals. These criteria are well matched by Intel's Concurrent Collections and\nAstraKahn, and also by OpenCL, POSIX and VMWare ESX."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.119.14", 
    "link": "http://arxiv.org/pdf/1307.5277v1", 
    "other_authors": "Neal Glew, Tim Sweeney, Leaf Petersen", 
    "title": "Formalisation of the lambda aleph Runtime", 
    "arxiv-id": "1307.5277v1", 
    "author": "Leaf Petersen", 
    "publish": "2013-07-19T16:38:25Z", 
    "summary": "In previous work we describe a novel approach to dependent typing based on a\nmultivalued term language. In this technical report we formalise the runtime, a\nkind of operational semantics, for that language. We describe a fairly\ncomprehensive core language, and then give a small-step operational semantics\nbased on an abstract machine. Errors are explicit in the semantics. We also\nprove several simple properties: that every non-terminated machine state steps\nto something and that reduction is deterministic once input is fixed."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628156", 
    "link": "http://arxiv.org/pdf/1307.6239v4", 
    "other_authors": "Phuc C. Nguyen, Sam Tobin-Hochstadt, David Van Horn", 
    "title": "Soft Contract Verification", 
    "arxiv-id": "1307.6239v4", 
    "author": "David Van Horn", 
    "publish": "2013-07-23T20:30:46Z", 
    "summary": "Behavioral software contracts are a widely used mechanism for governing the\nflow of values between components. However, run-time monitoring and enforcement\nof contracts imposes significant overhead and delays discovery of faulty\ncomponents to run-time.\n  To overcome these issues, we present soft contract verification, which aims\nto statically prove either complete or partial contract correctness of\ncomponents, written in an untyped, higher-order language with first-class\ncontracts. Our approach uses higher-order symbolic execution, leveraging\ncontracts as a source of symbolic values including unknown behavioral values,\nand employs an updatable heap of contract invariants to reason about\nflow-sensitive facts. We prove the symbolic execution soundly approximates the\ndynamic semantics and that verified programs can't be blamed.\n  The approach is able to analyze first-class contracts, recursive data\nstructures, unknown functions, and control-flow-sensitive refinements of\nvalues, which are all idiomatic in dynamic languages. It makes effective use of\nan off-the-shelf solver to decide problems without heavy encodings. The\napproach is competitive with a wide range of existing tools---including type\nsystems, flow analyzers, and model checkers---on their own benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628156", 
    "link": "http://arxiv.org/pdf/1307.7261v1", 
    "other_authors": "Jerzy Tyszkiewicz", 
    "title": "The Power of Spreadsheet Computations", 
    "arxiv-id": "1307.7261v1", 
    "author": "Jerzy Tyszkiewicz", 
    "publish": "2013-07-27T14:10:33Z", 
    "summary": "We investigate the expressive power of spreadsheets. We consider spreadsheets\nwhich contain only formulas, and assume that they are small templates, which\ncan be filled to a larger area of the grid to process input data of variable\nsize. Therefore we can compare them to well-known machine models of\ncomputation. We consider a number of classes of spreadsheets defined by\nrestrictions on their reference structure. Two of the classes correspond\nclosely to parallel complexity classes: we prove a direct correspondence\nbetween the dimensions of the spreadsheet and amount of hardware and time used\nby a parallel computer to compute the same function. As a tool, we produce\nspreadsheets which are universal in these classes, i.e. can emulate any other\nspreadsheet from them. In other cases we implement in the spreadsheets in\nquestion instances of a polynomial-time complete problem, which indicates that\nthe the spreadsheets are unlikely to have efficient parallel evaluation\nalgorithms. Thus we get a picture how the computational power of spreadsheets\ndepends on their dimensions and structure of references."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628156", 
    "link": "http://arxiv.org/pdf/1307.7281v2", 
    "other_authors": "Roopsha Samanta, Oswaldo Olivo, E. Allen Emerson", 
    "title": "Cost-Aware Automatic Program Repair", 
    "arxiv-id": "1307.7281v2", 
    "author": "E. Allen Emerson", 
    "publish": "2013-07-27T16:57:03Z", 
    "summary": "We present a formal framework for repairing infinite-state, imperative,\nsequential programs, with (possibly recursive) procedures and multiple\nassertions; the framework can generate repaired programs by modifying the\noriginal erroneous program in multiple program locations, and can ensure the\nreadability of the repaired program using user-defined expression templates;\nthe framework also generates a set of inductive assertions that serve as a\nproof of correctness of the repaired program. As a step toward integrating\nprogrammer intent and intuition in automated program repair, we present a \"\ncost-aware\" formulation - given a cost function associated with permissible\nstatement modifications, the goal is to ensure that the total program\nmodification cost does not exceed a given repair budget. As part of our\npredicate abstraction-based solution framework, we present a sound and complete\nalgorithm for repair of Boolean programs. We have developed a prototype tool\nbased on SMT solving and used it successfully to repair diverse errors in\nbenchmark C programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1307.8204v1", 
    "other_authors": "Joshua Dunfield", 
    "title": "Annotations for Intersection Typechecking", 
    "arxiv-id": "1307.8204v1", 
    "author": "Joshua Dunfield", 
    "publish": "2013-07-31T03:24:25Z", 
    "summary": "In functional programming languages, the classic form of annotation is a\nsingle type constraint on a term. Intersection types add complications: a\nsingle term may have to be checked several times against different types, in\ndifferent contexts, requiring annotation with several types. Moreover, it is\nuseful (in some systems, necessary) to indicate the context in which each such\ntype is to be used.\n  This paper explores the technical design space of annotations in systems with\nintersection types. Earlier work (Dunfield and Pfenning 2004) introduced\ncontextual typing annotations, which we now tease apart into more elementary\nmechanisms: a \"right hand\" annotation (the standard form), a \"left hand\"\nannotation (the context in which a right-hand annotation is to be used), a\nmerge that allows for multiple annotations, and an existential binder for index\nvariables. The most novel element is the left-hand annotation, which guards\nterms (and right-hand annotations) with a judgment that must follow from the\ncurrent context."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1307.8389v1", 
    "other_authors": "Ricardo Rocha, Christian Theil Have", 
    "title": "Proceedings of the 13th International Colloquium on Implementation of   Constraint and LOgic Programming Systems", 
    "arxiv-id": "1307.8389v1", 
    "author": "Christian Theil Have", 
    "publish": "2013-07-31T17:03:11Z", 
    "summary": "This volume contains the proceedings of the 13th International Colloquium on\nImplementation of Constraint and LOgic Programming Systems (CICLOPS 2013), held\nin Istanbul, Turkey during August 25, 2013. CICLOPS is a well established line\nof workshops, traditionally co-located with ICLP, that aims at discussing and\nexchanging experience on the design, implementation, and optimization of\nconstraint and logic programming systems, and other systems based on logic as a\nmeans of expressing computations. This year, CICLOPS received 8 paper\nsubmissions. Each submission was reviewed by at least 3 Program Committee\nmembers and, at the end, 6 papers were accepted for presentation at the\nworkshop. We would like to thank the ICLP organizers for their support, the\nEasyChair conference management system for making the life of the program\nchairs easier and arxiv.org for providing permanent hosting. Thanks should go\nalso to the authors of all submitted papers for their contribution to make\nCICLOPS alive and to the participants for making the event a meeting point for\na fruitful exchange of ideas and feedback on recent developments. Finally, we\nwant to express our gratitude to the Program Committee members, as the\nsymposium would not have been possible without their dedicated work."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1407.0729v6", 
    "other_authors": "Satoshi Egi", 
    "title": "Non-Linear Pattern-Matching against Unfree Data Types with Lexical   Scoping", 
    "arxiv-id": "1407.0729v6", 
    "author": "Satoshi Egi", 
    "publish": "2014-07-02T21:42:02Z", 
    "summary": "This paper proposes a pattern-matching system that enables non-linear\npattern-matching against unfree data types. The system allows multiple\noccurrences of the same variables in a pattern, multiple results of\npattern-matching and modularization of the way of pattern-matching for each\ndata type at the same time. It enables us to represent pattern-matching against\nnot only algebraic data types but also unfree data types such as sets, graphs\nand any other data types whose data have no canonical form and multiple ways of\ndecomposition. I have realized that with a rule that pattern-matching is\nexecuted from the left side of a pattern and a rule that a binding to a\nvariable in a pattern can be referred to in its right side of the pattern.\nFurthermore, I have realized modularization of these patterns with lexical\nscoping. In my system, a pattern is not a first class object, but a\npattern-function that obtains only patterns and returns a pattern is a first\nclass object. This restriction simplifies the non-linear pattern-matching\nsystem with lexical scoping. I have already implemented the pattern-matching\nsystem in the Egison programming language."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1407.0970v4", 
    "other_authors": "Mila Dalla Preda, Maurizio Gabbrielli, Saverio Giallorenzo, Ivan Lanese, Jacopo Mauro", 
    "title": "Dynamic Choreographies - Safe Runtime Updates of Distributed   Applications", 
    "arxiv-id": "1407.0970v4", 
    "author": "Jacopo Mauro", 
    "publish": "2014-07-03T16:12:12Z", 
    "summary": "Programming distributed applications free from communication deadlocks and\nraces is complex. Preserving these properties when applications are updated at\nruntime is even harder. We present DIOC, a language for programming distributed\napplications that are free from deadlocks and races by construction. A DIOC\nprogram describes a whole distributed application as a unique entity\n(choreography). DIOC allows the programmer to specify which parts of the\napplication can be updated. At runtime, these parts may be replaced by new DIOC\nfragments from outside the application. DIOC programs are compiled, generating\ncode for each site, in a lower-level language called DPOC. We formalise both\nDIOC and DPOC semantics as labelled transition systems and prove the\ncorrectness of the compilation as a trace equivalence result. As corollaries,\nDPOC applications are free from communication deadlocks and races, even in\npresence of runtime updates."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1407.0975v3", 
    "other_authors": "Mila Dalla Preda, Saverio Giallorenzo, Ivan Lanese, Jacopo Mauro, Maurizio Gabbrielli", 
    "title": "AIOCJ: A Choreographic Framework for Safe Adaptive Distributed   Applications", 
    "arxiv-id": "1407.0975v3", 
    "author": "Maurizio Gabbrielli", 
    "publish": "2014-07-03T16:14:57Z", 
    "summary": "We present AIOCJ, a framework for programming distributed adaptive\napplications. Applications are programmed using AIOC, a choreographic language\nsuited for expressing patterns of interaction from a global point of view. AIOC\nallows the programmer to specify which parts of the application can be adapted.\nAdaptation takes place at runtime by means of rules, which can change during\nthe execution to tackle possibly unforeseen adaptation needs. AIOCJ relies on a\nsolid theory that ensures applications to be deadlock-free by construction also\nafter adaptation. We describe the architecture of AIOCJ, the design of the AIOC\nlanguage, and an empirical validation of the framework."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1407.1545v1", 
    "other_authors": "Mary Southern, Gopalan Nadathur", 
    "title": "A Lambda Prolog Based Animation of Twelf Specifications", 
    "arxiv-id": "1407.1545v1", 
    "author": "Gopalan Nadathur", 
    "publish": "2014-07-06T20:56:24Z", 
    "summary": "Specifications in the Twelf system are based on a logic programming\ninterpretation of the Edinburgh Logical Framework or LF. We consider an\napproach to animating such specifications using a Lambda Prolog implementation.\nThis approach is based on a lossy translation of the dependently typed LF\nexpressions into the simply typed lambda calculus (STLC) terms of Lambda Prolog\nand a subsequent encoding of lost dependency information in predicates that are\ndefined by suitable clauses. To use this idea in an implementation of logic\nprogramming a la Twelf, it is also necessary to translate the results found for\nLambda Prolog queries back into LF expressions. We describe such an inverse\ntranslation and show that it has the necessary properties to facilitate an\nemulation of Twelf behavior through our translation of LF specifications into\nLambda Prolog programs. A characteristic of Twelf is that it permits queries to\nconsist of types which have unspecified parts represented by meta-variables for\nwhich values are to be found through computation. We show that this capability\ncan be supported within our translation based approach to animating Twelf\nspecifications."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.121.3", 
    "link": "http://arxiv.org/pdf/1407.1873v1", 
    "other_authors": "Olivier Bodini, Antoine Genitrini, Fr\u00e9d\u00e9ric Peschanski", 
    "title": "A Quantitative Study of Pure Parallel Processes", 
    "arxiv-id": "1407.1873v1", 
    "author": "Fr\u00e9d\u00e9ric Peschanski", 
    "publish": "2014-07-04T19:43:55Z", 
    "summary": "In this paper, we study the interleaving -- or pure merge -- operator that\nmost often characterizes parallelism in concurrency theory. This operator is a\nprincipal cause of the so-called combinatorial explosion that makes very hard -\nat least from the point of view of computational complexity - the analysis of\nprocess behaviours e.g. by model-checking. The originality of our approach is\nto study this combinatorial explosion phenomenon on average, relying on\nadvanced analytic combinatorics techniques. We study various measures that\ncontribute to a better understanding of the process behaviours represented as\nplane rooted trees: the number of runs (corresponding to the width of the\ntrees), the expected total size of the trees as well as their overall shape.\nTwo practical outcomes of our quantitative study are also presented: (1) a\nlinear-time algorithm to compute the probability of a concurrent run prefix,\nand (2) an efficient algorithm for uniform random sampling of concurrent runs.\nThese provide interesting responses to the combinatorial explosion problem."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/1407.3845v1", 
    "other_authors": "Jeff Bezanson, Jiahao Chen, Stefan Karpinski, Viral Shah, Alan Edelman", 
    "title": "Array operators using multiple dispatch: a design methodology for array   implementations in dynamic languages", 
    "arxiv-id": "1407.3845v1", 
    "author": "Alan Edelman", 
    "publish": "2014-07-14T23:13:17Z", 
    "summary": "Arrays are such a rich and fundamental data type that they tend to be built\ninto a language, either in the compiler or in a large low-level library.\nDefining this functionality at the user level instead provides greater\nflexibility for application domains not envisioned by the language designer.\nOnly a few languages, such as C++ and Haskell, provide the necessary power to\ndefine $n$-dimensional arrays, but these systems rely on compile-time\nabstraction, sacrificing some flexibility. In contrast, dynamic languages make\nit straightforward for the user to define any behavior they might want, but at\nthe possible expense of performance.\n  As part of the Julia language project, we have developed an approach that\nyields a novel trade-off between flexibility and compile-time analysis. The\ncore abstraction we use is multiple dispatch. We have come to believe that\nwhile multiple dispatch has not been especially popular in most kinds of\nprogramming, technical computing is its killer application. By expressing key\nfunctions such as array indexing using multi-method signatures, a surprising\nrange of behaviors can be obtained, in a way that is both relatively easy to\nwrite and amenable to compiler analysis. The compact factoring of concerns\nprovided by these methods makes it easier for user-defined types to behave\nconsistently with types in the standard library."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/1407.5670v1", 
    "other_authors": "Raphael Poss", 
    "title": "Rust for functional programmers", 
    "arxiv-id": "1407.5670v1", 
    "author": "Raphael Poss", 
    "publish": "2014-07-21T21:20:31Z", 
    "summary": "This article provides an introduction to Rust, a systems language by Mozilla,\nto programmers already familiar with Haskell, OCaml or other functional\nlanguages."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/1407.6124v1", 
    "other_authors": "Duc-Hiep Chu, Joxan Jaffar, Minh-Thai Trinh", 
    "title": "Automating Proofs of Data-Structure Properties in Imperative Programs", 
    "arxiv-id": "1407.6124v1", 
    "author": "Minh-Thai Trinh", 
    "publish": "2014-07-23T08:06:53Z", 
    "summary": "We consider the problem of automated reasoning about dynamically manipulated\ndata structures. The state-of-the-art methods are limited to the\nunfold-and-match (U+M) paradigm, where predicates are transformed via\n(un)folding operations induced from their definitions before being treated as\nuninterpreted. However, proof obligations from verifying programs with\niterative loops and multiple function calls often do not succumb to this\nparadigm. Our contribution is a proof method which -- beyond U+M -- performs\nautomatic formula re-writing by treating previously encountered obligations in\neach proof path as possible induction hypotheses. This enables us, for the\nfirst time, to systematically reason about a wide range of obligations, arising\nfrom practical program verification. We demonstrate the power of our proof\nrules on commonly used lemmas, thereby close the remaining gaps in existing\nstate-of-the-art systems. Another impact, probably more important, is that our\nmethod regains the power of compositional reasoning, and shows that the usage\nof user-provided lemmas is no longer needed for the existing set of benchmarks.\nThis not only removes the burden of coming up with the appropriate lemmas, but\nalso significantly boosts up the verification process, since lemma\napplications, coupled with unfolding, often induce very large search space."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/1407.6968v1", 
    "other_authors": "Dave Dice, Timothy L. Harris, Alex Kogan, Yossi Lev, Mark Moir", 
    "title": "Hardware extensions to make lazy subscription safe", 
    "arxiv-id": "1407.6968v1", 
    "author": "Mark Moir", 
    "publish": "2014-07-23T22:01:59Z", 
    "summary": "Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to\nexecute unmodified critical sections concurrently, even if they are protected\nby the same lock. To ensure correctness, the transactions used to execute these\ncritical sections \"subscribe\" to the lock by reading it and checking that it is\navailable. A recent paper proposed using the tempting \"lazy subscription\"\noptimization for a similar technique in a different context, namely\ntransactional systems that use a single global lock (SGL) to protect all\ntransactional data. We identify several pitfalls that show that lazy\nsubscription \\emph{is not safe} for TLE because unmodified critical sections\nexecuting before subscribing to the lock may behave incorrectly in a number of\nsubtle ways. We also show that recently proposed compiler support for modifying\ntransaction code to ensure subscription occurs before any incorrect behavior\ncould manifest is not sufficient to avoid all of the pitfalls we identify. We\nfurther argue that extending such compiler support to avoid all pitfalls would\nadd substantial complexity and would usually limit the extent to which\nsubscription can be deferred, undermining the effectiveness of the\noptimization. Hardware extensions suggested in the recent proposal also do not\naddress all of the pitfalls we identify. In this extended version of our WTTM\n2014 paper, we describe hardware extensions that make lazy subscription safe,\nboth for SGL-based transactional systems and for TLE, without the need for\nspecial compiler support. We also explain how nontransactional loads can be\nexploited, if available, to further enhance the effectiveness of lazy\nsubscription."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0806.4746v2", 
    "other_authors": "Alexandr Savinov", 
    "title": "Concept-Oriented Programming", 
    "arxiv-id": "0806.4746v2", 
    "author": "Alexandr Savinov", 
    "publish": "2008-06-29T10:56:41Z", 
    "summary": "Object-oriented programming (OOP) is aimed at describing the structure and\nbehaviour of objects by hiding the mechanism of their representation and access\nin primitive references. In this article we describe an approach, called\nconcept-oriented programming (COP), which focuses on modelling references\nassuming that they also possess application-specific structure and behaviour\naccounting for a great deal or even most of the overall program complexity.\nReferences in COP are completely legalized and get the same status as objects\nwhile the functions are distributed among both objects and references. In order\nto support this design we introduce a new programming construct, called\nconcept, which generalizes conventional classes and concept inclusion relation\ngeneralizing class inheritance. The main advantage of COP is that it allows\nprogrammers to describe two sides of any program: explicitly used functions of\nobjects and intermediate functionality of references having cross-cutting\nnature and executed implicitly behind the scenes during object access."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0807.3451v3", 
    "other_authors": "Etienne Payet, Fred Mesnard", 
    "title": "A Non-Termination Criterion for Binary Constraint Logic Programs", 
    "arxiv-id": "0807.3451v3", 
    "author": "Fred Mesnard", 
    "publish": "2008-07-22T13:51:33Z", 
    "summary": "On the one hand, termination analysis of logic programs is now a fairly\nestablished research topic within the logic programming community. On the other\nhand, non-termination analysis seems to remain a much less attractive subject.\nIf we divide this line of research into two kinds of approaches: dynamic versus\nstatic analysis, this paper belongs to the latter. It proposes a criterion for\ndetecting non-terminating atomic queries with respect to binary CLP rules,\nwhich strictly generalizes our previous works on this subject. We give a\ngeneric operational definition and an implemented logical form of this\ncriterion. Then we show that the logical form is correct and complete with\nrespect to the operational definition."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0807.3979v1", 
    "other_authors": "Maurizio Gabbrielli, Maria Chiara Meo, Paolo Tacchella", 
    "title": "Unfolding in CHR", 
    "arxiv-id": "0807.3979v1", 
    "author": "Paolo Tacchella", 
    "publish": "2008-07-25T15:21:46Z", 
    "summary": "Program transformation is an appealing technique which allows to improve\nrun-time efficiency, space-consumption and more generally to optimize a given\nprogram. Essentially it consists of a sequence of syntactic program\nmanipulations which preserves some kind of semantic equivalence. One of the\nbasic operations which is used by most program transformation systems is\nunfolding which consists in the replacement of a procedure call by its\ndefinition. While there is a large body of literature on transformation and\nunfolding of sequential programs, very few papers have addressed this issue for\nconcurrent languages and, to the best of our knowledge, no other has considered\nunfolding of CHR programs.\n  This paper defines a correct unfolding system for CHR programs. We define an\nunfolding rule, show its correctness and discuss some conditions which can be\nused to delete an unfolded rule while preserving the program meaning. We prove\nthat confluence and termination properties are preserved by the above\ntransformations."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0903.1598v6", 
    "other_authors": "Puri Arenas, Damiano Zanardini", 
    "title": "18th Workshop on Logic-based methods in Programming Environments (WLPE   2008)", 
    "arxiv-id": "0903.1598v6", 
    "author": "Damiano Zanardini", 
    "publish": "2009-03-09T17:26:36Z", 
    "summary": "This volume contains the papers presented at WLPE 2008: the 18th Workshop on\nLogic-based Methods in Programming Environments held on 12th December, 2008 in\nUdine, Italy. It was held as a satellite workshop of ICLP 2008, the 24th\nInternational Conference on Logic Programming."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0903.2202v1", 
    "other_authors": "Michael Leuschel, Salvador Tamarit, German Vidal", 
    "title": "Improving Size-Change Analysis in Offline Partial Evaluation", 
    "arxiv-id": "0903.2202v1", 
    "author": "German Vidal", 
    "publish": "2009-03-12T15:54:13Z", 
    "summary": "Some recent approaches for scalable offline partial evaluation of logic\nprograms include a size-change analysis for ensuring both so called local and\nglobal termination. In this work|inspired by experimental evaluation|we\nintroduce several improvements that may increase the accuracy of the analysis\nand, thus, the quality of the associated specialized programs. We aim to\nachieve this while maintaining the same complexity and scalability of the\nrecent works."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0903.2205v1", 
    "other_authors": "Francisco Javier Lopez-Fraguas, Juan Rodriguez-Hortala, Jaime Sanchez-Hernandez", 
    "title": "A Lightweight Combination of Semantics for Non-deterministic Functions", 
    "arxiv-id": "0903.2205v1", 
    "author": "Jaime Sanchez-Hernandez", 
    "publish": "2009-03-12T16:01:49Z", 
    "summary": "The use of non-deterministic functions is a distinctive feature of modern\nfunctional logic languages. The semantics commonly adopted is call-time choice,\na notion that at the operational level is related to the sharing mechanism of\nlazy evaluation in functional languages. However, there are situations where\nrun-time choice, closer to ordinary rewriting, is more appropriate. In this\npaper we propose an extension of existing call-time choice based languages, to\nprovide support for run-time choice in localized parts of a program. The\nextension is remarkably simple at three relevant levels: syntax, formal\noperational calculi and implementation, which is based on the system Toy."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627383", 
    "link": "http://arxiv.org/pdf/0903.2353v1", 
    "other_authors": "Andy King", 
    "title": "Relations, Constraints and Abstractions: Using the Tools of Logic   Programming in the Security Industry", 
    "arxiv-id": "0903.2353v1", 
    "author": "Andy King", 
    "publish": "2009-03-13T10:46:39Z", 
    "summary": "Logic programming is sometimes described as relational programming: a\nparadigm in which the programmer specifies and composes n-ary relations using\nsystems of constraints. An advanced logic programming environment will provide\ntools that abstract these relations to transform, optimise, or even verify the\ncorrectness of a logic program. This talk will show that these concepts, namely\nrelations, constraints and abstractions, turn out to also be important in the\nreverse engineer process that underpins the discovery of bugs within the\nsecurity industry."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2012.2.253", 
    "link": "http://arxiv.org/pdf/0909.2089v3", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Indirect jumps improve instruction sequence performance", 
    "arxiv-id": "0909.2089v3", 
    "author": "C. A. Middelburg", 
    "publish": "2009-09-11T05:51:09Z", 
    "summary": "Instruction sequences with direct and indirect jump instructions are as\nexpressive as instruction sequences with direct jump instructions only. We show\nthat, in the case where the number of instructions is not bounded, we are faced\nwith increases of the maximal internal delays of instruction sequences on\nexecution that are not bounded by a linear function if we strive for acceptable\nincreases of the lengths of instruction sequences on elimination of indirect\njump instructions."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2012.2.253", 
    "link": "http://arxiv.org/pdf/0909.2839v1", 
    "other_authors": "Jan A. Bergstra, Alban Ponse", 
    "title": "A progression ring for interfaces of instruction sequences, threads, and   services", 
    "arxiv-id": "0909.2839v1", 
    "author": "Alban Ponse", 
    "publish": "2009-09-15T16:59:57Z", 
    "summary": "We define focus-method interfaces and some connections between such\ninterfaces and instruction sequences, giving rise to instruction sequence\ncomponents. We provide a flexible and practical notation for interfaces using\nan abstract datatype specification comparable to that of basic process algebra\nwith deadlock. The structures thus defined are called progression rings. We\nalso define thread and service components. Two types of composition of\ninstruction sequences or threads and services (called `use' and `apply') are\nlifted to the level of components."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2012.2.253", 
    "link": "http://arxiv.org/pdf/1005.4022v1", 
    "other_authors": "Manas Ranjan Pradhan, E. G. Rajan", 
    "title": "Molecular Programming Pseudo-code Representation to Molecular   Electronics", 
    "arxiv-id": "1005.4022v1", 
    "author": "E. G. Rajan", 
    "publish": "2010-05-21T17:35:59Z", 
    "summary": "This research paper is proposing the idea of pseudo code representation to\nmolecular programming used in designing molecular electronics devices. Already\nthe schematic representation of logical gates like AND, OR, NOT etc.from\nmolecular diodes or resonant tunneling diode are available. This paper is\nsetting a generic pseudo code model so that various logic gates can be\nformulated. These molecular diodes have designed from organic molecules or\nBio-molecules. Our focus is on to give a scenario of molecular computation\nthrough molecular programming. We have restricted our study to molecular\nrectifying diode and logic device as AND gate from organic molecules only."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2012.2.253", 
    "link": "http://arxiv.org/pdf/1005.4798v2", 
    "other_authors": "Alexander Victor Berka", 
    "title": "Introduction to the Report \"Interlanguages and Synchronic Models of   Computation.\"", 
    "arxiv-id": "1005.4798v2", 
    "author": "Alexander Victor Berka", 
    "publish": "2010-05-26T12:34:37Z", 
    "summary": "A novel language system has given rise to promising alternatives to standard\nformal and processor network models of computation. An interstring linked with\na abstract machine environment, shares sub-expressions, transfers data, and\nspatially allocates resources for the parallel evaluation of dataflow. Formal\nmodels called the a-Ram family are introduced, designed to support interstring\nprogramming languages (interlanguages). Distinct from dataflow, graph\nrewriting, and FPGA models, a-Ram instructions are bit level and execute in\nsitu. They support sequential and parallel languages without the space/time\noverheads associated with the Turing Machine and lambda-calculus, enabling\nmassive programs to be simulated. The devices of one a-Ram model, called the\nSynchronic A-Ram, are fully connected and simpler than FPGA LUT's. A compiler\nfor an interlanguage called Space, has been developed for the Synchronic A-Ram.\nSpace is MIMD. strictly typed, and deterministic. Barring memory allocation and\ncompilation, modules are referentially transparent. At a high level of\nabstraction, modules exhibit a state transition system, aiding verification.\nData structures and parallel iteration are straightforward to implement, and\nallocations of sub-processes and data transfers to resources are implicit.\nSpace points towards highly connected architectures called Synchronic Engines,\nthat are more general purpose than systolic arrays and GPUs, and bypass\nprogrammability and conflict issues associated with multicores."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2012.2.253", 
    "link": "http://arxiv.org/pdf/1005.5183v1", 
    "other_authors": "Alexander Victor Berka", 
    "title": "Interlanguages and synchronic models of computation", 
    "arxiv-id": "1005.5183v1", 
    "author": "Alexander Victor Berka", 
    "publish": "2010-05-27T21:42:27Z", 
    "summary": "A novel language system has given rise to promising alternatives to standard\nformal and processor network models of computation. An interstring linked with\na abstract machine environment, shares sub-expressions, transfers data, and\nspatially allocates resources for the parallel evaluation of dataflow. Formal\nmodels called the a-Ram family are introduced, designed to support interstring\nprogramming languages (interlanguages). Distinct from dataflow, graph\nrewriting, and FPGA models, a-Ram instructions are bit level and execute in\nsitu. They support sequential and parallel languages without the space/time\noverheads associated with the Turing Machine and l-calculus, enabling massive\nprograms to be simulated. The devices of one a-Ram model, called the Synchronic\nA-Ram, are fully connected and simpler than FPGA LUT's. A compiler for an\ninterlanguage called Space, has been developed for the Synchronic A-Ram. Space\nis MIMD. strictly typed, and deterministic. Barring memory allocation and\ncompilation, modules are referentially transparent. At a high level of\nabstraction, modules exhibit a state transition system, aiding verification.\nData structures and parallel iteration are straightforward to implement, and\nallocations of sub-processes and data transfers to resources are implicit.\nSpace points towards highly connected architectures called Synchronic Engines,\nthat scale in a GALS manner. Synchronic Engines are more general purpose than\nsystolic arrays and GPUs, and bypass programmability and conflict issues\nassociated with multicores."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1005.5278v2", 
    "other_authors": "Peter A. Jonsson, Johan Nordlander", 
    "title": "Positive Supercompilation for a Higher-Order Call-By-Value Language", 
    "arxiv-id": "1005.5278v2", 
    "author": "Johan Nordlander", 
    "publish": "2010-05-28T12:33:37Z", 
    "summary": "Previous deforestation and supercompilation algorithms may introduce\naccidental termination when applied to call-by-value programs. This hides\nlooping bugs from the programmer, and changes the behavior of a program\ndepending on whether it is optimized or not. We present a supercompilation\nalgorithm for a higher-order call-by-value language and prove that the\nalgorithm both terminates and preserves termination properties. This algorithm\nutilizes strictness information to decide whether to substitute or not and\ncompares favorably with previous call-by-name transformations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1008.0909v1", 
    "other_authors": "Qing'an Li, Yanxiang He, Yong Chen, Wei Wu, Wenwen Xu", 
    "title": "A Heuristic Algorithm for optimizing Page Selection Instructions", 
    "arxiv-id": "1008.0909v1", 
    "author": "Wenwen Xu", 
    "publish": "2010-08-05T03:47:48Z", 
    "summary": "Page switching is a technique that increases the memory in microcontrollers\nwithout extending the address buses. This technique is widely used in the\ndesign of 8-bit MCUs. In this paper, we present an algorithm to reduce the\noverhead of page switching. To pursue small code size, we place the emphasis on\nthe allocation of functions into suitable pages with a heuristic algorithm,\nthereby the cost-effective placement of page selection instructions. Our\nexperimental results showed the optimization achieved a reduction in code size\nof 13.2 percent."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1008.1131v1", 
    "other_authors": "Chris Preston", 
    "title": "Computing with Equations", 
    "arxiv-id": "1008.1131v1", 
    "author": "Chris Preston", 
    "publish": "2010-08-06T07:55:16Z", 
    "summary": "The intention of these notes is to give a mathematical account of how I\nbelieve students could be taught to think about functional programming\nlanguages and to explain how such languages work."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1008.3431v1", 
    "other_authors": "Venkatreddy Dwarampudi, Shahbaz Singh Dhillon, Jivitesh Shah, Nikhil Joseph Sebastian, Nitin Kanigicharla", 
    "title": "Comparative study of the Pros and Cons of Programming languages Java,   Scala, C++, Haskell, VB .NET, AspectJ, Perl, Ruby, PHP & Scheme - a Team 11   COMP6411-S10 Term Report", 
    "arxiv-id": "1008.3431v1", 
    "author": "Nitin Kanigicharla", 
    "publish": "2010-08-20T03:17:43Z", 
    "summary": "With the advent of numerous languages it is difficult to realize the edge of\none language in a particular scope over another one. We are making an effort,\nrealizing these few issues and comparing some main stream languages like Java,\nScala, C++, Haskell, VB .NET, AspectJ, Perl, Ruby, PHP and Scheme keeping in\nmind some core issues in program development."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1008.3434v1", 
    "other_authors": "Sultan S. Al-Qahtani, Pawel Pietrzynski, Luis F. Guzman, Rafik Arif, Adrien Tevoedjre", 
    "title": "Comparing Selected Criteria of Programming Languages Java, PHP, C++,   Perl, Haskell, AspectJ, Ruby, COBOL, Bash Scripts and Scheme Revision 1.0 - a   Team CPLgroup COMP6411-S10 Term Report", 
    "arxiv-id": "1008.3434v1", 
    "author": "Adrien Tevoedjre", 
    "publish": "2010-08-20T04:10:27Z", 
    "summary": "Comparison of programming languages is a common topic of discussion among\nsoftware engineers. Few languages ever become sufficiently popular that they\nare used by more than a few people or find their niche in research or\neducation; but professional programmers can easily use dozens of different\nlanguages during their career. Multiple programming languages are designed,\nspecified, and implemented every year in order to keep up with the changing\nprogramming paradigms, hardware evolution, etc. In this paper we present a\ncomparative study between ten programming languages: Haskell, Java, Perl, C++,\nAspectJ, COBOL, Ruby, PHP, Bash Scripts, and Scheme; with respect of the\nfollowing criteria: Secure programming practices, web applications development,\nweb services design and composition, object oriented-based abstraction,\nreflection, aspect-orientation, functional programming, declarative\nprogramming, batch scripting, and user interface prototype design."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-6(3:5)2010", 
    "link": "http://arxiv.org/pdf/1008.3561v1", 
    "other_authors": "Rana Naim, Mohammad Fahim Nizam, Sheetal Hanamasagar, Jalal Noureddine, Marinela Miladinova", 
    "title": "Comparative Studies of 10 Programming Languages within 10 Diverse   Criteria - a Team 10 COMP6411-S10 Term Report", 
    "arxiv-id": "1008.3561v1", 
    "author": "Marinela Miladinova", 
    "publish": "2010-08-20T19:50:52Z", 
    "summary": "This is a survey on the programming languages: C++, JavaScript, AspectJ, C#,\nHaskell, Java, PHP, Scala, Scheme, and BPEL. Our survey work involves a\ncomparative study of these ten programming languages with respect to the\nfollowing criteria: secure programming practices, web application development,\nweb service composition, OOP-based abstractions, reflection, aspect\norientation, functional programming, declarative programming, batch scripting,\nand UI prototyping. We study these languages in the context of the above\nmentioned criteria and the level of support they provide for each one of them."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s11219-010-9116-5", 
    "link": "http://arxiv.org/pdf/1008.4188v1", 
    "other_authors": "Ralf L\u00e4mmel, Vadim Zaytsev", 
    "title": "Recovering Grammar Relationships for the Java Language Specification", 
    "arxiv-id": "1008.4188v1", 
    "author": "Vadim Zaytsev", 
    "publish": "2010-08-25T02:11:12Z", 
    "summary": "Grammar convergence is a method that helps discovering relationships between\ndifferent grammars of the same language or different language versions. The key\nelement of the method is the operational, transformation-based representation\nof those relationships. Given input grammars for convergence, they are\ntransformed until they are structurally equal. The transformations are composed\nfrom primitive operators; properties of these operators and the composed chains\nprovide quantitative and qualitative insight into the relationships between the\ngrammars at hand. We describe a refined method for grammar convergence, and we\nuse it in a major study, where we recover the relationships between all the\ngrammars that occur in the different versions of the Java Language\nSpecification (JLS). The relationships are represented as grammar\ntransformation chains that capture all accidental or intended differences\nbetween the JLS grammars. This method is mechanized and driven by nominal and\nstructural differences between pairs of grammars that are subject to\nasymmetric, binary convergence steps. We present the underlying operator suite\nfor grammar transformation in detail, and we illustrate the suite with many\nexamples of transformations on the JLS grammars. We also describe the\nextraction effort, which was needed to make the JLS grammars amenable to\nautomated processing. We include substantial metadata about the convergence\nprocess for the JLS so that the effort becomes reproducible and transparent."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(1:10)2011", 
    "link": "http://arxiv.org/pdf/1012.1802v3", 
    "other_authors": "Ross Tate, Michael Stepp, Zachary Tatlock, Sorin Lerner", 
    "title": "Equality Saturation: A New Approach to Optimization", 
    "arxiv-id": "1012.1802v3", 
    "author": "Sorin Lerner", 
    "publish": "2010-12-02T15:47:02Z", 
    "summary": "Optimizations in a traditional compiler are applied sequentially, with each\noptimization destructively modifying the program to produce a transformed\nprogram that is then passed to the next optimization. We present a new approach\nfor structuring the optimization phase of a compiler. In our approach,\noptimizations take the form of equality analyses that add equality information\nto a common intermediate representation. The optimizer works by repeatedly\napplying these analyses to infer equivalences between program fragments, thus\nsaturating the intermediate representation with equalities. Once saturated, the\nintermediate representation encodes multiple optimized versions of the input\nprogram. At this point, a profitability heuristic picks the final optimized\nprogram from the various programs represented in the saturated representation.\nOur proposed way of structuring optimizers has a variety of benefits over\nprevious approaches: our approach obviates the need to worry about optimization\nordering, enables the use of a global optimization heuristic that selects among\nfully optimized programs, and can be used to perform translation validation,\neven on compilers other than our own. We present our approach, formalize it,\nand describe our choice of intermediate representation. We also present\nexperimental results showing that our approach is practical in terms of time\nand space overhead, is effective at discovering intricate optimization\nopportunities, and is effective at performing translation validation for a\nrealistic optimizer."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(1:10)2011", 
    "link": "http://arxiv.org/pdf/1012.2294v4", 
    "other_authors": "Steven Obua", 
    "title": "Syntax and Semantics of Babel-17", 
    "arxiv-id": "1012.2294v4", 
    "author": "Steven Obua", 
    "publish": "2010-12-10T15:37:06Z", 
    "summary": "We present Babel-17, the first programming language for purely functional\nstructured programming (PFSP). Earlier work illustrated PFSP in the framework\nof a toy research language. Babel-17 takes this earlier work to a new level by\nshowing how PFSP can be combined with pattern matching, object oriented\nprogramming, and features like concurrency, lazy evaluation, memoization and\nsupport for lenses."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(1:10)2011", 
    "link": "http://arxiv.org/pdf/1012.4240v1", 
    "other_authors": "Joachim Schimpf, Kish Shen", 
    "title": "ECLiPSe - from LP to CLP", 
    "arxiv-id": "1012.4240v1", 
    "author": "Kish Shen", 
    "publish": "2010-12-20T05:11:57Z", 
    "summary": "ECLiPSe is a Prolog-based programming system, aimed at the development and\ndeployment of constraint programming applications. It is also used for teaching\nmost aspects of combinatorial problem solving, e.g. problem modelling,\nconstraint programming, mathematical programming, and search techniques. It\nuses an extended Prolog as its high-level modelling and control language,\ncomplemented by several constraint solver libraries, interfaces to third-party\nsolvers, an integrated development environment and interfaces for embedding\ninto host environments. This paper discusses language extensions,\nimplementation aspects, components and tools that we consider relevant on the\nway from Logic Programming to Constraint Logic Programming."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.42.4", 
    "link": "http://arxiv.org/pdf/1012.4893v1", 
    "other_authors": "Conrad Rau, Manfred Schmidt-Schau\u00df", 
    "title": "Towards Correctness of Program Transformations Through Unification and   Critical Pair Computation", 
    "arxiv-id": "1012.4893v1", 
    "author": "Manfred Schmidt-Schau\u00df", 
    "publish": "2010-12-22T07:07:59Z", 
    "summary": "Correctness of program transformations in extended lambda calculi with a\ncontextual semantics is usually based on reasoning about the operational\nsemantics which is a rewrite semantics. A successful approach to proving\ncorrectness is the combination of a context lemma with the computation of\noverlaps between program transformations and the reduction rules, and then of\nso-called complete sets of diagrams. The method is similar to the computation\nof critical pairs for the completion of term rewriting systems. We explore\ncases where the computation of these overlaps can be done in a first order way\nby variants of critical pair computation that use unification algorithms. As a\ncase study we apply the method to a lambda calculus with recursive\nlet-expressions and describe an effective unification algorithm to determine\nall overlaps of a set of transformations with all reduction rules. The\nunification algorithm employs many-sorted terms, the equational theory of\nleft-commutativity modelling multi-sets, context variables of different kinds\nand a mechanism for compactly representing binding chains in recursive\nlet-expressions."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.43.6", 
    "link": "http://arxiv.org/pdf/1012.4900v1", 
    "other_authors": "Aaron Stump, Vilhelm Sj\u00f6berg, Stephanie Weirich", 
    "title": "Termination Casts: A Flexible Approach to Termination with General   Recursion", 
    "arxiv-id": "1012.4900v1", 
    "author": "Stephanie Weirich", 
    "publish": "2010-12-22T07:09:39Z", 
    "summary": "This paper proposes a type-and-effect system called Teqt, which distinguishes\nterminating terms and total functions from possibly diverging terms and partial\nfunctions, for a lambda calculus with general recursion and equality types. The\ncentral idea is to include a primitive type-form \"Terminates t\", expressing\nthat term t is terminating; and then allow terms t to be coerced from possibly\ndiverging to total, using a proof of Terminates t. We call such coercions\ntermination casts, and show how to implement terminating recursion using them.\nFor the meta-theory of the system, we describe a translation from Teqt to a\nlogical theory of termination for general recursive, simply typed functions.\nEvery typing judgment of Teqt is translated to a theorem expressing the\nappropriate termination property of the computational part of the Teqt term."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.43.6", 
    "link": "http://arxiv.org/pdf/1012.5123v1", 
    "other_authors": "Terrance Swift, David S. Warren", 
    "title": "XSB: Extending Prolog with Tabled Logic Programming", 
    "arxiv-id": "1012.5123v1", 
    "author": "David S. Warren", 
    "publish": "2010-12-23T00:23:16Z", 
    "summary": "The paradigm of Tabled Logic Programming (TLP) is now supported by a number\nof Prolog systems, including XSB, YAP Prolog, B-Prolog, Mercury, ALS, and Ciao.\nThe reasons for this are partly theoretical: tabling ensures termination and\noptimal known complexity for queries to a large class of programs. However the\noverriding reasons are practical. TLP allows sophisticated programs to be\nwritten concisely and efficiently, especially when mechanisms such as tabled\nnegation and call and answer subsumption are supported. As a result TLP has now\nbeen used in a variety of applications from program analysis to querying over\nthe semantic web. This paper provides a survey of TLP and its applications as\nimplemented in XSB Prolog, along with discussion of how XSB supports tabling\nwith dynamically changing code, and in a multi-threaded environment."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.43.6", 
    "link": "http://arxiv.org/pdf/1012.5494v2", 
    "other_authors": "Serguei A. Mokhov", 
    "title": "Contents of COMP6411 Summer 2010 Final Reports on Comparative Studies of   Programming Languages", 
    "arxiv-id": "1012.5494v2", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-12-26T03:33:09Z", 
    "summary": "This index covers the lecture notes and the final course project reports for\nCOMP6411 Summer 2010 at Concordia University, Montreal, Canada, Comparative\nStudy of Programming Languages by 4 teams trying compare a set of common\ncriteria and their applicability to about 10 distinct programming languages,\nwhere 5 language choices were provided by the instructor and five were picked\nby each team and each student individually compared two of the 10 and then the\nteam did a summary synthesis across all 10 languages. Their findings are posted\nhere for further reference, comparative studies, and analysis."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1012.5560v1", 
    "other_authors": "Maribel Fern\u00e1ndez, Olivier Namet", 
    "title": "Strategic programming on graph rewriting systems", 
    "arxiv-id": "1012.5560v1", 
    "author": "Olivier Namet", 
    "publish": "2010-12-27T06:30:03Z", 
    "summary": "We describe a strategy language to control the application of graph rewriting\nrules, and show how this language can be used to write high-level declarative\nprograms in several application areas. This language is part of a graph-based\nprogramming tool built within the port-graph transformation and visualisation\nenvironment PORGY."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1106.0447v1", 
    "other_authors": "Umut A. Acar, Guy E. Blelloch, Robert Harper", 
    "title": "Selective Memoization", 
    "arxiv-id": "1106.0447v1", 
    "author": "Robert Harper", 
    "publish": "2011-06-02T15:03:15Z", 
    "summary": "This paper presents language techniques for applying memoization selectively.\nThe techniques provide programmer control over equality, space usage, and\nidentification of precise dependences so that memoization can be applied\naccording to the needs of an application. Two key properties of the approach\nare that it accepts and efficient implementation and yields programs whose\nperformance can be analyzed using standard analysis techniques. We describe our\napproach in the context of a functional language called MFL and an\nimplementation as a Standard ML library. The MFL language employs a modal type\nsystem to enable the programmer to express programs that reveal their true data\ndependences when executed. We prove that the MFL language is sound by showing\nthat that MFL programs yield the same result as they would with respect to a\nstandard, non-memoizing semantics. The SML implementation cannot support the\nmodal type system of MFL statically but instead employs run-time checks to\nensure correct usage of primitives."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1106.0478v1", 
    "other_authors": "Umut A. Acar, Matthias Blume, Jacob Donham", 
    "title": "A Consistent Semantics of Self-Adjusting Computation", 
    "arxiv-id": "1106.0478v1", 
    "author": "Jacob Donham", 
    "publish": "2011-06-02T18:13:08Z", 
    "summary": "This paper presents a semantics of self-adjusting computation and proves that\nthe semantics are correct and consistent. The semantics integrate change\npropagation with the classic idea of memoization to enable reuse of\ncomputations under mutation to memory. During evaluation, reuse of a\ncomputation via memoization triggers a change propagation that adjusts the\nreused computation to reflect the mutated memory. Since the semantics integrate\nmemoization and change-propagation, it involves both non-determinism (due to\nmemoization) and mutation (due to change propagation). Our consistency theorem\nstates that the non-determinism is not harmful: any two evaluations of the same\nprogram starting at the same state yield the same result. Our correctness\ntheorem states that mutation is not harmful: self-adjusting programs are\nconsistent with purely functional programming. We formalize the semantics and\ntheir meta-theory in the LF logical framework and machine check our proofs\nusing Twelf."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1106.1311v2", 
    "other_authors": "Phuong-Lan Nguyen, Bart Demoen", 
    "title": "Representation Sharing for Prolog", 
    "arxiv-id": "1106.1311v2", 
    "author": "Bart Demoen", 
    "publish": "2011-06-07T10:34:53Z", 
    "summary": "Representation sharing can reduce the memory footprint of a program by\nsharing one representation between duplicate terms. The most common\nimplementation of representation sharing in functional programming systems is\nknown as hash-consing. In the context of Prolog, representation sharing has\nbeen given little attention. Some current techniques that deal with\nrepresentation sharing are reviewed. The new contributions are: (1) an easy\nimplementation of {\\em input sharing} for {\\em findall/3}; (2) a description of\na {\\em sharer} module that introduces representation sharing at runtime. Their\nrealization is shown in the context of the WAM as implemented by hProlog. Both\ncan be adapted to any WAM-like Prolog implementation. The sharer works\nindependently of the garbage collector, but it can be made to cooperate with\nthe garbage collector. Benchmark results show that the sharer has a cost\ncomparable to the heap garbage collector, that its effectiveness is highly\napplication dependent, and that its policy must be tuned to the collector. To\nappear in Theory and Practice of Logic Programming (TPLP)"
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1106.2575v1", 
    "other_authors": "Sam Tobin-Hochstadt, Matthias Felleisen", 
    "title": "The Design and Implementation of Typed Scheme: From Scripts to Programs", 
    "arxiv-id": "1106.2575v1", 
    "author": "Matthias Felleisen", 
    "publish": "2011-06-13T23:41:05Z", 
    "summary": "When scripts in untyped languages grow into large programs, maintaining them\nbecomes difficult. A lack of explicit type annotations in typical scripting\nlanguages forces programmers to must (re)discover critical pieces of design\ninformation every time they wish to change a program. This analysis step both\nslows down the maintenance process and may even introduce mistakes due to the\nviolation of undiscovered invariants.\n  This paper presents Typed Scheme, an explicitly typed extension of PLT\nScheme, an untyped scripting language. Its type system is based on the novel\nnotion of occurrence typing, which we formalize and mechanically prove sound.\nThe implementation of Typed Scheme additionally borrows elements from a range\nof approaches, including recursive types, true unions and subtyping, plus\npolymorphism combined with a modicum of local inference.\n  The formulation of occurrence typing naturally leads to a simple and\nexpressive version of predicates to describe refinement types. A Typed Scheme\nprogram can use these refinement types to keep track of arbitrary classes of\nvalues via the type system. Further, we show how the Typed Scheme type system,\nin conjunction with simple recursive types, is able to encode refinements of\nexisting datatypes, thus expressing both proposed variations of refinement\ntypes."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.44.1", 
    "link": "http://arxiv.org/pdf/1106.2578v1", 
    "other_authors": "Sam Tobin-Hochstadt", 
    "title": "Extensible Pattern Matching in an Extensible Language", 
    "arxiv-id": "1106.2578v1", 
    "author": "Sam Tobin-Hochstadt", 
    "publish": "2011-06-14T00:06:58Z", 
    "summary": "Pattern matching is a widely used technique in functional languages,\nespecially those in the ML and Haskell traditions, where it is at the core of\nthe semantics. In languages in the Lisp tradition, in contrast, pattern\nmatching it typically provided by libraries built with macros. We present\nmatch, a sophisticated pattern matcher for Racket, implemented as language\nextension. using macros. The system supports novel and widely-useful\npattern-matching forms, and is itself extensible. The extensibility of match is\nimplemented via a general technique for creating extensible language\nextensions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1106.3445v2", 
    "other_authors": "Matthew R. Lakin", 
    "title": "Constraint solving in non-permutative nominal abstract syntax", 
    "arxiv-id": "1106.3445v2", 
    "author": "Matthew R. Lakin", 
    "publish": "2011-06-17T11:07:51Z", 
    "summary": "Nominal abstract syntax is a popular first-order technique for encoding, and\nreasoning about, abstract syntax involving binders. Many of its applications\ninvolve constraint solving. The most commonly used constraint solving algorithm\nover nominal abstract syntax is the Urban-Pitts-Gabbay nominal unification\nalgorithm, which is well-behaved, has a well-developed theory and is applicable\nin many cases. However, certain problems require a constraint solver which\nrespects the equivariance property of nominal logic, such as Cheney's\nequivariant unification algorithm. This is more powerful but is more\ncomplicated and computationally hard. In this paper we present a novel\nalgorithm for solving constraints over a simple variant of nominal abstract\nsyntax which we call non-permutative. This constraint problem has similar\ncomplexity to equivariant unification but without many of the additional\ncomplications of the equivariant unification term language. We prove our\nalgorithm correct, paying particular attention to issues of termination, and\npresent an explicit translation of name-name equivariant unification problems\ninto non-permutative constraints."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1106.3478v1", 
    "other_authors": "Joachim Breitner", 
    "title": "Conditional Elimination through Code Duplication", 
    "arxiv-id": "1106.3478v1", 
    "author": "Joachim Breitner", 
    "publish": "2011-06-15T08:27:04Z", 
    "summary": "We propose an optimizing transformation which reduces program runtime at the\nexpense of program size by eliminating conditional jumps."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1106.4200v1", 
    "other_authors": "Damien Cassou, Charles Consel, Emilie Balland, Julia Lawall", 
    "title": "Faire levier sur les architectures logicielles pour guider et v\u00e9rifier   le d\u00e9veloppement d'applications SCC", 
    "arxiv-id": "1106.4200v1", 
    "author": "Julia Lawall", 
    "publish": "2011-06-21T13:35:22Z", 
    "summary": "A software architecture describes the structure of a computing system by\nspecifying software components and their interactions. Mapping a software\narchitecture to an implementation is a well known challenge. A key element of\nthis mapping is the architecture's description of the data and control-flow\ninteractions between components. The characterization of these interactions can\nbe rather abstract or very concrete, providing more or less implementation\nguidance, programming support, and static verification. In this paper, we\nexplore one point in the design space between abstract and concrete component\ninteraction specifications. We introduce a notion of interaction contract that\nexpresses allowed interactions between components, describing both data and\ncontrol-flow constraints. This declaration is part of the architecture\ndescription, allows generation of extensive programming support, and enables\nvarious verifications. We instantiate our approach in an architecture\ndescription language for Sense/Compute/Control applications, and describe\nassociated compilation and verification strategies."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.0940v4", 
    "other_authors": "Joey Paquet, Serguei A. Mokhov", 
    "title": "Furthering Baseline Core Lucid Standard Specification in the Context of   the History of Lucid, Intensional Programming, and Context-Aware Computing", 
    "arxiv-id": "1107.0940v4", 
    "author": "Serguei A. Mokhov", 
    "publish": "2011-07-05T18:49:26Z", 
    "summary": "This work is multifold. We review the historical literature on the Lucid\nprogramming language, its dialects, intensional logic, intensional programming,\nthe implementing systems, and context-oriented and context-aware computing and\nso on that provide a contextual framework for the converging Core Lucid\nstandard programming model. We are designing a standard specification of a\nbaseline Lucid virtual machine for generic execution of Lucid programs. The\nresulting Core Lucid language would inherit the properties of generalization\nattempts of GIPL (1999-2013) and TransLucid (2008-2013) for all future and\nrecent Lucid implementing systems to follow. We also maintain this work across\nlocal research group in order to foster deeper collaboration, maintain a list\nof recent and historical bibliography and a reference manual and reading list\nfor students. We form a (for now informal) SIGLUCID group to keep track of this\nstandard and historical records with eventual long-term goal through iterative\nrevisions for this work to become a book or an encyclopedia of the referenced\ntopics, and perhaps, an RFC. We first begin small with this initial set of\nnotes."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.1398v1", 
    "other_authors": "Jan Obdrzalek, Marek Trtik", 
    "title": "Efficient Loop Navigation for Symbolic Execution", 
    "arxiv-id": "1107.1398v1", 
    "author": "Marek Trtik", 
    "publish": "2011-07-07T14:13:14Z", 
    "summary": "Symbolic execution is a successful and very popular technique used in\nsoftware verification and testing. A key limitation of symbolic execution is in\ndealing with code containing loops. The problem is that even a single loop can\ngenerate a huge number of different symbolic execution paths, corresponding to\ndifferent number of loop iterations and taking various paths through the loop.\n  We introduce a technique which, given a start location above some loops and a\ntarget location anywhere below these loops, returns a feasible path between\nthese two locations, if such a path exists. The technique infers a collection\nof constraint systems from the program and uses them to steer the symbolic\nexecution towards the target. On reaching a loop it iteratively solves the\nappropriate constraint system to find out which path through this loop to take,\nor, alternatively, whether to continue below the loop. To construct the\nconstraint systems we express the values of variables modified in a loop as\nfunctions of the number of times a given path through the loop was executed.\n  We have built a prototype implementation of our technique and compared it to\nstate-of-the-art symbolic execution tools on simple programs with loops. The\nresults show significant improvements in the running time. We found instances\nwhere our algorithm finished in seconds, whereas the other tools did not finish\nwithin an hour. Our approach also shows very good results in the case when the\ntarget location is not reachable by any feasible path."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.2157v1", 
    "other_authors": "Matthew J. Sottile, Craig E Rasmussen, Wayne N. Weseloh, Robert W. Robey, Daniel Quinlan, Jeffrey Overbey", 
    "title": "ForOpenCL: Transformations Exploiting Array Syntax in Fortran for   Accelerator Programming", 
    "arxiv-id": "1107.2157v1", 
    "author": "Jeffrey Overbey", 
    "publish": "2011-07-11T22:07:32Z", 
    "summary": "Emerging GPU architectures for high performance computing are well suited to\na data-parallel programming model. This paper presents preliminary work\nexamining a programming methodology that provides Fortran programmers with\naccess to these emerging systems. We use array constructs in Fortran to show\nhow this infrequently exploited, standardized language feature is easily\ntransformed to lower-level accelerator code. The transformations in ForOpenCL\nare based on a simple mapping from Fortran to OpenCL. We demonstrate, using a\nstencil code solving the shallow-water fluid equations, that the performance of\nthe ForOpenCL compiler-generated transformations is comparable with that of\nhand-optimized OpenCL code."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.2437v1", 
    "other_authors": "Harold V. McIntosh", 
    "title": "A CONVERT compiler of REC for PDP-8", 
    "arxiv-id": "1107.2437v1", 
    "author": "Harold V. McIntosh", 
    "publish": "2011-07-12T23:43:18Z", 
    "summary": "REC (REGULAR EXPRESSION COMPILER) is a programming language of simple\nstructure developed originally for the PDP-8 computer of the Digital Equipment,\nCorporation, but readily adaptable to any other general purpose computer. It\nhas been used extensively in teaching Algebra and Numerical Analysis in the\nEscuela Superior de F\\'isica y Matem\\'aticas of the Instituto Polit\\'ecnico\nNacional. Moreover, the fact that the same control language, REC, is equally\napplicable and equally efficient over the whole range of computer facilities\navailable to the students gives a very welcome coherence to the entire teaching\nprogram, including the course of Mathematical Logic which is devoted to the\ntheoretical aspects of such matters.\n  REC; derives its appeal from the fact that computers can be regarded\nreasonably well as Turing Machines. The REC notation is simply a manner of\nwriting regular expression, somewhat more amenable to programming the Turing\nMachine which they control. If one does not wish to think so strictly in terms\nof Turing Machines, REC expressions still provide a means of defining the flow\nof control in a program which is quite convenient for many applications."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.3193v10", 
    "other_authors": "Chengpu Wang", 
    "title": "Type Expressiveness and Its Application in Separation of Behavior   Programming and Data Management Programming", 
    "arxiv-id": "1107.3193v10", 
    "author": "Chengpu Wang", 
    "publish": "2011-07-16T03:03:10Z", 
    "summary": "A new behavior descriptive entity type called spec is proposed, which\ncombines the traditional interface with test rules and test cases, to\ncompletely specify the desired behavior of each method, and to enforce the\nbehavior-wise correctness of all compiled units. Using spec, a new programming\nparadigm is proposed, which allows the separation programming space into 1) a\nbehavior domain to aggregate all behavior programming in the format of specs,\n2) a object domain to bind each concrete spec to its data representation in a\nparticular address space, and 3) a realization domain to connect the behavior\ndomain and the object domain. Such separation guarantees the strictness of\nbehavior satisfaction at compile time, while allows flexibility of dynamical\nbinding of actual implementation at runtime. A new convention call type\nexpressiveness to allow data exchange between different programming languages\nand between different software environments is also proposed."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-7(3:6)2011", 
    "link": "http://arxiv.org/pdf/1107.3539v1", 
    "other_authors": "David Van Horn, Matthew Might", 
    "title": "Systematic Abstraction of Abstract Machines", 
    "arxiv-id": "1107.3539v1", 
    "author": "Matthew Might", 
    "publish": "2011-07-18T19:44:11Z", 
    "summary": "We describe a derivational approach to abstract interpretation that yields\nnovel and transparently sound static analyses when applied to well-established\nabstract machines for higher-order and imperative programming languages. To\ndemonstrate the technique and support our claim, we transform the CEK machine\nof Felleisen and Friedman, a lazy variant of Krivine's machine, and the\nstack-inspecting CM machine of Clements and Felleisen into abstract\ninterpretations of themselves. The resulting analyses bound temporal ordering\nof program events; predict return-flow and stack-inspection behavior; and\napproximate the flow and evaluation of by-need parameters. For all of these\nmachines, we find that a series of well-known concrete machine refactorings,\nplus a technique of store-allocated continuations, leads to machines that\nabstract into static analyses simply by bounding their stores. We demonstrate\nthat the technique scales up uniformly to allow static analysis of realistic\nlanguage features, including tail calls, conditionals, side effects,\nexceptions, first-class continuations, and even garbage collection. In order to\nclose the gap between formalism and implementation, we provide translations of\nthe mathematics as running Haskell code for the initial development of our\nmethod."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000202", 
    "link": "http://arxiv.org/pdf/1107.5408v1", 
    "other_authors": "Ant\u00f3nio Porto", 
    "title": "A structured alternative to Prolog with simple compositional semantics", 
    "arxiv-id": "1107.5408v1", 
    "author": "Ant\u00f3nio Porto", 
    "publish": "2011-07-27T08:24:34Z", 
    "summary": "Prolog's very useful expressive power is not captured by traditional logic\nprogramming semantics, due mainly to the cut and goal and clause order. Several\nalternative semantics have been put forward, exposing operational details of\nthe computation state. We propose instead to redesign Prolog around structured\nalternatives to the cut and clauses, keeping the expressive power and\ncomputation model but with a compositional denotational semantics over much\nsimpler states-just variable bindings. This considerably eases reasoning about\nprograms, by programmers and tools such as a partial evaluator, with safe\nunfolding of calls through predicate definitions. An if-then-else across\nclauses replaces most uses of the cut, but the cut's full power is achieved by\nan until construct. Disjunction, conjunction and until, along with unification,\nare the primitive goal types with a compositional semantics yielding sequences\nof variable-binding solutions. This extends to programs via the usual technique\nof a least fixpoint construction. A simple interpreter for Prolog in the\nalternative language, and a definition of until in Prolog, establish the\nidentical expressive power of the two languages. Many useful control constructs\nare derivable from the primitives, and the semantic framework illuminates the\ndiscussion of alternative ones. The formalisation rests on a term language with\nvariable abstraction as in the {\\lambda}-calculus. A clause is an abstraction\non the call arguments, a continuation, and the local variables. It can be\ninclusive or exclusive, expressing a local case bound to a continuation by\neither a disjunction or an if-then-else. Clauses are open definitions, composed\n(and closed) with simple functional application ({\\beta}-reduction). This paves\nthe way for a simple account of flexible module composition mechanisms. Cube, a\nconcrete language with the exposed principles, has been implemented."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068411000251", 
    "link": "http://arxiv.org/pdf/1107.5556v1", 
    "other_authors": "Flavio Cruz, Ricardo Rocha", 
    "title": "Efficient Instance Retrieval of Subgoals for Subsumptive Tabled   Evaluation of Logic Programs", 
    "arxiv-id": "1107.5556v1", 
    "author": "Ricardo Rocha", 
    "publish": "2011-07-27T18:31:13Z", 
    "summary": "Tabled evaluation is an implementation technique that solves some problems of\ntraditional Prolog systems in dealing with recursion and redundant\ncomputations. Most tabling engines determine if a tabled subgoal will produce\nor consume answers by using variant checks. A more refined method, named call\nsubsumption, considers that a subgoal A will consume from a subgoal B if A is\nsubsumed by (an instance of) B, thus allowing greater answer reuse. We recently\ndeveloped an extension, called Retroactive Call Subsumption, that improves upon\ncall subsumption by supporting bidirectional sharing of answers between\nsubsumed/subsuming subgoals. In this paper, we present both an algorithm and an\nextension to the table space data structures to efficiently implement instance\nretrieval of subgoals for subsumptive tabled evaluation of logic programs.\nExperiments results using the YapTab tabling system show that our\nimplementation performs quite well on some complex benchmarks and is robust\nenough to handle a large number of subgoals without performance degradation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66", 
    "link": "http://arxiv.org/pdf/1109.0323v1", 
    "other_authors": "Olivier Danvy, Chung-chieh Shan", 
    "title": "Proceedings IFIP Working Conference on Domain-Specific Languages", 
    "arxiv-id": "1109.0323v1", 
    "author": "Chung-chieh Shan", 
    "publish": "2011-09-01T22:59:50Z", 
    "summary": "This volume is the proceedings of the second IFIP Working Conference on\nDomain-Specific Languages (DSL 2011). It contains 2 abstracts of invited\npresentations, 7 peer-reviewed articles selected by the program committee from\n14 submissions, and 6 lecture notes for the distilled tutorials that we\nsolicited."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66", 
    "link": "http://arxiv.org/pdf/1109.0638v1", 
    "other_authors": "Masanobu Umeda, Ryoto Naruse, Hiroaki Sone, Keiichi Katamine", 
    "title": "Translating Nondeterministic Functional Language based on Attribute   Grammars into Java", 
    "arxiv-id": "1109.0638v1", 
    "author": "Keiichi Katamine", 
    "publish": "2011-09-03T16:54:11Z", 
    "summary": "Knowledge-based systems are suitable for realizing advanced functions that\nrequire domain-specific expert knowledge, while knowledge representation\nlanguages and their supporting environments are essential for realizing such\nsystems. Although Prolog is useful and effective in realizing such a supporting\nenvironment, the language interoperability with other implementation languages,\nsuch as Java, is often an important issue in practical application development.\nThis paper describes the techniques for translating a knowledge representation\nlanguage that is a nondeterministic functional language based on attribute\ngrammars into Java. The translation is based on binarization and the techniques\nproposed for Prolog to Java translation although the semantics are different\nfrom those of Prolog. A continuation unit is introduced to handle continuation\nefficiently, while the variable and register management on backtracking is\nsimplified by using the single and unidirectional assignment features of\nvariables. An experimental translator written in the language itself\nsuccessfully generates Java code, while experimental results show that the\ngenerated code is over 25 times faster than that of Prolog Cafe for\nnondeterministic programs, and over 2 times faster for deterministic programs.\nThe generated code is also over 2 times faster than B-Prolog for\nnondeterministic programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.1", 
    "link": "http://arxiv.org/pdf/1109.0774v1", 
    "other_authors": "Tim Bauer, Martin Erwig, Alan Fern, Jervis Pinto", 
    "title": "Adaptation-Based Programming in Haskell", 
    "arxiv-id": "1109.0774v1", 
    "author": "Jervis Pinto", 
    "publish": "2011-09-05T01:56:05Z", 
    "summary": "We present an embedded DSL to support adaptation-based programming (ABP) in\nHaskell. ABP is an abstract model for defining adaptive values, called\nadaptives, which adapt in response to some associated feedback. We show how our\ndesign choices in Haskell motivate higher-level combinators and constructs and\nhelp us derive more complicated compositional adaptives.\n  We also show an important specialization of ABP is in support of\nreinforcement learning constructs, which optimize adaptive values based on a\nprogrammer-specified objective function. This permits ABP users to easily\ndefine adaptive values that express uncertainty anywhere in their programs.\nOver repeated executions, these adaptive values adjust to more efficient ones\nand enable the user's programs to self optimize.\n  The design of our DSL depends significantly on the use of type classes. We\nwill illustrate, along with presenting our DSL, how the use of type classes can\nsupport the gradual evolution of DSLs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.4", 
    "link": "http://arxiv.org/pdf/1109.0777v1", 
    "other_authors": "Dominic Orchard, Alan Mycroft", 
    "title": "Efficient and Correct Stencil Computation via Pattern Matching and   Static Typing", 
    "arxiv-id": "1109.0777v1", 
    "author": "Alan Mycroft", 
    "publish": "2011-09-05T01:56:26Z", 
    "summary": "Stencil computations, involving operations over the elements of an array, are\na common programming pattern in scientific computing, games, and image\nprocessing. As a programming pattern, stencil computations are highly regular\nand amenable to optimisation and parallelisation. However, general-purpose\nlanguages obscure this regular pattern from the compiler, and even the\nprogrammer, preventing optimisation and obfuscating (in)correctness. This paper\nfurthers our work on the Ypnos domain-specific language for stencil\ncomputations embedded in Haskell. Ypnos allows declarative, abstract\nspecification of stencil computations, exposing the structure of a problem to\nthe compiler and to the programmer via specialised syntax. In this paper we\nshow the decidable safety guarantee that well-formed, well-typed Ypnos programs\ncannot index outside of array boundaries. Thus indexing in Ypnos is safe and\nrun-time bounds checking can be eliminated. Program information is encoded as\ntypes, using the advanced type-system features of the Glasgow Haskell Compiler,\nwith the safe-indexing invariant enforced at compile time via type checking."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.5", 
    "link": "http://arxiv.org/pdf/1109.0778v1", 
    "other_authors": "Tiark Rompf, Arvind K. Sujeeth, HyoukJoong Lee, Kevin J. Brown, Hassan Chafi, Martin Odersky, Kunle Olukotun", 
    "title": "Building-Blocks for Performance Oriented DSLs", 
    "arxiv-id": "1109.0778v1", 
    "author": "Kunle Olukotun", 
    "publish": "2011-09-05T01:56:34Z", 
    "summary": "Domain-specific languages raise the level of abstraction in software\ndevelopment. While it is evident that programmers can more easily reason about\nvery high-level programs, the same holds for compilers only if the compiler has\nan accurate model of the application domain and the underlying target platform.\nSince mapping high-level, general-purpose languages to modern, heterogeneous\nhardware is becoming increasingly difficult, DSLs are an attractive way to\ncapitalize on improved hardware performance, precisely by making the compiler\nreason on a higher level. Implementing efficient DSL compilers is a daunting\ntask however, and support for building performance-oriented DSLs is urgently\nneeded. To this end, we present the Delite Framework, an extensible toolkit\nthat drastically simplifies building embedded DSLs and compiling DSL programs\nfor execution on heterogeneous hardware. We discuss several building blocks in\nsome detail and present experimental results for the OptiML machine-learning\nDSL implemented on top of Delite."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.7", 
    "link": "http://arxiv.org/pdf/1109.0780v1", 
    "other_authors": "Eric Walkingshaw, Martin Erwig", 
    "title": "A DSEL for Studying and Explaining Causation", 
    "arxiv-id": "1109.0780v1", 
    "author": "Martin Erwig", 
    "publish": "2011-09-05T01:56:47Z", 
    "summary": "We present a domain-specific embedded language (DSEL) in Haskell that\nsupports the philosophical study and practical explanation of causation. The\nlanguage provides constructs for modeling situations comprised of events and\nfunctions for reliably determining the complex causal relationships that emerge\nbetween these events. It enables the creation of visual explanations of these\ncausal relationships and a means to systematically generate alternative,\nrelated scenarios, along with corresponding outcomes and causes. The DSEL is\nbased on neuron diagrams, a visual notation that is well established in\npractice and has been successfully employed for causation explanation and\nresearch. In addition to its immediate applicability by users of neuron\ndiagrams, the DSEL is extensible, allowing causation experts to extend the\nnotation to introduce special-purpose causation constructs. The DSEL also\nextends the notation of neuron diagrams to operate over non-boolean values,\nimproving its expressiveness and offering new possibilities for causation\nresearch and its applications."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.8", 
    "link": "http://arxiv.org/pdf/1109.0781v1", 
    "other_authors": "William R. Cook, Ralf L\u00e4mmel", 
    "title": "Tutorial on Online Partial Evaluation", 
    "arxiv-id": "1109.0781v1", 
    "author": "Ralf L\u00e4mmel", 
    "publish": "2011-09-05T01:56:53Z", 
    "summary": "This paper is a short tutorial introduction to online partial evaluation. We\nshow how to write a simple online partial evaluator for a simple, pure,\nfirst-order, functional programming language. In particular, we show that the\npartial evaluator can be derived as a variation on a compositionally defined\ninterpreter. We demonstrate the use of the resulting partial evaluator for\nprogram optimization in the context of model-driven development."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1109.0785v1", 
    "other_authors": "Keiko Nakata", 
    "title": "Resumption-based big-step and small-step interpreters for While with   interactive I/O", 
    "arxiv-id": "1109.0785v1", 
    "author": "Keiko Nakata", 
    "publish": "2011-09-05T01:57:20Z", 
    "summary": "In this tutorial, we program big-step and small-step total interpreters for\nthe While language extended with input and output primitives. While is a simple\nimperative language consisting of skip, assignment, sequence, conditional and\nloop. We first develop trace-based interpreters for While. Traces are\npotentially infinite nonempty sequences of states. The interpreters assign\ntraces to While programs: for us, traces are denotations of While programs. The\ntrace is finite if the program is terminating and infinite if the program is\nnon-terminating. However, we cannot decide (i.e., write a program to\ndetermine), for any given program, whether its trace is finite or infinite,\nwhich amounts to deciding the halting problem. We then extend While with\ninteractive input/output primitives. Accordingly, we extend the interpreters by\ngeneralizing traces to resumptions.\n  The tutorial is based on our previous work with T. Uustalu on reasoning about\ninteractive programs in the setting of constructive type theory."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1109.2405v1", 
    "other_authors": "David Monniaux, Julien Le Guen", 
    "title": "Stratified Static Analysis Based on Variable Dependencies", 
    "arxiv-id": "1109.2405v1", 
    "author": "Julien Le Guen", 
    "publish": "2011-09-12T08:48:00Z", 
    "summary": "In static analysis by abstract interpretation, one often uses widening\noperators in order to enforce convergence within finite time to an inductive\ninvariant. Certain widening operators, including the classical one over finite\npolyhedra, exhibit an unintuitive behavior: analyzing the program over a subset\nof its variables may lead a more precise result than analyzing the original\nprogram! In this article, we present simple workarounds for such behavior."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1109.4048v1", 
    "other_authors": "Shinji Kono, Kento Yogi", 
    "title": "Implementing Continuation based language in GCC", 
    "arxiv-id": "1109.4048v1", 
    "author": "Kento Yogi", 
    "publish": "2011-09-19T14:51:27Z", 
    "summary": "We have implemented C like Continuation based programming language.\nContinuation based C, CbC was implemented using micro-C on various\narchitecture, and we have tried several CbC programming experiments. Here we\nreport new implementation of CbC compiler based on GCC 4.2.3. Since it contains\nfull C capability, we can use CbC and C in a mixture."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1109.4467v2", 
    "other_authors": "David Van Horn, Matthew Might", 
    "title": "Pushdown Abstractions of JavaScript", 
    "arxiv-id": "1109.4467v2", 
    "author": "Matthew Might", 
    "publish": "2011-09-21T03:26:53Z", 
    "summary": "We design a family of program analyses for JavaScript that make no\napproximation in matching calls with returns, exceptions with handlers, and\nbreaks with labels. We do so by starting from an established reduction\nsemantics for JavaScript and systematically deriving its intensional abstract\ninterpretation. Our first step is to transform the semantics into an equivalent\nlow-level abstract machine: the JavaScript Abstract Machine (JAM). We then give\nan infinite-state yet decidable pushdown machine whose stack precisely models\nthe structure of the concrete program stack. The precise model of stack\nstructure in turn confers precise control-flow analysis even in the presence of\ncontrol effects, such as exceptions and finally blocks. We give pushdown\ngeneralizations of traditional forms of analysis such as k-CFA, and prove the\npushdown framework for abstract interpretation is sound and computable."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1109.5416v6", 
    "other_authors": "M. H. van Emden", 
    "title": "Matrix Code", 
    "arxiv-id": "1109.5416v6", 
    "author": "M. H. van Emden", 
    "publish": "2011-09-25T23:31:15Z", 
    "summary": "Matrix Code gives imperative programming a mathematical semantics and\nheuristic power comparable in quality to functional and logic programming. A\nprogram in Matrix Code is developed incrementally from a specification in\npre/post-condition form. The computations of a code matrix are characterized by\npowers of the matrix when it is interpreted as a transformation in a space of\nvectors of logical conditions. Correctness of a code matrix is expressed in\nterms of a fixpoint of the transformation. The abstract machine for Matrix Code\nis the dual-state machine, which we present as a variant of the classical\nfinite-state machine."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.66.12", 
    "link": "http://arxiv.org/pdf/1111.3673v1", 
    "other_authors": "N. Pataki", 
    "title": "C++ Standard Template Library by template specialized containers", 
    "arxiv-id": "1111.3673v1", 
    "author": "N. Pataki", 
    "publish": "2011-11-15T22:00:48Z", 
    "summary": "The C++ Standard Template Library is the flagship example for libraries based\non the generic programming paradigm. The usage of this library is intended to\nminimize the number of classical C/C++ errors, but does not warrant bug-free\nprograms. Furthermore, many new kinds of errors may arise from the inaccurate\nuse of the generic programming paradigm, like dereferencing invalid iterators\nor misunderstanding remove-like algorithms. In this paper we present some\ntypical scenarios that may cause runtime or portability problems. We emit\nwarnings and errors while these risky constructs are used. We also present a\ngeneral approach to emit \"customized\" warnings. We support the so-called\n\"believe-me marks\" to disable warnings. We present another typical usage of our\ntechnique, when classes become deprecated during the software lifecycle."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.74.2", 
    "link": "http://arxiv.org/pdf/1111.4737v1", 
    "other_authors": "Sebastian Buchwald, Edgar Jakumeit", 
    "title": "Compiler Optimization: A Case for the Transformation Tool Contest", 
    "arxiv-id": "1111.4737v1", 
    "author": "Edgar Jakumeit", 
    "publish": "2011-11-21T05:24:23Z", 
    "summary": "An optimizing compiler consists of a front end parsing a textual programming\nlanguage into an intermediate representation (IR), a middle end performing\noptimizations on the IR, and a back end lowering the IR to a target\nrepresentation (TR) built of operations supported by the target hardware. In\nmodern compiler construction graph-based IRs are employed. Optimization and\nlowering tasks can then be implemented with graph transformation rules. This\ncase provides two compiler tasks to evaluate the participating tools regarding\nperformance."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.74.7", 
    "link": "http://arxiv.org/pdf/1111.4742v1", 
    "other_authors": "Sebastian Buchwald, Edgar Jakumeit", 
    "title": "Solving the TTC 2011 Compiler Optimization Case with GrGen.NET", 
    "arxiv-id": "1111.4742v1", 
    "author": "Edgar Jakumeit", 
    "publish": "2011-11-21T05:24:59Z", 
    "summary": "The challenge of the Compiler Optimization Case is to perform local\noptimizations and instruction selection on the graph-based intermediate\nrepresentation of a compiler. The case is designed to compare participating\ntools regarding their performance. We tackle this task employing the general\npurpose graph rewrite system GrGen.NET (www.grgen.net)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.74.16", 
    "link": "http://arxiv.org/pdf/1111.4751v1", 
    "other_authors": "Edgar Jakumeit, Sebastian Buchwald", 
    "title": "Solving the TTC 2011 Reengineering Case with GrGen.NET", 
    "arxiv-id": "1111.4751v1", 
    "author": "Sebastian Buchwald", 
    "publish": "2011-11-21T05:26:14Z", 
    "summary": "The challenge of the Reengineering Case is to extract a state machine model\nout of the abstract syntax graph of a Java program. The extracted state machine\noffers a reduced view on the full program graph and thus helps to understand\nthe program regarding the question of interest. We tackle this task employing\nthe general purpose graph rewrite system GrGen.NET (www.grgen.net)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.74.23", 
    "link": "http://arxiv.org/pdf/1111.4757v1", 
    "other_authors": "Sebastian Buchwald, Edgar Jakumeit", 
    "title": "Saying Hello World with GrGen.NET - A Solution to the TTC 2011   Instructive Case", 
    "arxiv-id": "1111.4757v1", 
    "author": "Edgar Jakumeit", 
    "publish": "2011-11-21T05:27:20Z", 
    "summary": "We introduce the graph transformation tool GrGen.NET (www.grgen.net) by\nsolving the Hello World Case of the Transformation Tool Contest 2011 which\nconsists of a collection of small transformation tasks; for each task a section\nis given explaining our implementation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.85.5", 
    "link": "http://arxiv.org/pdf/1202.0693v3", 
    "other_authors": "Ivan Gazeau, Dale Miller, Catuscia Palamidessi", 
    "title": "A non-local method for robustness analysis of floating point programs", 
    "arxiv-id": "1202.0693v3", 
    "author": "Catuscia Palamidessi", 
    "publish": "2012-02-03T13:16:27Z", 
    "summary": "Robustness is a standard correctness property which intuitively means that if\nthe input to the program changes less than a fixed small amount then the output\nchanges only slightly. This notion is useful in the analysis of rounding error\nfor floating point programs because it helps to establish bounds on output\nerrors introduced by both measurement errors and by floating point computation.\nCompositional methods often do not work since key constructs---like the\nconditional and the while-loop---are not robust. We propose a method for\nproving the robustness of a while-loop. This method is non-local in the sense\nthat instead of breaking the analysis down to single lines of code, it checks\ncertain global properties of its structure. We show the applicability of our\nmethod on two standard algorithms: the CORDIC computation of the cosine and\nDijkstra's shortest path algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(1:17)2012", 
    "link": "http://arxiv.org/pdf/1202.2086v2", 
    "other_authors": "Viviana Bono, Luca Padovani", 
    "title": "Typing Copyless Message Passing", 
    "arxiv-id": "1202.2086v2", 
    "author": "Luca Padovani", 
    "publish": "2012-02-09T19:22:13Z", 
    "summary": "We present a calculus that models a form of process interaction based on\ncopyless message passing, in the style of Singularity OS. The calculus is\nequipped with a type system ensuring that well-typed processes are free from\nmemory faults, memory leaks, and communication errors. The type system is\nessentially linear, but we show that linearity alone is inadequate, because it\nleaves room for scenarios where well-typed processes leak significant amounts\nof memory. We address these problems basing the type system upon an original\nvariant of session types."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(1:14)2012", 
    "link": "http://arxiv.org/pdf/1202.2287v2", 
    "other_authors": "Jean Goubault-Larrecq", 
    "title": "QRB-Domains and the Probabilistic Powerdomain", 
    "arxiv-id": "1202.2287v2", 
    "author": "Jean Goubault-Larrecq", 
    "publish": "2012-02-10T15:46:29Z", 
    "summary": "Is there any Cartesian-closed category of continuous domains that would be\nclosed under Jones and Plotkin's probabilistic powerdomain construction? This\nis a major open problem in the area of denotational semantics of probabilistic\nhigher-order languages. We relax the question, and look for quasi-continuous\ndcpos instead. We introduce a natural class of such quasi-continuous dcpos, the\nomega-QRB-domains. We show that they form a category omega-QRB with pleasing\nproperties: omega-QRB is closed under the probabilistic powerdomain functor,\nunder finite products, under taking bilimits of expanding sequences, under\nretracts, and even under so-called quasi-retracts. But... omega-QRB is not\nCartesian closed. We conclude by showing that the QRB domains are just one half\nof an FS-domain, merely lacking control."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(1:14)2012", 
    "link": "http://arxiv.org/pdf/1202.2486v1", 
    "other_authors": "Neal Glew", 
    "title": "Subtyping for F-Bounded Quantifiers and Equirecursive Types (Extended   Version)", 
    "arxiv-id": "1202.2486v1", 
    "author": "Neal Glew", 
    "publish": "2012-02-12T03:15:17Z", 
    "summary": "This paper defines a notion of binding trees that provide a suitable model\nfor second-order type systems with F-bounded quantifiers and equirecursive\ntypes. It defines a notion of regular binding trees that correspond in the\nright way to notions of regularity in the first-order case. It defines a notion\nof subtyping on these trees and proves various properties of the subtyping\nrelation. It defines a mapping from types to trees and shows that types produce\nregular binding trees. It presents a set of type equality and subtyping rules,\nand proves them sound and complete with respect to the tree interpretation. It\ndefines a notion of binding-tree automata and how these generate regular\nbinding trees. It gives a polynomial-time algorithm for deciding when two\nautomata's trees are in the subtyping relation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.3", 
    "link": "http://arxiv.org/pdf/1202.2917v1", 
    "other_authors": "Patrick Bahr, Tom Hvitved", 
    "title": "Parametric Compositional Data Types", 
    "arxiv-id": "1202.2917v1", 
    "author": "Tom Hvitved", 
    "publish": "2012-02-14T03:01:17Z", 
    "summary": "In previous work we have illustrated the benefits that compositional data\ntypes (CDTs) offer for implementing languages and in general for dealing with\nabstract syntax trees (ASTs). Based on Swierstra's data types \\'a la carte,\nCDTs are implemented as a Haskell library that enables the definition of\nrecursive data types and functions on them in a modular and extendable fashion.\nAlthough CDTs provide a powerful tool for analysing and manipulating ASTs, they\nlack a convenient representation of variable binders. In this paper we remedy\nthis deficiency by combining the framework of CDTs with Chlipala's parametric\nhigher-order abstract syntax (PHOAS). We show how a generalisation from\nfunctors to difunctors enables us to capture PHOAS while still maintaining the\nfeatures of the original implementation of CDTs, in particular its modularity.\nUnlike previous approaches, we avoid so-called exotic terms without resorting\nto abstract types: this is crucial when we want to perform transformations on\nCDTs that inspect the recursively computed CDTs, e.g. constant folding."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.5", 
    "link": "http://arxiv.org/pdf/1202.2919v1", 
    "other_authors": "Mauro Jaskelioff, Ondrej Rypacek", 
    "title": "An Investigation of the Laws of Traversals", 
    "arxiv-id": "1202.2919v1", 
    "author": "Ondrej Rypacek", 
    "publish": "2012-02-14T03:01:31Z", 
    "summary": "Traversals of data structures are ubiquitous in programming. Consequently, it\nis important to be able to characterise those structures that are traversable\nand understand their algebraic properties. Traversable functors have been\ncharacterised by McBride and Paterson as those equipped with a distributive law\nover arbitrary applicative functors; however, laws that fully capture the\nintuition behind traversals are missing. This article is an attempt to remedy\nthis situation by proposing laws for characterising traversals that capture the\nintuition behind them. To support our claims, we prove that finitary containers\nare traversable in our sense and argue that elements in a traversable structure\nare visited exactly once."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.6", 
    "link": "http://arxiv.org/pdf/1202.2920v1", 
    "other_authors": "Jos\u00e9 Pedro Magalh\u00e3es, Andres L\u00f6h", 
    "title": "A Formal Comparison of Approaches to Datatype-Generic Programming", 
    "arxiv-id": "1202.2920v1", 
    "author": "Andres L\u00f6h", 
    "publish": "2012-02-14T03:01:41Z", 
    "summary": "Datatype-generic programming increases program abstraction and reuse by\nmaking functions operate uniformly across different types. Many approaches to\ngeneric programming have been proposed over the years, most of them for\nHaskell, but recently also for dependently typed languages such as Agda.\nDifferent approaches vary in expressiveness, ease of use, and implementation\ntechniques.\n  Some work has been done in comparing the different approaches informally.\nHowever, to our knowledge there have been no attempts to formally prove\nrelations between different approaches. We thus present a formal comparison of\ngeneric programming libraries. We show how to formalise different approaches in\nAgda, including a coinductive representation, and then establish theorems that\nrelate the approaches to each other. We provide constructive proofs of\ninclusion of one approach in another that can be used to convert between\napproaches, helping to reduce code duplication across different libraries. Our\nformalisation also helps in providing a clear picture of the potential of each\napproach, especially in relating different generic views and their\nexpressiveness."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.7", 
    "link": "http://arxiv.org/pdf/1202.2921v1", 
    "other_authors": "Tomas Petricek", 
    "title": "Evaluation strategies for monadic computations", 
    "arxiv-id": "1202.2921v1", 
    "author": "Tomas Petricek", 
    "publish": "2012-02-14T03:01:45Z", 
    "summary": "Monads have become a powerful tool for structuring effectful computations in\nfunctional programming, because they make the order of effects explicit. When\ntranslating pure code to a monadic version, we need to specify evaluation order\nexplicitly. Two standard translations give call-by-value and call-by-name\nsemantics. The resulting programs have different structure and types, which\nmakes revisiting the choice difficult.\n  In this paper, we translate pure code to monadic using an additional\noperation malias that abstracts out the evaluation strategy. The malias\noperation is based on computational comonads; we use a categorical framework to\nspecify the laws that are required to hold about the operation.\n  For any monad, we show implementations of malias that give call-by-value and\ncall-by-name semantics. Although we do not give call-by-need semantics for all\nmonads, we show how to turn certain monads into an extended monad with\ncall-by-need semantics, which partly answers an open question. Moreover, using\nour unified translation, it is possible to change the evaluation strategy of\nfunctional code translated to the monadic form without changing its structure\nor types."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.8", 
    "link": "http://arxiv.org/pdf/1202.2922v1", 
    "other_authors": "Maciej Pir\u00f3g, Jeremy Gibbons", 
    "title": "Tracing monadic computations and representing effects", 
    "arxiv-id": "1202.2922v1", 
    "author": "Jeremy Gibbons", 
    "publish": "2012-02-14T03:01:53Z", 
    "summary": "In functional programming, monads are supposed to encapsulate computations,\neffectfully producing the final result, but keeping to themselves the means of\nacquiring it. For various reasons, we sometimes want to reveal the internals of\na computation. To make that possible, in this paper we introduce monad\ntransformers that add the ability to automatically accumulate observations\nabout the course of execution as an effect. We discover that if we treat the\nresulting trace as the actual result of the computation, we can find new\nfunctionality in existing monads, notably when working with non-terminating\ncomputations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.9", 
    "link": "http://arxiv.org/pdf/1202.2923v1", 
    "other_authors": "Vilhelm Sj\u00f6berg, Chris Casinghino, Ki Yung Ahn, Nathan Collins, Harley D. Eades III, Peng Fu, Garrin Kimmell, Tim Sheard, Aaron Stump, Stephanie Weirich", 
    "title": "Irrelevance, Heterogeneous Equality, and Call-by-value Dependent Type   Systems", 
    "arxiv-id": "1202.2923v1", 
    "author": "Stephanie Weirich", 
    "publish": "2012-02-14T03:01:59Z", 
    "summary": "We present a full-spectrum dependently typed core language which includes\nboth nontermination and computational irrelevance (a.k.a. erasure), a\ncombination which has not been studied before. The two features interact: to\nprotect type safety we must be careful to only erase terminating expressions.\nOur language design is strongly influenced by the choice of CBV evaluation, and\nby our novel treatment of propositional equality which has a heterogeneous,\ncompletely erased elimination form."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1202.2924v1", 
    "other_authors": "Wouter Swierstra", 
    "title": "From Mathematics to Abstract Machine: A formal derivation of an   executable Krivine machine", 
    "arxiv-id": "1202.2924v1", 
    "author": "Wouter Swierstra", 
    "publish": "2012-02-14T03:02:06Z", 
    "summary": "This paper presents the derivation of an executable Krivine abstract machine\nfrom a small step interpreter for the simply typed lambda calculus in the\ndependently typed programming language Agda."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1202.3247v1", 
    "other_authors": "Gabriel Kerneis, Juliusz Chroboczek", 
    "title": "Lambda-lifting and CPS conversion in an imperative language", 
    "arxiv-id": "1202.3247v1", 
    "author": "Juliusz Chroboczek", 
    "publish": "2012-02-15T09:41:45Z", 
    "summary": "This paper is a companion technical report to the article\n\"Continuation-Passing C: from threads to events through continuations\". It\ncontains the complete version of the proofs of correctness of lambda-lifting\nand CPS-conversion presented in the article."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1202.5539v1", 
    "other_authors": "Yin Wang, R. Kent Dybvig", 
    "title": "Register Allocation By Model Transformer Semantics", 
    "arxiv-id": "1202.5539v1", 
    "author": "R. Kent Dybvig", 
    "publish": "2012-02-24T19:58:26Z", 
    "summary": "Register allocation has long been formulated as a graph coloring problem,\ncoloring the conflict graph with physical registers. Such a formulation does\nnot fully capture the goal of the allocation, which is to minimize the traffic\nbetween registers and memory. Linear scan has been proposed as an alternative\nto graph coloring, but in essence, it can be viewed as a greedy algorithm for\ngraph coloring: coloring the vertices not in the order of their degrees, but in\nthe order of their occurence in the program. Thus it suffers from almost the\nsame constraints as graph coloring. In this article, I propose a new method of\nregister allocation based on the ideas of model transformer semantics (MTS) and\nstatic cache replacement (SCR). Model transformer semantics captures the\nsemantics of registers and the stack. Static cache replacement relaxes the\nassumptions made by graph coloring and linear scan, aiming directly at reducing\nregister-memory traffic. The method explores a much larger solution space than\nthat of graph coloring and linear scan, thus providing more opportunities of\noptimization. It seamlessly performs live range splitting, an optimization\nfound in extensions to graph coloring and linear scan. Also, it simplifies the\ncompiler, and its semantics-based approach provides possibilities of\nsimplifying the formal verification of compilers."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1202.5959v2", 
    "other_authors": "Dariusz Biernacki, Serguei Lenglet", 
    "title": "Normal Form Bisimulations for Delimited-Control Operators", 
    "arxiv-id": "1202.5959v2", 
    "author": "Serguei Lenglet", 
    "publish": "2012-02-27T14:49:00Z", 
    "summary": "We define a notion of normal form bisimilarity for the untyped call-by-value\nlambda calculus extended with the delimited-control operators shift and reset.\nNormal form bisimilarities are simple, easy-to-use behavioral equivalences\nwhich relate terms without having to test them within all contexts (like\ncontextual equivalence), or by applying them to function arguments (like\napplicative bisimilarity). We prove that the normal form bisimilarity for shift\nand reset is sound but not complete w.r.t. contextual equivalence and we define\nup-to techniques that aim at simplifying bisimulation proofs. Finally, we\nillustrate the simplicity of the techniques we develop by proving several\nequivalences on terms."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1202.6593v1", 
    "other_authors": "Luis Quesada, Fernando Berzal, Juan-Carlos Cubero", 
    "title": "A Model-Driven Parser Generator, from Abstract Syntax Trees to Abstract   Syntax Graphs", 
    "arxiv-id": "1202.6593v1", 
    "author": "Juan-Carlos Cubero", 
    "publish": "2012-02-29T16:27:06Z", 
    "summary": "Model-based parser generators decouple language specification from language\nprocessing. The model-driven approach avoids the limitations that conventional\nparser generators impose on the language designer. Conventional tools require\nthe designed language grammar to conform to the specific kind of grammar\nsupported by the particular parser generator (being LL and LR parser generators\nthe most common). Model-driven parser generators, like ModelCC, do not require\na grammar specification, since that grammar can be automatically derived from\nthe language model and, if needed, adapted to conform to the requirements of\nthe given kind of parser, all of this without interfering with the conceptual\ndesign of the language and its associated applications. Moreover, model-driven\ntools such as ModelCC are able to automatically resolve references between\nlanguage elements, hence producing abstract syntax graphs instead of abstract\nsyntax trees as the result of the parsing process. Such graphs are not confined\nto directed acyclic graphs and they can contain cycles, since ModelCC supports\nanaphoric, cataphoric, and recursive references."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.76.10", 
    "link": "http://arxiv.org/pdf/1205.4672v1", 
    "other_authors": "Yaroub Elloumi, Mohamed Akil, Mohamed Hedi Bedoui", 
    "title": "Timing and Code Size Optimization on Achieving Full Parallelism in   Uniform Nested Loops", 
    "arxiv-id": "1205.4672v1", 
    "author": "Mohamed Hedi Bedoui", 
    "publish": "2012-05-17T22:45:45Z", 
    "summary": "Multidimensional Retiming is one of the most important optimization\ntechniques to improve timing parameters of nested loops. It consists in\nexploring the iterative and recursive structures of loops to redistribute\ncomputation nodes on cycle periods, and thus to achieve full parallelism.\nHowever, this technique introduces a large overhead in a loop generation due to\nthe loop transformation. The provided solutions are generally characterized by\nan important cycle number and a great code size. It represents the most\nlimiting factors while implementing them in embedded systems. In this paper, we\npresent a new Multidimensional Retiming technique, called \"Optimal\nMultidimensional Retiming\" (OMDR). It reveals the timing and data dependency\ncharacteristics of nodes, to minimize the overhead. The experimental results\nshow that the average improvement on the execution time of the nested loops by\nour technique is 19.31% compared to the experiments provided by an existent\nMultidimensional Retiming Technique. The average code size is reduced by 43.53%\ncompared to previous experiments."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-11(4:12)2015", 
    "link": "http://arxiv.org/pdf/1205.5344v5", 
    "other_authors": "Simon J. Gay, Nils Gesbert, Ant\u00f3nio Ravara, Vasco T. Vasconcelos", 
    "title": "Modular session types for objects", 
    "arxiv-id": "1205.5344v5", 
    "author": "Vasco T. Vasconcelos", 
    "publish": "2012-05-24T06:30:42Z", 
    "summary": "Session types allow communication protocols to be specified\ntype-theoretically so that protocol implementations can be verified by static\ntype checking. We extend previous work on session types for distributed\nobject-oriented languages in three ways. (1) We attach a session type to a\nclass definition, to specify the possible sequences of method calls. (2) We\nallow a session type (protocol) implementation to be modularized, i.e.\npartitioned into separately-callable methods. (3) We treat session-typed\ncommunication channels as objects, integrating their session types with the\nsession types of classes. The result is an elegant unification of communication\nchannels and their session types, distributed object-oriented programming, and\na form of typestate supporting non-uniform objects, i.e. objects that\ndynamically change the set of available methods. We define syntax, operational\nse-mantics, a sound type system, and a sound and complete type checking\nalgorithm for a small distributed class-based object-oriented language with\nstructural subtyping. Static typing guarantees that both sequences of messages\non channels, and sequences of method calls on objects, conform to\ntype-theoretic specifications, thus ensuring type-safety. The language includes\nexpected features of session types, such as delegation, and expected features\nof object-oriented programming, such as encapsulation of local state."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:12)2012", 
    "link": "http://arxiv.org/pdf/1206.0357v2", 
    "other_authors": "Neil Ghani, Patricia Johann, Clement Fumex", 
    "title": "Generic Fibrational Induction", 
    "arxiv-id": "1206.0357v2", 
    "author": "Clement Fumex", 
    "publish": "2012-06-02T09:26:53Z", 
    "summary": "This paper provides an induction rule that can be used to prove properties of\ndata structures whose types are inductive, i.e., are carriers of initial\nalgebras of functors. Our results are semantic in nature and are inspired by\nHermida and Jacobs' elegant algebraic formulation of induction for polynomial\ndata types. Our contribution is to derive, under slightly different\nassumptions, a sound induction rule that is generic over all inductive types,\npolynomial or not. Our induction rule is generic over the kinds of properties\nto be proved as well: like Hermida and Jacobs, we work in a general fibrational\nsetting and so can accommodate very general notions of properties on inductive\ntypes rather than just those of a particular syntactic form. We establish the\nsoundness of our generic induction rule by reducing induction to iteration. We\nthen show how our generic induction rule can be instantiated to give induction\nrules for the data types of rose trees, finite hereditary sets, and\nhyperfunctions. The first of these lies outside the scope of Hermida and\nJacobs' work because it is not polynomial, and as far as we are aware, no\ninduction rules have been known to exist for the second and third in a general\nfibrational framework. Our instantiation for hyperfunctions underscores the\nvalue of working in the general fibrational setting since this data type cannot\nbe interpreted as a set."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:12)2012", 
    "link": "http://arxiv.org/pdf/1206.1687v1", 
    "other_authors": "Silvia Crafa", 
    "title": "Behavioural Types for Actor Systems", 
    "arxiv-id": "1206.1687v1", 
    "author": "Silvia Crafa", 
    "publish": "2012-06-08T07:42:55Z", 
    "summary": "Recent mainstream programming languages such as Erlang or Scala have renewed\nthe interest on the Actor model of concurrency. However, the literature on the\nstatic analysis of actor systems is still lacking of mature formal methods. In\nthis paper we present a minimal actor calculus that takes as primitive the\nbasic constructs of Scala's Actors API. More precisely, actors can send\nasynchronous messages, process received messages according to a pattern\nmatching mechanism, and dynamically create new actors, whose scope can be\nextruded by passing actor names as message parameters. Drawing inspiration from\nthe linear types and session type theories developed for process calculi, we\nput forward a behavioural type system that addresses the key issues of an actor\ncalculus. We then study a safety property dealing with the determinism of\nfinite actor com- munication. More precisely, we show that well typed and\nbalanced actor systems are (i) deadlock-free and (ii) any message will\neventually be handled by the target actor, and dually no actor will\nindefinitely wait for an expected message"
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(2:12)2012", 
    "link": "http://arxiv.org/pdf/1206.1969v1", 
    "other_authors": "Iztok Fister Jr., Marjan Mernik, Iztok Fister, Dejan Hrn\u010di\u010d", 
    "title": "Implementation of EasyTime Formal Semantics using a LISA Compiler   Generator", 
    "arxiv-id": "1206.1969v1", 
    "author": "Dejan Hrn\u010di\u010d", 
    "publish": "2012-06-09T20:10:16Z", 
    "summary": "A manual measuring time tool in mass sporting competitions would not be\nimaginable nowadays, because many modern disciplines, such as IRONMAN, last a\nlong-time and, therefore, demand additional reliability. Moreover, automatic\ntiming-devices based on RFID technology, have become cheaper. However, these\ndevices cannot operate as stand-alone because they need a computer measuring\nsystem that is capable of processing incoming events, encoding the results,\nassigning them to the correct competitor, sorting the results according to the\nachieved times, and then providing a printout of the results. This article\npresents the domain-specific language EasyTime, which enables the controlling\nof an agent by writing the events within a database. It focuses, in particular,\non the implementation of EasyTime with a LISA tool that enables the automatic\nconstruction of compilers from language specifications, using Attribute\nGrammars."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.tcs.2012.12.018", 
    "link": "http://arxiv.org/pdf/1206.2188v2", 
    "other_authors": "Samir Genaim, Damiano Zanardini", 
    "title": "Reachability-based Acyclicity Analysis by Abstract Interpretation", 
    "arxiv-id": "1206.2188v2", 
    "author": "Damiano Zanardini", 
    "publish": "2012-06-11T12:56:28Z", 
    "summary": "In programming languages with dynamic use of memory, such as Java, knowing\nthat a reference variable x points to an acyclic data structure is valuable for\nthe analysis of termination and resource usage (e.g., execution time or memory\nconsumption). For instance, this information guarantees that the depth of the\ndata structure to which x points is greater than the depth of the data\nstructure pointed to by x.f for any field f of x. This, in turn, allows\nbounding the number of iterations of a loop which traverses the structure by\nits depth, which is essential in order to prove the termination or infer the\nresource usage of the loop. The present paper provides an\nAbstract-Interpretation-based formalization of a static analysis for inferring\nacyclicity, which works on the reduced product of two abstract domains:\nreachability, which models the property that the location pointed to by a\nvariable w can be reached by dereferencing another variable v (in this case, v\nis said to reach w); and cyclicity, modeling the property that v can point to a\ncyclic data structure. The analysis is proven to be sound and optimal with\nrespect to the chosen abstraction."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.tcs.2012.12.018", 
    "link": "http://arxiv.org/pdf/1206.2542v1", 
    "other_authors": "Iztok Fister Jr, Marjan Mernik, Iztok Fister, Dejan Hrn\u010di\u010d", 
    "title": "Implementation of the Domain-Specific Language EasyTime using a LISA   Compiler Generator", 
    "arxiv-id": "1206.2542v1", 
    "author": "Dejan Hrn\u010di\u010d", 
    "publish": "2012-06-12T14:25:17Z", 
    "summary": "A manually time-measuring tool in mass sporting competitions cannot be\nimagined nowadays because many modern disciplines, such as IronMan, take a long\ntime and, therefore, demand additional reliability. Moreover, automatic timing\ndevices, based on RFID technology, have become cheaper. However, these devices\ncannot operate stand-alone because they need a computer measuring system that\nis capable of processing the incoming events, encoding the results, assigning\nthem to the correct competitor, sorting the results according to the achieved\ntimes, and then providing a printout of the results. In this article, the\ndomain-specific language EasyTime is presented, which enables the controlling\nof an agent by writing the events in a database. In particular, we are focused\non the implementation of EasyTime with a LISA tool that enables the automatic\nconstruction of compilers from language specifications using Attribute\nGrammars. By using of EasyTime, we can also decrease the number of measuring\ndevices. Furthermore, EasyTime is universal and can be applied to many\ndifferent sporting competitions in practice."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2428116.2428123", 
    "link": "http://arxiv.org/pdf/1206.3523v3", 
    "other_authors": "N. Danner, J. Paykin, J. S. Royer", 
    "title": "A static cost analysis for a higher-order language", 
    "arxiv-id": "1206.3523v3", 
    "author": "J. S. Royer", 
    "publish": "2012-06-15T17:31:29Z", 
    "summary": "We develop a static complexity analysis for a higher-order functional\nlanguage with structural list recursion. The complexity of an expression is a\npair consisting of a cost and a potential. The former is defined to be the size\nof the expression's evaluation derivation in a standard big-step operational\nsemantics. The latter is a measure of the \"future\" cost of using the value of\nthat expression. A translation function tr maps target expressions to\ncomplexities. Our main result is the following Soundness Theorem: If t is a\nterm in the target language, then the cost component of tr(t) is an upper bound\non the cost of evaluating t. The proof of the Soundness Theorem is formalized\nin Coq, providing certified upper bounds on the cost of any expression in the\ntarget language."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2428116.2428123", 
    "link": "http://arxiv.org/pdf/1206.4234v1", 
    "other_authors": "Julien Henry, David Monniaux, Matthieu Moy", 
    "title": "Succinct Representations for Abstract Interpretation", 
    "arxiv-id": "1206.4234v1", 
    "author": "Matthieu Moy", 
    "publish": "2012-06-19T15:20:30Z", 
    "summary": "Abstract interpretation techniques can be made more precise by distinguishing\npaths inside loops, at the expense of possibly exponential complexity.\nSMT-solving techniques and sparse representations of paths and sets of paths\navoid this pitfall. We improve previously proposed techniques for guided static\nanalysis and the generation of disjunctive invariants by combining them with\ntechniques for succinct representations of paths and symbolic representations\nfor transitions based on static single assignment. Because of the\nnon-monotonicity of the results of abstract interpretation with widening\noperators, it is difficult to conclude that some abstraction is more precise\nthan another based on theoretical local precision results. We thus conducted\nextensive comparisons between our new techniques and previous ones, on a\nvariety of open-source packages."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796813000270", 
    "link": "http://arxiv.org/pdf/1206.5386v1", 
    "other_authors": "Joshua Dunfield", 
    "title": "Elaborating Intersection and Union Types", 
    "arxiv-id": "1206.5386v1", 
    "author": "Joshua Dunfield", 
    "publish": "2012-06-23T12:25:22Z", 
    "summary": "Designing and implementing typed programming languages is hard. Every new\ntype system feature requires extending the metatheory and implementation, which\nare often complicated and fragile. To ease this process, we would like to\nprovide general mechanisms that subsume many different features.\n  In modern type systems, parametric polymorphism is fundamental, but\nintersection polymorphism has gained little traction in programming languages.\nMost practical intersection type systems have supported only refinement\nintersections, which increase the expressiveness of types (more precise\nproperties can be checked) without altering the expressiveness of terms;\nrefinement intersections can simply be erased during compilation. In contrast,\nunrestricted intersections increase the expressiveness of terms, and can be\nused to encode diverse language features, promising an economy of both theory\nand implementation.\n  We describe a foundation for compiling unrestricted intersection and union\ntypes: an elaboration type system that generates ordinary lambda-calculus\nterms. The key feature is a Forsythe-like merge construct. With this construct,\nnot all reductions of the source program preserve types; however, we prove that\nordinary call-by-value evaluation of the elaborated program corresponds to a\ntype-preserving evaluation of the source program.\n  We also describe a prototype implementation and applications of unrestricted\nintersections and unions: records, operator overloading, and simulating dynamic\ntyping."
},{
    "category": "cs.PL", 
    "doi": "10.1109/WCRE.2012.12", 
    "link": "http://arxiv.org/pdf/1206.5648v3", 
    "other_authors": "Marco Trudel, Carlo A. Furia, Martin Nordio, Bertrand Meyer, Manuel Oriol", 
    "title": "C to O-O Translation: Beyond the Easy Stuff", 
    "arxiv-id": "1206.5648v3", 
    "author": "Manuel Oriol", 
    "publish": "2012-06-25T11:20:33Z", 
    "summary": "Can we reuse some of the huge code-base developed in C to take advantage of\nmodern programming language features such as type safety, object-orientation,\nand contracts? This paper presents a source-to-source translation of C code\ninto Eiffel, a modern object-oriented programming language, and the supporting\ntool C2Eif. The translation is completely automatic and supports the entire C\nlanguage (ANSI, as well as many GNU C Compiler extensions, through CIL) as used\nin practice, including its usage of native system libraries and inlined\nassembly code. Our experiments show that C2Eif can handle C applications and\nlibraries of significant size (such as vim and libgsl), as well as challenging\nbenchmarks such as the GCC torture tests. The produced Eiffel code is\nfunctionally equivalent to the original C code, and takes advantage of some of\nEiffel's object-oriented features to produce safe and easy-to-debug\ntranslations."
},{
    "category": "cs.PL", 
    "doi": "10.1109/WCRE.2012.12", 
    "link": "http://arxiv.org/pdf/1208.0515v2", 
    "other_authors": "Ugo Dal Lago, Simone Martini", 
    "title": "On Constructor Rewrite Systems and the Lambda Calculus", 
    "arxiv-id": "1208.0515v2", 
    "author": "Simone Martini", 
    "publish": "2012-08-02T15:21:44Z", 
    "summary": "We prove that orthogonal constructor term rewrite systems and lambda-calculus\nwith weak (i.e., no reduction is allowed under the scope of a\nlambda-abstraction) call-by-value reduction can simulate each other with a\nlinear overhead. In particular, weak call-by- value beta-reduction can be\nsimulated by an orthogonal constructor term rewrite system in the same number\nof reduction steps. Conversely, each reduction in a term rewrite system can be\nsimulated by a constant number of beta-reduction steps. This is relevant to\nimplicit computational complexity, because the number of beta steps to normal\nform is polynomially related to the actual cost (that is, as performed on a\nTuring machine) of normalization, under weak call-by-value reduction.\nOrthogonal constructor term rewrite systems and lambda-calculus are thus both\npolynomially related to Turing machines, taking as notion of cost their natural\nparameters."
},{
    "category": "cs.PL", 
    "doi": "10.1109/WCRE.2012.12", 
    "link": "http://arxiv.org/pdf/1208.0535v1", 
    "other_authors": "Christopher Schwaab, Jeremy G. Siek", 
    "title": "Modular Type-Safety Proofs using Dependant Types", 
    "arxiv-id": "1208.0535v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2012-08-02T16:38:13Z", 
    "summary": "While methods of code abstraction and reuse are widespread and well\nresearched, methods of proof abstraction and reuse are still emerging. We\nconsider the use of dependent types for this purpose, introducing a completely\nmechanical approach to proof composition. We show that common techniques for\nabstracting algorithms over data structures naturally translate to abstractions\nover proofs. We first introduce a language composed of a series of smaller\nlanguage components tied together by standard techniques from Malcom (1990). We\nproceed by giving proofs of type preservation for each language component and\nshow that the basic ideas used in composing the syntactic data structures can\nbe applied to their semantics as well."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.89.9", 
    "link": "http://arxiv.org/pdf/1208.2752v1", 
    "other_authors": "Matias David Lee, Daniel Gebler, Pedro R. D'Argenio", 
    "title": "Tree rules in probabilistic transition system specifications with   negative and quantitative premises", 
    "arxiv-id": "1208.2752v1", 
    "author": "Pedro R. D'Argenio", 
    "publish": "2012-08-14T01:52:18Z", 
    "summary": "Probabilistic transition system specifications (PTSSs) in the ntmufnu/ntmuxnu\nformat provide structural operational semantics for Segala-type systems that\nexhibit both probabilistic and nondeterministic behavior and guarantee that\nisimilarity is a congruence.Similar to the nondeterministic case of rule format\ntyft/tyxt, we show that the well-foundedness requirement is unnecessary in the\nprobabilistic setting. To achieve this, we first define an extended version of\nthe ntmufnu/ntmuxnu format in which quantitative premises and conclusions\ninclude nested convex combinations of distributions. This format also\nguarantees that bisimilarity is a congruence. Then, for a given (possibly\nnon-well-founded) PTSS in the new format, we construct an equivalent\nwell-founded transition system consisting of only rules of the simpler\n(well-founded) probabilistic ntree format. Furthermore, we develop a\nproof-theoretic notion for these PTSSs that coincides with the existing\nstratification-based meaning in case the PTSS is stratifiable. This continues\nthe line of research lifting structural operational semantic results from the\nnondeterministic setting to systems with both probabilistic and\nnondeterministic behavior."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS", 
    "link": "http://arxiv.org/pdf/1208.4035v1", 
    "other_authors": "Denis Barthou, Gilbert Grosdidier, Michael Kruse, Olivier P\u00e8ne, Claude Tadonki", 
    "title": "QIRAL: A High Level Language for Lattice QCD Code Generation", 
    "arxiv-id": "1208.4035v1", 
    "author": "Claude Tadonki", 
    "publish": "2012-08-16T12:14:39Z", 
    "summary": "Quantum chromodynamics (QCD) is the theory of subnuclear physics, aiming at\nmod- eling the strong nuclear force, which is responsible for the interactions\nof nuclear particles. Lattice QCD (LQCD) is the corresponding discrete\nformulation, widely used for simula- tions. The computational demand for the\nLQCD is tremendous. It has played a role in the history of supercomputers, and\nhas also helped defining their future. Designing efficient LQCD codes that\nscale well on large (probably hybrid) supercomputers requires to express many\nlevels of parallelism, and then to explore different algorithmic solutions.\nWhile al- gorithmic exploration is the key for efficient parallel codes, the\nprocess is hampered by the necessary coding effort. We present in this paper a\ndomain-specific language, QIRAL, for a high level expression of parallel\nalgorithms in LQCD. Parallelism is expressed through the mathematical struc-\nture of the sparse matrices defining the problem. We show that from these\nexpressions and from algorithmic and preconditioning formulations, a parallel\ncode can be automatically generated. This separates algorithms and mathematical\nformulations for LQCD (that be- long to the field of physics) from the\neffective orchestration of parallelism, mainly related to compilation and\noptimization for parallel architectures."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS", 
    "link": "http://arxiv.org/pdf/1208.4126v1", 
    "other_authors": "Iztok Fister Jr., Toma\u017e Kosar, Marjan Mernik, Iztok Fister", 
    "title": "Upgrading EasyTime: from a textual to a visual language", 
    "arxiv-id": "1208.4126v1", 
    "author": "Iztok Fister", 
    "publish": "2012-08-20T21:48:43Z", 
    "summary": "Measuring time in mass sports competitions is usually performed using\nexpensive measuring devices. Unfortunately, these solutions are not acceptable\nby many organizers of sporting competitions. In order to make the measuring\ntime as cheap as possible, the domain-specific language (DSL) EasyTime was\nproposed. In practice, it has been proven to be universal, flexible, and\nefficient. It can even reduce the number of required measuring devices. On the\nother hand, programming in EasyTime is not easy, because it requires a\ndomain-expert to program in a textual manner. In this paper, the\ndomain-specific modeling language (DSML) EasyTime II is proposed, which\nsimplifies the programming of the measuring system. First, the DSL EasyTime\ndomain analysis is presented. Then, the development of DSML is described in\ndetail. Finally, the DSML was tested by regular organizers of a sporting\ncompetition. This test showed that DSML can be used by end-users without any\nprevious programming knowledge."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(3:22)2012", 
    "link": "http://arxiv.org/pdf/1208.5895v2", 
    "other_authors": "Jacob Thamsborg, Lars Birkedal, Hongseok Yang", 
    "title": "Two for the Price of One: Lifting Separation Logic Assertions", 
    "arxiv-id": "1208.5895v2", 
    "author": "Hongseok Yang", 
    "publish": "2012-08-29T12:25:10Z", 
    "summary": "Recently, data abstraction has been studied in the context of separation\nlogic, with noticeable practical successes: the developed logics have enabled\nclean proofs of tricky challenging programs, such as subject-observer patterns,\nand they have become the basis of efficient verification tools for Java\n(jStar), C (VeriFast) and Hoare Type Theory (Ynot). In this paper, we give a\nnew semantic analysis of such logic-based approaches using Reynolds's\nrelational parametricity. The core of the analysis is our lifting theorems,\nwhich give a sound and complete condition for when a true implication between\nassertions in the standard interpretation entails that the same implication\nholds in a relational interpretation. Using these theorems, we provide an\nalgorithm for identifying abstraction-respecting client-side proofs; the proofs\nensure that clients cannot distinguish two appropriately-related module\nimplementations."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(3:22)2012", 
    "link": "http://arxiv.org/pdf/1208.6152v2", 
    "other_authors": "Ahmed Bouajjani, Egor Derevenetc, Roland Meyer", 
    "title": "Checking Robustness against TSO", 
    "arxiv-id": "1208.6152v2", 
    "author": "Roland Meyer", 
    "publish": "2012-08-30T12:32:01Z", 
    "summary": "We present algorithms for checking and enforcing robustness of concurrent\nprograms against the Total Store Ordering (TSO) memory model. A program is\nrobust if all its TSO computations correspond to computations under the\nSequential Consistency (SC) semantics.\n  We provide a complete characterization of non-robustness in terms of\nso-called attacks: a restricted form of (harmful) out-of-program-order\nexecutions. Then, we show that detecting attacks can be parallelized, and can\nbe solved using state reachability queries under SC semantics in a suitably\ninstrumented program obtained by a linear size source-to-source translation.\nImportantly, the construction is valid for an arbitrary number of addresses and\nan arbitrary number of parallel threads, and it is independent from the data\ndomain and from the size of store buffers in the TSO semantics. In particular,\nwhen the data domain is finite and the number of addresses is fixed, we obtain\ndecidability and complexity results for robustness, even for an arbitrary\nnumber of threads.\n  As a second contribution, we provide an algorithm for computing an optimal\nset of fences that enforce robustness. We consider two criteria of optimality:\nminimization of program size and maximization of its performance. The\nalgorithms we define are implemented, and we successfully applied them to\nanalyzing and correcting several concurrent algorithms."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-8(3:29)2012", 
    "link": "http://arxiv.org/pdf/1209.0643v2", 
    "other_authors": "Thomas Martin Gawlitza, David Monniaux", 
    "title": "Invariant Generation through Strategy Iteration in Succinctly   Represented Control Flow Graphs", 
    "arxiv-id": "1209.0643v2", 
    "author": "David Monniaux", 
    "publish": "2012-09-04T13:52:34Z", 
    "summary": "We consider the problem of computing numerical invariants of programs, for\ninstance bounds on the values of numerical program variables. More\nspecifically, we study the problem of performing static analysis by abstract\ninterpretation using template linear constraint domains. Such invariants can be\nobtained by Kleene iterations that are, in order to guarantee termination,\naccelerated by widening operators. In many cases, however, applying this form\nof extrapolation leads to invariants that are weaker than the strongest\ninductive invariant that can be expressed within the abstract domain in use.\nAnother well-known source of imprecision of traditional abstract interpretation\ntechniques stems from their use of join operators at merge nodes in the control\nflow graph. The mentioned weaknesses may prevent these methods from proving\nsafety properties. The technique we develop in this article addresses both of\nthese issues: contrary to Kleene iterations accelerated by widening operators,\nit is guaranteed to yield the strongest inductive invariant that can be\nexpressed within the template linear constraint domain in use. It also eschews\njoin operators by distinguishing all paths of loop-free code segments. Formally\nspeaking, our technique computes the least fixpoint within a given template\nlinear constraint domain of a transition relation that is succinctly expressed\nas an existentially quantified linear real arithmetic formula. In contrast to\npreviously published techniques that rely on quantifier elimination, our\nalgorithm is proved to have optimal complexity: we prove that the decision\nproblem associated with our fixpoint problem is in the second level of the\npolynomial-time hierarchy."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.91.6", 
    "link": "http://arxiv.org/pdf/1209.1422v1", 
    "other_authors": "Sung-Shik T. Q. Jongmans, Dave Clarke, Jos\u00e9 Proen\u00e7a", 
    "title": "A Procedure for Splitting Processes and its Application to Coordination", 
    "arxiv-id": "1209.1422v1", 
    "author": "Jos\u00e9 Proen\u00e7a", 
    "publish": "2012-09-06T22:52:39Z", 
    "summary": "We present a procedure for splitting processes in a process algebra with\nmulti-actions (a subset of the specification language mCRL2). This splitting\nprocedure cuts a process into two processes along a set of actions A: roughly,\none of these processes contains no actions from A, while the other process\ncontains only actions from A. We state and prove a theorem asserting that the\nparallel composition of these two processes equals the original process under\nappropriate synchronization.\n  We apply our splitting procedure to the process algebraic semantics of the\ncoordination language Reo: using this procedure and its related theorem, we\nformally establish the soundness of splitting Reo connectors along the\nboundaries of their (a)synchronous regions in implementations of Reo. Such\nsplitting can significantly improve the performance of connectors."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.91.6", 
    "link": "http://arxiv.org/pdf/1209.2617v2", 
    "other_authors": "Francisco J. L\u00f3pez-Fraguas, Enrique Martin-Martin, Juan Rodr\u00edguez-Hortal\u00e1, Jaime S\u00e1nchez-Hern\u00e1ndez", 
    "title": "Rewriting and narrowing for constructor systems with call-time choice   semantics", 
    "arxiv-id": "1209.2617v2", 
    "author": "Jaime S\u00e1nchez-Hern\u00e1ndez", 
    "publish": "2012-09-12T14:01:05Z", 
    "summary": "Non-confluent and non-terminating constructor-based term rewrite systems are\nuseful for the purpose of specification and programming. In particular,\nexisting functional logic languages use such kind of rewrite systems to define\npossibly non-strict non-deterministic functions. The semantics adopted for\nnon-determinism is call-time choice, whose combination with non-strictness is a\nnon trivial issue, addressed years ago from a semantic point of view with the\nConstructor-based Rewriting Logic (CRWL), a well-known semantic framework\ncommonly accepted as suitable semantic basis of modern functional logic\nlanguages. A drawback of CRWL is that it does not come with a proper notion of\none-step reduction, which would be very useful to understand and reason about\nhow computations proceed. In this paper we develop thoroughly the theory for\nthe first order version of let-rewriting, a simple reduction notion close to\nthat of classical term rewriting, but extended with a let-binding construction\nto adequately express the combination of call-time choice with non-strict\nsemantics. Let-rewriting can be seen as a particular textual presentation of\nterm graph rewriting. We investigate the properties of let-rewriting, most\nremarkably their equivalence with respect to a conservative extension of the\nCRWL-semantics coping with let-bindings, and we show by some case studies that\nhaving two interchangeable formal views (reduction/semantics) of the same\nlanguage is a powerful reasoning tool. After that, we provide a notion of\nlet-narrowing which is adequate for call-time choice as proved by soundness and\ncompleteness results of let-narrowing with respect to let-rewriting. Moreover,\nwe relate those let-rewriting and let-narrowing relations (and hence CRWL) with\nordinary term rewriting and narrowing (..)\n  To appear in Theory and Practice of Logic Programming (TPLP)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.91.6", 
    "link": "http://arxiv.org/pdf/1209.5851v1", 
    "other_authors": "Antoine Madet", 
    "title": "A polynomial time \u03bb-calculus with multithreading and side   effects", 
    "arxiv-id": "1209.5851v1", 
    "author": "Antoine Madet", 
    "publish": "2012-09-26T07:33:43Z", 
    "summary": "The framework of Light Logics has been extensively studied to control the\ncomplexity of higher-order functional programs. We propose an extension of this\nframework to multithreaded programs with side effects, focusing on the case of\npolynomial time. After introducing a modal \\lambda-calculus with parallel\ncomposition and regions, we prove that a realistic call-by-value evaluation\nstrategy can be computed in polynomial time for a class of well-formed\nprograms. The result relies on the simulation of call-by-value by a polynomial\nshallow-first strategy which preserves the evaluation order of side effects.\nThen, we provide a polynomial type system that guarantees that well-typed\nprograms do not go wrong. Finally, we illustrate the expressivity of the type\nsystem by giving a programming example of concurrent iteration producing side\neffects over an inductive data structure."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.91.6", 
    "link": "http://arxiv.org/pdf/1210.1611v1", 
    "other_authors": "Neng-Fa Zhou, Christian Theil Have", 
    "title": "Efficient Tabling of Structured Data with Enhanced Hash-Consing", 
    "arxiv-id": "1210.1611v1", 
    "author": "Christian Theil Have", 
    "publish": "2012-10-04T23:19:16Z", 
    "summary": "Current tabling systems suffer from an increase in space complexity, time\ncomplexity or both when dealing with sequences due to the use of data\nstructures for tabled subgoals and answers and the need to copy terms into and\nfrom the table area. This symptom can be seen in not only B-Prolog, which uses\nhash tables, but also systems that use tries such as XSB and YAP. In this\npaper, we apply hash-consing to tabling structured data in B-Prolog. While\nhash-consing can reduce the space consumption when sharing is effective, it\ndoes not change the time complexity. We enhance hash-consing with two\ntechniques, called input sharing and hash code memoization, for reducing the\ntime complexity by avoiding computing hash codes for certain terms. The\nimproved system is able to eliminate the extra linear factor in the old system\nfor processing sequences, thus significantly enhancing the scalability of\napplications such as language parsing and bio-sequence analysis applications.\nWe confirm this improvement with experimental results."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.127.6", 
    "link": "http://arxiv.org/pdf/1210.2094v4", 
    "other_authors": "Danko Ilik", 
    "title": "Type Directed Partial Evaluation for Level-1 Shift and Reset", 
    "arxiv-id": "1210.2094v4", 
    "author": "Danko Ilik", 
    "publish": "2012-10-07T18:56:54Z", 
    "summary": "We present an implementation in the Coq proof assistant of type directed\npartial evaluation (TDPE) algorithms for call-by-name and call-by-value\nversions of shift and reset delimited control operators, and in presence of\nstrong sum types. We prove that the algorithm transforms well-typed programs to\nones in normal form. These normal forms can not always be arrived at using the\nso far known equational theories. The typing system does not allow answer-type\nmodification for function types and allows delimiters to be set on at most one\natomic type. The semantic domain for evaluation is expressed in Constructive\nType Theory as a dependently typed monadic structure combining Kripke models\nand continuation passing style translations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000117", 
    "link": "http://arxiv.org/pdf/1210.2282v2", 
    "other_authors": "Miguel Areias, Ricardo Rocha", 
    "title": "Towards Multi-Threaded Local Tabling Using a Common Table Space", 
    "arxiv-id": "1210.2282v2", 
    "author": "Ricardo Rocha", 
    "publish": "2012-10-08T14:00:07Z", 
    "summary": "Multi-threading is currently supported by several well-known Prolog systems\nproviding a highly portable solution for applications that can benefit from\nconcurrency. When multi-threading is combined with tabling, we can exploit the\npower of higher procedural control and declarative semantics. However, despite\nthe availability of both threads and tabling in some Prolog systems, the\nimplementation of these two features implies complex ties to each other and to\nthe underlying engine. Until now, XSB was the only Prolog system combining\nmulti-threading with tabling. In XSB, tables may be either private or shared\nbetween threads. While thread-private tables are easier to implement, shared\ntables have all the associated issues of locking, synchronization and potential\ndeadlocks. In this paper, we propose an alternative view to XSB's approach. In\nour proposal, each thread views its tables as private but, at the engine level,\nwe use a common table space where tables are shared among all threads. We\npresent three designs for our common table space approach: No-Sharing (NS)\n(similar to XSB's private tables), Subgoal-Sharing (SS) and Full-Sharing (FS).\nThe primary goal of this work was to reduce the memory usage for the table\nspace but, our experimental results, using the YapTab tabling system with a\nlocal evaluation strategy, show that we can also achieve significant reductions\non running time."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000270", 
    "link": "http://arxiv.org/pdf/1210.2297v2", 
    "other_authors": "R\u00e9my Haemmerl\u00e9", 
    "title": "Diagrammatic confluence for Constraint Handling Rules", 
    "arxiv-id": "1210.2297v2", 
    "author": "R\u00e9my Haemmerl\u00e9", 
    "publish": "2012-10-08T14:34:25Z", 
    "summary": "Confluence is a fundamental property of Constraint Handling Rules (CHR)\nsince, as in other rewriting formalisms, it guarantees that the computations\nare not dependent on rule application order, and also because it implies the\nlogical consistency of the program declarative view. In this paper we are\nconcerned with proving the confluence of non-terminating CHR programs. For this\npurpose, we derive from van Oostrom's decreasing diagrams method a novel\ncriterion on CHR critical pairs that generalizes all preexisting criteria. We\nsubsequently improve on a result on the modularity of CHR confluence, which\npermits modular combinations of possibly non-terminating confluent programs,\nwithout loss of confluence."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000336", 
    "link": "http://arxiv.org/pdf/1210.2864v1", 
    "other_authors": "Jose F. Morales, R\u00e9my Haemmerl\u00e9, Manuel Carro, Manuel V. Hermenegildo", 
    "title": "Lightweight compilation of (C)LP to JavaScript", 
    "arxiv-id": "1210.2864v1", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2012-10-10T10:39:03Z", 
    "summary": "We present and evaluate a compiler from Prolog (and extensions) to JavaScript\nwhich makes it possible to use (constraint) logic programming to develop the\nclient side of web applications while being compliant with current industry\nstandards. Targeting JavaScript makes (C)LP programs executable in virtually\nevery modern computing device with no additional software requirements from the\npoint of view of the user. In turn, the use of a very high-level language\nfacilitates the development of high-quality, complex software. The compiler is\na back end of the Ciao system and supports most of its features, including its\nmodule system and its rich language extension mechanism based on packages. We\npresent an overview of the compilation process and a detailed description of\nthe run-time system, including the support for modular compilation into\nseparate JavaScript code. We demonstrate the maturity of the compiler by\ntesting it with complex code such as a CLP(FD) library written in Prolog with\nattributed variables. Finally, we validate our proposal by measuring the\nperformance of some LP and CLP(FD) benchmarks running on top of major\nJavaScript engines."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000336", 
    "link": "http://arxiv.org/pdf/1210.4263v2", 
    "other_authors": "Matthieu Boutier, Gabriel Kerneis", 
    "title": "Generating events with style", 
    "arxiv-id": "1210.4263v2", 
    "author": "Gabriel Kerneis", 
    "publish": "2012-10-16T06:35:59Z", 
    "summary": "Threads and events are two common abstractions for writing concurrent\nprograms. Because threads are often more convenient, but events more efficient,\nit is natural to want to translate the former into the latter. However, whereas\nthere are many different event-driven styles, existing translators often apply\nad-hoc rules which do not reflect this diversity. We analyse various\ncontrol-flow and data-flow encodings in real-world event-driven code, and we\nobserve that it is possible to generate any of these styles automatically from\nthreaded code, by applying certain carefully chosen classical program\ntransformations. In particular, we implement two of these transformations,\nlambda lifting and environments, in CPC, an extension of the C language for\nwriting concurrent systems. Finally, we find out that, although rarely used in\nreal-world programs because it is tedious to perform manually, lambda lifting\nyields better performance than environments in most of our benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000336", 
    "link": "http://arxiv.org/pdf/1210.5307v1", 
    "other_authors": "Gregory J. Duck", 
    "title": "SMCHR: Satisfiability Modulo Constraint Handling Rules", 
    "arxiv-id": "1210.5307v1", 
    "author": "Gregory J. Duck", 
    "publish": "2012-10-19T03:47:37Z", 
    "summary": "Constraint Handling Rules (CHRs) are a high-level rule-based programming\nlanguage for specification and implementation of constraint solvers. CHR\nmanipulates a global store representing a flat conjunction of constraints. By\ndefault, CHR does not support goals with a more complex propositional structure\nincluding disjunction, negation, etc., or CHR relies on the host system to\nprovide such features. In this paper we introduce Satisfiability Modulo\nConstraint Handling Rules (SMCHR): a tight integration of CHR with a modern\nBoolean Satisfiability (SAT) solver for quantifier-free formulae with an\narbitrary propositional structure. SMCHR is essentially a Satisfiability Modulo\nTheories (SMT) solver where the theory T is implemented in CHR. The execution\nalgorithm of SMCHR is based on lazy clause generation, where a new clause for\nthe SAT solver is generated whenever a rule is applied. We shall also explore\nthe practical aspects of building an SMCHR system, including extending a\n\"built-in\" constraint solver supporting equality with unification and\njustifications."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068412000336", 
    "link": "http://arxiv.org/pdf/1210.5935v1", 
    "other_authors": "Gabriel Scherer, Didier R\u00e9my", 
    "title": "GADT meet Subtyping", 
    "arxiv-id": "1210.5935v1", 
    "author": "Didier R\u00e9my", 
    "publish": "2012-10-22T15:36:56Z", 
    "summary": "While generalized abstract datatypes (GADT) are now considered\nwell-understood, adding them to a language with a notion of subtyping comes\nwith a few surprises. What does it mean for a GADT parameter to be covariant?\nThe answer turns out to be quite subtle. It involves fine-grained properties of\nthe subtyping relation that raise interesting design questions. We allow\nvariance annotations in GADT definitions, study their soundness, and present a\nsound and complete algorithm to check them. Our work may be applied to\nreal-world ML-like languages with explicit subtyping such as OCaml, or to\nlanguages with general subtyping constraints."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.98.7", 
    "link": "http://arxiv.org/pdf/1210.6114v1", 
    "other_authors": "Jonathan Michaux, Elie Najm, Alessandro Fantechi", 
    "title": "Adding Sessions to BPEL", 
    "arxiv-id": "1210.6114v1", 
    "author": "Alessandro Fantechi", 
    "publish": "2012-10-23T02:55:15Z", 
    "summary": "By considering an essential subset of the BPEL orchestration language, we\ndefine SeB, a session based style of this subset. We discuss the formal\nsemantics of SeB and we present its main properties. We use a new approach to\naddress the formal semantics, based on a translation into so-called control\ngraphs. Our semantics handles control links and addresses the static semantics\nthat prescribes the valid usage of variables. We also provide the semantics of\ncollections of networked services.\n  Relying on these semantics, we define precisely what is meant by interaction\nsafety, paving the way to the formal analysis of safe interactions between BPEL\nservices."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.98.7", 
    "link": "http://arxiv.org/pdf/1210.6390v2", 
    "other_authors": "Pierre-Evariste Dagand, Conor McBride", 
    "title": "Elaborating Inductive Definitions", 
    "arxiv-id": "1210.6390v2", 
    "author": "Conor McBride", 
    "publish": "2012-10-23T21:47:21Z", 
    "summary": "We present an elaboration of inductive definitions down to a universe of\ndatatypes. The universe of datatypes is an internal presentation of strictly\npositive families within type theory. By elaborating an inductive definition --\na syntactic artifact -- to its code -- its semantics -- we obtain an\ninternalized account of inductives inside the type theory itself: we claim that\nreasoning about inductive definitions could be carried in the type theory, not\nin the meta-theory as it is usually the case. Besides, we give a formal\nspecification of that elaboration process. It is therefore amenable to formal\nreasoning too. We prove the soundness of our translation and hint at its\ncorrectness with respect to Coq's Inductive definitions.\n  The practical benefits of this approach are numerous. For the type theorist,\nthis is a small step toward bootstrapping, ie. implementing the inductive\nfragment in the type theory itself. For the programmer, this means better\nsupport for generic programming: we shall present a lightweight deriving\nmechanism, entirely definable by the programmer and therefore not requiring any\nextension to the type theory."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-9(1:4)2013", 
    "link": "http://arxiv.org/pdf/1211.0865v2", 
    "other_authors": "Aaron Stump, Garrin Kimmell, Hans Zantema, Ruba El Haj Omar", 
    "title": "A Rewriting View of Simple Typing", 
    "arxiv-id": "1211.0865v2", 
    "author": "Ruba El Haj Omar", 
    "publish": "2012-11-05T14:05:51Z", 
    "summary": "This paper shows how a recently developed view of typing as small-step\nabstract reduction, due to Kuan, MacQueen, and Findler, can be used to recast\nthe development of simple type theory from a rewriting perspective. We show how\nstandard meta-theoretic results can be proved in a completely new way, using\nthe rewriting view of simple typing. These meta-theoretic results include\nstandard type preservation and progress properties for simply typed lambda\ncalculus, as well as generalized versions where typing is taken to include both\nabstract and concrete reduction. We show how automated analysis tools developed\nin the term-rewriting community can be used to help automate the proofs for\nthis meta-theory. Finally, we show how to adapt a standard proof of\nnormalization of simply typed lambda calculus, for the rewriting approach to\ntyping."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-9(1:4)2013", 
    "link": "http://arxiv.org/pdf/1211.2609v5", 
    "other_authors": "Massimo Bartoletti, Alceste Scalas, Emilio Tuosto, Roberto Zunino", 
    "title": "Honesty by Typing", 
    "arxiv-id": "1211.2609v5", 
    "author": "Roberto Zunino", 
    "publish": "2012-11-12T13:39:51Z", 
    "summary": "We propose a type system for a calculus of contracting processes. Processes\ncan establish sessions by stipulating contracts, and then can interact either\nby keeping the promises made, or not. Type safety guarantees that a typeable\nprocess is honest - that is, it abides by the contracts it has stipulated in\nall possible contexts, even in presence of dishonest adversaries. Type\ninference is decidable, and it allows to safely approximate the honesty of\nprocesses using either synchronous or asynchronous communication."
},{
    "category": "cs.PL", 
    "doi": "10.1145/1375657.1375672", 
    "link": "http://arxiv.org/pdf/1211.2776v1", 
    "other_authors": "Gwena\u00ebl Delaval, Alain Girault, Marc Pouzet", 
    "title": "A Type System for the Automatic Distribution of Higher-order Synchronous   Dataflow Programs", 
    "arxiv-id": "1211.2776v1", 
    "author": "Marc Pouzet", 
    "publish": "2012-11-12T20:40:59Z", 
    "summary": "We address the design of distributed systems with synchronous dataflow\nprogramming languages. As modular design entails handling both architectural\nand functional modularity, our first contribution is to extend an existing\nsynchronous dataflow programming language with primitives allowing the\ndescription of a distributed architecture and the localization of some\nexpressions onto some processors. We also present a distributed semantics to\nformalize the distributed execution of synchronous programs. Our second\ncontribution is to provide a type system, in order to infer the localization of\nnon-annotated values by means of type inference and to ensure, at compilation\ntime, the consistency of the distribution. Our third contribution is to provide\na type-directed projection operation to obtain automatically,from a centralized\ntyped program, the local program to be executed by each computing resource. The\ntype system as well as the automatic distribution mechanism has been fully\nimplemented in the compiler of an existing synchronous data-flow programming\nlanguage."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500604", 
    "link": "http://arxiv.org/pdf/1211.3722v4", 
    "other_authors": "J. Ian Johnson, Nicholas Labich, Matthew Might, David Van Horn", 
    "title": "Optimizing Abstract Abstract Machines", 
    "arxiv-id": "1211.3722v4", 
    "author": "David Van Horn", 
    "publish": "2012-11-15T20:37:01Z", 
    "summary": "The technique of abstracting abstract machines (AAM) provides a systematic\napproach for deriving computable approximations of evaluators that are easily\nproved sound. This article contributes a complementary step-by-step process for\nsubsequently going from a naive analyzer derived under the AAM approach, to an\nefficient and correct implementation. The end result of the process is a two to\nthree order-of-magnitude improvement over the systematically derived analyzer,\nmaking it competitive with hand-optimized implementations that compute\nfundamentally less precise results."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.102.11", 
    "link": "http://arxiv.org/pdf/1211.6193v1", 
    "other_authors": "Chris Hathhorn, Michela Becchi, William L. Harrison, Adam Procter", 
    "title": "Formal Semantics of Heterogeneous CUDA-C: A Modular Approach with   Applications", 
    "arxiv-id": "1211.6193v1", 
    "author": "Adam Procter", 
    "publish": "2012-11-27T02:37:06Z", 
    "summary": "We extend an off-the-shelf, executable formal semantics of C (Ellison and\nRosu's K Framework semantics) with the core features of CUDA-C. The hybrid\nCPU/GPU computation model of CUDA-C presents challenges not just for\nprogrammers, but also for practitioners of formal methods. Our formal semantics\nhelps expose and clarify these issues. We demonstrate the usefulness of our\nsemantics by generating a tool from it capable of detecting some race\nconditions and deadlocks in CUDA-C programs. We discuss limitations of our\nmodel and argue that its extensibility can easily enable a wider range of\nverification tasks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.102.11", 
    "link": "http://arxiv.org/pdf/1211.6418v1", 
    "other_authors": "Iztok Fister Jr., Iztok Fister", 
    "title": "Measuring Time in Sporting Competitions with the Domain-Specific   Language EasyTime", 
    "arxiv-id": "1211.6418v1", 
    "author": "Iztok Fister", 
    "publish": "2012-11-27T20:48:02Z", 
    "summary": "Measuring time in mass sporting competitions is unthinkable manually today\nbecause of their long duration and unreliability. Besides, automatic timing\ndevices based on the RFID technology have become cheaper. However, these\ndevices cannot operate stand-alone. To work efficiently, they need a computer\ntiming system for monitoring results. Such system should be capable of\nprocessing the incoming events, encoding and assigning results to a individual\ncompetitor, sorting results according to the achieved time and printing them.\nIn this paper, a domain-specific language named EasyTime will be defined. It\nenables controlling an agent by writing events to a database. Using the agent,\nthe number of measuring devices can be reduced. Also, EasyTime is of a\nuniversal type that can be applied to many different sporting competitions"
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.102.11", 
    "link": "http://arxiv.org/pdf/1212.2341v1", 
    "other_authors": "St\u00e9phane Ducasse, Nicolas Petton, Guillermo Polito, Damien Cassou", 
    "title": "Semantics and Security Issues in JavaScript", 
    "arxiv-id": "1212.2341v1", 
    "author": "Damien Cassou", 
    "publish": "2012-12-11T09:04:39Z", 
    "summary": "There is a plethora of research articles describing the deep semantics of\nJavaScript. Nevertheless, such articles are often difficult to grasp for\nreaders not familiar with formal semantics. In this report, we propose a digest\nof the semantics of JavaScript centered around security concerns. This document\nproposes an overview of the JavaScript language and the misleading semantic\npoints in its design. The first part of the document describes the main\ncharacteristics of the language itself. The second part presents how those\ncharacteristics can lead to problems. It finishes by showing some coding\npatterns to avoid certain traps and presents some ECMAScript 5 new features."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.102.11", 
    "link": "http://arxiv.org/pdf/1212.5210v5", 
    "other_authors": "Luca Saiu", 
    "title": "GNU epsilon - an extensible programming language", 
    "arxiv-id": "1212.5210v5", 
    "author": "Luca Saiu", 
    "publish": "2012-12-20T19:56:38Z", 
    "summary": "Reductionism is a viable strategy for designing and implementing practical\nprogramming languages, leading to solutions which are easier to extend,\nexperiment with and formally analyze. We formally specify and implement an\nextensible programming language, based on a minimalistic first-order imperative\ncore language plus strong abstraction mechanisms, reflection and\nself-modification features. The language can be extended to very high levels:\nby using Lisp-style macros and code-to-code transforms which automatically\nrewrite high-level expressions into core forms, we define closures and\nfirst-class continuations on top of the core. Non-self-modifying programs can\nbe analyzed and formally reasoned upon, thanks to the language simple\nsemantics. We formally develop a static analysis and prove a soundness property\nwith respect to the dynamic semantics. We develop a parallel garbage collector\nsuitable to multi-core machines to permit efficient execution of parallel\nprograms."
},{
    "category": "cs.PL", 
    "doi": "10.1587/transinf.E96.D.2036", 
    "link": "http://arxiv.org/pdf/1212.6844v1", 
    "other_authors": "Keehang Kwon, Sungwoo Hur, Mi-Young Park", 
    "title": "Improving Robustness via Disjunctive Statements in Imperative   Programming", 
    "arxiv-id": "1212.6844v1", 
    "author": "Mi-Young Park", 
    "publish": "2012-12-31T09:20:05Z", 
    "summary": "To deal with failures as simply as possible, we propose a new foun- dation\nfor the core (untyped) C, which is based on a new logic called task logic or\nimperative logic. We then introduce a sequential-disjunctive statement of the\nform S : R. This statement has the following semantics: execute S and R\nsequentially. It is considered a success if at least one of S;R is a success.\nThis statement is useful for dealing with inessential errors without explicitly\ncatching them."
},{
    "category": "cs.PL", 
    "doi": "10.1587/transinf.E96.D.2036", 
    "link": "http://arxiv.org/pdf/1301.0748v1", 
    "other_authors": "Dominik Charousset, Thomas C. Schmidt", 
    "title": "libcppa - Designing an Actor Semantic for C++11", 
    "arxiv-id": "1301.0748v1", 
    "author": "Thomas C. Schmidt", 
    "publish": "2013-01-04T15:23:43Z", 
    "summary": "Parallel hardware makes concurrency mandatory for efficient program\nexecution. However, writing concurrent software is both challenging and\nerror-prone. C++11 provides standard facilities for multiprogramming, such as\natomic operations with acquire/release semantics and RAII mutex locking, but\nthese primitives remain too low-level. Using them both correctly and\nefficiently still requires expert knowledge and hand-crafting. The actor model\nreplaces implicit communication by sharing with an explicit message passing\nmechanism. It applies to concurrency as well as distribution, and a lightweight\nactor model implementation that schedules all actors in a properly\npre-dimensioned thread pool can outperform equivalent thread-based\napplications. However, the actor model did not enter the domain of native\nprogramming languages yet besides vendor-specific island solutions. With the\nopen source library libcppa, we want to combine the ability to build reliable\nand distributed systems provided by the actor model with the performance and\nresource-efficiency of C++11."
},{
    "category": "cs.PL", 
    "doi": "10.1587/transinf.E96.D.2036", 
    "link": "http://arxiv.org/pdf/1301.2903v1", 
    "other_authors": "Gabriel Scherer, Didier R\u00e9my", 
    "title": "GADTs meet subtyping", 
    "arxiv-id": "1301.2903v1", 
    "author": "Didier R\u00e9my", 
    "publish": "2013-01-14T10:22:57Z", 
    "summary": "While generalized algebraic datatypes (\\GADTs) are now considered\nwell-understood, adding them to a language with a notion of subtyping comes\nwith a few surprises. What does it mean for a \\GADT parameter to be covariant?\nThe answer turns out to be quite subtle. It involves fine-grained properties of\nthe subtyping relation that raise interesting design questions. We allow\nvariance annotations in \\GADT definitions, study their soundness, and present a\nsound and complete algorithm to check them. Our work may be applied to\nreal-world ML-like languages with explicit subtyping such as OCaml, or to\nlanguages with general subtyping constraints."
},{
    "category": "cs.PL", 
    "doi": "10.1587/transinf.E96.D.2036", 
    "link": "http://arxiv.org/pdf/1301.4334v1", 
    "other_authors": "Matthew J. Sottile, Geoffrey C. Hulette", 
    "title": "Deriving program transformations by demonstration", 
    "arxiv-id": "1301.4334v1", 
    "author": "Geoffrey C. Hulette", 
    "publish": "2013-01-18T09:57:39Z", 
    "summary": "Automatic code transformation in which transformations are tuned for specific\napplications and contexts are difficult to achieve in an accessible manner. In\nthis paper, we present an approach to build application specific code\ntransformations. Our approach is based on analysis of the abstract syntax\nrepresentation of exemplars of the essential change to the code before and\nafter the transformation is applied. This analysis entails a sequence of steps\nto identify the change, determine how to generalize it, and map it to term\nrewriting rules for the Stratego term rewriting system. The methods described\nin this paper assume programs are represented in a language-neutral term\nformat, allowing tools based on our methods to be applied to programs written\nin the major languages used by computational scientists utilizing high\nperformance computing systems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.4779v1", 
    "other_authors": "Thomas Braibant, Adam Chlipala", 
    "title": "Formal Verification of Hardware Synthesis", 
    "arxiv-id": "1301.4779v1", 
    "author": "Adam Chlipala", 
    "publish": "2013-01-21T08:12:22Z", 
    "summary": "We report on the implementation of a certified compiler for a high-level\nhardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).\nFe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded\natomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.\nThe target language of the compiler corresponds to a synthesisable subset of\nVerilog or VHDL. A key aspect of our approach is that input programs to the\ncompiler can be defined and proved correct inside Coq. Then, we use extraction\nand a Verilog back-end (written in OCaml) to get a certified version of a\nhardware design."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.4858v1", 
    "other_authors": "Luis Quesada, Fernando Berzal, Juan-Carlos Cubero", 
    "title": "A DSL for Mapping Abstract Syntax Models to Concrete Syntax Models in   ModelCC", 
    "arxiv-id": "1301.4858v1", 
    "author": "Juan-Carlos Cubero", 
    "publish": "2013-01-21T13:18:10Z", 
    "summary": "ModelCC is a model-based parser generator that decouples language design from\nlanguage processing. ModelCC provides two different mechanisms to specify the\nmapping from an abstract syntax model to a concrete syntax model: metadata\nannotations defined on top of the abstract syntax model specification and a\ndomain-specific language for defining ASM-CSM mappings. Using a domain-specific\nlanguage to specify the mapping from abstract to concrete syntax models allows\nthe definition of multiple concrete syntax models for the same abstract syntax\nmodel. In this paper, we describe the ModelCC domain-specific language for\nabstract syntax model to concrete syntax model mappings and we showcase its\ncapabilities by providing a meta-definition of that domain-specific language."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.6260v1", 
    "other_authors": "Sim-Hui Tee", 
    "title": "Problems of Inheritance at Java Inner Class", 
    "arxiv-id": "1301.6260v1", 
    "author": "Sim-Hui Tee", 
    "publish": "2013-01-26T14:25:54Z", 
    "summary": "Single inheritance has been widely accepted in the current programming\npractice to avoid the complication that incurred by multiple inheritance.\nSingle inheritance enhances the reusability of codes and eliminates the\nconfusion of identical methods that possibly defined in two superclasses.\nHowever, the mechanism of inner class in Java potentially reintroduces the\nproblems encountered by multiple inheritance. When the depth of Java inner\nclass is increased, the problem becomes severe. This paper aims at exposing the\nproblems of inheritance at the Java inner class. In addition, a measure is\nproposed to evaluate the potential problem of inheritance for Java inner class"
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.6836v1", 
    "other_authors": "Keehang Kwon, Kyunghwan Park, Mi-Young Park", 
    "title": "Towards Interactive Object-Oriented Programming", 
    "arxiv-id": "1301.6836v1", 
    "author": "Mi-Young Park", 
    "publish": "2013-01-29T06:02:21Z", 
    "summary": "To represent interactive objects, we propose a choice-disjunctive declaration\nstatement of the form S R where S;R are the (procedure or field) declaration\nstatements within a class. This statement has the following semantics: request\nthe user to choose one between S and R when an object of this class is created.\nThis statement is useful for representing interactive objects that require\ninteractions with the user."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.7680v1", 
    "other_authors": "Jo\u00e3o Santos, Ricardo Rocha", 
    "title": "Efficient Support for Mode-Directed Tabling in the YapTab Tabling System", 
    "arxiv-id": "1301.7680v1", 
    "author": "Ricardo Rocha", 
    "publish": "2013-01-31T16:55:52Z", 
    "summary": "Mode-directed tabling is an extension to the tabling technique that supports\nthe definition of mode operators for specifying how answers are inserted into\nthe table space. In this paper, we focus our discussion on the efficient\nsupport for mode directed-tabling in the YapTab tabling system. We discuss 7\ndifferent mode operators and explain how we have extended and optimized\nYapTab's table space organization to support them. Initial experimental results\nshow that our implementation compares favorably with the B-Prolog and XSB\nstate-of-the-art Prolog tabling systems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.7690v1", 
    "other_authors": "Rui Vieira, Ricardo Rocha, Fernando Silva", 
    "title": "On Comparing Alternative Splitting Strategies for Or-Parallel Prolog   Execution on Multicores", 
    "arxiv-id": "1301.7690v1", 
    "author": "Fernando Silva", 
    "publish": "2013-01-31T17:19:21Z", 
    "summary": "Many or-parallel Prolog models exploiting implicit parallelism have been\nproposed in the past. Arguably, one of the most successful models is\nenvironment copying for shared memory architectures. With the increasing\navailability and popularity of multicore architectures, it makes sense to\nrecover the body of knowledge there is in this area and re-engineer prior\ncomputational models to evaluate their performance on newer architectures. In\nthis work, we focus on the implementation of splitting strategies for\nor-parallel Prolog execution on multicores and, for that, we develop a\nframework, on top of the YapOr system, that integrates and supports five\nalternative splitting strategies. Our implementation shares the underlying\nexecution environment and most of the data structures used to implement\nor-parallelism in YapOr. In particular, we took advantage of YapOr's\ninfrastructure for incremental copying and scheduling support, which we used\nwith minimal modifications. We thus argue that all these common support\nfeatures allow us to make a first and fair comparison between these five\nalternative splitting strategies and, therefore, better understand their\nadvantages and weaknesses."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.7694v1", 
    "other_authors": "Zo\u00e9 Drey, Jos\u00e9 F. Morales, Manuel V. Hermenegildo", 
    "title": "Reversible Language Extensions and their Application in Debugging", 
    "arxiv-id": "1301.7694v1", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2013-01-31T17:26:09Z", 
    "summary": "A range of methodologies and techniques are available to guide the design and\nimplementation of language extensions and domain-specific languages. A simple\nyet powerful technique is based on source-to-source transformations interleaved\nacross the compilation passes of a base language. Despite being a successful\napproach, it has the main drawback that the input source code is lost in the\nprocess. When considering the whole workflow of program development (warning\nand error reporting, debugging, or even program analysis), program translations\nare no more powerful than a glorified macro language. In this paper, we propose\nan augmented approach to language extensions for Prolog, where symbolic\nannotations are included in the target program. These annotations allow\nselectively reversing the translated code. We illustrate the approach by\nshowing that coupling it with minimal extensions to a generic Prolog debugger\nallows us to provide users with a familiar, source-level view during the\ndebugging of programs which use a variety of language extensions, such as\nfunctional notation, DCGs, or CLP{Q,R}."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1301.7702v1", 
    "other_authors": "Emilio Jes\u00fas Gallego Arias, R\u00e9my Haemmerl\u00e9, Manuel V. Hermenegildo, Jos\u00e9 F. Morales", 
    "title": "The Ciao clp(FD) Library. A Modular CLP Extension for Prolog", 
    "arxiv-id": "1301.7702v1", 
    "author": "Jos\u00e9 F. Morales", 
    "publish": "2013-01-31T17:49:20Z", 
    "summary": "We present a new free library for Constraint Logic Programming over Finite\nDomains, included with the Ciao Prolog system. The library is entirely written\nin Prolog, leveraging on Ciao's module system and code transformation\ncapabilities in order to achieve a highly modular design without compromising\nperformance. We describe the interface, implementation, and design rationale of\neach modular component.\n  The library meets several design goals: a high level of modularity, allowing\nthe individual components to be replaced by different versions;\nhigh-efficiency, being competitive with other FD implementations; a glass-box\napproach, so the user can specify new constraints at different levels; and a\nProlog implementation, in order to ease the integration with Ciao's code\nanalysis components.\n  The core is built upon two small libraries which implement integer ranges and\nclosures. On top of that, a finite domain variable datatype is defined, taking\ncare of constraint reexecution depending on range changes. These three\nlibraries form what we call the FD kernel of the library.\n  This FD kernel is used in turn to implement several higher-level finite\ndomain constraints, specified using indexicals. Together with a labeling module\nthis layer forms what we name \\emph{the FD solver}. A final level integrates\nthe clp(FD) paradigm with our FD solver. This is achieved using attributed\nvariables and a compiler from the clp(FD) language to the set of constraints\nprovided by the solver.\n  It should be noted that the user of the library is encouraged to work in any\nof those levels as seen convenient: from writing a new range module to\nenriching the set of FD constraints by writing new indexicals."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.0538v1", 
    "other_authors": "Sen Ma", 
    "title": "OESPA:A Theory of Programming that Support Software Engineering", 
    "arxiv-id": "1304.0538v1", 
    "author": "Sen Ma", 
    "publish": "2013-04-02T05:57:44Z", 
    "summary": "A new theory of programming is proposed. The theory consists of OE (Operation\nExpression), SP (Semantic Predicate) and A (Axiom), abbreviated as OESPA. OE is\nfor programming: its syntax is given by BNF formulas and its semantics is\ndefined by axioms on these formulas. Similar to predicates in logic, SP is for\ndescribing properties of OE (i.e. programs) and for program property analysis.\nBut SP is different from predicates, it directly relates the final values of\nvariables upon termination of a given OE with initial values of these variables\nbefore the same OE. As such, it is feasible to prove or disprove whether a\ngiven SP is a property of a given OE by computation based on A (Axioms). SP\ncalculus is proposed for program specification and specification analysis, that\nis missing in software engineering."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.0809v3", 
    "other_authors": "Guillaume Allais, Pierre Boutillier, Conor McBride", 
    "title": "New Equations for Neutral Terms: A Sound and Complete Decision   Procedure, Formalized", 
    "arxiv-id": "1304.0809v3", 
    "author": "Conor McBride", 
    "publish": "2013-04-02T22:55:32Z", 
    "summary": "The definitional equality of an intensional type theory is its test of type\ncompatibility. Today's systems rely on ordinary evaluation semantics to compare\nexpressions in types, frustrating users with type errors arising when\nevaluation fails to identify two `obviously' equal terms. If only the machine\ncould decide a richer theory! We propose a way to decide theories which\nsupplement evaluation with `$\\nu$-rules', rearranging the neutral parts of\nnormal forms, and report a successful initial experiment.\n  We study a simple -calculus with primitive fold, map and append operations on\nlists and develop in Agda a sound and complete decision procedure for an\nequational theory enriched with monoid, functor and fusion laws."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.1835v1", 
    "other_authors": "Eric Hielscher, Alex Rubinsteyn, Dennis Shasha", 
    "title": "Locality Optimization for Data Parallel Programs", 
    "arxiv-id": "1304.1835v1", 
    "author": "Dennis Shasha", 
    "publish": "2013-04-05T23:52:00Z", 
    "summary": "Productivity languages such as NumPy and Matlab make it much easier to\nimplement data-intensive numerical algorithms. However, these languages can be\nintolerably slow for programs that don't map well to their built-in primitives.\nIn this paper, we discuss locality optimizations for our system Parakeet, a\njust-in-time compiler and runtime system for an array-oriented subset of\nPython. Parakeet dynamically compiles whole user functions to high performance\nmulti-threaded native code. Parakeet makes extensive use of the classic data\nparallel operators Map, Reduce, and Scan. We introduce a new set of data\nparallel operators,TiledMap, TiledReduce, and TiledScan, that break up their\ncomputations into local pieces of bounded size so as better to make use of\nsmall fast memories. We introduce a novel tiling transformation to generate\ntiled operators automatically. Applying this transformation once tiles the\nprogram for cache, and applying it again enables tiling for registers. The\nsizes for cache tiles are left unspecified until runtime, when an autotuning\nsearch is performed. Finally, we evaluate our optimizations on benchmarks and\nshow significant speedups on programs that exhibit data locality."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.3596v1", 
    "other_authors": "Sandrine Blazy, Vincent Laporte, Andr\u00e9 Maroneze, David Pichardie", 
    "title": "Formal Verification of a C Value Analysis Based on Abstract   Interpretation", 
    "arxiv-id": "1304.3596v1", 
    "author": "David Pichardie", 
    "publish": "2013-04-12T10:32:40Z", 
    "summary": "Static analyzers based on abstract interpretation are complex pieces of\nsoftware implementing delicate algorithms. Even if static analysis techniques\nare well understood, their implementation on real languages is still\nerror-prone. This paper presents a formal verification using the Coq proof\nassistant: a formalization of a value analysis (based on abstract\ninterpretation), and a soundness proof of the value analysis. The formalization\nrelies on generic interfaces. The mechanized proof is facilitated by a\ntranslation validation of a Bourdoncle fixpoint iterator. The work has been\nintegrated into the CompCert verified C-compiler. Our verified analysis\ndirectly operates over an intermediate language of the compiler having the same\nexpressiveness as C. The automatic extraction of our value analysis into OCaml\nyields a program with competitive results, obtained from experiments on a\nnumber of benchmarks and comparisons with the Frama-C tool."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.5531v1", 
    "other_authors": "Edwin Westbrook, Swarat Chaudhuri", 
    "title": "A Semantics for Approximate Program Transformations", 
    "arxiv-id": "1304.5531v1", 
    "author": "Swarat Chaudhuri", 
    "publish": "2013-04-19T20:06:11Z", 
    "summary": "An approximate program transformation is a transformation that can change the\nsemantics of a program within a specified empirical error bound. Such\ntransformations have wide applications: they can decrease computation time,\npower consumption, and memory usage, and can, in some cases, allow\nimplementations of incomputable operations. Correctness proofs of approximate\nprogram transformations are by definition quantitative. Unfortunately, unlike\nwith standard program transformations, there is as of yet no modular way to\nprove correctness of an approximate transformation itself. Error bounds must be\nproved for each transformed program individually, and must be re-proved each\ntime a program is modified or a different set of approximations are applied. In\nthis paper, we give a semantics that enables quantitative reasoning about a\nlarge class of approximate program transformations in a local, composable way.\nOur semantics is based on a notion of distance between programs that defines\nwhat it means for an approximate transformation to be correct up to an error\nbound. The key insight is that distances between programs cannot in general be\nformulated in terms of metric spaces and real numbers. Instead, our semantics\nadmits natural notions of distance for each type construct; for example,\nnumbers are used as distances for numerical data, functions are used as\ndistances for functional data, an polymorphic lambda-terms are used as\ndistances for polymorphic data. We then show how our semantics applies to two\nexample approximations: replacing reals with floating-point numbers, and loop\nperforation."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.5893v1", 
    "other_authors": "Sabah Al-Fedaghi", 
    "title": "Conceptual Understanding of Computer Program Execution: Application to   C++", 
    "arxiv-id": "1304.5893v1", 
    "author": "Sabah Al-Fedaghi", 
    "publish": "2013-04-22T09:41:42Z", 
    "summary": "A visual programming language uses pictorial tools such as diagrams to\nrepresent its structural units and control stream. It is useful for enhancing\nunderstanding, maintenance, verification, testing, and parallelism. This paper\nproposes a diagrammatic methodology that produces a conceptual representation\nof instructions for programming source codes. Without loss of generality in the\npotential for using the methodology in a wider range of applications, this\npaper focuses on using these diagrams in teaching of C++ programming. C++\nprogramming constructs are represented in the proposed method in order to show\nthat it can provide a foundation for understanding the behavior of running\nprograms. Applying the method to actual C++ classes demonstrates that it\nimproves understanding of the activities in the computer system corresponding\nto a C++ program."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.6274v2", 
    "other_authors": "Rohan Padhye, Uday P. Khedker", 
    "title": "Interprocedural Data Flow Analysis in Soot using Value Contexts", 
    "arxiv-id": "1304.6274v2", 
    "author": "Uday P. Khedker", 
    "publish": "2013-04-23T13:02:09Z", 
    "summary": "An interprocedural analysis is precise if it is flow sensitive and fully\ncontext-sensitive even in the presence of recursion. Many methods of\ninterprocedural analysis sacrifice precision for scalability while some are\nprecise but limited to only a certain class of problems.\n  Soot currently supports interprocedural analysis of Java programs using graph\nreachability. However, this approach is restricted to IFDS/IDE problems, and is\nnot suitable for general data flow frameworks such as heap reference analysis\nand points-to analysis which have non-distributive flow functions.\n  We describe a general-purpose interprocedural analysis framework for Soot\nusing data flow values for context-sensitivity. This framework is not\nrestricted to problems with distributive flow functions, although the lattice\nmust be finite. It combines the key ideas of the tabulation method of the\nfunctional approach and the technique of value-based termination of call string\nconstruction.\n  The efficiency and precision of interprocedural analyses is heavily affected\nby the precision of the underlying call graph. This is especially important for\nobject-oriented languages like Java where virtual method invocations cause an\nexplosion of spurious call edges if the call graph is constructed naively. We\nhave instantiated our framework with a flow and context-sensitive points-to\nanalysis in Soot, which enables the construction of call graphs that are far\nmore precise than those constructed by Soot's SPARK engine."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.6284v3", 
    "other_authors": "Clemens Grabmayer, Jan Rochel", 
    "title": "Expressibility in the Lambda Calculus with mu", 
    "arxiv-id": "1304.6284v3", 
    "author": "Jan Rochel", 
    "publish": "2013-04-23T13:26:02Z", 
    "summary": "We address a problem connected to the unfolding semantics of functional\nprogramming languages: give a useful characterization of those infinite\nlambda-terms that are lambda_{letrec}-expressible in the sense that they arise\nas infinite unfoldings of terms in lambda_{letrec}, the lambda-calculus with\nletrec. We provide two characterizations, using concepts we introduce for\ninfinite lambda-terms: regularity, strong regularity, and binding-capturing\nchains. It turns out that lambda_{letrec}-expressible infinite lambda-terms\nform a proper subclass of the regular infinite lambda-terms. In this paper we\nestablish these characterizations only for expressibility in lambda_{mu}, the\nlambda-calculus with explicit mu-recursion. We show that for all infinite\nlambda-terms T the following are equivalent: (i): T is lambda_{mu}-expressible;\n(ii): T is strongly regular; (iii): T is regular, and it only has finite\nbinding-capturing chains.\n  We define regularity and strong regularity for infinite lambda-terms as two\ndifferent generalizations of regularity for infinite first-order terms: as the\nexistence of only finitely many subterms that are defined as the reducts of two\nrewrite relations for decomposing lambda-terms. These rewrite relations act on\ninfinite lambda-terms furnished with a marked prefix of abstractions for\ncollecting decomposed lambda-abstractions and keeping the terms closed under\ndecomposition. They differ in how vacuous abstractions in the prefix are\nremoved.\n  This report accompanies the article with the same title for the proceedings\nof the conference RTA 2013, and mainly differs from that by providing the proof\nof the characterization of lambda_{mu}-expressibility with binding-capturing\nchains."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-39799-8_14", 
    "link": "http://arxiv.org/pdf/1304.7600v1", 
    "other_authors": "Piotr Beling", 
    "title": "C++11 - okre\u015blanie typ\u00f3w", 
    "arxiv-id": "1304.7600v1", 
    "author": "Piotr Beling", 
    "publish": "2013-04-29T09:30:33Z", 
    "summary": "This paper presents a review of some new futures introduced to C++ language\nby ISO/IEC 14882:2011 standard (known as C++11). It describes new language\nelements which allow to easier expressed of types of variables: auto and\ndecltype keywords, new function declaration syntax, and tools which are\nincluded in type_traits header.\n  -----\n  Niniejszy artyku{\\l} jest jednym z serii artyku{\\l}\\'ow w kt\\'orych zawarto\nprzegl{\\ka}d nowych element\\'ow j{\\ke}zyka C++ wprowadzonych przez standard\nISO/IEC 14882:2011, znany pod nazw{\\ka} C++11. W artykule przedstawiono nowe\nmo\\.zliwo\\'sci zwi{\\ka}zane ze wskazywaniem typ\\'ow zmiennych. Opisano s{\\l}owa\nkluczowe auto i decltype, now{\\ka} sk{\\l}adnie deklarowania funkcji/metod oraz\nnarz{\\ke}dzia zawarte w pliku nag{\\l}\\'owkowym <type_traits>."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1305.3163v8", 
    "other_authors": "J. Ian Johnson, David Van Horn", 
    "title": "Abstracting Abstract Control (Extended)", 
    "arxiv-id": "1305.3163v8", 
    "author": "David Van Horn", 
    "publish": "2013-05-14T14:19:00Z", 
    "summary": "The strength of a dynamic language is also its weakness: run-time flexibility\ncomes at the cost of compile-time predictability. Many of the hallmarks of\ndynamic languages such as closures, continuations, various forms of reflection,\nand a lack of static types make many programmers rejoice, while compiler\nwriters, tool developers, and verification engineers lament. The dynamism of\nthese features simply confounds statically reasoning about programs that use\nthem. Consequently, static analyses for dynamic languages are few, far between,\nand seldom sound.\n  The \"abstracting abstract machines\" (AAM) approach to constructing static\nanalyses has recently been proposed as a method to ameliorate the difficulty of\ndesigning analyses for such language features. The approach, so called because\nit derives a function for the sound and computable approximation of program\nbehavior starting from the abstract machine semantics of a language, provides a\nviable approach to dynamic language analysis since all that is required is a\nmachine description of the interpreter.\n  The original AAM recipe produces finite state abstractions, which cannot\nfaithfully represent an interpreter's control stack. Recent advances have shown\nthat higher-order programs can be approximated with pushdown systems. However,\nthese automata theoretic models either break down on features that inspect or\nmodify the control stack.\n  In this paper, we tackle the problem of bringing pushdown flow analysis to\nthe domain of dynamic language features. We revise the abstracting abstract\nmachines technique to target the stronger computational model of pushdown\nsystems. In place of automata theory, we use only abstract machines and\nmemoization. As case studies, we show the technique applies to a language with\nclosures, garbage collection, stack-inspection, and first-class composable\ncontinuations."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1305.4584v1", 
    "other_authors": "Ludovic Court\u00e8s", 
    "title": "Functional Package Management with Guix", 
    "arxiv-id": "1305.4584v1", 
    "author": "Ludovic Court\u00e8s", 
    "publish": "2013-05-20T17:38:19Z", 
    "summary": "We describe the design and implementation of GNU Guix, a purely functional\npackage manager designed to support a complete GNU/Linux distribution. Guix\nsupports transactional upgrades and roll-backs, unprivileged package\nmanagement, per-user profiles, and garbage collection. It builds upon the\nlow-level build and deployment layer of the Nix package manager. Guix uses\nScheme as its programming interface. In particular, we devise an embedded\ndomain-specific language (EDSL) to describe and compose packages. We\ndemonstrate how it allows us to benefit from the host general-purpose\nprogramming language while not compromising on expressiveness. Second, we show\nthe use of Scheme to write build programs, leading to \"two-tier\" programming\nsystem."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1305.4957v1", 
    "other_authors": "Alexander Bau, Johannes Waldmann", 
    "title": "Propositional Encoding of Constraints over Tree-Shaped Data", 
    "arxiv-id": "1305.4957v1", 
    "author": "Johannes Waldmann", 
    "publish": "2013-05-21T20:35:20Z", 
    "summary": "We present a functional programming language for specifying constraints over\ntree-shaped data. The language allows for Haskell-like algebraic data types and\npattern matching. Our constraint compiler CO4 translates these programs into\nsatisfiability problems in propositional logic. We present an application from\nthe area of automated analysis of (non-)termination of rewrite systems."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1305.6108v2", 
    "other_authors": "Keehang Kwon", 
    "title": "Bounded Choice Queries for Logic Programming", 
    "arxiv-id": "1305.6108v2", 
    "author": "Keehang Kwon", 
    "publish": "2013-05-27T05:22:24Z", 
    "summary": "Adding versatile interactions to goals and queries in logic programming is an\nessential task. Unfortunately, existing logic languages can take input from the\nuser only via the $read$ construct.\n  We propose to add a new interactive goal to allow for more controlled and\nmore guided participation from the user. We illustrate our idea via \\muprolog,\nan extension of Prolog with bounded choice goals."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1305.6721v1", 
    "other_authors": "Matthias Keil, Peter Thiemann", 
    "title": "Type-based Dependency Analysis for JavaScript", 
    "arxiv-id": "1305.6721v1", 
    "author": "Peter Thiemann", 
    "publish": "2013-05-29T08:38:32Z", 
    "summary": "Dependency analysis is a program analysis that determines potential data flow\nbetween program points. While it is not a security analysis per se, it is a\nviable basis for investigating data integrity, for ensuring confidentiality,\nand for guaranteeing sanitization. A noninterference property can be stated and\nproved for the dependency analysis. We have designed and implemented a\ndependency analysis for JavaScript. We formalize this analysis as an\nabstraction of a tainting semantics. We prove the correctness of the tainting\nsemantics, the soundness of the abstraction, a noninterference property, and\nthe termination of the analysis."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2661088.2661098", 
    "link": "http://arxiv.org/pdf/1306.1870v1", 
    "other_authors": "Jos\u00e9 de Oliveira Guimar\u00e3es", 
    "title": "The Cyan Language", 
    "arxiv-id": "1306.1870v1", 
    "author": "Jos\u00e9 de Oliveira Guimar\u00e3es", 
    "publish": "2013-06-08T02:22:42Z", 
    "summary": "This is the manual of Cyan, a prototype-based object-oriented language. Cyan\nsupports static typing, single inheritance, mixin objects (similar to mixin\nclasses with mixin inheritance), generic prototypes, and Java-like interfaces.\nThe language has several innovations: a completely object-oriented exception\nsystem, statically-typed closures, a kind of graphical metaobjects called\ncodegs, optional dynamic typing, user-defined literal objects (an innovative\nway of creating objects), context objects (a generalization of closures), and\ngrammar methods and message sends (which makes it easy to define Domain\nSpecific Languages)."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2505879.2505884", 
    "link": "http://arxiv.org/pdf/1306.1901v1", 
    "other_authors": "Roberto Bagnara, Fred Mesnard", 
    "title": "Eventual Linear Ranking Functions", 
    "arxiv-id": "1306.1901v1", 
    "author": "Fred Mesnard", 
    "publish": "2013-06-08T10:10:05Z", 
    "summary": "Program termination is a hot research topic in program analysis. The last few\nyears have witnessed the development of termination analyzers for programming\nlanguages such as C and Java with remarkable precision and performance. These\nsystems are largely based on techniques and tools coming from the field of\ndeclarative constraint programming. In this paper, we first recall an algorithm\nbased on Farkas' Lemma for discovering linear ranking functions proving\ntermination of a certain class of loops. Then we propose an extension of this\nmethod for showing the existence of eventual linear ranking functions, i.e.,\nlinear functions that become ranking functions after a finite unrolling of the\nloop. We show correctness and completeness of this algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2505879.2505884", 
    "link": "http://arxiv.org/pdf/1306.1919v1", 
    "other_authors": "Lars Bergstrom, Matthew Fluet, John Reppy, Nora Sandler", 
    "title": "Practical Inlining of Functions with Free Variables", 
    "arxiv-id": "1306.1919v1", 
    "author": "Nora Sandler", 
    "publish": "2013-06-08T13:31:59Z", 
    "summary": "A long-standing practical challenge in the optimization of higher-order\nlanguages is inlining functions with free variables. Inlining code statically\nat a function call site is safe if the compiler can guarantee that the free\nvariables have the same bindings at the inlining point as they do at the point\nwhere the function is bound as a closure (code and free variables). There have\nbeen many attempts to create a heuristic to check this correctness condition,\nfrom Shivers' kCFA-based reflow analysis to Might's Delta-CFA and anodization,\nbut all of those have performance unsuitable for practical compiler\nimplementations. In practice, modern language implementations rely on a series\nof tricks to capture some common cases (e.g., closures whose free variables are\nonly top-level identifiers such as +) and rely on hand-inlining by the\nprogrammer for anything more complicated.\n  This work provides the first practical, general approach for inlining\nfunctions with free variables. We also provide a proof of correctness, an\nevaluation of both the execution time and performance impact of this\noptimization, and some tips and tricks for implementing an efficient and\nprecise control-flow analysis."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000070", 
    "link": "http://arxiv.org/pdf/1306.2291v1", 
    "other_authors": "Gianluca Amato, Francesca Scozzari", 
    "title": "Optimal multi-binding unification for sharing and linearity analysis", 
    "arxiv-id": "1306.2291v1", 
    "author": "Francesca Scozzari", 
    "publish": "2013-06-10T19:12:57Z", 
    "summary": "In the analysis of logic programs, abstract domains for detecting sharing\nproperties are widely used. Recently the new domain $\\Linp$ has been introduced\nto generalize both sharing and linearity information. This domain is endowed\nwith an optimal abstract operator for single-binding unification. The authors\nclaim that the repeated application of this operator is also optimal for\nmulti-binding unification. This is the proof of such a claim."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.117.2", 
    "link": "http://arxiv.org/pdf/1306.2692v1", 
    "other_authors": "Paolo Tranquilli", 
    "title": "Indexed Labels for Loop Iteration Dependent Costs", 
    "arxiv-id": "1306.2692v1", 
    "author": "Paolo Tranquilli", 
    "publish": "2013-06-12T01:55:23Z", 
    "summary": "We present an extension to the labelling approach, a technique for lifting\nresource consumption information from compiled to source code. This approach,\nwhich is at the core of the annotating compiler from a large fragment of C to\n8051 assembly of the CerCo project, looses preciseness when differences arise\nas to the cost of the same portion of code, whether due to code transformation\nsuch as loop optimisations or advanced architecture features (e.g. cache). We\npropose to address this weakness by formally indexing cost labels with the\niterations of the containing loops they occur in. These indexes can be\ntransformed during the compilation, and when lifted back to source code they\nproduce dependent costs.\n  The proposed changes have been implemented in CerCo's untrusted prototype\ncompiler from a large fragment of C to 8051 assembly."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.117.2", 
    "link": "http://arxiv.org/pdf/1306.4473v2", 
    "other_authors": "Hugo Pacheco, Nuno Macedo, Alcino Cunha, Janis Voigtl\u00e4nder", 
    "title": "A Generic Scheme and Properties of Bidirectional Transformations", 
    "arxiv-id": "1306.4473v2", 
    "author": "Janis Voigtl\u00e4nder", 
    "publish": "2013-06-19T10:03:01Z", 
    "summary": "The recent rise of interest in bidirectional transformations (BXs) has led to\nthe development of many BX frameworks, originating in diverse computer science\ndisciplines. From a user perspective, these frameworks vary significantly in\nboth interface and predictability of the underlying bidirectionalization\ntechnique. In this paper we start by presenting a generic BX scheme that can be\ninstantiated to different concrete interfaces, by plugging-in the desired\nnotion of update and traceability. Based on that scheme, we then present\nseveral desirable generic properties that may characterize a BX framework, and\nshow how they can be instantiated to concrete interfaces. This generic\npresentation is useful when exploring the BX design space: it might help\ndevelopers when designing new frameworks and end-users when comparing existing\nones. We support the latter claim, by applying it in a comparative survey of\npopular existing BX frameworks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.136.1", 
    "link": "http://arxiv.org/pdf/1306.4713v2", 
    "other_authors": "Sam Tobin-Hochstadt, David Van Horn", 
    "title": "From Principles to Practice with Class in the First Year", 
    "arxiv-id": "1306.4713v2", 
    "author": "David Van Horn", 
    "publish": "2013-06-19T22:33:39Z", 
    "summary": "We propose a bridge between functional and object-oriented programming in the\nfirst-year curriculum. Traditionally, curricula that begin with functional\nprogramming transition to a professional, usually object-oriented, language in\nthe second course. This transition poses obstacles for students, and often\nresults in confusing the details of development environments, syntax, and\nlibraries with the fundamentals of OO programming that the course should focus\non. Instead, we propose to begin the second course with a sequence of custom\nteaching languages which minimize the transition from the first course, and\nallow students to focus on core ideas. After working through the sequence of\npedagogical languages, we then transition to Java, at which point students have\na strong command of the basic principles. We have 3 years of experience with\nthis course, with notable success."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.136.1", 
    "link": "http://arxiv.org/pdf/1306.4898v2", 
    "other_authors": "Gabriel S. Hjort Blindell", 
    "title": "Survey on Instruction Selection: An Extensive and Modern Literature   Review", 
    "arxiv-id": "1306.4898v2", 
    "author": "Gabriel S. Hjort Blindell", 
    "publish": "2013-06-20T14:49:35Z", 
    "summary": "Instruction selection is one of three optimisation problems involved in the\ncode generator backend of a compiler. The instruction selector is responsible\nof transforming an input program from its target-independent representation\ninto a target-specific form by making best use of the available machine\ninstructions. Hence instruction selection is a crucial part of efficient code\ngeneration.\n  Despite on-going research since the late 1960s, the last, comprehensive\nsurvey on the field was written more than 30 years ago. As new approaches and\ntechniques have appeared since its publication, this brings forth a need for a\nnew, up-to-date review of the current body of literature. This report addresses\nthat need by performing an extensive review and categorisation of existing\nresearch. The report therefore supersedes and extends the previous surveys, and\nalso attempts to identify where future research should be directed."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.136.1", 
    "link": "http://arxiv.org/pdf/1306.5061v1", 
    "other_authors": "Robert Jakob, Peter Thiemann", 
    "title": "Towards Tree Automata-based Success Types", 
    "arxiv-id": "1306.5061v1", 
    "author": "Peter Thiemann", 
    "publish": "2013-06-21T06:53:04Z", 
    "summary": "Error detection facilities for dynamic languages are often based on unit\ntesting. Thus, the advantage of rapid prototyping and flexibility must be\nweighed against cumbersome and time consuming test suite development. Lindahl\nand Sagonas' success typings provide a means of static must-fail detection in\nErlang. Due to the constraint-based nature of the approach, some errors\ninvolving nested tuples and recursion cannot be detected.\n  We propose an approach that uses an extension of model checking for\npattern-matching recursion schemes with context-aware ranked tree automata to\nprovide improved success typings for a constructor-based first-order prototype\nlanguage."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.5142v1", 
    "other_authors": "Daniel Langr, Pavel Tvrd\u00edk, Tom\u00e1\u0161 Dytrych, Jerry P. Draayer", 
    "title": "Fake Run-Time Selection of Template Arguments in C++", 
    "arxiv-id": "1306.5142v1", 
    "author": "Jerry P. Draayer", 
    "publish": "2013-06-21T14:01:43Z", 
    "summary": "C++ does not support run-time resolution of template type arguments. To\ncircumvent this restriction, we can instantiate a template for all possible\ncombinations of type arguments at compile time and then select the proper\ninstance at run time by evaluation of some provided conditions. However, for\ntemplates with multiple type parameters such a solution may easily result in a\nbranching code bloat. We present a template metaprogramming algorithm called\nfor_id that allows the user to select the proper template instance at run time\nwith theoretical minimum sustained complexity of the branching code."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.5930v1", 
    "other_authors": "Jos\u00e9 de Oliveira Guimar\u00e3es", 
    "title": "The Green Language", 
    "arxiv-id": "1306.5930v1", 
    "author": "Jos\u00e9 de Oliveira Guimar\u00e3es", 
    "publish": "2013-06-25T11:54:48Z", 
    "summary": "Green is a statically-typed object-oriented language that separates subtyping\nfrom inheritance. It supports garbage collection, classes as first-class\nobjects, parameterized classes, introspective reflection and a kind of run-time\nmetaobjects called shells."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.6029v4", 
    "other_authors": "Alex Shafarenko", 
    "title": "AstraKahn: A Coordination Language for Streaming Networks", 
    "arxiv-id": "1306.6029v4", 
    "author": "Alex Shafarenko", 
    "publish": "2013-06-25T16:49:16Z", 
    "summary": "This is a preliminary version of the language report. It contains key\ndefinitions, specifications and some examples, but lacks completeness. The full\ndocument will include Chapter 3 (Data and Instrumentation Layer) and will\ncomprise an appendix giving the complete syntax and some whole program\nexamples. The purpose of the present document is to fix the concepts and major\nfeatures of the language and to enable the production of the definition\ndocument that is required for implementation."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.6032v1", 
    "other_authors": "Joshua Dunfield, Neelakantan R. Krishnaswami", 
    "title": "Complete and Easy Bidirectional Typechecking for Higher-Rank   Polymorphism", 
    "arxiv-id": "1306.6032v1", 
    "author": "Neelakantan R. Krishnaswami", 
    "publish": "2013-06-25T16:52:51Z", 
    "summary": "Bidirectional typechecking, in which terms either synthesize a type or are\nchecked against a known type, has become popular for its scalability (unlike\nDamas-Milner type inference, bidirectional typing remains decidable even for\nvery expressive type systems), its error reporting, and its relative ease of\nimplementation. Following design principles from proof theory, bidirectional\ntyping can be applied to many type constructs. The principles underlying a\nbidirectional approach to polymorphism, however, are less obvious. We give a\ndeclarative, bidirectional account of higher-rank polymorphism, grounded in\nproof theory; this calculus enjoys many properties such as eta-reduction and\npredictability of annotations. We give an algorithm for implementing the\ndeclarative system; our algorithm is remarkably simple and well-behaved,\ndespite being both sound and complete."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.6047v2", 
    "other_authors": "Russell Power, Alex Rubinsteyn", 
    "title": "How fast can we make interpreted Python?", 
    "arxiv-id": "1306.6047v2", 
    "author": "Alex Rubinsteyn", 
    "publish": "2013-06-25T17:57:00Z", 
    "summary": "Python is a popular dynamic language with a large part of its appeal coming\nfrom powerful libraries and extension modules. These augment the language and\nmake it a productive environment for a wide variety of tasks, ranging from web\ndevelopment (Django) to numerical analysis (NumPy). Unfortunately, Python's\nperformance is quite poor when compared to modern implementations of languages\nsuch as Lua and JavaScript.\n  Why does Python lag so far behind these other languages? As we show, the very\nsame API and extension libraries that make Python a powerful language also make\nit very difficult to efficiently execute. Given that we want to retain access\nto the great extension libraries that already exist for Python, how fast can we\nmake it?\n  To evaluate this, we designed and implemented Falcon, a high-performance\nbytecode interpreter fully compatible with the standard CPython interpreter.\nFalcon applies a number of well known optimizations and introduces several new\ntechniques to speed up execution of Python bytecode. In our evaluation, we\nfound Falcon an average of 25% faster than the standard Python interpreter on\nmost benchmarks and in some cases about 2.5X faster."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.6526v4", 
    "other_authors": "Damiano Zanardini, Samir Genaim", 
    "title": "Inference of Field-Sensitive Reachability and Cyclicity", 
    "arxiv-id": "1306.6526v4", 
    "author": "Samir Genaim", 
    "publish": "2013-06-27T14:48:27Z", 
    "summary": "In heap-based languages, knowing that a variable x points to an acyclic data\nstructure is useful for analyzing termination: this information guarantees that\nthe depth of the data structure to which x points is greater than the depth of\nthe structure pointed to by x.fld, and allows bounding the number of iterations\nof a loop which traverses the data structure on fld. In general, proving\ntermination needs acyclicity, unless program-specific or non-automated\nreasoning is performed. However, recent work could prove that certain loops\nterminate even without inferring acyclicity, because they traverse data\nstructures \"acyclically\". Consider a double-linked list: if it is possible to\ndemonstrate that every cycle involves both the \"next\" and the \"prev\" field,\nthen a traversal on \"next\" terminates since no cycle will be traversed\ncompletely. This paper develops a static analysis inferring field-sensitive\nreachability and cyclicity information, which is more general than existing\napproaches. Propositional formulae are computed, which describe which fields\nmay or may not be traversed by paths in the heap. Consider a tree with edges\n\"left\" and \"right\" to the left and right sub-trees, and \"parent\" to the parent\nnode: termination of a loop traversing leaf-up cannot be guaranteed by\nstate-of-the-art analyses. Instead, propositional formulae computed by this\nanalysis indicate that cycles must traverse \"parent\" and at least one between\n\"left\" and \"right\": termination is guaranteed as no cycle is traversed\ncompletely. This paper defines the necessary abstract domains and builds an\nabstract semantics on them. A prototypical implementation provides the expected\nresult on relevant examples."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30561-0_11", 
    "link": "http://arxiv.org/pdf/1306.6856v1", 
    "other_authors": "Marco Gaboardi", 
    "title": "Linear Dependent Types for Domain Specific Program Analysis (Extended   Abstract)", 
    "arxiv-id": "1306.6856v1", 
    "author": "Marco Gaboardi", 
    "publish": "2013-06-28T14:38:13Z", 
    "summary": "In this tutorial I will present how a combination of linear and dependent\ntype can be useful to describe different properties about higher order\nprograms. Linear types have been proved particularly useful to express\nproperties of functions; dependent types are useful to describe the behavior of\nthe program in terms of its control flow. This two ideas fits together well\nwhen one is interested in analyze properties of functions depending on the\ncontrol flow of the program. I will present these ideas with example taken by\ncomplexity analysis and sensitivity analysis. I will conclude the tutorial by\narguing about the generality of this approach."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.0339v2", 
    "other_authors": "Taisuke Sato, Philipp Meyer", 
    "title": "Infinite probability computation by cyclic explanation graphs", 
    "arxiv-id": "1309.0339v2", 
    "author": "Philipp Meyer", 
    "publish": "2013-09-02T09:44:28Z", 
    "summary": "Tabling in logic programming has been used to eliminate redundant computation\nand also to stop infinite loop. In this paper we investigate another\npossibility of tabling, i.e. to compute an infinite sum of probabilities for\nprobabilistic logic programs. Using PRISM, a logic-based probabilistic modeling\nlanguage with a tabling mechanism, we generalize prefix probability computation\nfor probabilistic context free grammars (PCFGs) to probabilistic logic\nprograms. Given a top-goal, we search for all proofs with tabling and obtain an\nexplanation graph which compresses them and may be cyclic. We then convert the\nexplanation graph to a set of linear probability equations and solve them by\nmatrix operation. The solution gives us the probability of the top-goal, which,\nin nature, is an infinite sum of probabilities. Our general approach to prefix\nprobability computation through tabling not only allows to deal with non-PCFGs\nsuch as probabilistic left-corner grammars (PLCGs) but has applications such as\nplan recognition and probabilistic model checking and makes it possible to\ncompute probability for probabilistic models describing cyclic relations. To\nappear in Theory and Practice of Logic Programming (TPLP)."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.1251v1", 
    "other_authors": "Keehang Kwon", 
    "title": "Pattern Matching via Choice Existential Quantifications in Imperative   Languages", 
    "arxiv-id": "1309.1251v1", 
    "author": "Keehang Kwon", 
    "publish": "2013-09-05T07:45:00Z", 
    "summary": "Selection statements -- if-then-else, switch and try-catch -- are commonly\nused in modern imperative programming languages. We propose another selection\nstatement called a {\\it choice existentially quantified statement}. This\nstatement turns out to be quite useful for pattern matching among several\nmerits. Examples will be provided for this statement."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.1307v1", 
    "other_authors": "Piotr Beling", 
    "title": "C++11 -- idea r-warto\u015bci i przenoszenia", 
    "arxiv-id": "1309.1307v1", 
    "author": "Piotr Beling", 
    "publish": "2013-09-05T10:48:52Z", 
    "summary": "This paper presents a review of some new futures introduced to C++ language\nby ISO/IEC 14882:2011 standard (known as C++11). It describes the ideas of\nr-values and move constructors.\n  ----\n  Niniejszy artyku{\\l} jest jednym z serii artyku{\\l}\\'ow w kt\\'orych zawarto\nprzegl{\\ka}d nowych element\\'ow j{\\ke}zyka C++ wprowadzonych przez standard\nISO/IEC 14882:2011, znany pod nazw{\\ka} C++11. W artykule przedstawiono nowe\nmo\\.zliwo\\'sci zwi{\\ka}zane z przekazywaniem parametr\\'ow i pisaniem\nkonstruktor\\'ow. Zawarto w nim dok{\\l}adne om\\'owienie idei r-warto\\'sci i\nprzenoszenia obiekt\\'ow."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.2348v2", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "An Overview of Nominal-Typing versus Structural-Typing in   Object-Oriented Programming (with code examples)", 
    "arxiv-id": "1309.2348v2", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2013-09-10T00:10:04Z", 
    "summary": "NOOP is a mathematical model of nominally-typed OOP that proves the\nidentification of inheritance and subtyping in mainstream nominally-typed OO\nprogramming languages and the validity of this identification. This report\ngives an overview of the main notions in OOP relevant to constructing a\nmathematical model of OOP such as NOOP. The emphasis in this report is on\ndefining nominality, nominal typing and nominal subtyping of mainstream\nnominally-typed OO languages, and on contrasting the three notions with their\ncounterparts in structurally-typed OO languages, i.e., with structurality,\nstructural typing and structural subtyping, respectively. An additional\nappendix demonstrates these notions and other related notions, and the\ndifferences between them, using some simple code examples. A detailed, more\ntechnical comparison between nominal typing and structural typing in OOP is\npresented in other publications."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.2511v1", 
    "other_authors": "Eva Darulova, Viktor Kuncak", 
    "title": "On Sound Compilation of Reals", 
    "arxiv-id": "1309.2511v1", 
    "author": "Viktor Kuncak", 
    "publish": "2013-09-10T13:53:09Z", 
    "summary": "Writing accurate numerical software is hard because of many sources of\nunavoidable uncertainties, including finite numerical precision of\nimplementations. We present a programming model where the user writes a program\nin a real-valued implementation and specification language that explicitly\nincludes different types of uncertainties. We then present a compilation\nalgorithm that generates a conventional implementation that is guaranteed to\nmeet the desired precision with respect to real numbers. Our verification step\ngenerates verification conditions that treat different uncertainties in a\nunified way and encode reasoning about floating-point roundoff errors into\nreasoning about real numbers. Such verification conditions can be used as a\nstandardized format for verifying the precision and the correctness of\nnumerical programs. Due to their often non-linear nature, precise reasoning\nabout such verification conditions remains difficult. We show that current\nstate-of-the art SMT solvers do not scale well to solving such verification\nconditions. We propose a new procedure that combines exact SMT solving over\nreals with approximate and sound affine and interval arithmetic. We show that\nthis approach overcomes scalability limitations of SMT solvers while providing\nimproved precision over affine and interval arithmetic. Using our initial\nimplementation we show the usefullness and effectiveness of our approach on\nseveral examples, including those containing non-linear computation."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.3128v1", 
    "other_authors": "Ton Chanh Le", 
    "title": "Preliminary Notes on Termination and Non-Termination Reasoning", 
    "arxiv-id": "1309.3128v1", 
    "author": "Ton Chanh Le", 
    "publish": "2013-09-12T12:04:49Z", 
    "summary": "In this preliminary note, we will illustrate our ideas on automated\nmechanisms for termination and non-termination reasoning."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068413000562", 
    "link": "http://arxiv.org/pdf/1309.4334v1", 
    "other_authors": "Martin Dias, Damien Cassou, St\u00e9phane Ducasse", 
    "title": "Representing Code History with Development Environment Events", 
    "arxiv-id": "1309.4334v1", 
    "author": "St\u00e9phane Ducasse", 
    "publish": "2013-09-17T14:50:22Z", 
    "summary": "Modern development environments handle information about the intent of the\nprogrammer: for example, they use abstract syntax trees for providing\nhigh-level code manipulation such as refactorings; nevertheless, they do not\nkeep track of this information in a way that would simplify code sharing and\nchange understanding. In most Smalltalk systems, source code modifications are\nimmediately registered in a transaction log often called a ChangeSet. Such\nmechanism has proven reliability, but it has several limitations. In this paper\nwe analyse such limitations and describe scenarios and requirements for\ntracking fine-grained code history with a semantic representation. We present\nEpicea, an early prototype implementation. We want to enrich code sharing with\nextra information from the IDE, which will help understanding the intention of\nthe changes and let a new generation of tools act in consequence."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.4", 
    "link": "http://arxiv.org/pdf/1309.5131v1", 
    "other_authors": "Isabella Mastroeni", 
    "title": "Abstract interpretation-based approaches to Security - A Survey on   Abstract Non-Interference and its Challenging Applications", 
    "arxiv-id": "1309.5131v1", 
    "author": "Isabella Mastroeni", 
    "publish": "2013-09-20T01:43:25Z", 
    "summary": "In this paper we provide a survey on the framework of abstract\nnon-interference. In particular, we describe a general formalization of\nabstract non-interference by means of three dimensions (observation, protection\nand semantics) that can be instantiated in order to obtain well known or even\nnew weakened non-interference properties. Then, we show that the notions of\nabstract non-interference introduced in language-based security are instances\nof this more general framework which allows to better understand the different\ncomponents of a non-interference policy. Finally, we consider two challenging\nresearch fields concerning security where abstract non-interference seems a\npromising approach providing new perspectives and new solutions to open\nproblems: Code injection and code obfuscation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.6", 
    "link": "http://arxiv.org/pdf/1309.5132v1", 
    "other_authors": "Philip Mulry", 
    "title": "Notions of Monad Strength", 
    "arxiv-id": "1309.5132v1", 
    "author": "Philip Mulry", 
    "publish": "2013-09-20T01:43:32Z", 
    "summary": "Over the past two decades the notion of a strong monad has found wide\napplicability in computing. Arising out of a need to interpret products in\ncomputational and semantic settings, different approaches to this concept have\narisen. In this paper we introduce and investigate the connections between\nthese approaches and also relate the results to monad composition. We also\nintroduce new methods for checking and using the required laws associated with\nsuch compositions, as well as provide examples illustrating problems and issues\nthat arise."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.7", 
    "link": "http://arxiv.org/pdf/1309.5133v1", 
    "other_authors": "Mads Rosendahl", 
    "title": "Abstract Interpretation as a Programming Language", 
    "arxiv-id": "1309.5133v1", 
    "author": "Mads Rosendahl", 
    "publish": "2013-09-20T01:43:40Z", 
    "summary": "In David Schmidt's PhD work he explored the use of denotational semantics as\na programming language. It was part of an effort to not only treat formal\nsemantics as specifications but also as interpreters and input to compiler\ngenerators. The semantics itself can be seen as a program and one may examine\ndifferent programming styles and ways to represent states.\n  Abstract interpretation is primarily a technique for derivation and\nspecification of program analysis. As with denotational semantics we may also\nview abstract interpretations as programs and examine the implementation. The\nmain focus in this paper is to show that results from higher-order strictness\nanalysis may be used more generally as fixpoint operators for higher-order\nfunctions over lattices and thus provide a technique for immediate\nimplementation of a large class of abstract interpretations. Furthermore, it\nmay be seen as a programming paradigm and be used to write programs in a\ncircular style."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.17", 
    "link": "http://arxiv.org/pdf/1309.5144v1", 
    "other_authors": "Anindya Banerjee, David A. Naumann", 
    "title": "A Simple Semantics and Static Analysis for Stack Inspection", 
    "arxiv-id": "1309.5144v1", 
    "author": "David A. Naumann", 
    "publish": "2013-09-20T01:45:17Z", 
    "summary": "The Java virtual machine and the .NET common language runtime feature an\naccess control mechanism specified operationally in terms of run-time stack\ninspection. We give a denotational semantics in \"eager\" form, and show that it\nis equivalent to the \"lazy\" semantics using stack inspection. We give a static\nanalysis of safety, i.e., the absence of security errors, that is simpler than\nprevious proposals. We identify several program transformations that can be\nused to remove run-time checks. We give complete, detailed proofs for safety of\nthe analysis and for the transformations, exploiting compositionality of the\neager semantics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.20", 
    "link": "http://arxiv.org/pdf/1309.5147v1", 
    "other_authors": "Chris Hankin", 
    "title": "A short note on Simulation and Abstraction", 
    "arxiv-id": "1309.5147v1", 
    "author": "Chris Hankin", 
    "publish": "2013-09-20T01:45:44Z", 
    "summary": "This short note is written in celebration of David Schmidt's sixtieth\nbirthday. He has now been active in the program analysis research community for\nover thirty years and we have enjoyed many interactions with him. His work on\ncharacterising simulations between Kripke structures using Galois connections\nwas particularly influential in our own work on using probabilistic abstract\ninterpretation to study Larsen and Skou's notion of probabilistic bisimulation.\nWe briefly review this work and discuss some recent applications of these ideas\nin a variety of different application areas."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.20", 
    "link": "http://arxiv.org/pdf/1309.5500v1", 
    "other_authors": "Judith Bishop, Nikolai Tillmann, Arno Puder, Vinayak Naik", 
    "title": "PRoMoTo 2013 proceedings", 
    "arxiv-id": "1309.5500v1", 
    "author": "Vinayak Naik", 
    "publish": "2013-09-21T16:48:22Z", 
    "summary": "Programming for Mobile and Touch (PRoMoTo'13) was held at the 2013 ACM\nSIGPLAN conference on Systems, Programming, Languages and Applications (SPLASH\n2013), October 2013 in Indianapolis, USA. Submissions for this event were\ninvited in the general area of mobile and touch-oriented programming languages\nand programming environments, and teaching of programming for mobile devices.\nThese are proceedings of the PRoMoTo'13."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.20", 
    "link": "http://arxiv.org/pdf/1310.2300v1", 
    "other_authors": "Stefan Brunthaler", 
    "title": "Speculative Staging for Interpreter Optimization", 
    "arxiv-id": "1310.2300v1", 
    "author": "Stefan Brunthaler", 
    "publish": "2013-10-08T22:44:44Z", 
    "summary": "Interpreters have a bad reputation for having lower performance than\njust-in-time compilers. We present a new way of building high performance\ninterpreters that is particularly effective for executing dynamically typed\nprogramming languages. The key idea is to combine speculative staging of\noptimized interpreter instructions with a novel technique of incrementally and\niteratively concerting them at run-time.\n  This paper introduces the concepts behind deriving optimized instructions\nfrom existing interpreter instructions---incrementally peeling off layers of\ncomplexity. When compiling the interpreter, these optimized derivatives will be\ncompiled along with the original interpreter instructions. Therefore, our\ntechnique is portable by construction since it leverages the existing\ncompiler's backend. At run-time we use instruction substitution from the\ninterpreter's original and expensive instructions to optimized instruction\nderivatives to speed up execution.\n  Our technique unites high performance with the simplicity and portability of\ninterpreters---we report that our optimization makes the CPython interpreter up\nto more than four times faster, where our interpreter closes the gap between\nand sometimes even outperforms PyPy's just-in-time compiler."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.129.20", 
    "link": "http://arxiv.org/pdf/1310.2741v1", 
    "other_authors": "Guido Chari, Diego Garbervetsky, Camillo Bruni, Marcus Denker, St\u00e9phane Ducasse", 
    "title": "Waterfall: Primitives Generation on the Fly", 
    "arxiv-id": "1310.2741v1", 
    "author": "St\u00e9phane Ducasse", 
    "publish": "2013-10-10T09:24:15Z", 
    "summary": "Modern languages are typically supported by managed runtimes (Virtual\nMachines). Since VMs have to deal with many concepts such as memory management,\nabstract execution model and scheduling, they tend to be very complex.\nAdditionally, VMs have to meet strong performance requirements. This demand of\nperformance is one of the main reasons why many VMs are built statically. Thus,\ndesign decisions are frozen at compile time preventing changes at runtime. One\nclear example is the impossibility to dynamically adapt or change primitives of\nthe VM once it has been compiled. In this work we present a toolchain that\nallows for altering and configuring components such as primitives and plug-ins\nat runtime. The main contribution is Waterfall, a dynamic and reflective\ntranslator from Slang, a restricted subset of Smalltalk, to native code.\nWaterfall generates primitives on demand and executes them on the fly. We\nvalidate our approach by implementing dynamic primitive modification and\nruntime customization of VM plug-ins."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2543728.2543733", 
    "link": "http://arxiv.org/pdf/1310.3404v2", 
    "other_authors": "Gabriel Kerneis, Charlie Shepherd, Stefan Hajnoczi", 
    "title": "QEMU/CPC: Static Analysis and CPS Conversion for Safe, Portable, and   Efficient Coroutines", 
    "arxiv-id": "1310.3404v2", 
    "author": "Stefan Hajnoczi", 
    "publish": "2013-10-12T16:46:28Z", 
    "summary": "Coroutines and events are two common abstractions for writing concurrent\nprograms. Because coroutines are often more convenient, but events more\nportable and efficient, it is natural to want to translate the former into the\nlatter. CPC is such a source-to-source translator for C programs, based on a\npartial conversion into continuation-passing style (CPS conversion) of\nfunctions annotated as cooperative.\n  In this article, we study the application of the CPC translator to QEMU, an\nopen-source machine emulator which also uses annotated coroutine functions for\nconcurrency. We first propose a new type of annotations to identify functions\nwhich never cooperate, and we introduce CoroCheck, a tool for the static\nanalysis and inference of cooperation annotations. Then, we improve the CPC\ntranslator, defining CPS conversion as a calling convention for the C language,\nwith support for indirect calls to CPS-converted function through function\npointers. Finally, we apply CoroCheck and CPC to QEMU (750 000 lines of C\ncode), fixing hundreds of missing annotations and comparing performance of the\ntranslated code with existing implementations of coroutines in QEMU.\n  Our work shows the importance of static annotation checking to prevent actual\nconcurrency bugs, and demonstrates that CPS conversion is a flexible, portable,\nand efficient compilation technique, even for very large programs written in an\nimperative language."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2543728.2543733", 
    "link": "http://arxiv.org/pdf/1310.3481v1", 
    "other_authors": "Azadeh Farzan, Zachary Kincaid", 
    "title": "An Algebraic Framework for Compositional Program Analysis", 
    "arxiv-id": "1310.3481v1", 
    "author": "Zachary Kincaid", 
    "publish": "2013-10-13T13:41:25Z", 
    "summary": "The purpose of a program analysis is to compute an abstract meaning for a\nprogram which approximates its dynamic behaviour. A compositional program\nanalysis accomplishes this task with a divide-and-conquer strategy: the meaning\nof a program is computed by dividing it into sub-programs, computing their\nmeaning, and then combining the results. Compositional program analyses are\ndesirable because they can yield scalable (and easily parallelizable) program\nanalyses.\n  This paper presents algebraic framework for designing, implementing, and\nproving the correctness of compositional program analyses. A program analysis\nin our framework defined by an algebraic structure equipped with sequencing,\nchoice, and iteration operations. From the analysis design perspective, a\nparticularly interesting consequence of this is that the meaning of a loop is\ncomputed by applying the iteration operator to the loop body. This style of\ncompositional loop analysis can yield interesting ways of computing loop\ninvariants that cannot be defined iteratively. We identify a class of\nalgorithms, the so-called path-expression algorithms [Tarjan1981,Scholz2007],\nwhich can be used to efficiently implement analyses in our framework. Lastly,\nwe develop a theory for proving the correctness of an analysis by establishing\nan approximation relationship between an algebra defining a concrete semantics\nand an algebra defining an analysis."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2543728.2543733", 
    "link": "http://arxiv.org/pdf/1310.4283v2", 
    "other_authors": "Arnaud Spiwack", 
    "title": "Abstract interpretation as anti-refinement", 
    "arxiv-id": "1310.4283v2", 
    "author": "Arnaud Spiwack", 
    "publish": "2013-10-16T07:16:49Z", 
    "summary": "This article shows a correspondence between abstract interpretation of\nimperative programs and the refinement calculus: in the refinement calculus, an\nabstract interpretation of a program is a specification which is a function.\n  This correspondence can be used to guide the design of mechanically verified\nstatic analyses, keeping the correctness proof well separated from the\nheuristic parts of the algorithms."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2543728.2543738", 
    "link": "http://arxiv.org/pdf/1310.4780v3", 
    "other_authors": "James Cheney, Sam Lindley, Gabriel Radanne, Philip Wadler", 
    "title": "Effective Quotation: relating approaches to language-integrated query", 
    "arxiv-id": "1310.4780v3", 
    "author": "Philip Wadler", 
    "publish": "2013-10-17T17:29:40Z", 
    "summary": "Language-integrated query techniques have been explored in a number of\ndifferent language designs. We consider two different, type-safe approaches\nemployed by Links and F#. Both approaches provide rich dynamic query generation\ncapabilities, and thus amount to a form of heterogeneous staged computation,\nbut to date there has been no formal investigation of their relative\nexpressiveness. We present two core calculi Eff and Quot, respectively\ncapturing the essential aspects of language-integrated querying using effects\nin Links and quotation in LINQ. We show via translations from Eff to Quot and\nback that the two approaches are equivalent in expressiveness. Based on the\ntranslation from Eff to Quot, we extend a simple Links compiler to handle\nqueries."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2543728.2543738", 
    "link": "http://arxiv.org/pdf/1310.5538v1", 
    "other_authors": "Vijay Saraswat, Radha Jagadeesan, Vineet Gupta", 
    "title": "TCC, with History", 
    "arxiv-id": "1310.5538v1", 
    "author": "Vineet Gupta", 
    "publish": "2013-10-21T13:33:21Z", 
    "summary": "Modern computer systems are awash in a sea of asynchronous events. There is\nan increasing need for a declarative language that can permit business users to\nspecify complex event-processing rules. Such rules should be able to correlate\ndifferent event streams, detect absence of events (negative information),\npermit aggregations over sliding windows, specify dependent sliding windows\netc. For instance it should be possible to precisely state a rule such as\n\"Every seventh trading session that DowJones has risen consecutively, and IBM's\nstock is off 3% over its average in this period, evaluate IBM position\",\n\"Declare the sensor as faulty if no reading has been received for 500 ms\", etc.\nFurther, the language should be implementable efficiently in an event-driven\nfashion.\n  We propose the Timed (Default) Concurrent Constraint, TCC, programming\nframework as a foundation for such complex event processing. While very rich,\nthe TCC framework \"forgets\" information from one instant to the next. We make\ntwo extensions. First, we extend the TCC model to carry the store from previous\ntime instants as \"past\" information in the current time instant. This permits\nrules to to be written with rich queries over the past. Second, we show that\nmany of the powerful properties of the agent language can be folded into the\nquery language by permitting agents and queries to be defined mutually\nrecursively, building on the testing interpretation of intuitionistic logic\ndescribed in RCC \\cite{radha-fsttcs05}. We show that this permits queries to\nmove \"back and forth\" in the past, e.g.{} \"Order a review if the last time that\nIBM stock price dropped by 10% in a day, there was more than 20% increase in\ntrading volume for Oracle the following day.\"\n  We provide a formal semantics for TCC + Histories and establish some basic\nproperties."
},{
    "category": "cs.PL", 
    "doi": "10.3233/JCS-130487", 
    "link": "http://arxiv.org/pdf/1310.6299v2", 
    "other_authors": "Umut A. Acar, Amal Ahmed, James Cheney, Roly Perera", 
    "title": "A Core Calculus for Provenance", 
    "arxiv-id": "1310.6299v2", 
    "author": "Roly Perera", 
    "publish": "2013-10-23T17:14:33Z", 
    "summary": "Provenance is an increasing concern due to the ongoing revolution in sharing\nand processing scientific data on the Web and in other computer systems. It is\nproposed that many computer systems will need to become provenance-aware in\norder to provide satisfactory accountability, reproducibility, and trust for\nscientific or other high-value data. To date, there is not a consensus\nconcerning appropriate formal models or security properties for provenance. In\nprevious work, we introduced a formal framework for provenance security and\nproposed formal definitions of properties called disclosure and obfuscation.\n  In this article, we study refined notions of positive and negative disclosure\nand obfuscation in a concrete setting, that of a general-purpose programing\nlanguage. Previous models of provenance have focused on special-purpose\nlanguages such as workflows and database queries. We consider a higher-order,\nfunctional language with sums, products, and recursive types and functions, and\nequip it with a tracing semantics in which traces themselves can be replayed as\ncomputations. We present an annotation-propagation framework that supports many\nprovenance views over traces, including standard forms of provenance studied\npreviously. We investigate some relationships among provenance views and\ndevelop some partial solutions to the disclosure and obfuscation problems,\nincluding correct algorithms for disclosure and positive obfuscation based on\ntrace slicing."
},{
    "category": "cs.PL", 
    "doi": "10.3233/JCS-130487", 
    "link": "http://arxiv.org/pdf/1310.7774v1", 
    "other_authors": "Mariano Martinez Peck, Noury Bouraqadi, St\u00e9phane Ducasse, Luc Fabresse, Marcus Denker", 
    "title": "Ghost: A Uniform and General-Purpose Proxy Implementation", 
    "arxiv-id": "1310.7774v1", 
    "author": "Marcus Denker", 
    "publish": "2013-10-29T12:03:53Z", 
    "summary": "A proxy object is a surrogate or placeholder that controls access to another\ntarget object. Proxy objects are a widely used solution for different scenarios\nsuch as remote method invocation, future objects, behavioral reflection, object\ndatabases, inter-languages communications and bindings, access control, lazy or\nparallel evaluation, security, among others. Most proxy implementations support\nproxies for regular objects but are unable to create proxies for objects with\nan important role in the runtime infrastructure such as classes or methods.\nProxies can be complex to install, they can have a significant overhead, they\ncan be limited to certain kind of classes, etc. Moreover, proxy implementations\nare often not stratified and they do not have a clear separation between\nproxies (the objects intercepting messages) and handlers (the objects handling\ninterceptions). In this paper, we present Ghost: a uniform and general-purpose\nproxy implementation for the Pharo programming language. Ghost provides low\nmemory consuming proxies for regular objects as well as for classes and\nmethods. When a proxy takes the place of a class, it intercepts both the\nmessages received by the class and the lookup of methods for messages received\nby its instances. Similarly, if a proxy takes the place of a method, then the\nmethod execution is intercepted too."
},{
    "category": "cs.PL", 
    "doi": "10.3233/JCS-130487", 
    "link": "http://arxiv.org/pdf/1311.0768v2", 
    "other_authors": "Bertrand Jeannet, Peter Schrammel, Sriram Sankaranarayanan", 
    "title": "Abstract Acceleration of General Linear Loops", 
    "arxiv-id": "1311.0768v2", 
    "author": "Sriram Sankaranarayanan", 
    "publish": "2013-11-04T17:03:17Z", 
    "summary": "We present abstract acceleration techniques for computing loop invariants for\nnumerical programs with linear assignments and conditionals. Whereas abstract\ninterpretation techniques typically over-approximate the set of reachable\nstates iteratively, abstract acceleration captures the effect of the loop with\na single, non-iterative transfer function applied to the initial states at the\nloop head. In contrast to previous acceleration techniques, our approach\napplies to any linear loop without restrictions. Its novelty lies in the use of\nthe Jordan normal form decomposition of the loop body to derive symbolic\nexpressions for the entries of the matrix modeling the effect of n>=0\niterations of the loop. The entries of such a matrix depend on $n$ through\ncomplex polynomial, exponential and trigonometric functions. Therefore, we\nintroduces an abstract domain for matrices that captures the linear inequality\nrelations between these complex expressions. This results in an abstract matrix\nfor describing the fixpoint semantics of the loop.\n  Our approach integrates smoothly into standard abstract interpreters and can\nhandle programs with nested loops and loops containing conditional branches. We\nevaluate it over small but complex loops that are commonly found in control\nsoftware, comparing it with other tools for computing linear loop invariants.\nThe loops in our benchmarks typically exhibit polynomial, exponential and\noscillatory behaviors that present challenges to existing approaches. Our\napproach finds non-trivial invariants to prove useful bounds on the values of\nvariables for such loops, clearly outperforming the existing approaches in\nterms of precision while exhibiting good performance."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.3293v1", 
    "other_authors": "Dr. Brijender Kahanwal", 
    "title": "Abstraction Level Taxonomy of Programming Language Frameworks", 
    "arxiv-id": "1311.3293v1", 
    "author": "Dr. Brijender Kahanwal", 
    "publish": "2013-11-13T10:21:31Z", 
    "summary": "The main purpose of this article is to describe the taxonomy of computer\nlanguages according to the levels of abstraction. There exists so many computer\nlanguages because of so many reasons like the evolution of better computer\nlanguages over the time; the socio-economic factors as the proprietary\ninterests, commercial advantages; expressive power; ease of use of novice;\norientation toward special purposes; orientation toward special hardware; and\ndiverse ideas about most suitability. Moreover, the important common properties\nof most of these languages are discussed here. No programming language is\ndesigned in a vacuity, but it solves some specific kinds of problems. There is\na different framework for each problem and best suitable framework for each\nproblem. A single framework is not best for all types of problems. So, it is\nimportant to select vigilantly the frameworks supported by the language. The\nfive generation of the computer programming languages are explored in this\npaper to some extent."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.4198v1", 
    "other_authors": "Shuying Liang, Matthew Might, David Van Horn", 
    "title": "AnaDroid: Malware Analysis of Android with User-supplied Predicates", 
    "arxiv-id": "1311.4198v1", 
    "author": "David Van Horn", 
    "publish": "2013-11-17T18:47:15Z", 
    "summary": "Today's mobile platforms provide only coarse-grained permissions to users\nwith regard to how third- party applications use sensitive private data.\nUnfortunately, it is easy to disguise malware within the boundaries of\nlegitimately-granted permissions. For instance, granting access to \"contacts\"\nand \"internet\" may be necessary for a text-messaging application to function,\neven though the user does not want contacts transmitted over the internet. To\nunderstand fine-grained application use of permissions, we need to statically\nanalyze their behavior. Even then, malware detection faces three hurdles: (1)\nanalyses may be prohibitively expensive, (2) automated analyses can only find\nbehaviors that they are designed to find, and (3) the maliciousness of any\ngiven behavior is application-dependent and subject to human judgment. To\nremedy these issues, we propose semantic-based program analysis, with a human\nin the loop as an alternative approach to malware detection. In particular, our\nanalysis allows analyst-crafted semantic predicates to search and filter\nanalysis results. Human-oriented semantic-based program analysis can\nsystematically, quickly and concisely characterize the behaviors of mobile\napplications. We describe a tool that provides analysts with a library of the\nsemantic predicates and the ability to dynamically trade speed and precision.\nIt also provides analysts the ability to statically inspect details of every\nsuspicious state of (abstract) execution in order to make a ruling as to\nwhether or not the behavior is truly malicious with respect to the intent of\nthe application. In addition, permission and profiling reports are generated to\naid analysts in identifying common malicious behaviors."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.4231v1", 
    "other_authors": "Matthew Might, Yannis Smaragdakis, David Van Horn", 
    "title": "Resolving and Exploiting the $k$-CFA Paradox", 
    "arxiv-id": "1311.4231v1", 
    "author": "David Van Horn", 
    "publish": "2013-11-17T23:59:28Z", 
    "summary": "Low-level program analysis is a fundamental problem, taking the shape of\n\"flow analysis\" in functional languages and \"points-to\" analysis in imperative\nand object-oriented languages. Despite the similarities, the vocabulary and\nresults in the two communities remain largely distinct, with limited\ncross-understanding. One of the few links is Shivers's $k$-CFA work, which has\nadvanced the concept of \"context-sensitive analysis\" and is widely known in\nboth communities.\n  Recent results indicate that the relationship between the functional and\nobject-oriented incarnations of $k$-CFA is not as well understood as thought.\nVan Horn and Mairson proved $k$-CFA for $k \\geq 1$ to be EXPTIME-complete;\nhence, no polynomial-time algorithm can exist. Yet, there are several\npolynomial-time formulations of context-sensitive points-to analyses in\nobject-oriented languages. Thus, it seems that functional $k$-CFA may actually\nbe a profoundly different analysis from object-oriented $k$-CFA. We resolve\nthis paradox by showing that the exact same specification of $k$-CFA is\npolynomial-time for object-oriented languages yet exponential- time for\nfunctional ones: objects and closures are subtly different, in a way that\ninteracts crucially with context-sensitivity and complexity. This illumination\nleads to an immediate payoff: by projecting the object-oriented treatment of\nobjects onto closures, we derive a polynomial-time hierarchy of\ncontext-sensitive CFAs for functional programs."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.4733v1", 
    "other_authors": "David Van Horn", 
    "title": "The Complexity of Flow Analysis in Higher-Order Languages", 
    "arxiv-id": "1311.4733v1", 
    "author": "David Van Horn", 
    "publish": "2013-11-19T13:31:54Z", 
    "summary": "This dissertation proves lower bounds on the inherent difficulty of deciding\nflow analysis problems in higher-order programming languages. We give exact\ncharacterizations of the computational complexity of 0CFA, the $k$CFA\nhierarchy, and related analyses. In each case, we precisely capture both the\nexpressiveness and feasibility of the analysis, identifying the elements\nresponsible for the trade-off.\n  0CFA is complete for polynomial time. This result relies on the insight that\nwhen a program is linear (each bound variable occurs exactly once), the\nanalysis makes no approximation; abstract and concrete interpretation coincide,\nand therefore pro- gram analysis becomes evaluation under another guise.\nMoreover, this is true not only for 0CFA, but for a number of further\napproximations to 0CFA. In each case, we derive polynomial time completeness\nresults.\n  For any $k > 0$, $k$CFA is complete for exponential time. Even when $k = 1$,\nthe distinction in binding contexts results in a limited form of closures,\nwhich do not occur in 0CFA. This theorem validates empirical observations that\n$k$CFA is intractably slow for any $k > 0$. There is, in the worst case---and\nplausibly, in practice---no way to tame the cost of the analysis. Exponential\ntime is required. The empirically observed intractability of this analysis can\nbe understood as being inherent in the approximation problem being solved,\nrather than reflecting unfortunate gaps in our programming abilities."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.5810v1", 
    "other_authors": "David Van Horn, Harry G. Mairson", 
    "title": "Deciding $k$CFA is complete for EXPTIME", 
    "arxiv-id": "1311.5810v1", 
    "author": "Harry G. Mairson", 
    "publish": "2013-11-22T16:50:48Z", 
    "summary": "We give an exact characterization of the computational complexity of the\n$k$CFA hierarchy. For any $k > 0$, we prove that the control flow decision\nproblem is complete for deterministic exponential time. This theorem validates\nempirical observations that such control flow analysis is intractable. It also\nprovides more general insight into the complexity of abstract interpretation."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.5825v1", 
    "other_authors": "David Van Horn, Harry G. Mairson", 
    "title": "Flow analysis, linearity, and PTIME", 
    "arxiv-id": "1311.5825v1", 
    "author": "Harry G. Mairson", 
    "publish": "2013-11-22T17:23:55Z", 
    "summary": "Flow analysis is a ubiquitous and much-studied component of compiler\ntechnology---and its variations abound. Amongst the most well known is Shivers'\n0CFA; however, the best known algorithm for 0CFA requires time cubic in the\nsize of the analyzed program and is unlikely to be improved. Consequently,\nseveral analyses have been designed to approximate 0CFA by trading precision\nfor faster computation. Henglein's simple closure analysis, for example,\nforfeits the notion of directionality in flows and enjoys an \"almost linear\"\ntime algorithm. But in making trade-offs between precision and complexity, what\nhas been given up and what has been gained? Where do these analyses differ and\nwhere do they coincide?\n  We identify a core language---the linear $\\lambda$-calculus---where 0CFA,\nsimple closure analysis, and many other known approximations or restrictions to\n0CFA are rendered identical. Moreover, for this core language, analysis\ncorresponds with (instrumented) evaluation. Because analysis faithfully\ncaptures evaluation, and because the linear $\\lambda$-calculus is complete for\nPTIME, we derive PTIME-completeness results for all of these analyses."
},{
    "category": "cs.PL", 
    "doi": "10.4230/OASIcs.FSFMA.2013.68", 
    "link": "http://arxiv.org/pdf/1311.6929v1", 
    "other_authors": "Jonathan Protzenko", 
    "title": "Illustrating the Mezzo programming language", 
    "arxiv-id": "1311.6929v1", 
    "author": "Jonathan Protzenko", 
    "publish": "2013-11-27T10:57:10Z", 
    "summary": "When programmers want to prove strong program invariants, they are usually\nfaced with a choice between using theorem provers and using traditional\nprogramming languages. The former requires them to provide program proofs,\nwhich, for many applications, is considered a heavy burden. The latter provides\nless guarantees and the programmer usually has to write run-time assertions to\ncompensate for the lack of suitable invariants expressible in the type system.\n  We introduce Mezzo, a programming language in the tradition of ML, in which\nthe usual concept of a type is replaced by a more precise notion of a\npermission. Programs written in Mezzo usually enjoy stronger guarantees than\nprograms written in pure ML. However, because Mezzo is based on a type system,\nthe reasoning requires no user input. In this paper, we illustrate the key\nconcepts of Mezzo, highlighting the static guarantees our language provides."
},{
    "category": "cs.PL", 
    "doi": "10.4230/OASIcs.FSFMA.2013.68", 
    "link": "http://arxiv.org/pdf/1311.7203v1", 
    "other_authors": "Dr. Brijender Kahanwal, Dr. T. P. Singh", 
    "title": "Function Overloading Implementation in C++", 
    "arxiv-id": "1311.7203v1", 
    "author": "Dr. T. P. Singh", 
    "publish": "2013-11-28T04:16:49Z", 
    "summary": "In this article the function overloading in object-oriented programming is\nelaborated and how they are implemented in C++. The language supports a variety\nof programming styles. Here we are describing the polymorphism and its types in\nbrief. The main stress is given on the function overloading implementation\nstyles in the language. The polymorphic nature of languages has advantages like\nthat we can add new code without requiring changes to the other classes and\ninterfaces (in Java language) are easily implemented. In this technique, the\nrun-time overhead is also introduced in dynamic binding which increases the\nexecution time of the software. But there are no such types of overheads in\nthis static type of polymorphism because everything is resolved at the time of\ncompile time. Polymorphism; Function Overloading; Static Polymorphism;\nOverloading; Compile Time Polymorphism."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500598", 
    "link": "http://arxiv.org/pdf/1311.7242v1", 
    "other_authors": "Jonathan Protzenko, Fran\u00e7ois Pottier", 
    "title": "Programming with Permissions in Mezzo", 
    "arxiv-id": "1311.7242v1", 
    "author": "Fran\u00e7ois Pottier", 
    "publish": "2013-11-28T09:15:55Z", 
    "summary": "We present Mezzo, a typed programming language of ML lineage. Mezzo is\nequipped with a novel static discipline of duplicable and affine permissions,\nwhich controls aliasing and ownership. This rules out certain mistakes,\nincluding representation exposure and data races, and enables new idioms, such\nas gradual initialization, memory re-use, and (type)state changes. Although the\ncore static discipline disallows sharing a mutable data structure, Mezzo offers\nseveral ways of working around this restriction, including a novel dynamic\nownership control mechanism which we dub \"adoption and abandon\"."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500598", 
    "link": "http://arxiv.org/pdf/1311.7256v1", 
    "other_authors": "Arma\u00ebl Gu\u00e9neau, Fran\u00e7ois Pottier, Jonathan Protzenko", 
    "title": "The ins and outs of iteration in Mezzo", 
    "arxiv-id": "1311.7256v1", 
    "author": "Jonathan Protzenko", 
    "publish": "2013-11-28T09:58:55Z", 
    "summary": "This is a talk proposal for HOPE 2013. Using iteration over a collection as a\ncase study, we wish to illustrate the strengths and weaknesses of the prototype\nprogramming language Mezzo."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500598", 
    "link": "http://arxiv.org/pdf/1312.0018v1", 
    "other_authors": "Gabriel Scherer, Jan Hoffmann", 
    "title": "Tracking Data-Flow with Open Closure Types", 
    "arxiv-id": "1312.0018v1", 
    "author": "Jan Hoffmann", 
    "publish": "2013-11-29T21:06:29Z", 
    "summary": "Type systems hide data that is captured by function closures in function\ntypes. In most cases this is a beneficial design that favors simplicity and\ncompositionality. However, some applications require explicit information about\nthe data that is captured in closures. This paper introduces open closure\ntypes, that is, function types that are decorated with type contexts. They are\nused to track data-flow from the environment into the function closure. A\nsimply-typed lambda calculus is used to study the properties of the type theory\nof open closure types. A distinctive feature of this type theory is that an\nopen closure type of a function can vary in different type contexts. To present\nan application of the type theory, it is shown that a type derivation\nestablishes a simple non-interference property in the sense of information-flow\ntheory. A publicly available prototype implementation of the system can be used\nto experiment with type derivations for example programs."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500598", 
    "link": "http://arxiv.org/pdf/1312.0078v1", 
    "other_authors": "G\u00e9rard Berry, Manuel Serrano", 
    "title": "Hop and HipHop : Multitier Web Orchestration", 
    "arxiv-id": "1312.0078v1", 
    "author": "Manuel Serrano", 
    "publish": "2013-11-30T08:48:02Z", 
    "summary": "Rich applications merge classical computing, client-server concurrency,\nweb-based interfaces, and the complex time- and event-based reactive\nprogramming found in embedded systems. To handle them, we extend the Hop web\nprogramming platform by HipHop, a domain-specific language dedicated to\nevent-based process orchestration. Borrowing the synchronous reactive model of\nEsterel, HipHop is based on synchronous concurrency and preemption primitives\nthat are known to be key components for the modular design of complex reactive\nbehaviors. HipHop departs from Esterel by its ability to handle the dynamicity\nof Web applications, thanks to the reflexivity of Hop. Using a music player\nexample, we show how to modularly build a non-trivial Hop application using\nHipHop orchestration code."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2500365.2500598", 
    "link": "http://arxiv.org/pdf/1312.0138v1", 
    "other_authors": "Krasimir Yordzhev", 
    "title": "The bitwise operations related to a fast sorting algorithm", 
    "arxiv-id": "1312.0138v1", 
    "author": "Krasimir Yordzhev", 
    "publish": "2013-11-30T18:43:31Z", 
    "summary": "In the work we discuss the benefit of using bitwise operations in\nprogramming. Some interesting examples in this respect have been shown. What is\ndescribed in detail is an algorithm for sorting an integer array with the\nsubstantial use of the bitwise operations. Besides its correctness we strictly\nprove that the described algorithm works in time O(n). In the work during the\nrealisation of each of the examined algorithms we use the apparatus of the\nobject-oriented programming with the syntax and the semantics of the\nprogramming language C++."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.135.3", 
    "link": "http://arxiv.org/pdf/1312.0342v1", 
    "other_authors": "Pieter Van Gorp, Louis M. Rose", 
    "title": "The Petri-Nets to Statecharts Transformation Case", 
    "arxiv-id": "1312.0342v1", 
    "author": "Louis M. Rose", 
    "publish": "2013-12-02T06:59:06Z", 
    "summary": "This paper describes a case study for the sixth Transformation Tool Contest.\nThe case is based on a mapping from Petri-Nets to statecharts (i.e., from flat\nprocess models to hierarchical ones). The case description separates a simple\nmapping phase from a phase that involves the step by step destruction Petri-Net\nelements and the corresponding construction of a hierarchy of statechart\nelements. Although the focus of this case study is on the comparison of the\nruntime performance of solutions, we also include correctness tests as well as\nbonus criteria for evaluating transformation language and tool features."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.135.3", 
    "link": "http://arxiv.org/pdf/1312.0658v1", 
    "other_authors": "Yufei Cai, Paolo G. Giarrusso, Tillmann Rendel, Klaus Ostermann", 
    "title": "A Theory of Changes for Higher-Order Languages - Incrementalizing   \u03bb-Calculi by Static Differentiation", 
    "arxiv-id": "1312.0658v1", 
    "author": "Klaus Ostermann", 
    "publish": "2013-12-02T23:23:36Z", 
    "summary": "If the result of an expensive computation is invalidated by a small change to\nthe input, the old result should be updated incrementally instead of\nreexecuting the whole computation. We incrementalize programs through their\nderivative. A derivative maps changes in the program's input directly to\nchanges in the program's output, without reexecuting the original program. We\npresent a program transformation taking programs to their derivatives, which is\nfully static and automatic, supports first-class functions, and produces\nderivatives amenable to standard optimization.\n  We prove the program transformation correct in Agda for a family of\nsimply-typed {\\lambda}-calculi, parameterized by base types and primitives. A\nprecise interface specifies what is required to incrementalize the chosen\nprimitives.\n  We investigate performance by a case study: We implement in Scala the program\ntransformation, a plugin and improve performance of a nontrivial program by\norders of magnitude."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.135.3", 
    "link": "http://arxiv.org/pdf/1312.0694v2", 
    "other_authors": "Jeremy G. Siek, Michael M. Vitousek", 
    "title": "Monotonic References for Gradual Typing", 
    "arxiv-id": "1312.0694v2", 
    "author": "Michael M. Vitousek", 
    "publish": "2013-12-03T04:24:44Z", 
    "summary": "We describe an alternative approach to handling mutable references (aka.\npointers) within a gradually typed language that has different efficiency\ncharacteristics than the prior approach of Herman et al. [2010]. In particular,\nwe reduce the costs of reading and writing through references in statically\ntyped regions of code. We reduce the costs to be the same as they would in a\nstatically typed language, that is, simply the cost of a load or store\ninstruction (for primitive data types). This reduction in cost is especially\nimportant for programmers who would like to use gradual typing to facilitate\ntransitioning from a dynamically-typed prototype of an algorithm to a\nstatically-typed, high-performance implementation. The programmers we have in\nmind are scientists and engineers who currently prototype in Matlab and then\nmanually translate their algorithms into Fortran. We present the static and\ndynamic semantics for mutable references and a mechanized proof of type safety\nusing the Isabelle proof assistant."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.135.3", 
    "link": "http://arxiv.org/pdf/1312.1529v1", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Instruction sequence expressions for the Karatsuba multiplication   algorithm", 
    "arxiv-id": "1312.1529v1", 
    "author": "C. A. Middelburg", 
    "publish": "2013-12-05T12:53:56Z", 
    "summary": "The Karatsuba multiplication algorithm is an algorithm for computing the\nproduct of two natural numbers represented in the binary number system. This\nmeans that the algorithm actually computes a function on bit strings. The\nrestriction of this function to bit strings of any given length can be computed\naccording to the Karatsuba multiplication algorithm by a finite instruction\nsequence that contains only instructions to set and get the content of Boolean\nregisters, forward jump instructions, and a termination instruction. We\ndescribe the instruction sequences concerned for the restrictions to bit\nstrings of the different lengths by uniform terms from an algebraic theory."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.135.3", 
    "link": "http://arxiv.org/pdf/1312.1812v3", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "Long multiplication by instruction sequences with backward jump   instructions", 
    "arxiv-id": "1312.1812v3", 
    "author": "C. A. Middelburg", 
    "publish": "2013-12-06T09:41:06Z", 
    "summary": "For each function on bit strings, its restriction to bit strings of any given\nlength can be computed by a finite instruction sequence that contains only\ninstructions to set and get the content of Boolean registers, forward jump\ninstructions, and a termination instruction. Backward jump instructions are not\nnecessary for this, but instruction sequences can be significantly shorter with\nthem. We take the function on bit strings that models the multiplication of\nnatural numbers on their representation in the binary number system to\ndemonstrate this by means of a concrete example. The example is reason to\ndiscuss points concerning the halting problem and the concept of an algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137", 
    "link": "http://arxiv.org/pdf/1312.2218v1", 
    "other_authors": "Nobuko Yoshida, Wim Vanderbauwhede", 
    "title": "Proceedings 5th Workshop on Programming Language Approaches to   Concurrency and Communication-cEntric Software", 
    "arxiv-id": "1312.2218v1", 
    "author": "Wim Vanderbauwhede", 
    "publish": "2013-12-08T14:19:11Z", 
    "summary": "PLACES 2013 (full title: Programming Language Approaches to Concurrency- and\nCommunication-cEntric Software) was the sixth edition of the PLACES workshop\nseries. After the first PLACES, which was affiliated to DisCoTec in 2008, the\nworkshop has been part of ETAPS every year since 2009 and is now an established\npart of the ETAPS satellite events. This year, PLACES was the best attended\nworkshop at ETAPS 2013.\n  The workshop series was started in order to promote the application of novel\nprogramming language ideas to the increasingly important problem of developing\nsoftware for systems in which concurrency and communication are intrinsic\naspects. This includes software for multi- and many-core systems, accelerators\nand large-scale distributed and/or service-oriented systems. The scope of\nPLACES includes new programming language features, whole new programming\nlanguage designs, new type systems, new semantic approaches, new program\nanalysis techniques, and new implementation mechanisms."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.8", 
    "link": "http://arxiv.org/pdf/1312.2704v1", 
    "other_authors": "Rumyana Neykova", 
    "title": "Session Types Go Dynamic or How to Verify Your Python Conversations", 
    "arxiv-id": "1312.2704v1", 
    "author": "Rumyana Neykova", 
    "publish": "2013-12-10T08:04:18Z", 
    "summary": "This paper presents the first implementation of session types in a\ndynamically-typed language - Python. Communication safety of the whole system\nis guaranteed at runtime by monitors that check the execution traces comply\nwith an associated protocol. Protocols are written in Scribble, a choreography\ndescription language based on multiparty session types, with addition of logic\nformulas for more precise behaviour properties. The presented framework\novercomes the limitations of previous works on the session types where all\nendpoints should be statically typed so that they do not permit\ninteroperability with untyped participants. The advantages, expressiveness and\nperformance of dynamic protocol checking are demonstrated through use case and\nbenchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.2707v1", 
    "other_authors": "Simon Dobson, Alan Dearle, Barry Porter", 
    "title": "Minimising virtual machine support for concurrency", 
    "arxiv-id": "1312.2707v1", 
    "author": "Barry Porter", 
    "publish": "2013-12-10T08:04:46Z", 
    "summary": "Co-operative and pre-emptive scheduling are usually considered to be\ncomplementary models of threading. In the case of virtual machines, we show\nthat they can be unified using a single concept, the bounded execution of a\nthread of control, essentially providing a first-class representation of a\ncomputation as it is reduced. Furthermore this technique can be used to surface\nthe thread scheduler of a language into the language itself, allowing programs\nto provide their own schedulers without any additional support in the virtual\nmachine, and allowing the same virtual machine to support different thread\nmodels simultaneously and without re-compilation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.3184v1", 
    "other_authors": "Matthias Keil, Peter Thiemann", 
    "title": "Efficient Dynamic Access Analysis Using JavaScript Proxies", 
    "arxiv-id": "1312.3184v1", 
    "author": "Peter Thiemann", 
    "publish": "2013-12-11T14:24:53Z", 
    "summary": "JSConTest introduced the notions of effect monitoring and dynamic effect\ninference for JavaScript. It enables the description of effects with path\nspecifications resembling regular expressions. It is implemented by an offline\nsource code transformation.\n  To overcome the limitations of the JSConTest implementation, we redesigned\nand reimplemented effect monitoring by taking advantange of JavaScript proxies.\nOur new design avoids all drawbacks of the prior implementation. It guarantees\nfull interposition; it is not restricted to a subset of JavaScript; it is\nself-maintaining; and its scalability to large programs is significantly better\nthan with JSConTest.\n  The improved scalability has two sources. First, the reimplementation is\nsignificantly faster than the original, transformation-based implementation.\nSecond, the reimplementation relies on the fly-weight pattern and on trace\nreduction to conserve memory. Only the combination of these techniques enables\nmonitoring and inference for large programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.3739v1", 
    "other_authors": "Silvia Crafa, David Cunningham, Vijay Saraswat, Avraham Shinnar, Olivier Tardieu", 
    "title": "Semantics of (Resilient) X10", 
    "arxiv-id": "1312.3739v1", 
    "author": "Olivier Tardieu", 
    "publish": "2013-12-13T09:22:30Z", 
    "summary": "We present a formal small-step structural operational semantics for a large\nfragment of X10, unifying past work. The fragment covers multiple places,\nmutable objects on the heap, sequencing, \\code{try/catch}, \\code{async},\n\\code{finish}, and \\code{at} constructs. This model accurately captures the\nbehavior of a large class of concurrent, multi-place X10 programs. Further, we\nintroduce a formal model of resilience in X10. During execution of an X10\nprogram, a place may fail for many reasons. Resilient X10 permits the program\nto continue executing, losing the data at the failed place, and most of the\ncontrol state, and repairing the global control state in such a way that key\nsemantic principles hold, the Invariant Happens Before Principle, and the\nFailure Masking Principle. These principles permit an X10 programmer to write\nclean code that continues to work in the presence of place failure. The given\nsemantics have additionally been mechanized in Coq."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.4328v1", 
    "other_authors": "Luc De Raedt, Angelika Kimmig", 
    "title": "Probabilistic Programming Concepts", 
    "arxiv-id": "1312.4328v1", 
    "author": "Angelika Kimmig", 
    "publish": "2013-12-16T12:04:39Z", 
    "summary": "A multitude of different probabilistic programming languages exists today,\nall extending a traditional programming language with primitives to support\nmodeling of complex, structured probability distributions. Each of these\nlanguages employs its own probabilistic primitives, and comes with a particular\nsyntax, semantics and inference procedure. This makes it hard to understand the\nunderlying programming concepts and appreciate the differences between the\ndifferent languages. To obtain a better understanding of probabilistic\nprogramming, we identify a number of core programming concepts underlying the\nprimitives used by various probabilistic languages, discuss the execution\nmechanisms that they require and use these to position state-of-the-art\nprobabilistic languages and their implementation. While doing so, we focus on\nprobabilistic extensions of logic programming languages such as Prolog, which\nhave been developed since more than 20 years."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.4454v2", 
    "other_authors": "Ekaterina Komendantskaya, Martin Schmidt, J\u00f3nathan Heras", 
    "title": "Exploiting Parallelism in Coalgebraic Logic Programming", 
    "arxiv-id": "1312.4454v2", 
    "author": "J\u00f3nathan Heras", 
    "publish": "2013-12-16T18:39:55Z", 
    "summary": "We present a parallel implementation of Coalgebraic Logic Programming (CoALP)\nin the programming language Go. CoALP was initially introduced to reflect\ncoalgebraic semantics of logic programming, with coalgebraic derivation\nalgorithm featuring both corecursion and parallelism. Here, we discuss how the\ncoalgebraic semantics influenced our parallel implementation of logic\nprogramming."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.5429v1", 
    "other_authors": "Matthias Keil, Peter Thiemann", 
    "title": "On the Proxy Identity Crisis", 
    "arxiv-id": "1312.5429v1", 
    "author": "Peter Thiemann", 
    "publish": "2013-12-19T07:56:11Z", 
    "summary": "A proxy, in general, is an object mediating access to an arbitrary target\nobject. The proxy is then intended to be used in place of the target object.\nIdeally, a proxy is not distinguishable from other objects. Running a program\nwith a proxy leads to the same outcome as running the program with the target\nobject. Even though the approach provides a lot of power to the user, proxies\ncome with a limitation. Because a proxy, wrapping a target object, is a new\nobject and different from its target, the interposition changes the behaviour\nof some core components. For distinct proxies the double == and triple ===\nequal operator returns false, even if the target object is the same. More\nprecisely, the expected result depends on use case. To overcome this limitation\nwe will discuss alternatives."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.6568v1", 
    "other_authors": "Ekaterina Komendantskaya, John Power, Martin Schmidt", 
    "title": "Coalgebraic Logic Programming: from Semantics to Implementation", 
    "arxiv-id": "1312.6568v1", 
    "author": "Martin Schmidt", 
    "publish": "2013-12-23T15:19:27Z", 
    "summary": "Coinductive definitions, such as that of an infinite stream, may often be\ndescribed by elegant logic programs, but ones for which SLD-refutation is of no\nvalue as SLD-derivations fall into infinite loops. Such definitions give rise\nto questions of lazy corecursive derivations and parallelism, as execution of\nsuch logic programs can have both recursive and corecursive features at once.\nObservational and coalgebraic semantics have been used to study them\nabstractly. The programming developments have often occurred separately and\nhave usually been implementation-led. Here, we give a coherent semantics-led\naccount of the issues, starting with abstract category theoretic semantics,\ndeveloping coalgebra to characterise naturally arising trees, and proceeding\ntowards implementation of a new dialect, CoALP, of logic programming,\ncharacterised by guarded lazy corecursion and parallelism."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.137.11", 
    "link": "http://arxiv.org/pdf/1312.6624v3", 
    "other_authors": "Diego Calvanese, Tomer Kotek, Mantas \u0160imkus, Helmut Veith, Florian Zuleger", 
    "title": "Shape and Content: Incorporating Domain Knowledge into Shape Analysis", 
    "arxiv-id": "1312.6624v3", 
    "author": "Florian Zuleger", 
    "publish": "2013-12-23T18:07:08Z", 
    "summary": "The verification community has studied dynamic data structures primarily in a\nbottom-up way by analyzing pointers and the shapes induced by them. Recent work\nin fields such as separation logic has made significant progress in extracting\nshapes from program source code. Many real world programs however manipulate\ncomplex data whose structure and content is most naturally described by\nformalisms from object oriented programming and databases. In this paper, we\nlook at the verification of programs with dynamic data structures from the\nperspective of content representation. Our approach is based on description\nlogic, a widely used knowledge representation paradigm which gives a logical\nunderpinning for diverse modeling frameworks such as UML and ER. Technically,\nwe assume that we have separation logic shape invariants obtained from a shape\nanalysis tool, and requirements on the program data in terms of description\nlogic. We show that the two-variable fragment of first order logic with\ncounting and trees %(whose decidability was proved at LICS 2013) can be used as\na joint framework to embed suitable fragments of description logic and\nseparation logic."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628148", 
    "link": "http://arxiv.org/pdf/1401.1460v5", 
    "other_authors": "Clemens Grabmayer, Jan Rochel", 
    "title": "Maximal Sharing in the Lambda Calculus with letrec", 
    "arxiv-id": "1401.1460v5", 
    "author": "Jan Rochel", 
    "publish": "2014-01-07T18:01:41Z", 
    "summary": "Increasing sharing in programs is desirable to compactify the code, and to\navoid duplication of reduction work at run-time, thereby speeding up execution.\nWe show how a maximal degree of sharing can be obtained for programs expressed\nas terms in the lambda calculus with letrec. We introduce a notion of `maximal\ncompactness' for lambda-letrec-terms among all terms with the same infinite\nunfolding. Instead of defined purely syntactically, this notion is based on a\ngraph semantics. lambda-letrec-terms are interpreted as first-order term graphs\nso that unfolding equivalence between terms is preserved and reflected through\nbisimilarity of the term graph interpretations. Compactness of the term graphs\ncan then be compared via functional bisimulation.\n  We describe practical and efficient methods for the following two problems:\ntransforming a lambda-letrec-term into a maximally compact form; and deciding\nwhether two lambda-letrec-terms are unfolding-equivalent. The transformation of\na lambda-letrec-term $L$ into maximally compact form $L_0$ proceeds in three\nsteps:\n  (i) translate L into its term graph $G = [[ L ]]$; (ii) compute the maximally\nshared form of $G$ as its bisimulation collapse $G_0$; (iii) read back a\nlambda-letrec-term $L_0$ from the term graph $G_0$ with the property $[[ L_0 ]]\n= G_0$. This guarantees that $L_0$ and $L$ have the same unfolding, and that\n$L_0$ exhibits maximal sharing.\n  The procedure for deciding whether two given lambda-letrec-terms $L_1$ and\n$L_2$ are unfolding-equivalent computes their term graph interpretations $[[\nL_1 ]]$ and $[[ L_2 ]]$, and checks whether these term graphs are bisimilar.\n  For illustration, we also provide a readily usable implementation."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628148", 
    "link": "http://arxiv.org/pdf/1401.5097v1", 
    "other_authors": "Olle Fredriksson", 
    "title": "Distributed call-by-value machines", 
    "arxiv-id": "1401.5097v1", 
    "author": "Olle Fredriksson", 
    "publish": "2014-01-20T21:32:38Z", 
    "summary": "We present a new abstract machine, called DCESH, which describes the\nexecution of higher-order programs running in distributed architectures. DCESH\nimplements a generalised form of Remote Procedure Call that supports calling\nhigher-order functions across node boundaries, without sending actual code. Our\nstarting point is a variant of the SECD machine that we call the CES machine,\nwhich implements reduction for untyped call-by-value PCF. We successively add\nthe features that we need for distributed execution and show the correctness of\neach addition. First we add heaps, forming the CESH machine, which provides\nfeatures necessary for more efficient execution, and show that there is a\nbisimulation between the CES and the CESH machine. Then we construct a\ntwo-level operational semantics, where the high level is a network of\ncommunicating machines, and the low level is given by local machine\ntransitions. Using these networks, we arrive at our final system, the\ndistributed CESH machine (DCESH). We show that there is a bisimulation relation\nalso between the CESH machine and the DCESH machine. All the technical results\nhave been formalised and proved correct in Agda, and a prototype compiler has\nbeen developed."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628148", 
    "link": "http://arxiv.org/pdf/1401.5292v1", 
    "other_authors": "\u00c9tienne Payet, Fred Mesnard, Fausto Spoto", 
    "title": "Non-Termination Analysis of Java Bytecode", 
    "arxiv-id": "1401.5292v1", 
    "author": "Fausto Spoto", 
    "publish": "2014-01-21T12:35:09Z", 
    "summary": "We introduce a fully automated static analysis that takes a sequential Java\nbytecode program P as input and attempts to prove that there exists an infinite\nexecution of P. The technique consists in compiling P into a constraint logic\nprogram P_CLP and in proving non-termination of P_CLP; when P consists of\ninstructions that are exactly compiled into constraints, the non-termination of\nP_CLP entails that of P. Our approach can handle method calls; to the best of\nour knowledge, it is the first static approach for Java bytecode able to prove\nthe existence of infinite recursions. We have implemented our technique inside\nthe Julia analyser. We have compared the results of Julia on a set of 113\nprograms with those provided by AProVE and Invel, the only freely usable\nnon-termination analysers comparable to ours that we are aware of. Only Julia\ncould detect non-termination due to infinite recursion."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628148", 
    "link": "http://arxiv.org/pdf/1401.5391v1", 
    "other_authors": "Dominic Orchard, Tomas Petricek, Alan Mycroft", 
    "title": "The semantic marriage of monads and effects", 
    "arxiv-id": "1401.5391v1", 
    "author": "Alan Mycroft", 
    "publish": "2014-01-21T17:23:45Z", 
    "summary": "Wadler and Thiemann unified type-and-effect systems with monadic semantics\nvia a syntactic correspondence and soundness results with respect to an\noperational semantics. They conjecture that a general, \"coherent\" denotational\nsemantics can be given to unify effect systems with a monadic-style semantics.\nWe provide such a semantics based on the novel structure of an indexed monad,\nwhich we introduce. We redefine the semantics of Moggi's computational\nlambda-calculus in terms of (strong) indexed monads which gives a one-to-one\ncorrespondence between indices of the denotations and the effect annotations of\ntraditional effect systems. Dually, this approach yields indexed comonads which\ngives a unified semantics and effect system to contextual notions of effect\n(called coeffects), which we have previously described."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2628136.2628148", 
    "link": "http://arxiv.org/pdf/1401.6227v1", 
    "other_authors": "Niki Vazou, Eric L. Seidel, Ranjit Jhala", 
    "title": "From Safety To Termination And Back: SMT-Based Verification For Lazy   Languages", 
    "arxiv-id": "1401.6227v1", 
    "author": "Ranjit Jhala", 
    "publish": "2014-01-24T00:45:45Z", 
    "summary": "SMT-based verifiers have long been an effective means of ensuring safety\nproperties of programs. While these techniques are well understood, we show\nthat they implicitly require eager semantics; directly applying them to a lazy\nlanguage is unsound due to the presence of divergent sub-computations. We\nrecover soundness by composing the safety analysis with a termination analysis.\nOf course, termination is itself a challenging problem, but we show how the\nsafety analysis can be used to ensure termination, thereby bootstrapping\nsoundness for the entire system. Thus, while safety invariants have long been\nrequired to prove termination, we show how termination proofs can be to soundly\nestablish safety. We have implemented our approach in liquidHaskell, a\nRefinement Type-based verifier for Haskell. We demonstrate its effectiveness\nvia an experimental evaluation using liquidHaskell to verify safety, functional\ncorrectness and termination properties of real-world Haskell libraries,\ntotaling over 10,000 lines of code."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-10(1:17)2014", 
    "link": "http://arxiv.org/pdf/1403.1477v3", 
    "other_authors": "Rasmus Ejlers M\u00f8gelberg, Sam Staton", 
    "title": "Linear usage of state", 
    "arxiv-id": "1403.1477v3", 
    "author": "Sam Staton", 
    "publish": "2014-03-06T16:18:12Z", 
    "summary": "We investigate the phenomenon that \"every monad is a linear state monad\". We\ndo this by studying a fully-complete state-passing translation from an impure\ncall-by-value language to a new linear type theory: the enriched call-by-value\ncalculus. The results are not specific to store, but can be applied to any\ncomputational effect expressible using algebraic operations, even to effects\nthat are not usually thought of as stateful. There is a bijective\ncorrespondence between generic effects in the source language and state access\noperations in the enriched call-by-value calculus. From the perspective of\ncategorical models, the enriched call-by-value calculus suggests a refinement\nof the traditional Kleisli models of effectful call-by-value languages. The new\nmodels can be understood as enriched adjunctions."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-10(1:17)2014", 
    "link": "http://arxiv.org/pdf/1403.2765v2", 
    "other_authors": "Christophe Rhodes, Jan Moringen, David Lichteblau", 
    "title": "Generalizers: New Metaobjects for Generalized Dispatch", 
    "arxiv-id": "1403.2765v2", 
    "author": "David Lichteblau", 
    "publish": "2014-03-11T21:35:41Z", 
    "summary": "This paper introduces a new metaobject, the generalizer, which complements\nthe existing specializer metaobject. With the help of examples, we show that\nthis metaobject allows for the efficient implementation of complex\nnon-class-based dispatch within the framework of existing metaobject protocols.\nWe present our modifications to the generic function invocation protocol from\nthe Art of the Metaobject Protocol; in combination with previous work, this\nproduces a fully-functional extension of the existing mechanism for method\nselection and combination, including support for method combination completely\nindependent from method selection. We discuss our implementation, within the\nSBCL implementation of Common Lisp, and in that context compare the performance\nof the new protocol with the standard one, demonstrating that the new protocol\ncan be tolerably efficient."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-10(1:17)2014", 
    "link": "http://arxiv.org/pdf/1403.3336v1", 
    "other_authors": "Kenneth Knowles", 
    "title": "Executable Refinement Types", 
    "arxiv-id": "1403.3336v1", 
    "author": "Kenneth Knowles", 
    "publish": "2014-03-13T17:31:39Z", 
    "summary": "This dissertation introduces executable refinement types, which refine\nstructural types by semi-decidable predicates, and establishes their metatheory\nand accompanying implementation techniques. These results are useful for\nundecidable type systems in general.\n  Particular contributions include: (1) Type soundness and a logical relation\nfor extensional equivalence for executable refinement types (though type\nchecking is undecidable); (2) hybrid type checking for executable refinement\ntypes, which blends static and dynamic checks in a novel way, in some sense\nperforming better statically than any decidable approximation; (3) a type\nreconstruction algorithm - reconstruction is decidable even though type\nchecking is not, when suitably redefined to apply to undecidable type systems;\n(4) a novel use of existential types with dependent types to ensure that the\nlanguage of logical formulae is closed under type checking (5) a prototype\nimplementation, Sage, of executable refinement types such that all dynamic\nerrors are communicated back to the compiler and are thenceforth static errors."
},{
    "category": "cs.PL", 
    "doi": "10.2168/LMCS-10(1:17)2014", 
    "link": "http://arxiv.org/pdf/1403.3996v1", 
    "other_authors": "Vineeth Kashyap, Kyle Dewey, Ethan A. Kuefner, John Wagner, Kevin Gibbons, John Sarracino, Ben Wiedermann, Ben Hardekopf", 
    "title": "JSAI: Designing a Sound, Configurable, and Efficient Static Analyzer for   JavaScript", 
    "arxiv-id": "1403.3996v1", 
    "author": "Ben Hardekopf", 
    "publish": "2014-03-17T04:29:25Z", 
    "summary": "We describe JSAI, an abstract interpreter for JavaScript. JSAI uses novel\nabstract domains to compute a reduced product of type inference, pointer\nanalysis, string analysis, integer and boolean constant propagation, and\ncontrol-flow analysis. In addition, JSAI allows for analysis control-flow\nsensitivity (i.e., context-, path-, and heap-sensitivity) to be modularly\nconfigured without requiring any changes to the analysis implementation. JSAI\nis designed to be provably sound with respect to a specific concrete semantics\nfor JavaScript, which has been extensively tested against existing\nproduction-quality JavaScript implementations.\n  We provide a comprehensive evaluation of JSAI's performance and precision\nusing an extensive benchmark suite. This benchmark suite includes real-world\nJavaScript applications, machine-generated JavaScript code via Emscripten, and\nbrowser addons. We use JSAI's configurability to evaluate a large number of\nanalysis sensitivities (some well-known, some novel) and observe some\nsurprising results. We believe that JSAI's configurability and its formal\nspecifications position it as a useful research platform to experiment on novel\nsensitivities, abstract domains, and client analyses for JavaScript."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2635868.2635912", 
    "link": "http://arxiv.org/pdf/1403.4064v2", 
    "other_authors": "Sumit Gulwani, Ivan Radi\u010dek, Florian Zuleger", 
    "title": "Feedback Generation for Performance Problems in Introductory Programming   Assignments", 
    "arxiv-id": "1403.4064v2", 
    "author": "Florian Zuleger", 
    "publish": "2014-03-17T11:09:59Z", 
    "summary": "Providing feedback on programming assignments manually is a tedious, error\nprone, and time-consuming task. In this paper, we motivate and address the\nproblem of generating feedback on performance aspects in introductory\nprogramming assignments. We studied a large number of functionally correct\nstudent solutions to introductory programming assignments and observed: (1)\nThere are different algorithmic strategies, with varying levels of efficiency,\nfor solving a given problem. These different strategies merit different\nfeedback. (2) The same algorithmic strategy can be implemented in countless\ndifferent ways, which are not relevant for reporting feedback on the student\nprogram.\n  We propose a light-weight programming language extension that allows a\nteacher to define an algorithmic strategy by specifying certain key values that\nshould occur during the execution of an implementation. We describe a dynamic\nanalysis based approach to test whether a student's program matches a teacher's\nspecification. Our experimental results illustrate the effectiveness of both\nour specification language and our dynamic analysis. On one of our benchmarks\nconsisting of 2316 functionally correct implementations to 3 programming\nproblems, we identified 16 strategies that we were able to describe using our\nspecification language (in 95 minutes after inspecting 66, i.e., around 3%,\nimplementations). Our dynamic analysis correctly matched each implementation\nwith its corresponding specification, thereby automatically producing the\nintended feedback."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2931098", 
    "link": "http://arxiv.org/pdf/1403.4910v5", 
    "other_authors": "Vini Kanvar, Uday P. Khedker", 
    "title": "Heap Abstractions for Static Analysis", 
    "arxiv-id": "1403.4910v5", 
    "author": "Uday P. Khedker", 
    "publish": "2014-03-19T18:59:56Z", 
    "summary": "Heap data is potentially unbounded and seemingly arbitrary. As a consequence,\nunlike stack and static memory, heap memory cannot be abstracted directly in\nterms of a fixed set of source variable names appearing in the program being\nanalysed. This makes it an interesting topic of study and there is an abundance\nof literature employing heap abstractions. Although most studies have addressed\nsimilar concerns, their formulations and formalisms often seem dissimilar and\nsome times even unrelated. Thus, the insights gained in one description of heap\nabstraction may not directly carry over to some other description. This survey\nis a result of our quest for a unifying theme in the existing descriptions of\nheap abstractions. In particular, our interest lies in the abstractions and not\nin the algorithms that construct them.\n  In our search of a unified theme, we view a heap abstraction as consisting of\ntwo features: a heap model to represent the heap memory and a summarization\ntechnique for bounding the heap representation. We classify the models as\nstoreless, store based, and hybrid. We describe various summarization\ntechniques based on k-limiting, allocation sites, patterns, variables, other\ngeneric instrumentation predicates, and higher-order logics. This approach\nallows us to compare the insights of a large number of seemingly dissimilar\nheap abstractions and also paves way for creating new abstractions by\nmix-and-match of models and summarization techniques."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2737924.2737980", 
    "link": "http://arxiv.org/pdf/1403.5843v3", 
    "other_authors": "Jedidiah McClurg, Hossein Hojjat, Pavol Cerny, Nate Foster", 
    "title": "Efficient Synthesis of Network Updates", 
    "arxiv-id": "1403.5843v3", 
    "author": "Nate Foster", 
    "publish": "2014-03-24T03:34:49Z", 
    "summary": "Software-defined networking (SDN) is revolutionizing the networking industry,\nbut current SDN programming platforms do not provide automated mechanisms for\nupdating global configurations on the fly. Implementing updates by hand is\nchallenging for SDN programmers because networks are distributed systems with\nhundreds or thousands of interacting nodes. Even if initial and final\nconfigurations are correct, naively updating individual nodes can lead to\nincorrect transient behaviors, including loops, black holes, and access control\nviolations. This paper presents an approach for automatically synthesizing\nupdates that are guaranteed to preserve specified properties. We formalize\nnetwork updates as a distributed programming problem and develop a synthesis\nalgorithm based on counterexample-guided search and incremental model checking.\nWe describe a prototype implementation, and present results from experiments on\nreal-world topologies and properties demonstrating that our tool scales to\nupdates involving over one-thousand nodes."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2737924.2737980", 
    "link": "http://arxiv.org/pdf/1403.5952v1", 
    "other_authors": "Andr\u00e9 Tavares, Benoit Boissinot, Fernando Pereira, Fabrice Rastello", 
    "title": "Parameterized Construction of Program Representations for Sparse   Dataflow Analyses", 
    "arxiv-id": "1403.5952v1", 
    "author": "Fabrice Rastello", 
    "publish": "2014-03-21T18:44:06Z", 
    "summary": "Data-flow analyses usually associate information with control flow regions.\nInformally, if these regions are too small, like a point between two\nconsecutive statements, we call the analysis dense. On the other hand, if these\nregions include many such points, then we call it sparse. This paper presents a\nsystematic method to build program representations that support sparse\nanalyses. To pave the way to this framework we clarify the bibliography about\nwell-known intermediate program representations. We show that our approach, up\nto parameter choice, subsumes many of these representations, such as the SSA,\nSSI and e-SSA forms. In particular, our algorithms are faster, simpler and more\nfrugal than the previous techniques used to construct SSI - Static Single\nInformation - form programs. We produce intermediate representations isomorphic\nto Choi et al.'s Sparse Evaluation Graphs (SEG) for the family of data-flow\nproblems that can be partitioned per variables. However, contrary to SEGs, we\ncan handle - sparsely - problems that are not in this family."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2737924.2737980", 
    "link": "http://arxiv.org/pdf/1403.6997v1", 
    "other_authors": "Martin Li\u0161ka", 
    "title": "Optimizing large applications", 
    "arxiv-id": "1403.6997v1", 
    "author": "Martin Li\u0161ka", 
    "publish": "2014-03-27T12:24:41Z", 
    "summary": "Both uppermost open source compilers, GCC and LLVM, are mature enough to\nlink-time optimize large applications. In case of large applications, we must\ntake into account, except standard speed efficiency and memory consumption,\ndifferent aspects. We focus on size of the code, cold start-up time, etc.\nDevelopers of applications often come up with ad-hoc solutions such as Elfhack\nutility, start-up of an application via a pre-loading utility and dlopen;\nprelinking and variety of different tools that reorder functions to fit the\norder of execution. The goal of the thesis is to analyse all existing\ntechniques of optimization, evaluate their efficiency and design new solutions\nbased on the link-time optimization platform."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.143.1", 
    "link": "http://arxiv.org/pdf/1404.0073v1", 
    "other_authors": "Abeer S. Al-Humaimeedy, Maribel Fern\u00e1ndez", 
    "title": "General dynamic recovery for compensating CSP", 
    "arxiv-id": "1404.0073v1", 
    "author": "Maribel Fern\u00e1ndez", 
    "publish": "2014-04-01T00:36:54Z", 
    "summary": "Compensation is a technique to roll-back a system to a consistent state in\ncase of failure. Recovery mechanisms for compensating calculi specify the order\nof execution of compensation sequences. Dynamic recovery means that the order\nof execution is determined at runtime. In this paper, we define an extension of\nCompensating CSP, called DEcCSP, with general dynamic recovery. We provide a\nformal, operational semantics for the calculus, and illustrate its expressive\npower with a case study. In contrast with previous versions of Compensating\nCSP, DEcCSP provides mechanisms to replace or discard compensations at runtime.\nAdditionally, we bring back to DEcCSP standard CSP operators that are not\navailable in other compensating CSP calculi, and introduce channel\ncommunication."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.0089v1", 
    "other_authors": "Mladen Skelin, Marc Geilen, Francky Catthoor, Sverre Hendseth", 
    "title": "Worst-case Throughput Analysis for Parametric Rate and Parametric Actor   Execution Time Scenario-Aware Dataflow Graphs", 
    "arxiv-id": "1404.0089v1", 
    "author": "Sverre Hendseth", 
    "publish": "2014-04-01T00:40:11Z", 
    "summary": "Scenario-aware dataflow (SADF) is a prominent tool for modeling and analysis\nof dynamic embedded dataflow applications. In SADF the application is\nrepresented as a finite collection of synchronous dataflow (SDF) graphs, each\nof which represents one possible application behaviour or scenario. A finite\nstate machine (FSM) specifies the possible orders of scenario occurrences. The\nSADF model renders the tightest possible performance guarantees, but is limited\nby its finiteness. This means that from a practical point of view, it can only\nhandle dynamic dataflow applications that are characterized by a reasonably\nsized set of possible behaviours or scenarios. In this paper we remove this\nlimitation for a class of SADF graphs by means of SADF model parametrization in\nterms of graph port rates and actor execution times. First, we formally define\nthe semantics of the model relevant for throughput analysis based on (max,+)\nlinear system theory and (max,+) automata. Second, by generalizing some of the\nexisting results, we give the algorithms for worst-case throughput analysis of\nparametric rate and parametric actor execution time acyclic SADF graphs with a\nfully connected, possibly infinite state transition system. Third, we\ndemonstrate our approach on a few realistic applications from digital signal\nprocessing (DSP) domain mapped onto an embedded multi-processor architecture."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.1830v1", 
    "other_authors": "Silviu S. Craciunas, Christoph M. Kirsch, Hannes Payer, Harald R\u00f6ck, Ana Sokolova", 
    "title": "Concurrency and Scalability versus Fragmentation and Compaction with   Compact-fit", 
    "arxiv-id": "1404.1830v1", 
    "author": "Ana Sokolova", 
    "publish": "2014-04-07T16:07:54Z", 
    "summary": "We study, formally and experimentally, the trade-off in temporal and spatial\noverhead when managing contiguous blocks of memory using the explicit, dynamic\nand real-time heap management system Compact-fit (CF). The key property of CF\nis that temporal and spatial overhead can be bounded, related, and predicted in\nconstant time through the notion of partial and incremental compaction. Partial\ncompaction determines the maximally tolerated degree of memory fragmentation.\nIncremental compaction of objects, introduced here, determines the maximal\namount of memory involved in any, logically atomic, portion of a compaction\noperation. We explore CF's potential application space on (1) multiprocessor\nand multicore systems as well as on (2) memory-constrained uniprocessor\nsystems. For (1), we argue that little or no compaction is likely to avoid the\nworst case in temporal as well as spatial overhead but also observe that\nscalability only improves by a constant factor. Scalability can be further\nimproved significantly by reducing overall data sharing through separate\ninstances of Compact-fit. For (2), we observe that incremental compaction can\neffectively trade-off throughput and memory fragmentation for lower latency."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.3407v1", 
    "other_authors": "Ruslan Shevchenko", 
    "title": "Annotated imports", 
    "arxiv-id": "1404.3407v1", 
    "author": "Ruslan Shevchenko", 
    "publish": "2014-04-13T18:20:40Z", 
    "summary": "Presented simple extensions to scala language related to import statements:\nexported imports, which provide ability to reuse sequence of import clauses in\ncomposable form and default rewriters, which provide mechanism for pluggable\nmacro-based AST transformation of overall compilation unit, activated by import\nof library object. Using these facilities not only allows more compact code, it\nprevents application programmer from producing certain type of errors too and\nallows to implement local language extension as libraries on top of standard\ncompiler. Part of discussed extensions is submitted to scala language committee\nas pre-sip \\cite{ai-presip} and can be used as first step for refining imports\nsemantics in the future version of scala language."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.4246v1", 
    "other_authors": "Nataliia Stulova, Jos\u00e9 F. Morales, Manuel V. Hermenegildo", 
    "title": "An Approach to Assertion-based Debugging of Higher-Order (C)LP Programs", 
    "arxiv-id": "1404.4246v1", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2014-04-16T13:53:02Z", 
    "summary": "Higher-order constructs extend the expressiveness of first-order (Constraint)\nLogic Programming ((C)LP) both syntactically and semantically. At the same time\nassertions have been in use for some time in (C)LP systems helping programmers\ndetect errors and validate programs. However, these assertion-based extensions\nto (C)LP have not been integrated well with higher-order to date. This paper\ncontributes to filling this gap by extending the assertion-based approach to\nerror detection and program validation to the higher-order context within\n(C)LP. We propose an extension of properties and assertions as used in (C)LP in\norder to be able to fully describe arguments that are predicates. The extension\nmakes the full power of the assertion language available when describing\nhigher-order arguments. We provide syntax and semantics for (higher-order)\nproperties and assertions, as well as for programs which contain such\nassertions, including the notions of error and partial correctness and provide\nsome formal results. We also discuss several alternatives for performing\nrun-time checking of such programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.4666v1", 
    "other_authors": "Edward Givelberg", 
    "title": "Object-Oriented Parallel Programming", 
    "arxiv-id": "1404.4666v1", 
    "author": "Edward Givelberg", 
    "publish": "2014-04-17T22:36:44Z", 
    "summary": "We introduce an object-oriented framework for parallel programming, which is\nbased on the observation that programming objects can be naturally interpreted\nas processes. A parallel program consists of a collection of persistent\nprocesses that communicate by executing remote methods. We discuss code\nparallelization and process persistence, and explain the main ideas in the\ncontext of computations with very large data objects."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.5770v1", 
    "other_authors": "Sebastian Erdweg, Tijs van der Storm, Yi Dai", 
    "title": "Capture-Avoiding and Hygienic Program Transformations (incl. Proofs)", 
    "arxiv-id": "1404.5770v1", 
    "author": "Yi Dai", 
    "publish": "2014-04-23T10:08:11Z", 
    "summary": "Program transformations in terms of abstract syntax trees compromise\nreferential integrity by introducing variable capture. Variable capture occurs\nwhen in the generated program a variable declaration accidentally shadows the\nintended target of a variable reference. Existing transformation systems either\ndo not guarantee the avoidance of variable capture or impair the implementation\nof transformations.\n  We present an algorithm called name-fix that automatically eliminates\nvariable capture from a generated program by systematically renaming variables.\nname-fix is guided by a graph representation of the binding structure of a\nprogram, and requires name-resolution algorithms for the source language and\nthe target language of a transformation. name-fix is generic and works for\narbitrary transformations in any transformation system that supports origin\ntracking for names. We verify the correctness of name-fix and identify an\ninteresting class of transformations for which name-fix provides hygiene. We\ndemonstrate the applicability of name-fix for implementing capture-avoiding\nsubstitution, inlining, lambda lifting, and compilers for two domain-specific\nlanguages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.6390v2", 
    "other_authors": "Stefan Richthofer", 
    "title": "JyNI - Using native CPython-Extensions in Jython", 
    "arxiv-id": "1404.6390v2", 
    "author": "Stefan Richthofer", 
    "publish": "2014-04-25T10:56:33Z", 
    "summary": "Jython is a Java based Python implementation and the most seamless way to\nintegrate Python and Java. However, it does not support native extensions\nwritten for CPython like NumPy or SciPy. Since most scientific Python code\nfundamentally depends on exactly such native extensions directly or indirectly,\nit usually cannot be run with Jython. JyNI (Jython Native Interface) aims to\nclose this gap. It is a layer that enables Jython users to load native CPython\nextensions and access them from Jython the same way as they would do in\nCPython. In order to leverage the JyNI functionality, you just have to put it\non the Java classpath when Jython is launched. It neither requires you to\nrecompile the extension code, nor to build a customized Jython fork. That\nmeans, it is binary compatible with existing extension builds. At the time of\nwriting, JyNI does not fully implement the Python C-API and it is only capable\nof loading simple examples that only involve most basic built-in types. The\nconcept is rather complete though and our goal is to provide the C-API needed\nto load NumPy as soon as possible. After that we will focus on SciPy and\nothers. We expect that our work will also enable Java developers to use CPython\nextensions like NumPy in their Java code."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1404.6838v1", 
    "other_authors": "Mathieu Acher, Benoit Combemale, Philippe Collet", 
    "title": "Metamorphic Domain-Specific Languages: A Journey Into the Shapes of a   Language", 
    "arxiv-id": "1404.6838v1", 
    "author": "Philippe Collet", 
    "publish": "2014-04-27T22:51:24Z", 
    "summary": "External or internal domain-specific languages (DSLs) or (fluent) APIs?\nWhoever you are -- a developer or a user of a DSL -- you usually have to choose\nyour side; you should not! What about metamorphic DSLs that change their shape\naccording to your needs? We report on our 4-years journey of providing the\n\"right\" support (in the domain of feature modeling), leading us to develop an\nexternal DSL, different shapes of an internal API, and maintain all these\nlanguages. A key insight is that there is no one-size-fits-all solution or no\nclear superiority of a solution compared to another. On the contrary, we found\nthat it does make sense to continue the maintenance of an external and internal\nDSL. The vision that we foresee for the future of software languages is their\nability to be self-adaptable to the most appropriate shape (including the\ncorresponding integrated development environment) according to a particular\nusage or task. We call metamorphic DSL such a language, able to change from one\nshape to another shape."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1405.0166v1", 
    "other_authors": "Pierre de Buyl, Nelle Varoquaux", 
    "title": "Proceedings of the 6th European Conference on Python in Science   (EuroSciPy 2013)", 
    "arxiv-id": "1405.0166v1", 
    "author": "Nelle Varoquaux", 
    "publish": "2014-05-01T14:22:09Z", 
    "summary": "These are the proceedings of the 6th European Conference on Python in\nScience, EuroSciPy 2013, that was held in Brussels (21-25 August 2013)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1405.1302v1", 
    "other_authors": "Brijender kahanwal", 
    "title": "Comparative Study of the Function Overloading and Function Overriding   Using C++", 
    "arxiv-id": "1405.1302v1", 
    "author": "Brijender kahanwal", 
    "publish": "2014-03-21T05:33:03Z", 
    "summary": "In the Object-Oriented Programming Systems (OOPS), these two concepts namely\nfunction overloading and function overriding are a bit confusing to the\nprogrammers. In this article this confusion is tried to be removed. Both of\nthese are the concepts which come under the polymorphism (poly means many and\nmorph mean forms). In the article the comparison is done in between them. For\nthe new programmers and the learners, it is important to understand them. The\nfunction overloading [1] is achieved at the time of the compile and the\nfunction overriding is achieved at the run time. The function overriding always\ntakes place in inheritance, but the function overloading can also take place\nwithout inheritance."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1405.2538v1", 
    "other_authors": "Neng-Fa Zhou", 
    "title": "Combinatorial Search With Picat", 
    "arxiv-id": "1405.2538v1", 
    "author": "Neng-Fa Zhou", 
    "publish": "2014-05-11T14:45:07Z", 
    "summary": "Picat, a new member of the logic programming family, follows a different\ndoctrine than Prolog in offering the core logic programming concepts: arrays\nand maps as built-in data types; implicit pattern matching with explicit\nunification and explicit non-determinism; functions for deterministic\ncomputations; and loops for convenient scripting and modeling purposes. Picat\nprovides facilities for solving combinatorial search problems, including a\ncommon interface with CP, SAT, and MIP solvers, tabling for dynamic\nprogramming, and a module for planning. Picat's planner module, which is\nimplemented by the use of tabling, has produced surprising and encouraging\nresults. Thanks to term-sharing and resource-bounded tabled search, Picat\noverwhelmingly outperforms the cutting-edge ASP and PDDL planners on the\nplanning benchmarks used in recent ASP competitions."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1405.2564v1", 
    "other_authors": "George Souza Oliveira, Anderson Faustino da Silva", 
    "title": "Towards an Efficient Prolog System by Code Introspection", 
    "arxiv-id": "1405.2564v1", 
    "author": "Anderson Faustino da Silva", 
    "publish": "2014-05-11T18:39:26Z", 
    "summary": "To appear in Theory and Practice of Logic Programming (TPLP). Several Prolog\ninterpreters are based on the Warren Abstract Machine (WAM), an elegant model\nto compile Prolog programs. In order to improve the performance several\nstrategies have been proposed, such as: optimize the selection of clauses,\nspecialize the unification, global analysis, native code generation and\ntabling. This paper proposes a different strategy to implement an efficient\nProlog System, the creation of specialized emulators on the fly. The proposed\nstrategy was implemented and evaluated at YAP Prolog System, and the\nexperimental evaluation showed interesting results."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.145.7", 
    "link": "http://arxiv.org/pdf/1405.2693v2", 
    "other_authors": "Sergio Castro, Kim Mens, Paulo Moura", 
    "title": "Customisable Handling of Java References in Prolog Programs", 
    "arxiv-id": "1405.2693v2", 
    "author": "Paulo Moura", 
    "publish": "2014-05-12T10:12:14Z", 
    "summary": "Integration techniques for combining programs written in distinct language\nparadigms facilitate the implementation of specialised modules in the best\nlanguage for their task. In the case of Java-Prolog integration, a known\nproblem is the proper representation of references to Java objects on the\nProlog side. To solve it adequately, multiple dimensions should be considered,\nincluding reference representation, opacity of the representation, identity\npreservation, reference life span, and scope of the inter-language conversion\npolicies. This paper presents an approach that addresses all these dimensions,\ngeneralising and building on existing representation patterns of foreign\nreferences in Prolog, and taking inspiration from similar inter-language\nrepresentation techniques found in other domains. Our approach maximises\nportability by making few assumptions about the Prolog engine interacting with\nJava (e.g., embedded or executed as an external process). We validate our work\nby extending JPC, an open-source integration library, with features supporting\nour approach. Our JPC library is currently compatible with three different open\nsource Prolog engines (SWI, YAP} and XSB) by means of drivers. To appear in\nTheory and Practice of Logic Programming (TPLP)."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.2794v2", 
    "other_authors": "Thepfrastos Mantadelis, Ricardo Rocha, Paulo Moura", 
    "title": "Tabling, Rational Terms, and Coinduction Finally Together!", 
    "arxiv-id": "1405.2794v2", 
    "author": "Paulo Moura", 
    "publish": "2014-05-09T17:07:26Z", 
    "summary": "To appear in Theory and Practice of Logic Programming (TPLP). Tabling is a\ncommonly used technique in logic programming for avoiding cyclic behavior of\nlogic programs and enabling more declarative program definitions. Furthermore,\ntabling often improves computational performance. Rational term are terms with\none or more infinite sub-terms but with a finite representation. Rational terms\ncan be generated in Prolog by omitting the occurs check when unifying two\nterms. Applications of rational terms include definite clause grammars,\nconstraint handling systems, and coinduction. In this paper, we report our\nextension of YAP's Prolog tabling mechanism to support rational terms. We\ndescribe the internal representation of rational terms within the table space\nand prove its correctness. We then use this extension to implement a tabling\nbased approach to coinduction. We compare our approach with current coinductive\ntransformations and describe the implementation. In addition, we present an\nalgorithm that ensures a canonical representation for rational terms."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.2850v2", 
    "other_authors": "Miguel Areias, Ricardo Rocha", 
    "title": "A Simple and Efficient Lock-Free Hash Trie Design for Concurrent Tabling", 
    "arxiv-id": "1405.2850v2", 
    "author": "Ricardo Rocha", 
    "publish": "2014-05-09T19:19:21Z", 
    "summary": "A critical component in the implementation of a concurrent tabling system is\nthe design of the table space. One of the most successful proposals for\nrepresenting tables is based on a two-level trie data structure, where one trie\nlevel stores the tabled subgoal calls and the other stores the computed\nanswers. In this work, we present a simple and efficient lock-free design where\nboth levels of the tries can be shared among threads in a concurrent\nenvironment. To implement lock-freedom we took advantage of the CAS atomic\ninstruction that nowadays can be widely found on many common architectures. CAS\nreduces the granularity of the synchronization when threads access concurrent\nareas, but still suffers from low-level problems such as false sharing or cache\nmemory side-effects. In order to be as effective as possible in the concurrent\nsearch and insert operations over the table space data structures, we based our\ndesign on a hash trie data structure in such a way that it minimizes potential\nlow-level synchronization problems by dispersing as much as possible the\nconcurrent areas. Experimental results in the Yap Prolog system show that our\nnew lock-free hash trie design can effectively reduce the execution time and\nscale better than previous designs."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.3072v2", 
    "other_authors": "Raphael Poss", 
    "title": "Haskell for OCaml programmers", 
    "arxiv-id": "1405.3072v2", 
    "author": "Raphael Poss", 
    "publish": "2014-05-13T09:10:32Z", 
    "summary": "This introduction to Haskell is written to optimize learning by programmers\nwho already know OCaml."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.3099v1", 
    "other_authors": "Joachim Breitner", 
    "title": "The Correctness of Launchbury's Natural Semantics for Lazy Evaluation", 
    "arxiv-id": "1405.3099v1", 
    "author": "Joachim Breitner", 
    "publish": "2014-05-13T10:47:12Z", 
    "summary": "In his seminal paper \"A Natural Semantics for Lazy Evaluation\", John\nLaunchbury proves his semantics correct with respect to a denotational\nsemantics. We machine-checked the proof and found it to fail, and provide two\nways to fix it: One by taking a detour via a modified natural semantics with an\nexplicit stack, and one by adjusting the denotational semantics of heaps."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.3323v1", 
    "other_authors": "Niall Douglas", 
    "title": "Large Code Base Change Ripple Management in C++: My thoughts on how a   new Boost C++ Library could help", 
    "arxiv-id": "1405.3323v1", 
    "author": "Niall Douglas", 
    "publish": "2014-05-13T22:44:11Z", 
    "summary": "C++ 98/03 already has a reputation for overwhelming complexity compared to\nother programming languages. The raft of new features in C++ 11/14 suggests\nthat the complexity in the next generation of C++ code bases will overwhelm\nstill further. The planned C++ 17 will probably worsen matters in ways\ndifficult to presently imagine.\n  Countervailing against this rise in software complexity is the hard\nde-exponentialisation of computer hardware capacity growth expected no later\nthan 2020, and which will have even harder to imagine consequences on all\ncomputer software. WG21 C++ 17 study groups SG2 (Modules), SG7 (Reflection),\nSG8 (Concepts), and to a lesser extent SG10 (Feature Test) and SG12 (Undefined\nBehaviour), are all fundamentally about significantly improving complexity\nmanagement in C++ 17, yet WG21's significant work on improving C++ complexity\nmanagement is rarely mentioned explicitly.\n  This presentation pitches a novel implementation solution for some of these\ncomplexity scaling problems, tying together SG2 and SG7 with parts of SG3\n(Filesystem): a standardised but very lightweight transactional graph database\nbased on Boost.ASIO, Boost.AFIO and Boost.Graph at the very core of the C++\nruntime, making future C++ codebases considerably more tractable and affordable\nto all users of C++."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S147106841400012X", 
    "link": "http://arxiv.org/pdf/1405.3365v1", 
    "other_authors": "Yi Bi, Jia-Huai You, Zhiyong Feng", 
    "title": "A Well-Founded Semantics for FOL-Programs", 
    "arxiv-id": "1405.3365v1", 
    "author": "Zhiyong Feng", 
    "publish": "2014-05-14T05:29:40Z", 
    "summary": "An FOL-program consists of a background theory in a decidable fragment of\nfirst-order logic and a collection of rules possibly containing first-order\nformulas. The formalism stems from recent approaches to tight integrations of\nASP with description logics. In this paper, we define a well-founded semantics\nfor FOL-programs based on a new notion of unfounded sets on consistent as well\nas inconsistent sets of literals, and study some of its properties. The\nsemantics is defined for all FOL-programs, including those where it is\nnecessary to represent inconsistencies explicitly. The semantics supports a\nform of combined reasoning by rules under closed world as well as open world\nassumptions, and it is a generalization of the standard well-founded semantics\nfor normal logic programs. We also show that the well-founded semantics defined\nhere approximates the well-supported answer set semantics for normal DL\nprograms."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000246", 
    "link": "http://arxiv.org/pdf/1405.3393v1", 
    "other_authors": "Gregory J. Duck, Remy Haemmerle, Martin Sulzmann", 
    "title": "On Termination, Confluence and Consistent CHR-based Type Inference", 
    "arxiv-id": "1405.3393v1", 
    "author": "Martin Sulzmann", 
    "publish": "2014-05-14T07:51:39Z", 
    "summary": "We consider the application of Constraint Handling Rules (CHR) for the\nspecification of type inference systems, such as that used by Haskell.\nConfluence of CHR guarantees that the answer provided by type inference is\ncorrect and consistent. The standard method for establishing confluence relies\non an assumption that the CHR program is terminating. However, many examples in\npractice give rise to non-terminating CHR programs, rendering this method\ninapplicable. Despite no guarantee of termination or confluence, the Glasgow\nHaskell Compiler (GHC) supports options that allow the user to proceed with\ntype inference anyway, e.g. via the use of the UndecidableInstances flag. In\nthis paper we formally identify and verify a set of relaxed criteria, namely\nrange-restrictedness, local confluence, and ground termination, that ensure the\nconsistency of CHR-based type inference that maps to potentially\nnon-terminating CHR programs."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000209", 
    "link": "http://arxiv.org/pdf/1405.3547v1", 
    "other_authors": "Terrance Swift", 
    "title": "Incremental Tabling in Support of Knowledge Representation and Reasoning", 
    "arxiv-id": "1405.3547v1", 
    "author": "Terrance Swift", 
    "publish": "2014-05-14T15:49:49Z", 
    "summary": "Resolution-based Knowledge Representation and Reasoning (KRR) systems, such\nas Flora-2, Silk or Ergo, can scale to tens or hundreds of millions of facts,\nwhile supporting reasoning that includes Hilog, inheritance, defeasibility\ntheories, and equality theories. These systems handle the termination and\ncomplexity issues that arise from the use of these features by a heavy use of\ntabled resolution. In fact, such systems table by default all rules defined by\nusers, unless they are simple facts.\n  Performing dynamic updates within such systems is nearly impossible unless\nthe tables themselves can be made to react to changes. Incremental tabling as\nfirst implemented in XSB (Saha 2006) partially addressed this problem, but the\nimplementation was limited in scope and not always easy to use. In this paper,\nwe introduce transparent incremental tabling which at the semantic level\nsupports updates in the 3-valued well-founded semantics, while guaranteeing\nfull consistency of all tabled queries. Transparent incremental tabling also\nhas significant performance improvements over previous implementations,\nincluding lazy recomputation, and control over the dependency structures used\nto determine how tables are updated."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000167", 
    "link": "http://arxiv.org/pdf/1405.3556v1", 
    "other_authors": "Flavio Cruz, Ricardo Rocha, Seth Copen Goldstein, Frank Pfenning", 
    "title": "A Linear Logic Programming Language for Concurrent Programming over   Graph Structures", 
    "arxiv-id": "1405.3556v1", 
    "author": "Frank Pfenning", 
    "publish": "2014-05-14T16:02:41Z", 
    "summary": "We have designed a new logic programming language called LM (Linear Meld) for\nprogramming graph-based algorithms in a declarative fashion. Our language is\nbased on linear logic, an expressive logical system where logical facts can be\nconsumed. Because LM integrates both classical and linear logic, LM tends to be\nmore expressive than other logic programming languages. LM programs are\nnaturally concurrent because facts are partitioned by nodes of a graph data\nstructure. Computation is performed at the node level while communication\nhappens between connected nodes. In this paper, we present the syntax and\noperational semantics of our language and illustrate its use through a number\nof examples."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3675v1", 
    "other_authors": "Marco Comini, Laura Titolo, Alicia Villanueva", 
    "title": "Abstract Diagnosis for tccp using a Linear Temporal Logic", 
    "arxiv-id": "1405.3675v1", 
    "author": "Alicia Villanueva", 
    "publish": "2014-05-14T20:18:48Z", 
    "summary": "Automatic techniques for program verification usually suffer the well-known\nstate explosion problem. Most of the classical approaches are based on browsing\nthe structure of some form of model (which represents the behavior of the\nprogram) to check if a given specification is valid. This implies that a part\nof the model has to be built, and sometimes the needed fragment is quite huge.\n  In this work, we provide an alternative automatic decision method to check\nwhether a given property, specified in a linear temporal logic, is valid w.r.t.\na tccp program. Our proposal (based on abstract interpretation techniques) does\nnot require to build any model at all. Our results guarantee correctness but,\nas usual when using an abstract semantics, completeness is lost."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3694v1", 
    "other_authors": "Martin Gebser, Roland Kaminski, Benjamin Kaufmann, Torsten Schaub", 
    "title": "Clingo = ASP + Control: Preliminary Report", 
    "arxiv-id": "1405.3694v1", 
    "author": "Torsten Schaub", 
    "publish": "2014-05-14T21:33:07Z", 
    "summary": "We present the new ASP system clingo 4. Unlike its predecessors, being mere\nmonolithic combinations of the grounder gringo with the solver clasp, the new\nclingo 4 series offers high-level constructs for realizing complex reasoning\nprocesses. Among others, such processes feature advanced forms of search, as in\noptimization or theory solving, or even interact with an environment, as in\nrobotics or query-answering. Common to them is that the problem specification\nevolves during the reasoning process, either because data or constraints are\nadded, deleted, or replaced. In fact, clingo 4 carries out such complex\nreasoning within a single integrated ASP grounding and solving process. This\navoids redundancies in relaunching grounder and solver programs and benefits\nfrom the solver's learning capacities. clingo 4 accomplishes this by\ncomplementing ASP's declarative input language by control capacities expressed\nvia the embedded scripting languages lua and python. On the declarative side,\nclingo 4 offers a new directive that allows for structuring logic programs into\nnamed and parameterizable subprograms. The grounding and integration of these\nsubprograms into the solving process is completely modular and fully\ncontrollable from the procedural side, viz. the scripting languages. By\nstrictly separating logic and control programs, clingo 4 also abolishes the\nneed for dedicated systems for incremental and reactive reasoning, like iclingo\nand oclingo, respectively, and its flexibility goes well beyond the advanced\nyet still rigid solving processes of the latter."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3791v1", 
    "other_authors": "Iulia Teodora Banu-Demergian, Gheorghe Stefanescu", 
    "title": "On contour representation of two dimensional patterns", 
    "arxiv-id": "1405.3791v1", 
    "author": "Gheorghe Stefanescu", 
    "publish": "2014-05-15T10:27:52Z", 
    "summary": "Two-dimensional patterns are used in many research areas in computer science,\nranging from image processing to specification and verification of complex\nsoftware systems (via scenarios). The contribution of this paper is twofold.\nFirst, we present the basis of a new formal representation of two-dimensional\npatterns based on contours and their compositions. Then, we present efficient\nalgorithms to verify correctness of the contour-representation. Finally, we\nbriefly discuss possible applications, in particular using them as a basic\ninstrument in developing software tools for handling two dimensional words."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3793v1", 
    "other_authors": "Nada Sharaf, Slim Abdennadher, Thom Fruehwirth", 
    "title": "Visualization of Constraint Handling Rules", 
    "arxiv-id": "1405.3793v1", 
    "author": "Thom Fruehwirth", 
    "publish": "2014-05-15T10:42:42Z", 
    "summary": "Constraint Handling Rules (CHR) has matured into a general purpose language\nover the past two decades. Any general purpose language requires its own\ndevelopment tools. Visualization tools, in particular, facilitate many tasks\nfor programmers as well as beginners to the language. The article presents\non-going work towards the visualization of CHR programs. The process is done\nthrough source-to-source transformation. It aims towards reaching a generic\ntransformer to visualize different algorithms implemented in CHR. Note: An\nextended abstract / full version of a paper accepted to be presented at the\nDoctoral Consortium of the 30th International Conference on Logic Programming\n(ICLP 2014), July 19-22, Vienna, Austria."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3795v1", 
    "other_authors": "Grzegorz Ja\u015bkiewicz", 
    "title": "Logic Programming as Scripting Language for Bots in Computer Games --   Research Overview", 
    "arxiv-id": "1405.3795v1", 
    "author": "Grzegorz Ja\u015bkiewicz", 
    "publish": "2014-05-15T10:48:10Z", 
    "summary": "This publication is to present a summary of research (referred as Klabs -\nhttp://www.kappalabs.org) carried out in author's Ph.D studies on topic of\napplication of Logic Programming as scripting language for virtual character\nbehavior control in First Person Shooter (FPS) games. The research goal is to\napply reasoning and knowledge representation techniques to create character\nbehavior, which results in increased players' engagement. An extended abstract\n/ full version of a paper accepted to be presented at the Doctoral Consortium\nof the 30th International Conference on Logic Programming (ICLP 2014), July\n19-22, Vienna, Austria"
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000349", 
    "link": "http://arxiv.org/pdf/1405.3883v1", 
    "other_authors": "John P. Gallagher, Bishoksan Kafle", 
    "title": "Analysis and Transformation Tools for Constrained Horn Clause   Verification", 
    "arxiv-id": "1405.3883v1", 
    "author": "Bishoksan Kafle", 
    "publish": "2014-05-15T15:39:46Z", 
    "summary": "Several techniques and tools have been developed for verification of\nproperties expressed as Horn clauses with constraints over a background theory\n(CHC). Current CHC verification tools implement intricate algorithms and are\noften limited to certain subclasses of CHC problems. Our aim in this work is to\ninvestigate the use of a combination of off-the-shelf techniques from the\nliterature in analysis and transformation of Constraint Logic Programs (CLPs)\nto solve challenging CHC verification problems. We find that many problems can\nbe solved using a combination of tools based on well-known techniques from\nabstract interpretation, semantics-preserving transformations, program\nspecialisation and query-answer transformations. This gives insights into the\ndesign of automatic, more general CHC verification tools based on a library of\ncomponents."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000192", 
    "link": "http://arxiv.org/pdf/1405.3953v1", 
    "other_authors": "Torbj\u00f6rn Lager, Jan Wielemaker", 
    "title": "Pengines: Web Logic Programming Made Easy", 
    "arxiv-id": "1405.3953v1", 
    "author": "Jan Wielemaker", 
    "publish": "2014-05-14T15:22:30Z", 
    "summary": "When developing a (web) interface for a deductive database, functionality\nrequired by the client is provided by means of HTTP handlers that wrap the\nlogical data access predicates. These handlers are responsible for converting\nbetween client and server data representations and typically include options\nfor paginating results. Designing the web accessible API is difficult because\nit is hard to predict the exact requirements of clients. Pengines changes this\npicture. The client provides a Prolog program that selects the required data by\naccessing the logical API of the server. The pengine infrastructure provides\ngeneral mechanisms for converting Prolog data and handling Prolog\nnon-determinism. The Pengines library is small (2000 lines Prolog, 150 lines\nJavaScript). It greatly simplifies defining an AJAX based client for a Prolog\nprogram and provides non-deterministic RPC between Prolog processes as well as\ninteraction with Prolog engines similar to Paul Tarau's engines. Pengines are\navailable as a standard package for SWI-Prolog 7."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000337", 
    "link": "http://arxiv.org/pdf/1405.4041v1", 
    "other_authors": "Ethan K. Jackson", 
    "title": "A Module System for Domain-Specific Languages", 
    "arxiv-id": "1405.4041v1", 
    "author": "Ethan K. Jackson", 
    "publish": "2014-05-16T00:47:10Z", 
    "summary": "Domain-specific languages (DSLs) are routinely created to simplify difficult\nor specialized programming tasks. They expose useful abstractions and design\npatterns in the form of language constructs, provide static semantics to\neagerly detect misuse of these constructs, and dynamic semantics to completely\ndefine how language constructs interact. However, implementing and composing\nDSLs is a non-trivial task, and there is a lack of tools and techniques.\n  We address this problem by presenting a complete module system over LP for\nDSL construction, reuse, and composition. LP is already useful for DSL design,\nbecause it supports executable language specifications using notations familiar\nto language designers. We extend LP with a module system that is simple (with a\nfew concepts), succinct (for key DSL specification scenarios), and composable\n(on the level of languages, compilers, and programs). These design choices\nreflect our use of LP for industrial DSL design. Our module system has been\nimplemented in the FORMULA language, and was used to build key Windows 8 device\ndrivers via DSLs. Though we present our module system as it actually appears in\nour FORMULA language, our emphasis is on concepts adaptable to other LP\nlanguages."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1405.4565v3", 
    "other_authors": "Neville Grech, Kyriakos Georgiou, James Pallister, Steve Kerrison, Jeremy Morse, Kerstin Eder", 
    "title": "Static analysis of energy consumption for LLVM IR programs", 
    "arxiv-id": "1405.4565v3", 
    "author": "Kerstin Eder", 
    "publish": "2014-05-18T23:32:10Z", 
    "summary": "Energy models can be constructed by characterizing the energy consumed by\nexecuting each instruction in a processor's instruction set. This can be used\nto determine how much energy is required to execute a sequence of assembly\ninstructions, without the need to instrument or measure hardware.\n  However, statically analyzing low-level program structures is hard, and the\ngap between the high-level program structure and the low-level energy models\nneeds to be bridged. We have developed techniques for performing a static\nanalysis on the intermediate compiler representations of a program.\nSpecifically, we target LLVM IR, a representation used by modern compilers,\nincluding Clang. Using these techniques we can automatically infer an estimate\nof the energy consumed when running a function under different platforms, using\ndifferent compilers.\n  One of the challenges in doing so is that of determining an energy cost of\nexecuting LLVM IR program segments, for which we have developed two different\napproaches. When this information is used in conjunction with our analysis, we\nare able to infer energy formulae that characterize the energy consumption for\na particular program. This approach can be applied to any languages targeting\nthe LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or\nXMOS xCORE, with a focus towards embedded platforms. Our techniques are\nvalidated on these platforms by comparing the static analysis results to the\nphysical measurements taken from the hardware. Static energy consumption\nestimation enables energy-aware software development, without requiring\nhardware knowledge."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1405.5590v2", 
    "other_authors": "Mukund Raghothaman, Abhishek Udupa", 
    "title": "Language to Specify Syntax-Guided Synthesis Problems", 
    "arxiv-id": "1405.5590v2", 
    "author": "Abhishek Udupa", 
    "publish": "2014-05-22T01:50:47Z", 
    "summary": "We present a language to specify syntax guided synthesis (SyGuS) problems.\nSyntax guidance is a prominent theme in contemporary program synthesis\napproaches, and SyGuS was first described in [1]. This paper describes\nconcretely the input format of a SyGuS solver.\n  [1] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund\nRaghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina\nTorlak, and Abhishek Udupa. Syntax-guided synthesis. In FMCAD, pages 1--17,\n2013."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1405.7898v2", 
    "other_authors": "Nataliia Stulova, Jos\u00e9 F. Morales, Manuel V. Hermenegildo", 
    "title": "Towards Assertion-based Debugging of Higher-Order (C)LP Programs", 
    "arxiv-id": "1405.7898v2", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2014-05-14T08:25:55Z", 
    "summary": "Higher-order constructs extend the expressiveness of first-order (Constraint)\nLogic Programming ((C)LP) both syntactically and semantically. At the same time\nassertions have been in use for some time in (C)LP systems helping programmers\ndetect errors and validate programs. However, these assertion-based extensions\nto (C)LP have not been integrated well with higher order to date. Our work\ncontributes to filling this gap by extending the assertion-based approach to\nerror detection and program validation to the higher-order context, within\n(C)LP. It is based on an extension of properties and assertions as used in\n(C)LP in order to be able to fully describe arguments that are predicates."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1406.0582v1", 
    "other_authors": "Lukasz Domagala, Fabrice Rastello, Sadayappan Ponnuswany, Duco Van Amstel", 
    "title": "A Tiling Perspective for Register Optimization", 
    "arxiv-id": "1406.0582v1", 
    "author": "Duco Van Amstel", 
    "publish": "2014-06-03T06:04:52Z", 
    "summary": "Register allocation is a much studied problem. A particularly important\ncontext for optimizing register allocation is within loops, since a significant\nfraction of the execution time of programs is often inside loop code. A variety\nof algorithms have been proposed in the past for register allocation, but the\ncomplexity of the problem has resulted in a decoupling of several important\naspects, including loop unrolling, register promotion, and instruction\nreordering. In this paper, we develop an approach to register allocation and\npromotion in a unified optimization framework that simultaneously considers the\nimpact of loop unrolling and instruction scheduling. This is done via a novel\ninstruction tiling approach where instructions within a loop are represented\nalong one dimension and innermost loop iterations along the other dimension. By\nexploiting the regularity along the loop dimension, and imposing essential\ndependence based constraints on intra-tile execution order, the problem of\noptimizing register pressure is cast in a constraint programming formalism.\nExperimental results are provided from thousands of innermost loops extracted\nfrom the SPEC benchmarks, demonstrating improvements over the current\nstate-of-the-art."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1406.1393v1", 
    "other_authors": "Paul Tarau, Fahmida Hamid", 
    "title": "Interclausal Logic Variables", 
    "arxiv-id": "1406.1393v1", 
    "author": "Fahmida Hamid", 
    "publish": "2014-06-05T14:09:30Z", 
    "summary": "Unification of logic variables instantly connects present and future\nobservations of their value, independently of their location in the data areas\nof the runtime system. The paper extends this property to \"interclausal logic\nvariables\", an easy to implement Prolog extension that supports instant global\ninformation exchanges without dynamic database updates. We illustrate their\nusefulness with two of algorithms, {\\em graph coloring} and {\\em minimum\nspanning tree}. Implementations of interclausal variables as source-level\ntransformations and as abstract machine adaptations are given. To address the\nneed for globally visible chained transitions of logic variables we describe a\nDCG-based program transformation that extends the functionality of interclausal\nvariables."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2764967.2764974", 
    "link": "http://arxiv.org/pdf/1406.1510v1", 
    "other_authors": "R\u00e9my Haemmerl\u00e9, Jon Sneyers", 
    "title": "Proceedings of the Eleventh Workshop on Constraint Handling Rules", 
    "arxiv-id": "1406.1510v1", 
    "author": "Jon Sneyers", 
    "publish": "2014-06-05T20:11:27Z", 
    "summary": "This volume contains the papers presented at the eleventh Workshop on\nConstraint Handling Rules (CHR 2014), which will be held in Vienna at the\noccasion of the Vienna Summer of Logic (VSL)"
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.153.3", 
    "link": "http://arxiv.org/pdf/1406.2058v1", 
    "other_authors": "Jules Hedges", 
    "title": "Monad Transformers for Backtracking Search", 
    "arxiv-id": "1406.2058v1", 
    "author": "Jules Hedges", 
    "publish": "2014-06-09T03:29:29Z", 
    "summary": "This paper extends Escardo and Oliva's selection monad to the selection monad\ntransformer, a general monadic framework for expressing backtracking search\nalgorithms in Haskell. The use of the closely related continuation monad\ntransformer for similar purposes is also discussed, including an implementation\nof a DPLL-like SAT solver with no explicit recursion. Continuing a line of work\nexploring connections between selection functions and game theory, we use the\nselection monad transformer with the nondeterminism monad to obtain an\nintuitive notion of backward induction for a certain class of nondeterministic\ngames."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.153.7", 
    "link": "http://arxiv.org/pdf/1406.2060v1", 
    "other_authors": "Michael Hicks, Gavin Bierman, Nataliya Guts, Daan Leijen, Nikhil Swamy", 
    "title": "Polymonadic Programming", 
    "arxiv-id": "1406.2060v1", 
    "author": "Nikhil Swamy", 
    "publish": "2014-06-09T03:30:05Z", 
    "summary": "Monads are a popular tool for the working functional programmer to structure\neffectful computations. This paper presents polymonads, a generalization of\nmonads. Polymonads give the familiar monadic bind the more general type forall\na,b. L a -> (a -> M b) -> N b, to compose computations with three different\nkinds of effects, rather than just one. Polymonads subsume monads and\nparameterized monads, and can express other constructions, including precise\ntype-and-effect systems and information flow tracking; more generally,\npolymonads correspond to Tate's productoid semantic model. We show how to equip\na core language (called lambda-PM) with syntactic support for programming with\npolymonads. Type inference and elaboration in lambda-PM allows programmers to\nwrite polymonadic code directly in an ML-like syntax--our algorithms compute\nprincipal types and produce elaborated programs wherein the binds appear\nexplicitly. Furthermore, we prove that the elaboration is coherent: no matter\nwhich (type-correct) binds are chosen, the elaborated program's semantics will\nbe the same. Pleasingly, the inferred types are easy to read: the polymonad\nlaws justify (sometimes dramatic) simplifications, but with no effect on a\ntype's generality."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.153.8", 
    "link": "http://arxiv.org/pdf/1406.2061v1", 
    "other_authors": "Daan Leijen", 
    "title": "Koka: Programming with Row Polymorphic Effect Types", 
    "arxiv-id": "1406.2061v1", 
    "author": "Daan Leijen", 
    "publish": "2014-06-09T03:30:31Z", 
    "summary": "We propose a programming model where effects are treated in a disciplined\nway, and where the potential side-effects of a function are apparent in its\ntype signature. The type and effect of expressions can also be inferred\nautomatically, and we describe a polymorphic type inference system based on\nHindley-Milner style inference. A novel feature is that we support polymorphic\neffects through row-polymorphism using duplicate labels. Moreover, we show that\nour effects are not just syntactic labels but have a deep semantic connection\nto the program. For example, if an expression can be typed without an exn\neffect, then it will never throw an unhandled exception. Similar to Haskell's\n`runST` we show how we can safely encapsulate stateful operations. Through the\nstate effect, we can also safely combine state with let-polymorphism without\nneeding either imperative type variables or a syntactic value restriction.\nFinally, our system is implemented fully in a new language called Koka and has\nbeen used successfully on various small to medium-sized sample programs ranging\nfrom a Markdown processor to a tier-splitted chat application. You can try out\nKoka live at www.rise4fun.com/koka/tutorial."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.153.8", 
    "link": "http://arxiv.org/pdf/1406.2121v1", 
    "other_authors": "Edmund S. L. Lam, Iliano Cervesato", 
    "title": "Constraint Handling Rules with Multiset Comprehension Patterns", 
    "arxiv-id": "1406.2121v1", 
    "author": "Iliano Cervesato", 
    "publish": "2014-06-09T10:03:26Z", 
    "summary": "CHR is a declarative, concurrent and committed choice rule-based constraint\nprogramming language. We extend CHR with multiset comprehension patterns,\nproviding the programmer with the ability to write multiset rewriting rules\nthat can match a variable number of constraints in the store. This enables\nwriting more readable, concise and declarative code for algorithms that\ncoordinate large amounts of data or require aggregate operations. We call this\nextension $\\mathit{CHR}^\\mathit{cp}$. We give a high-level abstract semantics\nof $\\mathit{CHR}^\\mathit{cp}$, followed by a lower-level operational semantics.\nWe then show the soundness of this operational semantics with respect to the\nabstract semantics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.153.8", 
    "link": "http://arxiv.org/pdf/1406.2370v1", 
    "other_authors": "Beniamino Accattoli, Pablo Barenbaum, Damiano Mazza", 
    "title": "Distilling Abstract Machines (Long Version)", 
    "arxiv-id": "1406.2370v1", 
    "author": "Damiano Mazza", 
    "publish": "2014-06-09T21:44:34Z", 
    "summary": "It is well-known that many environment-based abstract machines can be seen as\nstrategies in lambda calculi with explicit substitutions (ES). Recently,\ngraphical syntaxes and linear logic led to the linear substitution calculus\n(LSC), a new approach to ES that is halfway between big-step calculi and\ntraditional calculi with ES. This paper studies the relationship between the\nLSC and environment-based abstract machines. While traditional calculi with ES\nsimulate abstract machines, the LSC rather distills them: some transitions are\nsimulated while others vanish, as they map to a notion of structural\ncongruence. The distillation process unveils that abstract machines in fact\nimplement weak linear head reduction, a notion of evaluation having a central\nrole in the theory of linear logic. We show that such a pattern applies\nuniformly in call-by-name, call-by-value, and call-by-need, catching many\nmachines in the literature. We start by distilling the KAM, the CEK, and the\nZINC, and then provide simplified versions of the SECD, the lazy KAM, and\nSestoft's machine. Along the way we also introduce some new machines with\nglobal environments. Moreover, we show that distillation preserves the time\ncomplexity of the executions, i.e. the LSC is a complexity-preserving\nabstraction of abstract machines."
},{
    "category": "cs.PL", 
    "doi": "10.5121/ijpla.2014.4201", 
    "link": "http://arxiv.org/pdf/1406.4087v1", 
    "other_authors": "Artem Melentyev", 
    "title": "Java Modular Extension for Operator Overloading", 
    "arxiv-id": "1406.4087v1", 
    "author": "Artem Melentyev", 
    "publish": "2014-06-16T18:04:54Z", 
    "summary": "The paper introduces a modular extension (plugin) for Java language compilers\nand Integrated Development Environments (IDE) which adds operator overloading\nfeature to Java language while preserving backward compatibility.\n  The extension use the idea of library-based language extensibility similar to\nSugarJ. But unlike most language extensions, it works directly inside the\ncompiler and does not have any external preprocessors. This gives much faster\ncompilation, better language compatibility and support of native developer\ntools (IDE, build tools).\n  The extension plugs into javac and Eclipse Java compilers as well as in all\ntools whose use the compilers such as IDEs (Netbeans, Eclipse, IntelliJ IDEA),\nbuild tools (ant, maven, gradle), etc. No compiler, IDE, build tools\nmodification needed. Just add a jar library to classpath and/or install a\nplugin to your IDE.\n  The paper also discuss on how to build such Java compiler extensions.\n  The extension source code is open on http://amelentev.github.io/java-oo/"
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.5106v1", 
    "other_authors": "J. Ian Johnson, Ilya Sergey, Christopher Earl, Matthew Might, David Van Horn", 
    "title": "Pushdown flow analysis with abstract garbage collection", 
    "arxiv-id": "1406.5106v1", 
    "author": "David Van Horn", 
    "publish": "2014-06-19T16:51:12Z", 
    "summary": "In the static analysis of functional programs, pushdown flow analysis and\nabstract garbage collection push the boundaries of what we can learn about\nprograms statically. This work illuminates and poses solutions to theoretical\nand practical challenges that stand in the way of combining the power of these\ntechniques. Pushdown flow analysis grants unbounded yet computable polyvariance\nto the analysis of return-flow in higher-order programs. Abstract garbage\ncollection grants unbounded polyvariance to abstract addresses which become\nunreachable between invocations of the abstract contexts in which they were\ncreated. Pushdown analysis solves the problem of precisely analyzing recursion\nin higher-order languages; abstract garbage collection is essential in solving\nthe \"stickiness\" problem. Alone, our benchmarks demonstrate that each method\ncan reduce analysis times and boost precision by orders of magnitude. We\ncombine these methods. The challenge in marrying these techniques is not\nsubtle: computing the reachable control states of a pushdown system relies on\nlimiting access during transition to the top of the stack; abstract garbage\ncollection, on the other hand, needs full access to the entire stack to compute\na root set, just as concrete collection does. Conditional pushdown systems were\ndeveloped for just such a conundrum, but existing methods are ill-suited for\nthe dynamic nature of garbage collection. We show fully precise and approximate\nsolutions to the feasible paths problem for pushdown garbage-collecting\ncontrol-flow analysis. Experiments reveal synergistic interplay between garbage\ncollection and pushdown techniques, and the fusion demonstrates\n\"better-than-both-worlds\" precision."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.5457v1", 
    "other_authors": "Thomas M. Gawlitza, Martin D. Schwarz, Helmut Seidl", 
    "title": "Parametric Strategy Iteration", 
    "arxiv-id": "1406.5457v1", 
    "author": "Helmut Seidl", 
    "publish": "2014-06-19T19:35:27Z", 
    "summary": "Program behavior may depend on parameters, which are either configured before\ncompilation time, or provided at run-time, e.g., by sensors or other input\ndevices. Parametric program analysis explores how different parameter settings\nmay affect the program behavior.\n  In order to infer invariants depending on parameters, we introduce parametric\nstrategy iteration. This algorithm determines the precise least solution of\nsystems of integer equations depending on surplus parameters. Conceptually, our\nalgorithm performs ordinary strategy iteration on the given integer system for\nall possible parameter settings in parallel. This is made possible by means of\nregion trees to represent the occurring piecewise affine functions. We indicate\nthat each required operation on these trees is polynomial-time if only\nconstantly many parameters are involved.\n  Parametric strategy iteration for systems of integer equations allows to\nconstruct parametric integer interval analysis as well as parametric analysis\nof differences of integer variables. It thus provides a general technique to\nrealize precise parametric program analysis if numerical properties of integer\nvariables are of concern."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.5715v2", 
    "other_authors": "Alexander Kaiser, Daniel Kroening, Thomas Wahl", 
    "title": "Lost in Abstraction: Monotonicity in Multi-Threaded Programs (Extended   Technical Report)", 
    "arxiv-id": "1406.5715v2", 
    "author": "Thomas Wahl", 
    "publish": "2014-06-22T12:34:17Z", 
    "summary": "Monotonicity in concurrent systems stipulates that, in any global state,\nextant system actions remain executable when new processes are added to the\nstate. This concept is not only natural and common in multi-threaded software,\nbut also useful: if every thread's memory is finite, monotonicity often\nguarantees the decidability of safety property verification even when the\nnumber of running threads is unknown. In this paper, we show that the act of\nobtaining finite-data thread abstractions for model checking can be at odds\nwith monotonicity: Predicate-abstracting certain widely used monotone software\nresults in non-monotone multi-threaded Boolean programs - the monotonicity is\nlost in the abstraction. As a result, well-established sound and complete\nsafety checking algorithms become inapplicable; in fact, safety checking turns\nout to be undecidable for the obtained class of unbounded-thread Boolean\nprograms. We demonstrate how the abstract programs can be modified into\nmonotone ones, without affecting safety properties of the non-monotone\nabstraction. This significantly improves earlier approaches of enforcing\nmonotonicity via overapproximations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.6163v1", 
    "other_authors": "Felix P. Hargreaves, Daniel Merkle, Peter Schneider-Kamp", 
    "title": "Group Communication Patterns for High Performance Computing in Scala", 
    "arxiv-id": "1406.6163v1", 
    "author": "Peter Schneider-Kamp", 
    "publish": "2014-06-24T08:04:36Z", 
    "summary": "We developed a Functional object-oriented Parallel framework (FooPar) for\nhigh-level high-performance computing in Scala. Central to this framework are\nDistributed Memory Parallel Data structures (DPDs), i.e., collections of data\ndistributed in a shared nothing system together with parallel operations on\nthese data. In this paper, we first present FooPar's architecture and the idea\nof DPDs and group communications. Then, we show how DPDs can be implemented\nelegantly and efficiently in Scala based on the Traversable/Builder pattern,\nunifying Functional and Object-Oriented Programming. We prove the correctness\nand safety of one communication algorithm and show how specification testing\n(via ScalaCheck) can be used to bridge the gap between proof and\nimplementation. Furthermore, we show that the group communication operations of\nFooPar outperform those of the MPJ Express open source MPI-bindings for Java,\nboth asymptotically and empirically. FooPar has already been shown to be\ncapable of achieving close-to-optimal performance for dense matrix-matrix\nmultiplication via JNI. In this article, we present results on a parallel\nimplementation of the Floyd-Warshall algorithm in FooPar, achieving more than\n94 % efficiency compared to the serial version on a cluster using 100 cores for\nmatrices of dimension 38000 x 38000."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.6631v2", 
    "other_authors": "Aggelos Biboudis, Nick Palladinos, Yannis Smaragdakis", 
    "title": "Clash of the Lambdas", 
    "arxiv-id": "1406.6631v2", 
    "author": "Yannis Smaragdakis", 
    "publish": "2014-06-25T16:33:11Z", 
    "summary": "The introduction of lambdas in Java 8 completes the slate of\nstatically-typed, mainstream languages with both object-oriented and functional\nfeatures. The main motivation for lambdas in Java has been to facilitate\nstream-based declarative APIs, and, therefore, easier parallelism. In this\npaper, we evaluate the performance impact of lambda abstraction employed in\nstream processing, for a variety of high-level languages that run on a virtual\nmachine (C#, F#, Java and Scala) and runtime platforms (JVM on Linux and\nWindows, .NET CLR for Windows, Mono for Linux). Furthermore, we evaluate the\nperformance gain that two optimizing libraries (ScalaBlitz and LinqOptimizer)\ncan offer for C#, F# and Scala. Our study is based on small-scale\nthroughput-benchmarking, with significant care to isolate different factors,\nconsult experts on the systems involved, and identify causes and opportunities.\nWe find that Java exhibits high implementation maturity, which is a dominant\nfactor in benchmarks. At the same time, optimizing frameworks can be highly\neffective for common query patterns."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796814000100", 
    "link": "http://arxiv.org/pdf/1406.7497v1", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "Domain Theory for Modeling OOP: A Summary", 
    "arxiv-id": "1406.7497v1", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2014-06-29T12:23:25Z", 
    "summary": "Domain theory is `a mathematical theory that serves as a foundation for the\nsemantics of programming languages'. Domains form the basis of a theory of\npartial information, which extends the familiar notion of partial function to\nencompass a whole spectrum of \"degrees of definedness\", so as to model\nincremental higher-order computation (i.e., computing with infinite data\nvalues, such as functions defined over an infinite domain like the domain of\nintegers, infinite trees, and such as objects of object-oriented programming).\nGeneral considerations from recursion theory dictate that partial functions are\nunavoidable in any discussion of computability. Domain theory provides an\nappropriately abstract setting in which the notion of a partial function can be\nlifted and used to give meaning to higher types, recursive types, etc. NOOP is\na domain-theoretic model of nominally-typed OOP. NOOP was used to prove the\nidentification of inheritance and subtyping in mainstream nominally-typed OO\nprogramming languages and the validity of this identification. In this report\nwe first present the definitions of basic domain theoretic notions and domain\nconstructors used in the construction of NOOP, then we present the construction\nof a simple structural model of OOP called COOP as a step towards the\nconstruction of NOOP. Like the construction of NOOP, the construction of COOP\nuses earlier presented domain constructors."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.160.8", 
    "link": "http://arxiv.org/pdf/1408.1458v1", 
    "other_authors": "Bartek Klin, Beata Nachy\u0142a", 
    "title": "Distributive Laws and Decidable Properties of SOS Specifications", 
    "arxiv-id": "1408.1458v1", 
    "author": "Beata Nachy\u0142a", 
    "publish": "2014-08-07T01:57:17Z", 
    "summary": "Some formats of well-behaved operational specifications, correspond to\nnatural transformations of certain types (for example, GSOS and coGSOS laws).\nThese transformations have a common generalization: distributive laws of monads\nover comonads. We prove that this elegant theoretical generalization has\nlimited practical benefits: it does not translate to any concrete rule format\nthat would be complete for specifications that contain both GSOS and coGSOS\nrules. This is shown for the case of labeled transition systems and\ndeterministic stream systems."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.160.8", 
    "link": "http://arxiv.org/pdf/1408.1754v1", 
    "other_authors": "Graeme Gange, Jorge A. Navas, Peter Schachte, Harald Sondergaard, Peter J. Stuckey", 
    "title": "A Partial-Order Approach to Array Content Analysis", 
    "arxiv-id": "1408.1754v1", 
    "author": "Peter J. Stuckey", 
    "publish": "2014-08-08T03:11:41Z", 
    "summary": "We present a parametric abstract domain for array content analysis. The\nmethod maintains invariants for contiguous regions of the array, similar to the\nmethods of Gopan, Reps and Sagiv, and of Halbwachs and Peron. However, it\nintroduces a novel concept of an array content graph, avoiding the need for an\nup-front factorial partitioning step. The resulting analysis can be used with\narbitrary numeric relational abstract domains; we evaluate the domain on a\nrange of array manipulating program fragments."
},{
    "category": "cs.PL", 
    "doi": "10.14445/22312803/IJCTT-V13P136", 
    "link": "http://arxiv.org/pdf/1408.2564v1", 
    "other_authors": "Ally S. Nyamawe", 
    "title": "A Proposed Framework for Development of a Visualizer Based on Memory   Transfer Language (MTL)", 
    "arxiv-id": "1408.2564v1", 
    "author": "Ally S. Nyamawe", 
    "publish": "2014-08-11T21:35:00Z", 
    "summary": "Computer programming is among the fundamental aspects of computer science\ncurriculum. Many students first introduced to introductory computer programming\ncourses experience difficulties in learning and comprehending. Vast amount of\nresearches have revealed that, generally programming courses are regarded as\ndifficult and challenging and thus often have the highest dropout rates.\nMoreover, numerous researches have devoted in delivering new approaches and\ntools in enhancing the process of teaching and learning computer programming to\nnovice programmers. One among the tools that have emerged to offer positive\nresults is Program Visualization tool (Visualizer). Visualizers have shown\nremarkable contributions in facilitating novices to learn and comprehend\ncomputer programming. In addition to that, an approach to visualize codes\nexecution, Memory Transfer Language (MTL), allows a novice to animate the code\nthrough paper and pencil mechanism without actively involving the machine. MTL\ndepends on the concepts of RAM (Random Access Memory) to interpret the code\nline by line. Programming requires effort and special approach in the way it is\nlearned and taught, thus this paper aimed at presenting a proposed framework\nfor developing a visualizer that employs the use of MTL to enhance teaching and\nlearning programming."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.162.4", 
    "link": "http://arxiv.org/pdf/1408.5980v1", 
    "other_authors": "Ornela Dardha", 
    "title": "Recursive Session Types Revisited", 
    "arxiv-id": "1408.5980v1", 
    "author": "Ornela Dardha", 
    "publish": "2014-08-26T02:15:51Z", 
    "summary": "Session types model structured communication-based programming. In\nparticular, binary session types for the pi-calculus describe communication\nbetween exactly two participants in a distributed scenario. Adding sessions to\nthe pi-calculus means augmenting it with type and term constructs. In a\nprevious paper, we tried to understand to which extent the session constructs\nare more complex and expressive than the standard pi-calculus constructs. Thus,\nwe presented an encoding of binary session pi-calculus to the standard typed\npi-calculus by adopting linear and variant types and the continuation-passing\nprinciple. In the present paper, we focus on recursive session types and we\npresent an encoding into recursive linear pi-calculus. This encoding is a\nconservative extension of the former in that it preserves the results therein\nobtained. Most importantly, it adopts a new treatment of the duality relation,\nwhich in the presence of recursive types has been proven to be quite\nchallenging."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S0956796816000058", 
    "link": "http://arxiv.org/pdf/1409.0393v2", 
    "other_authors": "Catalin Hritcu, Leonidas Lampropoulos, Antal Spector-Zabusky, Arthur Azevedo de Amorim, Maxime D\u00e9n\u00e8s, John Hughes, Benjamin C. Pierce, Dimitrios Vytiniotis", 
    "title": "Testing Noninterference, Quickly", 
    "arxiv-id": "1409.0393v2", 
    "author": "Dimitrios Vytiniotis", 
    "publish": "2014-09-01T12:53:17Z", 
    "summary": "Information-flow control mechanisms are difficult both to design and to prove\ncorrect. To reduce the time wasted on doomed proof attempts due to broken\ndefinitions, we advocate modern random testing techniques for finding\ncounterexamples during the design process. We show how to use QuickCheck, a\nproperty-based random-testing tool, to guide the design of increasingly complex\ninformation-flow abstract machines, leading up to a sophisticated register\nmachine with a novel and highly permissive flow-sensitive dynamic enforcement\nmechanism that is sound in the presence of first-class public labels. We find\nthat both sophisticated strategies for generating well-distributed random\nprograms and readily falsifiable formulations of noninterference properties are\ncritically important for efficient testing. We propose several approaches and\nevaluate their effectiveness on a collection of injected bugs of varying\nsubtlety. We also present an effective technique for shrinking large\ncounterexamples to minimal, easily comprehensible ones. Taken together, our\nbest methods enable us to quickly and automatically generate simple\ncounterexamples for more than 45 bugs. Moreover, we show how testing guides the\ndiscovery of the sophisticated invariants needed for the noninterference proof\nof our most complex machine."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2015.03.001", 
    "link": "http://arxiv.org/pdf/1409.0757v3", 
    "other_authors": "Edd Barrett, Carl Friedrich Bolz, Laurence Tratt", 
    "title": "Approaches to Interpreter Composition", 
    "arxiv-id": "1409.0757v3", 
    "author": "Laurence Tratt", 
    "publish": "2014-09-02T15:29:23Z", 
    "summary": "In this paper, we compose six different Python and Prolog VMs into 4 pairwise\ncompositions: one using C interpreters; one running on the JVM; one using\nmeta-tracing interpreters; and one using a C interpreter and a meta-tracing\ninterpreter. We show that programs that cross the language barrier frequently\nexecute faster in a meta-tracing composition, and that meta-tracing imposes a\nsignificantly lower overhead on composed programs relative to mono-language\nprograms."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2015.03.001", 
    "link": "http://arxiv.org/pdf/1409.2089v1", 
    "other_authors": "Michael Kruse", 
    "title": "Perfrewrite -- Program Complexity Analysis via Source Code   Instrumentation", 
    "arxiv-id": "1409.2089v1", 
    "author": "Michael Kruse", 
    "publish": "2014-09-07T06:42:09Z", 
    "summary": "Most program profiling methods output the execution time of one specific\nprogram execution, but not its computational complexity class in terms of the\nbig-O notation. Perfrewrite is a tool based on LLVM's Clang compiler to rewrite\na program such that it tracks semantic information while the program executes\nand uses it to guess memory usage, communication and computational complexity.\nWhile source code instrumentation is a standard technique for profiling, using\nit for deriving formulas is an uncommon approach."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2015.03.001", 
    "link": "http://arxiv.org/pdf/1409.2764v1", 
    "other_authors": "Takeshi Tsukada, C. -H. Luke Ong", 
    "title": "Innocent Strategies are Sheaves over Plays---Deterministic,   Non-deterministic and Probabilistic Innocence", 
    "arxiv-id": "1409.2764v1", 
    "author": "C. -H. Luke Ong", 
    "publish": "2014-09-09T14:56:12Z", 
    "summary": "Although the HO/N games are fully abstract for PCF, the traditional notion of\ninnocence (which underpins these games) is not satisfactory for such language\nfeatures as non-determinism and probabilistic branching, in that there are\nstateless terms that are not innocent. Based on a category of P-visible plays\nwith a notion of embedding as morphisms, we propose a natural generalisation by\nviewing innocent strategies as sheaves over (a site of) plays, echoing a slogan\nof Hirschowitz and Pous. Our approach gives rise to fully complete game models\nin each of the three cases of deterministic, nondeterministic and probabilistic\nbranching. To our knowledge, in the second and third cases, ours are the first\nsuch factorisation-free constructions."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1409.3108v1", 
    "other_authors": "Shuying Liang, Weibin Sun, Matthew Might, Andy Keep, David Van Horn", 
    "title": "Pruning, Pushdown Exception-Flow Analysis", 
    "arxiv-id": "1409.3108v1", 
    "author": "David Van Horn", 
    "publish": "2014-09-10T15:10:35Z", 
    "summary": "Statically reasoning in the presence of exceptions and about the effects of\nexceptions is challenging: exception-flows are mutually determined by\ntraditional control-flow and points-to analyses. We tackle the challenge of\nanalyzing exception-flows from two angles. First, from the angle of pruning\ncontrol-flows (both normal and exceptional), we derive a pushdown framework for\nan object-oriented language with full-featured exceptions. Unlike traditional\nanalyses, it allows precise matching of throwers to catchers. Second, from the\nangle of pruning points-to information, we generalize abstract garbage\ncollection to object-oriented programs and enhance it with liveness analysis.\nWe then seamlessly weave the techniques into enhanced reachability computation,\nyielding highly precise exception-flow analysis, without becoming intractable,\neven for large applications. We evaluate our pruned, pushdown exception-flow\nanalysis, comparing it with an established analysis on large scale standard\nJava benchmarks. The results show that our analysis significantly improves\nanalysis precision over traditional analysis within a reasonable analysis time."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1409.3184v1", 
    "other_authors": "Rachid Rebiha, Arnaldo Vieira Moura, Nadir Matringe", 
    "title": "Characterization of Termination for Linear Loop Programs", 
    "arxiv-id": "1409.3184v1", 
    "author": "Nadir Matringe", 
    "publish": "2014-09-10T18:39:44Z", 
    "summary": "We present necessary and sufficient conditions for the termination of linear\nhomogeneous programs. We also develop a complete method to check termination\nfor this class of programs. Our complete characterization of termination for\nsuch programs is based on linear algebraic methods. We reduce the verification\nof the termination problem to checking the orthogonality of a well determined\nvector space and a certain vector, both related to loops in the program.\nMoreover, we provide theoretical results and symbolic computational methods\nguaranteeing the soundness, completeness and numerical stability of the\napproach. Finally, we show that it is enough to interpret variable values over\na specific countable number field, or even over its ring of integers, when one\nwants to check termination over the reals."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1409.7514v3", 
    "other_authors": "Georgiana Caltais, Bertrand Meyer", 
    "title": "Coffman deadlocks in SCOOP", 
    "arxiv-id": "1409.7514v3", 
    "author": "Bertrand Meyer", 
    "publish": "2014-09-26T09:29:09Z", 
    "summary": "In this paper we address the deadlock detection problem in the context of\nSCOOP - an OO-programming model for concurrency, recently formalized in Maude.\nWe present the integration of a deadlock detection mechanism on top of the\naforementioned formalization and analyze how an abstract semantics of SCOOP\nbased on a notion of \"may alias expressions\" can contribute to improving the\ndeadlock detection procedure."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1409.7628v1", 
    "other_authors": "Roberto Casta\u00f1eda Lozano, Christian Schulte", 
    "title": "Survey on Combinatorial Register Allocation and Instruction Scheduling", 
    "arxiv-id": "1409.7628v1", 
    "author": "Christian Schulte", 
    "publish": "2014-09-26T16:32:07Z", 
    "summary": "Register allocation and instruction scheduling are two central compiler\nback-end problems that are critical for quality. In the last two decades,\ncombinatorial optimization has emerged as an alternative approach to\ntraditional, heuristic algorithms for these problems. Combinatorial approaches\nare generally slower but more flexible than their heuristic counterparts and\nhave the potential to generate optimal code. This paper surveys existing\nliterature on combinatorial register allocation and instruction scheduling. The\nsurvey covers approaches that solve each problem in isolation as well as\napproaches that integrate both problems. The latter have the potential to\ngenerate code that is globally optimal by capturing the trade-off between\nconflicting register allocation and instruction scheduling decisions."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1409.7841v1", 
    "other_authors": "Nadezhda Baklanova, Wilmer Ricciotti, Jan-Georg Smaus, Martin Strecker", 
    "title": "Abstracting an operational semantics to finite automata", 
    "arxiv-id": "1409.7841v1", 
    "author": "Martin Strecker", 
    "publish": "2014-09-27T21:05:38Z", 
    "summary": "There is an apparent similarity between the descriptions of small-step\noperational semantics of imperative programs and the semantics of finite\nautomata, so defining an abstraction mapping from semantics to automata and\nproving a simulation property seems to be easy. This paper aims at identifying\nthe reasons why simple proofs break, among them artifacts in the semantics that\nlead to stuttering steps in the simulation. We then present a semantics based\non the zipper data structure, with a direct interpretation of evaluation as\nnavigation in the syntax tree. The abstraction function is then defined by\nequivalence class construction."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.0198v3", 
    "other_authors": "Eva Darulova, Viktor Kuncak", 
    "title": "Towards a Compiler for Reals", 
    "arxiv-id": "1410.0198v3", 
    "author": "Viktor Kuncak", 
    "publish": "2014-10-01T12:17:40Z", 
    "summary": "Numerical software, common in scientific computing or embedded systems,\ninevitably uses an approximation of the real arithmetic in which most\nalgorithms are designed. In many domains, roundoff errors are not the only\nsource of inaccuracy and measurement and other input errors further increase\nthe uncertainty of the computed results. Adequate tools are needed to help\nusers select suitable approximations, especially for safety-critical\napplications.\n  We present the source-to-source compiler Rosa which takes as input a\nreal-valued program with error specifications and synthesizes code over an\nappropriate floating-point or fixed-point data type. The main challenge of such\na compiler is a fully automated, sound and yet accurate enough numerical error\nestimation. We present a unified technique for floating-point and fixed-point\narithmetic of various precisions which can handle nonlinear arithmetic,\ndetermine closed- form symbolic invariants for unbounded loops and quantify the\neffects of discontinuities on numerical errors. We evaluate Rosa on a number of\nbenchmarks from scientific computing and embedded systems and, comparing it to\nstate-of-the-art in automated error estimation, show it presents an interesting\ntrade-off between accuracy and performance."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.2813v1", 
    "other_authors": "Michael Greenberg", 
    "title": "Space-Efficient Manifest Contracts", 
    "arxiv-id": "1410.2813v1", 
    "author": "Michael Greenberg", 
    "publish": "2014-10-10T15:43:34Z", 
    "summary": "The standard algorithm for higher-order contract checking can lead to\nunbounded space consumption and can destroy tail recursion, altering a\nprogram's asymptotic space complexity. While space efficiency for gradual\ntypes---contracts mediating untyped and typed code---is well studied, sound\nspace efficiency for manifest contracts---contracts that check stronger\nproperties than simple types, e.g., \"is a natural\" instead of \"is an\ninteger\"---remains an open problem.\n  We show how to achieve sound space efficiency for manifest contracts with\nstrong predicate contracts. The essential trick is breaking the contract\nchecking down into coercions: structured, blame-annotated lists of checks. By\ncarefully preventing duplicate coercions from appearing, we can restore space\nefficiency while keeping the same observable behavior.\n  Along the way, we define a framework for space efficiency, traversing the\ndesign space with three different space-efficient manifest calculi. We examine\nthe diverse correctness criteria for contract semantics; we conclude with a\ncoercion-based language whose contracts enjoy (galactically) bounded, sound\nspace consumption---they are observationally equivalent to the standard,\nspace-inefficient semantics."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.3772v1", 
    "other_authors": "Rishabh Jain, Sakshi Gupta", 
    "title": "Optimizing the For loop: Comparison of For loop and micro For loop", 
    "arxiv-id": "1410.3772v1", 
    "author": "Sakshi Gupta", 
    "publish": "2014-10-12T15:16:55Z", 
    "summary": "Looping is one of the fundamental logical instructions used for repeating a\nblock of code. It is used in programs across all programming languages.\nTraditionally, in languages like C, the for loop is used extensively for\nrepeated execution of a block of code, due to its ease for use and simplified\nrepresentation. This paper proposes a new way of representing the for loop to\nimprove its runtime efficiency and compares the experimental statistics with\nthe traditional for loop representation. It is found that for small number of\niterations, the difference in computational time may not be considerable. But\ngiven any large number of iterations, the difference is noticeable."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.4956v2", 
    "other_authors": "David Insa, Josep Silva", 
    "title": "Transforming while/do/for/foreach-Loops into Recursive Methods", 
    "arxiv-id": "1410.4956v2", 
    "author": "Josep Silva", 
    "publish": "2014-10-18T13:43:53Z", 
    "summary": "In software engineering, taking a good election between recursion and\niteration is essential because their efficiency and maintenance are different.\nIn fact, developers often need to transform iteration into recursion (e.g., in\ndebugging, to decompose the call graph into iterations); thus, it is quite\nsurprising that there does not exist a public transformation from loops to\nrecursion that handles all kinds of loops. This article describes a\ntransformation able to transform iterative loops into equivalent recursive\nmethods. The transformation is described for the programming language Java, but\nit is general enough as to be adapted to many other languages that allow\niteration and recursion. We describe the changes needed to transform loops of\ntypes while/do/for/foreach into recursion. Each kind of loop requires a\nparticular treatment that is described and exemplified."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.5370v2", 
    "other_authors": "Eric L. Seidel, Niki Vazou, Ranjit Jhala", 
    "title": "Type Targeted Testing", 
    "arxiv-id": "1410.5370v2", 
    "author": "Ranjit Jhala", 
    "publish": "2014-10-20T17:48:20Z", 
    "summary": "We present a new technique called type targeted testing, which translates\nprecise refinement types into comprehensive test-suites. The key insight behind\nour approach is that through the lens of SMT solvers, refinement types can also\nbe viewed as a high-level, declarative, test generation technique, wherein\ntypes are converted to SMT queries whose models can be decoded into concrete\nprogram inputs. Our approach enables the systematic and exhaustive testing of\nimplementations from high-level declarative specifications, and furthermore,\nprovides a gradual path from testing to full verification. We have implemented\nour approach as a Haskell testing tool called TARGET, and present an evaluation\nthat shows how TARGET can be used to test a wide variety of properties and how\nit compares against state-of-the-art testing approaches."
},{
    "category": "cs.PL", 
    "doi": "10.1109/SCAM.2014.44", 
    "link": "http://arxiv.org/pdf/1410.6449v2", 
    "other_authors": "Jiahao Chen, Alan Edelman", 
    "title": "Parallel Prefix Polymorphism Permits Parallelization, Presentation &   Proof", 
    "arxiv-id": "1410.6449v2", 
    "author": "Alan Edelman", 
    "publish": "2014-10-23T18:42:57Z", 
    "summary": "Polymorphism in programming languages enables code reuse. Here, we show that\npolymorphism has broad applicability far beyond computations for technical\ncomputing: parallelism in distributed computing, presentation of visualizations\nof runtime data flow, and proofs for formal verification of correctness. The\nability to reuse a single codebase for all these purposes provides new ways to\nunderstand and verify parallel programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.166.5", 
    "link": "http://arxiv.org/pdf/1410.7467v1", 
    "other_authors": "Sung-Shik T. Q. Jongmans, Farhad Arbab", 
    "title": "Toward Sequentializing Overparallelized Protocol Code", 
    "arxiv-id": "1410.7467v1", 
    "author": "Farhad Arbab", 
    "publish": "2014-10-28T00:40:55Z", 
    "summary": "In our ongoing work, we use constraint automata to compile protocol\nspecifications expressed as Reo connectors into efficient executable code,\ne.g., in C. We have by now studied this automata based compilation approach\nrather well, and have devised effective solutions to some of its problems.\nBecause our approach is based on constraint automata, the approach, its\nproblems, and our solutions are in fact useful and relevant well beyond the\nspecific case of compiling Reo. In this short paper, we identify and analyze\ntwo such rather unexpected problems."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.166.5", 
    "link": "http://arxiv.org/pdf/1411.0352v2", 
    "other_authors": "Maxime Chevalier-Boisvert, Marc Feeley", 
    "title": "Simple and Effective Type Check Removal through Lazy Basic Block   Versioning", 
    "arxiv-id": "1411.0352v2", 
    "author": "Marc Feeley", 
    "publish": "2014-11-03T03:46:18Z", 
    "summary": "Dynamically typed programming languages such as JavaScript and Python defer\ntype checking to run time. In order to maximize performance, dynamic language\nVM implementations must attempt to eliminate redundant dynamic type checks.\nHowever, type inference analyses are often costly and involve tradeoffs between\ncompilation time and resulting precision. This has lead to the creation of\nincreasingly complex multi-tiered VM architectures.\n  This paper introduces lazy basic block versioning, a simple JIT compilation\ntechnique which effectively removes redundant type checks from critical code\npaths. This novel approach lazily generates type-specialized versions of basic\nblocks on-the-fly while propagating context-dependent type information. This\ndoes not require the use of costly program analyses, is not restricted by the\nprecision limitations of traditional type analyses and avoids the\nimplementation complexity of speculative optimization techniques.\n  We have implemented intraprocedural lazy basic block versioning in a\nJavaScript JIT compiler. This approach is compared with a classical flow-based\ntype analysis. Lazy basic block versioning performs as well or better on all\nbenchmarks. On average, 71% of type tests are eliminated, yielding speedups of\nup to 50%. We also show that our implementation generates more efficient\nmachine code than TraceMonkey, a tracing JIT compiler for JavaScript, on\nseveral benchmarks. The combination of implementation simplicity, low\nalgorithmic complexity and good run time performance makes basic block\nversioning attractive for baseline JIT compilers."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814308", 
    "link": "http://arxiv.org/pdf/1411.3962v5", 
    "other_authors": "David Darais, Matthew Might, David Van Horn", 
    "title": "Galois Transformers and Modular Abstract Interpreters", 
    "arxiv-id": "1411.3962v5", 
    "author": "David Van Horn", 
    "publish": "2014-11-14T16:33:11Z", 
    "summary": "The design and implementation of static analyzers has become increasingly\nsystematic. Yet for a given language or analysis feature, it often requires\ntedious and error prone work to implement an analyzer and prove it sound. In\nshort, static analysis features and their proofs of soundness do not compose\nwell, causing a dearth of reuse in both implementation and metatheory.\n  We solve the problem of systematically constructing static analyzers by\nintroducing Galois transformers: monad transformers that transport Galois\nconnection properties. In concert with a monadic interpreter, we define a\nlibrary of monad transformers that implement building blocks for classic\nanalysis parameters like context, path, and heap (in)sensitivity. Moreover,\nthese can be composed together independent of the language being analyzed.\n  Significantly, a Galois transformer can be proved sound once and for all,\nmaking it a reusable analysis component. As new analysis features and\nabstractions are developed and mixed in, soundness proofs need not be\nreconstructed, as the composition of a monad transformer stack is sound by\nvirtue of its constituents. Galois transformers provide a viable foundation for\nreusable and composable metatheory for program analysis.\n  Finally, these Galois transformers shift the level of abstraction in analysis\ndesign and implementation to a level where non-specialists have the ability to\nsynthesize sound analyzers over a number of parameters."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814308", 
    "link": "http://arxiv.org/pdf/1411.3967v2", 
    "other_authors": "Phuc C. Nguyen, David Van Horn", 
    "title": "Relatively Complete Counterexamples for Higher-Order Programs", 
    "arxiv-id": "1411.3967v2", 
    "author": "David Van Horn", 
    "publish": "2014-11-14T16:44:31Z", 
    "summary": "In this paper, we study the problem of generating inputs to a higher-order\nprogram causing it to error. We first study the problem in the setting of PCF,\na typed, core functional language and contribute the first relatively complete\nmethod for constructing counterexamples for PCF programs. The method is\nrelatively complete in the sense of Hoare logic; completeness is reduced to the\ncompleteness of a first-order solver over the base types of PCF. In practice,\nthis means an SMT solver can be used for the effective, automated generation of\nhigher-order counterexamples for a large class of programs.\n  We achieve this result by employing a novel form of symbolic execution for\nhigher-order programs. The remarkable aspect of this symbolic execution is that\neven though symbolic higher-order inputs and values are considered, the path\ncondition remains a first-order formula. Our handling of symbolic function\napplication enables the reconstruction of higher-order counterexamples from\nthis first-order formula.\n  After establishing our main theoretical results, we sketch how to apply the\napproach to untyped, higher-order, stateful languages with first-class\ncontracts and show how counterexample generation can be used to detect contract\nviolations in this setting. To validate our approach, we implement a tool\ngenerating counterexamples for erroneous modules written in Racket."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814308", 
    "link": "http://arxiv.org/pdf/1411.5110v1", 
    "other_authors": "Keehang Kwon", 
    "title": "Mutually Exclusive Procedures in Imperative Languages", 
    "arxiv-id": "1411.5110v1", 
    "author": "Keehang Kwon", 
    "publish": "2014-11-19T04:34:35Z", 
    "summary": "To represent mutually exclusive procedures, we propose a choice-conjunctive\ndeclaration statement of the form $uchoo(S,R)$ where $S, R$ are the procedure\ndeclaration statements within a module. This statement has the following\nsemantics: request the machine to choose a successful one between $S$ and $R$.\nThis statement is useful for representing objects with mutually exclusive\nprocedures. We illustrate our idea via C^uchoo, an extension of the core C with\na new statement."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814308", 
    "link": "http://arxiv.org/pdf/1411.5289v2", 
    "other_authors": "Uday P. Khedker, Vini Kanvar", 
    "title": "Generalizing the Liveness Based Points-to Analysis", 
    "arxiv-id": "1411.5289v2", 
    "author": "Vini Kanvar", 
    "publish": "2014-11-19T16:58:18Z", 
    "summary": "The original liveness based flow and context sensitive points-to analysis\n(LFCPA) is restricted to scalar pointer variables and scalar pointees on stack\nand static memory. In this paper, we extend it to support heap memory and\npointer expressions involving structures, unions, arrays, and pointer\narithmetic. The key idea behind these extensions involves constructing bounded\nnames for locations in terms of compile time constants (names and fixed\noffsets), and introducing sound approximations when it is not possible to do\nso. We achieve this by defining a grammar for pointer expressions, suitable\nmemory models and location naming conventions, and some key evaluations of\npointer expressions that compute the named locations. These extensions preserve\nthe spirit of the original LFCPA which is evidenced by the fact that although\nthe lattices and flow functions change, the overall data flow equations remain\nunchanged."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1411.5573v1", 
    "other_authors": "Jose F. Morales, Manuel Carro, Manuel Hermenegildo", 
    "title": "Description and Optimization of Abstract Machines in a Dialect of Prolog", 
    "arxiv-id": "1411.5573v1", 
    "author": "Manuel Hermenegildo", 
    "publish": "2014-11-20T15:22:52Z", 
    "summary": "In order to achieve competitive performance, abstract machines for Prolog and\nrelated languages end up being large and intricate, and incorporate\nsophisticated optimizations, both at the design and at the implementation\nlevels. At the same time, efficiency considerations make it necessary to use\nlow-level languages in their implementation. This makes them laborious to code,\noptimize, and, especially, maintain and extend. Writing the abstract machine\n(and ancillary code) in a higher-level language can help tame this inherent\ncomplexity. We show how the semantics of most basic components of an efficient\nvirtual machine for Prolog can be described using (a variant of) Prolog. These\ndescriptions are then compiled to C and assembled to build a complete bytecode\nemulator. Thanks to the high level of the language used and its closeness to\nProlog, the abstract machine description can be manipulated using standard\nProlog compilation and optimization techniques with relative ease. We also show\nhow, by applying program transformations selectively, we obtain abstract\nmachine implementations whose performance can match and even exceed that of\nstate-of-the-art, highly-tuned, hand-crafted emulators."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1411.6361v1", 
    "other_authors": "Baptiste Wicht, Roberto A. Vitillo, Dehao Chen, David Levinthal", 
    "title": "Hardware Counted Profile-Guided Optimization", 
    "arxiv-id": "1411.6361v1", 
    "author": "David Levinthal", 
    "publish": "2014-11-24T07:01:31Z", 
    "summary": "Profile-Guided Optimization (PGO) is an excellent means to improve the\nperformance of a compiled program. Indeed, the execution path data it provides\nhelps the compiler to generate better code and better cacheline packing.\n  At the time of this writing, compilers only support instrumentation-based\nPGO. This proved effective for optimizing programs. However, few projects use\nit, due to its complicated dual-compilation model and its high overhead. Our\nsolution of sampling Hardware Performance Counters overcome these drawbacks. In\nthis paper, we propose a PGO solution for GCC by sampling Last Branch Record\n(LBR) events and using debug symbols to recreate source locations of binary\ninstructions.\n  By using LBR-Sampling, the generated profiles are very accurate. This\nsolution achieved an average of 83% of the gains obtained with\ninstrumentation-based PGO and 93% on C++ benchmarks only. The profiling\noverhead is only 1.06% on average whereas instrumentation incurs a 16% overhead\non average."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1411.7839v2", 
    "other_authors": "Stefano Dissegna, Francesco Logozzo, Francesco Ranzato", 
    "title": "An Abstract Interpretation-based Model of Tracing Just-In-Time   Compilation", 
    "arxiv-id": "1411.7839v2", 
    "author": "Francesco Ranzato", 
    "publish": "2014-11-28T12:32:39Z", 
    "summary": "Tracing just-in-time compilation is a popular compilation technique for the\nefficient implementation of dynamic languages, which is commonly used for\nJavaScript, Python and PHP. We provide a formal model of tracing JIT\ncompilation of programs using abstract interpretation. Hot path detection\ncorresponds to an abstraction of the trace semantics of the program. The\noptimization phase corresponds to a transform of the original program that\npreserves its trace semantics up to an observation modeled by some abstraction.\nWe provide a generic framework to express dynamic optimizations and prove them\ncorrect. We instantiate it to prove the correctness of dynamic type\nspecialization and constant variable folding. We show that our framework is\nmore general than the model of tracing compilation introduced by Guo and\nPalsberg [2011] based on operational bisimulations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.0961v1", 
    "other_authors": "Jingshu Chen, Marie Duflot, Stephan Merz", 
    "title": "Analyzing Conflict Freedom For Multi-threaded Programs With Time   Annotations", 
    "arxiv-id": "1412.0961v1", 
    "author": "Stephan Merz", 
    "publish": "2014-11-30T07:22:03Z", 
    "summary": "Avoiding access conflicts is a major challenge in the design of\nmulti-threaded programs. In the context of real-time systems, the absence of\nconflicts can be guaranteed by ensuring that no two potentially conflicting\naccesses are ever scheduled concurrently.In this paper, we analyze programs\nthat carry time annotations specifying the time for executing each statement.\nWe propose a technique for verifying that a multi-threaded program with time\nannotations is free of access conflicts. In particular, we generate constraints\nthat reflect the possible schedules for executing the program and the required\nproperties. We then invoke an SMT solver in order to verify that no execution\ngives rise to concurrent conflicting accesses. Otherwise, we obtain a trace\nthat exhibits the access conflict."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.1221v1", 
    "other_authors": "Daeseong Kang, Keehang Kwon, Zulkarnine Mahmud", 
    "title": "Sequential Operations in LogicWeb", 
    "arxiv-id": "1412.1221v1", 
    "author": "Zulkarnine Mahmud", 
    "publish": "2014-12-03T07:54:13Z", 
    "summary": "Sequential tasks cannot be effectively handled in logic programming based on\nclassical logic or linear logic. This limitation can be addressed by using a\nfragment of Japaridze'sSequential tasks cannot be effectively handled in logic\nprogramming based on classical logic or linear logic. This limitation can be\naddressed by using a fragment of Japaridze's computability logic. We propose\n\\seqweb, an extension to LogicWeb with sequential goal formulas. SeqWeb extends\nthe LogicWeb by allowing goals of the form $G\\seqand G$ and $G\\seqor G$ where\n$G$ is a goal. These goals allow us to specify both sequential-conjunctive and\nsequential-disjunctive tasks. computability logic. We propose \\seqweb, an\nextension to LogicWeb with sequential goal formulas. SeqWeb extends the\nLogicWeb by allowing goals of the form $G\\seqand G$ and $G\\seqor G$ where $G$\nis a goal. These goals allow us to specify both sequential-conjunctive and\nsequential-disjunctive tasks."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.1393v1", 
    "other_authors": "Marco Antoniotti", 
    "title": "CLAZY: Lazy Calling for Common Lisp", 
    "arxiv-id": "1412.1393v1", 
    "author": "Marco Antoniotti", 
    "publish": "2014-12-03T16:34:11Z", 
    "summary": "This document contains a description of a Common Lisp extension that allows a\nprogrammer to write functional programs that use \"normal order\" evaluation, as\nin \"non-strict\" languages like Haskell. The extension is relatively\nstraightforward, and it appears to be the first one such that is integrated in\nthe overall Common Lisp framework."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.2304v2", 
    "other_authors": "Sergii Dymchenko, Mariia Mykhailova", 
    "title": "Declaratively solving tricky Google Code Jam problems with Prolog-based   ECLiPSe CLP system", 
    "arxiv-id": "1412.2304v2", 
    "author": "Mariia Mykhailova", 
    "publish": "2014-12-07T02:24:21Z", 
    "summary": "In this paper we demonstrate several examples of solving challenging\nalgorithmic problems from the Google Code Jam programming contest with the\nProlog-based ECLiPSe system using declarative techniques like constraint logic\nprogramming and linear (integer) programming. These problems were designed to\nbe solved by inventing clever algorithms and efficiently implementing them in a\nconventional imperative programming language, but we present relatively simple\ndeclarative programs in ECLiPSe that are fast enough to find answers within the\ntime limit imposed by the contest rules. We claim that declarative programming\nwith ECLiPSe is better suited for solving certain common kinds of programming\nproblems offered in Google Code Jam than imperative programming. We show this\nby comparing the mental steps required to come up with both kinds of solutions."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.3729v1", 
    "other_authors": "Etienne Payet, Fred Mesnard", 
    "title": "Non-termination of Dalvik bytecode via compilation to CLP", 
    "arxiv-id": "1412.3729v1", 
    "author": "Fred Mesnard", 
    "publish": "2014-12-10T12:20:13Z", 
    "summary": "We present a set of rules for compiling a Dalvik bytecode program into a\nlogic program with array constraints. Non-termination of the resulting program\nentails that of the original one, hence the techniques we have presented before\nfor proving non-termination of constraint logic programs can be used for\nproving non-termination of Dalvik programs."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.4053v2", 
    "other_authors": "Neil Toronto, Jay McCarthy, David Van Horn", 
    "title": "Running Probabilistic Programs Backwards", 
    "arxiv-id": "1412.4053v2", 
    "author": "David Van Horn", 
    "publish": "2014-12-12T16:59:14Z", 
    "summary": "Many probabilistic programming languages allow programs to be run under\nconstraints in order to carry out Bayesian inference. Running programs under\nconstraints could enable other uses such as rare event simulation and\nprobabilistic verification---except that all such probabilistic languages are\nnecessarily limited because they are defined or implemented in terms of an\nimpoverished theory of probability. Measure-theoretic probability provides a\nmore general foundation, but its generality makes finding computational content\ndifficult.\n  We develop a measure-theoretic semantics for a first-order probabilistic\nlanguage with recursion, which interprets programs as functions that compute\npreimages. Preimage functions are generally uncomputable, so we derive an\nabstract semantics. We implement the abstract semantics and use the\nimplementation to carry out Bayesian inference, stochastic ray tracing (a rare\nevent simulation), and probabilistic verification of floating-point error\nbounds."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.4184v1", 
    "other_authors": "Kostadin Kratchanov, Emilia Golemanova, Tzanko Golemanov, Tuncay Ercan, Burak Ekici", 
    "title": "Procedural and Non-Procedural Implementation of Search Strategies in   Control Network Programming", 
    "arxiv-id": "1412.4184v1", 
    "author": "Burak Ekici", 
    "publish": "2014-12-13T02:39:20Z", 
    "summary": "This report presents the general picture of how Control Network Programming\ncan be effectively used for implementing various search strategies, both blind\nand informed. An interesting possibility is non - procedural solutions that can\nbe developed for most local search algorithms. A generic solution is described\nfor procedural implementations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.4395v1", 
    "other_authors": "Rachel Gauci", 
    "title": "Dafny: Statically Verifying Functional Correctness", 
    "arxiv-id": "1412.4395v1", 
    "author": "Rachel Gauci", 
    "publish": "2014-12-14T19:04:47Z", 
    "summary": "This report presents the Dafny language and verifier, with a focus on\ndescribing the main features of the language, including pre- and\npostconditions, assertions, loop invariants, termination metrics, quantifiers,\npredicates and frames. Examples of Dafny code are provided to illustrate the\nuse of each feature, and an overview of how Dafny translates programming code\ninto a mathematical proof of functional verification is presented. The report\nalso includes references to useful resources on Dafny, with mentions of related\nworks in the domain of specification languages."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.4480v2", 
    "other_authors": "Long Zheng, Xiaofei Liao, Bingsheng He, Song Wu, Hai Jin", 
    "title": "On Performance Debugging of Unnecessary Lock Contentions on Multicore   Processors: A Replay-based Approach", 
    "arxiv-id": "1412.4480v2", 
    "author": "Hai Jin", 
    "publish": "2014-12-15T07:22:40Z", 
    "summary": "Locks have been widely used as an effective synchronization mechanism among\nprocesses and threads. However, we observe that a large number of false\ninter-thread dependencies (i.e., unnecessary lock contentions) exist during the\nprogram execution on multicore processors, thereby incurring significant\nperformance overhead. This paper presents a performance debugging framework,\nPERFPLAY, to facilitate a comprehensive and in-depth understanding of the\nperformance impact of unnecessary lock contentions. The core technique of our\ndebugging framework is trace replay. Specifically, PERFPLAY records the program\nexecution trace, on the basis of which the unnecessary lock contentions can be\nidentified through trace analysis. We then propose a novel technique of trace\ntransformation to transform these identified unnecessary lock contentions in\nthe original trace into the correct pattern as a new trace free of unnecessary\nlock contentions. Through replaying both traces, PERFPLAY can quantify the\nperformance impact of unnecessary lock contentions. To demonstrate the\neffectiveness of our debugging framework, we study five real-world programs and\nPARSEC benchmarks. Our experimental results demonstrate the significant\nperformance overhead of unnecessary lock contentions, and the effectiveness of\nPERFPLAY in identifying the performance critical unnecessary lock contentions\nin real applications."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.5150v1", 
    "other_authors": "Vassilis Vassiliadis, Konstantinos Parasyris, Charalambos Chalios, Christos D. Antonopoulos, Spyros Lalis, Nikolaos Bellas, Hans Vandierendonck, Dimitrios S. Nikolopoulos", 
    "title": "A Programming Model and Runtime System for Significance-Aware   Energy-Efficient Computing", 
    "arxiv-id": "1412.5150v1", 
    "author": "Dimitrios S. Nikolopoulos", 
    "publish": "2014-12-15T17:46:42Z", 
    "summary": "Reducing energy consumption is one of the key challenges in computing\ntechnology. One factor that contributes to high energy consumption is that all\nparts of the program are considered equally significant for the accuracy of the\nend-result. However, in many cases, parts of computations can be performed in\nan approximate way, or even dropped, without affecting the quality of the final\noutput to a significant degree.\n  In this paper, we introduce a task-based programming model and runtime system\nthat exploit this observation to trade off the quality of program outputs for\nincreased energy-efficiency. This is done in a structured and flexible way,\nallowing for easy exploitation of different execution points in the\nquality/energy space, without code modifications and without adversely\naffecting application performance. The programmer specifies the significance of\ntasks, and optionally provides approximations for them. Moreover, she provides\nhints to the runtime on the percentage of tasks that should be executed\naccurately in order to reach the target quality of results. The runtime system\ncan apply a number of different policies to decide whether it will execute each\nindividual less-significant task in its accurate form, or in its approximate\nversion. Policies differ in terms of their runtime overhead but also the degree\nto which they manage to execute tasks according to the programmer's\nspecification.\n  The results from experiments performed on top of an Intel-based\nmulticore/multiprocessor platform show that, depending on the runtime policy\nused, our system can achieve an energy reduction of up to 83% compared with a\nfully accurate execution and up to 35% compared with an approximate version\nemploying loop perforation. At the same time, our approach always results in\ngraceful quality degradation."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.5400v4", 
    "other_authors": "Uday P. Khedker", 
    "title": "Buffer Overflow Analysis for C", 
    "arxiv-id": "1412.5400v4", 
    "author": "Uday P. Khedker", 
    "publish": "2014-12-17T14:12:12Z", 
    "summary": "Buffer overflow detection and mitigation for C programs has been an important\nconcern for a long time. This paper defines a string buffer overflow analysis\nfor C programs. The key ideas of our formulation are (a) separating buffers\nfrom the pointers that point to them, (b) modelling buffers in terms of sizes\nand sets of positions of null characters, and (c) defining stateless functions\nto compute the sets of null positions and mappings between buffers and\npointers.\n  This exercise has been carried out to test the feasibility of describing such\nan analysis in terms of lattice valued functions and relations to facilitate\nautomatic construction of an analyser without the user having to write\nC/C++/Java code. This is facilitated by devising stateless formulations because\nstateful formulations combine features through side effects in states raising a\nnatural requirement of C/C++/Java code to be written to describe them. Given\nthe above motivation, the focus of this paper is not to build good static\napproximations for buffer overflow analysis but to show how given static\napproximations could be formalized in terms of stateless formulations so that\nthey become amenable to automatic construction of analysers."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.7664v1", 
    "other_authors": "M. G. G. C. R. Salgado, R. G. Ragel", 
    "title": "Register Spilling for Specific Application Domains in Application   Specific Instruction-set Processors", 
    "arxiv-id": "1412.7664v1", 
    "author": "R. G. Ragel", 
    "publish": "2014-12-24T14:15:19Z", 
    "summary": "An Application Specific Instruction set Processor (ASIP) is an important\ncomponent in designing embedded systems. One of the problems in designing an\ninstruction set for such processors is determining the number of registers is\nneeded in the processor that will optimize the computational time and the cost.\nThe performance of a processor may fall short due to register spilling, which\nis caused by the lack of available registers in a processor. In the design\nperspective, it will result in processors with great performance and low power\nconsumption if we can avoid register spilling by deciding a value for the\nnumber of registers needed in an ASIP. However, as of now, it has not clearly\nbeen recognized how the number of registers changes with different application\ndomains. In this paper, we evaluated whether different application domains have\nany significant effect on register spilling and therefore the performance of a\nprocessor so that we could use different number of registers when building\nASIPs for different application domains rather than using a constant set of\nregisters. Such utilization of registers will result in processors with high\nperformance, low cost and low power consumption."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1412.8120v1", 
    "other_authors": "Amlan Kusum, Iulian Neamtiu, Rajiv Gupta", 
    "title": "Adapting Graph Application Performance via Alternate Data Structure   Representation", 
    "arxiv-id": "1412.8120v1", 
    "author": "Rajiv Gupta", 
    "publish": "2014-12-28T06:49:23Z", 
    "summary": "Graph processing is used extensively in areas from social networking mining\nto web indexing. We demonstrate that the performance and dependability of such\napplications critically hinges on the graph data structure used, because a\nfixed, compile-time choice of data structure can lead to poor performance or\napplications unable to complete. To address this problem, we introduce an\napproach that helps programmers transform regular, off-the-shelf graph\napplications into adaptive, more dependable applications where adaptations are\nperformed via runtime selection from alternate data structure representations.\nUsing our approach, applications dynamically adapt to the input graph's\ncharacteristics and changes in available memory so they continue to run when\nfaced with adverse conditions such as low memory. Experiments with graph\nalgorithms on real-world (e.g., Wikipedia metadata, Gnutella topology) and\nsynthetic graph datasets show that our adaptive applications run to completion\nwith lower execution time and/or memory utilization in comparison to their\nnon-adaptive versions."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1501.00669v1", 
    "other_authors": "Mohamed A. El-Zawawy", 
    "title": "Asynchronous Programming in a Prioritized Form", 
    "arxiv-id": "1501.00669v1", 
    "author": "Mohamed A. El-Zawawy", 
    "publish": "2015-01-04T12:19:57Z", 
    "summary": "Asynchronous programming has appeared as a programming style that overcomes\nundesired properties of concurrent programming. Typically in asynchronous\nmodels of programming, methods are posted into a post list for latter\nexecution. The order of method executions is serial, but nondeterministic. This\npaper presents a new and simple, yet powerful, model for asynchronous\nprogramming. The proposed model consists of two components; a context-free\ngrammar and an operational semantics. The model is supported by the ability to\nexpress important applications. An advantage of our model over related work is\nthat the model simplifies the way posted methods are assigned priorities.\nAnother advantage is that the operational semantics uses the simple concept of\nsingly linked list to simulate the prioritized process of methods posting and\nexecution. The simplicity and expressiveness make it relatively easy for\nanalysis algorithms to disclose the otherwise un-captured programming bugs in\nasynchronous programs."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068414000672", 
    "link": "http://arxiv.org/pdf/1501.00720v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Concept-oriented programming: from classes to concepts and from   inheritance to inclusion", 
    "arxiv-id": "1501.00720v1", 
    "author": "Alexandr Savinov", 
    "publish": "2015-01-04T21:02:34Z", 
    "summary": "For the past several decades, programmers have been modeling things in the\nworld with trees using hierarchies of classes and object-oriented programming\n(OOP) languages. In this paper, we describe a novel approach to programming,\ncalled concept-oriented programming (COP), which generalizes classes and\ninheritance by introducing concepts and inclusion, respectively."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.02035v1", 
    "other_authors": "Adri\u00e1n Riesco, Juan Rodr\u00edguez-Hortal\u00e1", 
    "title": "Lifting Term Rewriting Derivations in Constructor Systems by Using   Generators", 
    "arxiv-id": "1501.02035v1", 
    "author": "Juan Rodr\u00edguez-Hortal\u00e1", 
    "publish": "2015-01-09T04:00:31Z", 
    "summary": "Narrowing is a procedure that was first studied in the context of equational\nE-unification and that has been used in a wide range of applications. The\nclassic completeness result due to Hullot states that any term rewriting\nderivation starting from an instance of an expression can be \"lifted\" to a\nnarrowing derivation, whenever the substitution employed is normalized. In this\npaper we adapt the generator- based extra-variables-elimination transformation\nused in functional-logic programming to overcome that limitation, so we are\nable to lift term rewriting derivations starting from arbitrary instances of\nexpressions. The proposed technique is limited to left-linear constructor\nsystems and to derivations reaching a ground expression. We also present a\nMaude-based implementation of the technique, using natural rewriting for the\non-demand evaluation strategy."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.02683v1", 
    "other_authors": "Ahmed Bouajjani, Georgel Calin, Egor Derevenetc, Roland Meyer", 
    "title": "Lazy TSO Reachability", 
    "arxiv-id": "1501.02683v1", 
    "author": "Roland Meyer", 
    "publish": "2015-01-12T15:50:07Z", 
    "summary": "We address the problem of checking state reachability for programs running\nunder Total Store Order (TSO). The problem has been shown to be decidable but\nthe cost is prohibitive, namely non-primitive recursive. We propose here to\ngive up completeness. Our contribution is a new algorithm for TSO reachability:\nit uses the standard SC semantics and introduces the TSO semantics lazily and\nonly where needed. At the heart of our algorithm is an iterative refinement of\nthe program of interest. If the program's goal state is SC-reachable, we are\ndone. If the goal state is not SC-reachable, this may be due to the fact that\nSC under-approximates TSO. We employ a second algorithm that determines TSO\ncomputations which are infeasible under SC, and hence likely to lead to new\nstates. We enrich the program to emulate, under SC, these TSO computations.\nAltogether, this yields an iterative under-approximation that we prove sound\nand complete for bug hunting, i.e., a semi-decision procedure halting for\npositive cases of reachability. We have implemented the procedure as an\nextension to the tool Trencher and compared it to the Memorax and CBMC model\ncheckers."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.03839v1", 
    "other_authors": "Arkady Klimov", 
    "title": "Yet Another Way of Building Exact Polyhedral Model for Weakly Dynamic   Affine Programs", 
    "arxiv-id": "1501.03839v1", 
    "author": "Arkady Klimov", 
    "publish": "2015-01-15T22:07:16Z", 
    "summary": "Exact polyhedral model (PM) can be built in the general case if the only\ncontrol structures are {\\tt do}-loops and structured {\\tt if}s, and if loop\ncounter bounds, array subscripts and {\\tt if}-conditions are affine expressions\nof enclosing loop counters and possibly some integer constants. In more general\ndynamic control programs, where arbitrary {\\tt if}s and {\\tt while}s are\nallowed, in the general case the usual dataflow analysis can be only fuzzy.\nThis is not a problem when PM is used just for guiding the parallelizing\ntransformations, but is insufficient for transforming source programs to other\ncomputation models (CM) relying on the PM, such as our version of dataflow CM\nor the well-known KPN.\n  The paper presents a novel way of building the exact polyhedral model and an\nextension of the concept of the exact PM, which allowed us to add in a natural\nway all the processing related to the data dependent conditions. Currently, in\nour system, only arbirary {\\tt if}s (not {\\tt while}s) are allowed in input\nprograms. The resulting polyhedral model can be easily put out as an equivalent\nprogram with the dataflow computation semantics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.04730v2", 
    "other_authors": "Raveendra Kumar Medicherla, Raghavan Komondoor, S. Narendran", 
    "title": "Static Analysis of File-Processing Programs using File Format   Specifications", 
    "arxiv-id": "1501.04730v2", 
    "author": "S. Narendran", 
    "publish": "2015-01-20T07:53:09Z", 
    "summary": "Programs that process data that reside in files are widely used in varied\ndomains, such as banking, healthcare, and web-traffic analysis. Precise static\nanalysis of these programs in the context of software verification and\ntransformation tasks is a challenging problem. Our key insight is that static\nanalysis of file-processing programs can be made more useful if knowledge of\nthe input file formats of these programs is made available to the analysis. We\npropose a generic framework that is able to perform any given underlying\nabstract interpretation on the program, while restricting the attention of the\nanalysis to program paths that are potentially feasible when the program's\ninput conforms to the given file format specification. We describe an\nimplementation of our approach, and present empirical results using real and\nrealistic programs that show how our approach enables novel verification and\ntransformation tasks, and also improves the precision of standard analysis\nproblems."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.05338v2", 
    "other_authors": "Michael Ernst, Damiano Macedonio, Massimo Merro, Fausto Spoto", 
    "title": "Semantics for Locking Specifications", 
    "arxiv-id": "1501.05338v2", 
    "author": "Fausto Spoto", 
    "publish": "2015-01-21T21:56:06Z", 
    "summary": "To prevent concurrency errors, programmers need to obey a locking discipline.\nAnnotations that specify that discipline, such as Java's @GuardedBy, are\nalready widely used. Unfortunately, their semantics is expressed informally and\nis consequently ambiguous. This article highlights such ambiguities and\nformalizes the semantics of @GuardedBy in two alternative ways, building on an\noperational semantics for a small concurrent fragment of a Java-like language.\nIt also identifies when such annotations are actual guarantees against data\nraces. Our work aids in understanding the annotations and supports the\ndevelopment of sound formal tools that verify or infer such annotations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.05425v1", 
    "other_authors": "Jasmin Christian Blanchette, Andrei Popescu, Dmitriy Traytel", 
    "title": "Foundational Extensible Corecursion", 
    "arxiv-id": "1501.05425v1", 
    "author": "Dmitriy Traytel", 
    "publish": "2015-01-22T08:45:10Z", 
    "summary": "This paper presents a formalized framework for defining corecursive functions\nsafely in a total setting, based on corecursion up-to and relational\nparametricity. The end product is a general corecursor that allows corecursive\n(and even recursive) calls under well-behaved operations, including\nconstructors. Corecursive functions that are well behaved can be registered as\nsuch, thereby increasing the corecursor's expressiveness. The metatheory is\nformalized in the Isabelle proof assistant and forms the core of a prototype\ntool. The corecursor is derived from first principles, without requiring new\naxioms or extensions of the logic."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.173.7", 
    "link": "http://arxiv.org/pdf/1501.06743v1", 
    "other_authors": "Esraa Alwan, John Fitch, Julian Padget", 
    "title": "Enhancing the performance of Decoupled Software Pipeline through   Backward Slicing", 
    "arxiv-id": "1501.06743v1", 
    "author": "Julian Padget", 
    "publish": "2015-01-27T11:04:59Z", 
    "summary": "The rapidly increasing number of cores available in multicore processors does\nnot necessarily lead directly to a commensurate increase in performance:\nprograms written in conventional languages, such as C, need careful\nrestructuring, preferably automatically, before the benefits can be observed in\nimproved run-times. Even then, much depends upon the intrinsic capacity of the\noriginal program for concurrent execution. The subject of this paper is the\nperformance gains from the combined effect of the complementary techniques of\nthe Decoupled Software Pipeline (DSWP) and (backward) slicing. DSWP extracts\nthreadlevel parallelism from the body of a loop by breaking it into stages\nwhich are then executed pipeline style: in effect cutting across the control\nchain. Slicing, on the other hand, cuts the program along the control chain,\nteasing out finer threads that depend on different variables (or locations).\nparts that depend on different variables. The main contribution of this paper\nis to demonstrate that the application of DSWP, followed by slicing offers\nnotable improvements over DSWP alone, especially when there is a loop-carried\ndependence that prevents the application of the simpler DOALL optimization.\nExperimental results show an improvement of a factor of ?1.6 for DSWP + slicing\nover DSWP alone and a factor of ?2.4 for DSWP + slicing over the original\nsequential code."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2016.1.1", 
    "link": "http://arxiv.org/pdf/1502.00238v2", 
    "other_authors": "J. A. Bergstra, C. A. Middelburg", 
    "title": "On instruction sets for Boolean registers in program algebra", 
    "arxiv-id": "1502.00238v2", 
    "author": "C. A. Middelburg", 
    "publish": "2015-02-01T12:17:12Z", 
    "summary": "In previous work carried out in the setting of program algebra, including\nwork in the area of instruction sequence size complexity, we chose instruction\nsets for Boolean registers that contain only instructions of a few of the\npossible kinds. In the current paper, we study instruction sequence size\nbounded functional completeness of all possible instruction sets for Boolean\nregisters. We expect that the results of this study will turn out to be useful\nto adequately assess results of work that is concerned with lower bounds of\ninstruction sequence size complexity."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2016.1.1", 
    "link": "http://arxiv.org/pdf/1502.01278v2", 
    "other_authors": "Robert Jakob, Peter Thiemann", 
    "title": "A Falsification View of Success Typing", 
    "arxiv-id": "1502.01278v2", 
    "author": "Peter Thiemann", 
    "publish": "2015-02-04T18:10:21Z", 
    "summary": "Dynamic languages are praised for their flexibility and expressiveness, but\nstatic analysis often yields many false positives and verification is\ncumbersome for lack of structure. Hence, unit testing is the prevalent\nincomplete method for validating programs in such languages.\n  Falsification is an alternative approach that uncovers definite errors in\nprograms. A falsifier computes a set of inputs that definitely crash a program.\n  Success typing is a type-based approach to document programs in dynamic\nlanguages. We demonstrate that success typing is, in fact, an instance of\nfalsification by mapping success (input) types into suitable logic formulae.\nOutput types are represented by recursive types. We prove the correctness of\nour mapping (which establishes that success typing is falsification) and we\nreport some experiences with a prototype implementation."
},{
    "category": "cs.PL", 
    "doi": "10.7561/SACS.2016.1.1", 
    "link": "http://arxiv.org/pdf/1502.02519v2", 
    "other_authors": "Fabrizio Montesi", 
    "title": "Kickstarting Choreographic Programming", 
    "arxiv-id": "1502.02519v2", 
    "author": "Fabrizio Montesi", 
    "publish": "2015-02-09T15:20:03Z", 
    "summary": "We present an overview of some recent efforts aimed at the development of\nChoreographic Programming, a programming paradigm for the production of\nconcurrent software that is guaranteed to be correct by construction from\nglobal descriptions of communication behaviour."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1502.04772v1", 
    "other_authors": "Edward Gan, Jesse A. Tov, Greg Morrisett", 
    "title": "Type Classes for Lightweight Substructural Types", 
    "arxiv-id": "1502.04772v1", 
    "author": "Greg Morrisett", 
    "publish": "2015-02-17T02:27:31Z", 
    "summary": "Linear and substructural types are powerful tools, but adding them to\nstandard functional programming languages often means introducing extra\nannotations and typing machinery. We propose a lightweight substructural type\nsystem design that recasts the structural rules of weakening and contraction as\ntype classes; we demonstrate this design in a prototype language, Clamp.\n  Clamp supports polymorphic substructural types as well as an expressive\nsystem of mutable references. At the same time, it adds little additional\noverhead to a standard Damas-Hindley-Milner type system enriched with type\nclasses. We have established type safety for the core model and implemented a\ntype checker with type inference in Haskell."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1502.05094v1", 
    "other_authors": "Christopher A. Stone, Melissa E. O'Neill, Sonja A. Bohr, Adam M. Cozzette, M. Joe DeBlasio, Julia Matsieva, Stuart A. Pernsteiner, Ari D. Schumer", 
    "title": "Observationally Cooperative Multithreading", 
    "arxiv-id": "1502.05094v1", 
    "author": "Ari D. Schumer", 
    "publish": "2015-02-18T01:06:43Z", 
    "summary": "Despite widespread interest in multicore computing, concur- rency models in\nmainstream languages often lead to subtle, error-prone code.\n  Observationally Cooperative Multithreading (OCM) is a new approach to\nshared-memory parallelism. Programmers write code using the well-understood\ncooperative (i.e., nonpreemptive) multithreading model for uniprocessors. OCM\nthen allows threads to run in parallel, so long as results remain consistent\nwith the cooperative model.\n  Programmers benefit because they can reason largely sequentially. Remaining\ninterthread interactions are far less chaotic than in other models, permitting\neasier reasoning and debugging. Programmers can also defer the choice of\nconcurrency-control mechanism (e.g., locks or transactions) until after they\nhave written their programs, at which point they can compare\nconcurrency-control strategies and choose the one that offers the best\nperformance. Implementers and researchers also benefit from the agnostic nature\nof OCM -- it provides a level of abstraction to investigate, compare, and\ncombine a variety of interesting concurrency-control techniques."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1502.06286v1", 
    "other_authors": "Yixiao Lin, Sayan Mitra", 
    "title": "StarL: Towards a Unified Framework for Programming, Simulating and   Verifying Distributed Robotic Systems", 
    "arxiv-id": "1502.06286v1", 
    "author": "Sayan Mitra", 
    "publish": "2015-02-22T23:05:54Z", 
    "summary": "We developed StarL as a framework for programming, simulating, and verifying\ndistributed systems that interacts with physical processes. StarL framework has\n(a) a collection of distributed primitives for coordination, such as mutual\nexclusion, registration and geocast that can be used to build sophisticated\napplications, (b) theory libraries for verifying StarL applications in the PVS\ntheorem prover, and (c) an execution environment that can be used to deploy the\napplications on hardware or to execute them in a discrete event simulator. The\nprimitives have (i) abstract, nondeterministic specifications in terms of\ninvariants, and assume-guarantee style progress properties, (ii)\nimplementations in Java/Android that always satisfy the invariants and attempt\nprogress using best effort strategies. The PVS theories specify the invariant\nand progress properties of the primitives, and have to be appropriately\ninstantiated and composed with the application's state machine to prove\nproperties about the application. We have built two execution environments: one\nfor deploying applications on Android/iRobot Create platform and a second one\nfor simulating large instantiations of the applications in a discrete even\nsimulator. The capabilities are illustrated with a StarL application for\nvehicle to vehicle coordination in a automatic intersection that uses\nprimitives for point-to-point motion, mutual exclusion, and registration."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1503.00375v4", 
    "other_authors": "M. H. van Emden", 
    "title": "The lambda mechanism in lambda calculus and in other calculi", 
    "arxiv-id": "1503.00375v4", 
    "author": "M. H. van Emden", 
    "publish": "2015-03-01T23:27:46Z", 
    "summary": "A comparison of Landin's form of lambda calculus with Church's shows that,\nindependently of the lambda calculus, there exists a mechanism for converting\nfunctions with arguments indexed by variables to the usual kind of function\nwhere the arguments are indexed numerically. We call this the \"lambda\nmechanism\" and show how it can be used in other calculi. In first-order\npredicate logic it can be used to define new functions and new predicates in\nterms of existing ones. In a purely imperative programming language it can be\nused to provide an Algol-like procedure facility."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1503.00622v4", 
    "other_authors": "Pavel Zaichenkov, Olga Tveretina, Alex Shafarenko", 
    "title": "Interface Reconciliation in Kahn Process Networks using CSP and SAT", 
    "arxiv-id": "1503.00622v4", 
    "author": "Alex Shafarenko", 
    "publish": "2015-03-02T17:11:03Z", 
    "summary": "We present a new CSP- and SAT-based approach for coordinating interfaces of\ndistributed stream-connected components provided as closed-source services. The\nKahn Process Network (KPN) is taken as a formal model of computation and a\nMessage Definition Language (MDL) is introduced to describe the format of\nmessages communicated between the processes. MDL links input and output\ninterfaces of a node to support flow inheritance and contextualisation. Since\ninterfaces can also be linked by the existence of a data channel between them,\nthe match is generally not only partial but also substantially nonlocal. The\nKPN communication graph thus becomes a graph of interlocked constraints to be\nsatisfied by specific instances of the variables. We present an algorithm that\nsolves the CSP by iterative approximation while generating an adjunct Boolean\nSAT problem on the way. We developed a solver in OCaml as well as tools that\nanalyse the source code of KPN vertices to derive MDL terms and automatically\nmodify the code by propagating type definitions back to the vertices after the\nCSP has been solved. Techniques and approaches are illustrated on a KPN\nimplementing an image processing algorithm as a running example."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1503.00883v1", 
    "other_authors": "Gianluca Amato, Francesca Scozzari, Helmut Seidl, Kalmer Apinis, Vesal Vojdani", 
    "title": "Efficiently intertwining widening and narrowing", 
    "arxiv-id": "1503.00883v1", 
    "author": "Vesal Vojdani", 
    "publish": "2015-03-03T10:35:47Z", 
    "summary": "Non-trivial analysis problems require posets with infinite ascending and\ndescending chains. In order to compute reasonably precise post-fixpoints of the\nresulting systems of equations, Cousot and Cousot have suggested accelerated\nfixpoint iteration by means of widening and narrowing.\n  The strict separation into phases, however, may unnecessarily give up\nprecision that cannot be recovered later, as over-approximated interim results\nhave to be fully propagated through the equation the system. Additionally,\nclassical two-phased approach is not suitable for equation systems with\ninfinitely many unknowns---where demand driven solving must be used.\nConstruction of an intertwined approach must be able to answer when it is safe\nto apply narrowing---or when widening must be applied. In general, this is a\ndifficult problem. In case the right-hand sides of equations are monotonic,\nhowever, we can always apply narrowing whenever we have reached a post-fixpoint\nfor an equation. The assumption of monotonicity, though, is not met in presence\nof widening. It is also not met by equation systems corresponding to\ncontext-sensitive inter-procedural analysis, possibly combining\ncontext-sensitive analysis of local information with flow-insensitive analysis\nof globals.\n  As a remedy, we present a novel operator that combines a given widening\noperator with a given narrowing operator. We present adapted versions of\nround-robin as well as of worklist iteration, local and side-effecting solving\nalgorithms for the combined operator and prove that the resulting solvers\nalways return sound results and are guaranteed to terminate for monotonic\nsystems whenever only finitely many unknowns (constraint variables) are\nencountered. Practical remedies are proposed for termination in the\nnon-monotonic case."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.176.4", 
    "link": "http://arxiv.org/pdf/1503.04608v1", 
    "other_authors": "Aleksandar S. Dimovski, Claus Brabrand, Andrzej W\u0105sowski", 
    "title": "Variability Abstractions: Trading Precision for Speed in Family-Based   Analyses (Extended Version)", 
    "arxiv-id": "1503.04608v1", 
    "author": "Andrzej W\u0105sowski", 
    "publish": "2015-03-16T11:16:36Z", 
    "summary": "Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is\ncapable of analyzing all valid products (variants) without generating any of\nthem explicitly. It takes as input only the common code base, which encodes all\nvariants of a SPL, and produces analysis results corresponding to all variants.\nHowever, the computational cost of the lifted analysis still depends inherently\non the number of variants (which is exponential in the number of features, in\nthe worst case). For a large number of features, the lifted analysis may be too\ncostly or even infeasible. In this paper, we introduce variability abstractions\ndefined as Galois connections and use abstract interpretation as a formal\nmethod for the calculational-based derivation of approximate (abstracted)\nlifted analyses of SPL programs, which are sound by construction. Moreover,\ngiven an abstraction we define a syntactic transformation that translates any\nSPL program into an abstracted version of it, such that the analysis of the\nabstracted SPL coincides with the corresponding abstracted analysis of the\noriginal SPL. We implement the transformation in a tool, reconfigurator that\nworks on Object-Oriented Java program families, and evaluate the practicality\nof this approach on three Java SPL benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.177.3", 
    "link": "http://arxiv.org/pdf/1503.04908v1", 
    "other_authors": "M\u00e1rio Pereira, Sandra Alves, M\u00e1rio Florido", 
    "title": "Liquid Intersection Types", 
    "arxiv-id": "1503.04908v1", 
    "author": "M\u00e1rio Florido", 
    "publish": "2015-03-17T03:58:44Z", 
    "summary": "We present a new type system combining refinement types and the\nexpressiveness of intersection type discipline. The use of such features makes\nit possible to derive more precise types than in the original refinement\nsystem. We have been able to prove several interesting properties for our\nsystem (including subject reduction) and developed an inference algorithm,\nwhich we proved to be sound."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.177.3", 
    "link": "http://arxiv.org/pdf/1503.05445v1", 
    "other_authors": "Cristina David, Daniel Kroening, Matt Lewis", 
    "title": "Danger Invariants", 
    "arxiv-id": "1503.05445v1", 
    "author": "Matt Lewis", 
    "publish": "2015-03-18T15:13:12Z", 
    "summary": "Static analysers search for overapproximating proofs of safety commonly known\nas safety invariants. Fundamentally, such analysers summarise traces into sets\nof states, thus trading the ability to distinguish traces for computational\ntractability. Conversely, static bug finders (e.g. Bounded Model Checking) give\nevidence for the failure of an assertion in the form of a counterexample, which\ncan be inspected by the user. However, static bug finders fail to scale when\nanalysing programs with bugs that require many iterations of a loop as the\ncomputational effort grows exponentially with the depth of the bug. We propose\na novel approach for finding bugs, which delivers the performance of abstract\ninterpretation together with the concrete precision of BMC. To do this, we\nintroduce the concept of danger invariants -- the dual to safety invariants.\nDanger invariants summarise sets of traces that are guaranteed to reach an\nerror state. This summarisation allows us to find deep bugs without false\nalarms and without explicitly unwinding loops. We present a second-order\nformulation of danger invariants and use the Second-Order SAT solver described\nin previous work to compute danger invariants for intricate programs taken from\nthe literature."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.177.3", 
    "link": "http://arxiv.org/pdf/1503.06061v1", 
    "other_authors": "Jules Hedges", 
    "title": "The selection monad as a CPS transformation", 
    "arxiv-id": "1503.06061v1", 
    "author": "Jules Hedges", 
    "publish": "2015-03-20T13:28:02Z", 
    "summary": "A computation in the continuation monad returns a final result given a\ncontinuation, ie. it is a function with type $(X \\to R) \\to R$. If we instead\nreturn the intermediate result at $X$ then our computation is called a\nselection function. Selection functions appear in diverse areas of mathematics\nand computer science (especially game theory, proof theory and topology) but\nthe existing literature does not heavily emphasise the fact that the selection\nmonad is a CPS translation. In particular it has so far gone unnoticed that the\nselection monad has a call/cc-like operator with interesting similarities and\ndifferences to the usual call/cc, which we explore experimentally using\nHaskell.\n  Selection functions can be used whenever we find the intermediate result more\ninteresting than the final result. For example a SAT solver computes an\nassignment to a boolean function, and then its continuation decides whether it\nis a satisfying assignment, and we find the assignment itself more interesting\nthan the fact that it is or is not satisfying. In game theory we find the move\nchosen by a player more interesting than the outcome that results from that\nmove. The author and collaborators are developing a theory of games in which\nselection functions are viewed as generalised notions of rationality, used to\nmodel players. By realising that strategic contexts in game theory are examples\nof continuations we can see that classical game theory narrowly misses being in\nCPS, and that a small change of viewpoint yields a theory of games that is\nbetter behaved, and especially more compositional."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837637", 
    "link": "http://arxiv.org/pdf/1503.07073v3", 
    "other_authors": "Mark Batty, Alastair F. Donaldson, John Wickerson", 
    "title": "Overhauling SC Atomics in C11 and OpenCL", 
    "arxiv-id": "1503.07073v3", 
    "author": "John Wickerson", 
    "publish": "2015-03-24T15:23:16Z", 
    "summary": "Despite the conceptual simplicity of sequential consistency (SC), the\nsemantics of SC atomic operations and fences in the C11 and OpenCL memory\nmodels is subtle, leading to convoluted prose descriptions that translate to\ncomplex axiomatic formalisations. We conduct an overhaul of SC atomics in C11,\nreducing the associated axioms in both number and complexity. A consequence of\nour simplification is that the SC operations in an execution no longer need to\nbe totally ordered. This relaxation enables, for the first time, efficient and\nexhaustive simulation of litmus tests that use SC atomics. We extend our\nimproved C11 model to obtain the first rigorous memory model formalisation for\nOpenCL (which extends C11 with support for heterogeneous many-core\nprogramming). In the OpenCL setting, we refine the SC axioms still further to\ngive a sensible semantics to SC operations that employ a 'memory scope' to\nrestrict their visibility to specific threads. Our overhaul requires slight\nstrengthenings of both the C11 and the OpenCL memory models, causing some\nbehaviours to become disallowed. We argue that these strengthenings are\nnatural, and that all of the formalised C11 and OpenCL compilation schemes of\nwhich we are aware (Power and x86 CPUs for C11, AMD GPUs for OpenCL) remain\nvalid in our revised models. Using the Herd memory model simulator, we show\nthat our overhaul leads to an exponential improvement in simulation time for\nC11 litmus tests compared with the original model, making exhaustive simulation\ncompetitive, time-wise, with the non-exhaustive CDSChecker tool."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814305", 
    "link": "http://arxiv.org/pdf/1503.07792v5", 
    "other_authors": "Matthew A. Hammer, Joshua Dunfield, Kyle Headley, Nicholas Labich, Jeffrey S. Foster, Michael Hicks, David Van Horn", 
    "title": "Incremental Computation with Names", 
    "arxiv-id": "1503.07792v5", 
    "author": "David Van Horn", 
    "publish": "2015-03-26T17:13:34Z", 
    "summary": "Over the past thirty years, there has been significant progress in developing\ngeneral-purpose, language-based approaches to incremental computation, which\naims to efficiently update the result of a computation when an input is\nchanged. A key design challenge in such approaches is how to provide efficient\nincremental support for a broad range of programs. In this paper, we argue that\nfirst-class names are a critical linguistic feature for efficient incremental\ncomputation. Names identify computations to be reused across differing runs of\na program, and making them first class gives programmers a high level of\ncontrol over reuse. We demonstrate the benefits of names by presenting NOMINAL\nADAPTON, an ML-like language for incremental computation with names. We\ndescribe how to use NOMINAL ADAPTON to efficiently incrementalize several\nstandard programming patterns---including maps, folds, and unfolds---and show\nhow to build efficient, incremental probabilistic trees and tries. Since\nNOMINAL ADAPTON's implementation is subtle, we formalize it as a core calculus\nand prove it is from-scratch consistent, meaning it always produces the same\nanswer as simply re-running the computation. Finally, we demonstrate that\nNOMINAL ADAPTON can provide large speedups over both from-scratch computation\nand ADAPTON, a previous state-of-the-art incremental computation system."
},{
    "category": "cs.PL", 
    "doi": "10.4230/LIPIcs.ECOOP.2016.3", 
    "link": "http://arxiv.org/pdf/1503.08623v3", 
    "other_authors": "Edd Barrett, Carl Friedrich Bolz, Lukas Diekmann, Laurence Tratt", 
    "title": "Fine-grained Language Composition: A Case Study", 
    "arxiv-id": "1503.08623v3", 
    "author": "Laurence Tratt", 
    "publish": "2015-03-30T10:10:18Z", 
    "summary": "Although run-time language composition is common, it normally takes the form\nof a crude Foreign Function Interface (FFI). While useful, such compositions\ntend to be coarse-grained and slow. In this paper we introduce a novel\nfine-grained syntactic composition of PHP and Python which allows users to\nembed each language inside the other, including referencing variables across\nlanguages. This composition raises novel design and implementation challenges.\nWe show that good solutions can be found to the design challenges; and that the\nresulting implementation imposes an acceptable performance overhead of, at\nmost, 2.6x."
},{
    "category": "cs.PL", 
    "doi": "10.4230/LIPIcs.ECOOP.2016.3", 
    "link": "http://arxiv.org/pdf/1503.08665v2", 
    "other_authors": "Sigurd Schneider, Gert Smolka, Sebastian Hack", 
    "title": "A Linear First-Order Functional Intermediate Language for Verified   Compilers", 
    "arxiv-id": "1503.08665v2", 
    "author": "Sebastian Hack", 
    "publish": "2015-03-30T13:31:38Z", 
    "summary": "We present the linear first-order intermediate language IL for verified\ncompilers. IL is a functional language with calls to a nondeterministic\nenvironment. We give IL terms a second, imperative semantic interpretation and\nobtain a register transfer language. For the imperative interpretation we\nestablish a notion of live variables. Based on live variables, we formulate a\ndecidable property called coherence ensuring that the functional and the\nimperative interpretation of a term coincide. We formulate a register\nassignment algorithm for IL and prove its correctness. The algorithm translates\na functional IL program into an equivalent imperative IL program. Correctness\nfollows from the fact that the algorithm reaches a coherent program after\nconsistently renaming local variables. We prove that the maximal number of live\nvariables in the initial program bounds the number of different variables in\nthe final coherent program. The entire development is formalized in Coq."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814270.2814294", 
    "link": "http://arxiv.org/pdf/1503.09006v2", 
    "other_authors": "Martin Aigner, Christoph M. Kirsch, Michael Lippautz, Ana Sokolova", 
    "title": "Fast, Multicore-Scalable, Low-Fragmentation Memory Allocation through   Large Virtual Memory and Global Data Structures", 
    "arxiv-id": "1503.09006v2", 
    "author": "Ana Sokolova", 
    "publish": "2015-03-31T11:36:46Z", 
    "summary": "We demonstrate that general-purpose memory allocation involving many threads\non many cores can be done with high performance, multicore scalability, and low\nmemory consumption. For this purpose, we have designed and implemented scalloc,\na concurrent allocator that generally performs and scales in our experiments\nbetter than other allocators while using less memory, and is still competitive\notherwise. The main ideas behind the design of scalloc are: uniform treatment\nof small and big objects through so-called virtual spans, efficiently and\neffectively reclaiming free memory through fast and scalable global data\nstructures, and constant-time (modulo synchronization) allocation and\ndeallocation operations that trade off memory reuse and spatial locality\nwithout being subject to false sharing."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19282-6_14", 
    "link": "http://arxiv.org/pdf/1503.09097v1", 
    "other_authors": "Marino Miculan, Marco Peressotti, Andrea Toneguzzo", 
    "title": "Open Transactions on Shared Memory", 
    "arxiv-id": "1503.09097v1", 
    "author": "Andrea Toneguzzo", 
    "publish": "2015-03-31T15:49:55Z", 
    "summary": "Transactional memory has arisen as a good way for solving many of the issues\nof lock-based programming. However, most implementations admit isolated\ntransactions only, which are not adequate when we have to coordinate\ncommunicating processes. To this end, in this paper we present OCTM, an\nHaskell-like language with open transactions over shared transactional memory:\nprocesses can join transactions at runtime just by accessing to shared\nvariables. Thus a transaction can co-operate with the environment through\nshared variables, but if it is rolled-back, also all its effects on the\nenvironment are retracted. For proving the expressive power of TCCS we give an\nimplementation of TCCS, a CCS-like calculus with open transactions."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19282-6_14", 
    "link": "http://arxiv.org/pdf/1504.00198v1", 
    "other_authors": "Friedrich Gretz, Nils Jansen, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Annabelle McIver, Federico Olmedo", 
    "title": "Conditioning in Probabilistic Programming", 
    "arxiv-id": "1504.00198v1", 
    "author": "Federico Olmedo", 
    "publish": "2015-04-01T12:29:10Z", 
    "summary": "We investigate the semantic intricacies of conditioning, a main feature in\nprobabilistic programming. We provide a weakest (liberal) pre-condition (w(l)p)\nsemantics for the elementary probabilistic programming language pGCL extended\nwith conditioning. We prove that quantitative weakest (liberal) pre-conditions\ncoincide with conditional (liberal) expected rewards in Markov chains and show\nthat semantically conditioning is a truly conservative extension. We present\ntwo program transformations which entirely eliminate conditioning from any\nprogram and prove their correctness using the w(l)p-semantics. Finally, we show\nhow the w(l)p-semantics can be used to determine conditional probabilities in a\nparametric anonymity protocol and show that an inductive w(l)p-semantics for\nconditioning in non-deterministic probabilistic programs cannot exist."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19282-6_14", 
    "link": "http://arxiv.org/pdf/1504.00527v1", 
    "other_authors": "Francis Sergeraert", 
    "title": "Functional Programming is Free", 
    "arxiv-id": "1504.00527v1", 
    "author": "Francis Sergeraert", 
    "publish": "2015-04-02T12:40:18Z", 
    "summary": "A paper has recently been published in SIAM-JC. This paper is faulty: 1) The\nstandard requirements about the definition of an algorithm are not respected,\n2) The main point in the complexity study, namely the functional programming\ncomponent, is absent. The Editorial Board of the SIAM JC had been warned a\nconfirmed publication would be openly commented, it is the role of this text."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19282-6_14", 
    "link": "http://arxiv.org/pdf/1504.00693v1", 
    "other_authors": "Zakaria Alomari, Oualid El Halimi, Kaushik Sivaprasad, Chitrang Pandit", 
    "title": "Comparative Studies of Six Programming Languages", 
    "arxiv-id": "1504.00693v1", 
    "author": "Chitrang Pandit", 
    "publish": "2015-04-02T21:14:38Z", 
    "summary": "Comparison of programming languages is a common topic of discussion among\nsoftware engineers. Multiple programming languages are designed, specified, and\nimplemented every year in order to keep up with the changing programming\nparadigms, hardware evolution, etc. In this paper we present a comparative\nstudy between six programming languages: C++, PHP, C#, Java, Python, VB ; These\nlanguages are compared under the characteristics of reusability, reliability,\nportability, availability of compilers and tools, readability, efficiency,\nfamiliarity and expressiveness."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19686-2_4", 
    "link": "http://arxiv.org/pdf/1504.00977v1", 
    "other_authors": "Sergii Dymchenko, Mariia Mykhailova", 
    "title": "Declaratively solving Google Code Jam problems with Picat", 
    "arxiv-id": "1504.00977v1", 
    "author": "Mariia Mykhailova", 
    "publish": "2015-04-04T03:30:22Z", 
    "summary": "In this paper we present several examples of solving algorithmic problems\nfrom the Google Code Jam programming contest with Picat programming language\nusing declarative techniques: constraint logic programming and tabled logic\nprogramming. In some cases the use of Picat simplifies the implementation\ncompared to conventional imperative programming languages, while in others it\nallows to directly convert the problem statement into an efficiently solvable\ndeclarative problem specification without inventing an imperative algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-19686-2_4", 
    "link": "http://arxiv.org/pdf/1504.01842v5", 
    "other_authors": "Hendra Gunadi, Alwen Tiu, Rajeev Gore", 
    "title": "Formal Certification of Android Bytecode", 
    "arxiv-id": "1504.01842v5", 
    "author": "Rajeev Gore", 
    "publish": "2015-04-08T06:24:38Z", 
    "summary": "Android is an operating system that has been used in a majority of mobile\ndevices. Each application in Android runs in an instance of the Dalvik virtual\nmachine, which is a register-based virtual machine (VM). Most applications for\nAndroid are developed using Java, compiled to Java bytecode and then translated\nto DEX bytecode using the dx tool in the Android SDK. In this work, we aim to\ndevelop a type-based method for certifying non-interference properties of DEX\nbytecode, following a methodology that has been developed for Java bytecode\ncertification by Barthe et al. To this end, we develop a formal operational\nsemantics of the Dalvik VM, a type system for DEX bytecode, and prove the\nsoundness of the type system with respect to a notion of non-interference. We\nthen study the translation process from Java bytecode to DEX bytecode, as\nimplemented in the dx tool in the Android SDK. We show that an abstracted\nversion of the translation from Java bytecode to DEX bytecode preserves the\nnon-interference property. More precisely, we show that if the Java bytecode is\ntypable in Barthe et al's type system (which guarantees non-interference) then\nits translation is typable in our type system. This result opens up the\npossibility to leverage existing bytecode verifiers for Java to certify\nnon-interference properties of Android bytecode."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.179.3", 
    "link": "http://arxiv.org/pdf/1504.02603v1", 
    "other_authors": "Wolfram Kahl", 
    "title": "A Simple Parallel Implementation of Interaction Nets in Haskell", 
    "arxiv-id": "1504.02603v1", 
    "author": "Wolfram Kahl", 
    "publish": "2015-04-10T09:31:32Z", 
    "summary": "Due to their \"inherent parallelism\", interaction nets have since their\nintroduction been considered as an attractive implementation mechanism for\nfunctional programming. We show that a simple highly-concurrent implementation\nin Haskell can achieve promising speed-ups on multiple cores."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.02621v1", 
    "other_authors": "Christopher Bak, Glyn Faulkner, Detlef Plump, Colin Runciman", 
    "title": "A Reference Interpreter for the Graph Programming Language GP 2", 
    "arxiv-id": "1504.02621v1", 
    "author": "Colin Runciman", 
    "publish": "2015-04-10T09:50:23Z", 
    "summary": "GP 2 is an experimental programming language for computing by graph\ntransformation. An initial interpreter for GP 2, written in the functional\nlanguage Haskell, provides a concise and simply structured reference\nimplementation. Despite its simplicity, the performance of the interpreter is\nsufficient for the comparative investigation of a range of test programs. It\nalso provides a platform for the development of more sophisticated\nimplementations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.03239v1", 
    "other_authors": "Rekha R Pai", 
    "title": "Global Value Numbering: A Precise and Efficient Algorithm", 
    "arxiv-id": "1504.03239v1", 
    "author": "Rekha R Pai", 
    "publish": "2015-04-13T16:14:43Z", 
    "summary": "Global Value Numbering (GVN) is an important static analysis to detect\nequivalent expressions in a program. We present an iterative data-flow analysis\nGVN algorithm in SSA for the purpose of detecting total redundancies. The\ncentral challenge is defining a join operation to detect equivalences at a join\npoint in polynomial time such that later occurrences of redundant expressions\ncould be detected. For this purpose, we introduce the novel concept of value\n$\\phi$-function. We claim the algorithm is precise and takes only polynomial\ntime."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.05372v1", 
    "other_authors": "Wim Vanderbauwhede", 
    "title": "Inferring Program Transformations from Type Transformations for   Partitioning of Ordered Sets", 
    "arxiv-id": "1504.05372v1", 
    "author": "Wim Vanderbauwhede", 
    "publish": "2015-04-21T10:19:57Z", 
    "summary": "In this paper I introduce a mechanism to derive program transforma- tions\nfrom order-preserving transformations of vector types. The purpose of this work\nis to allow automatic generation of correct-by-construction instances of\nprograms in a streaming data processing paradigm suitable for FPGA processing.\nWe show that for it is possible to automatically derive instances for programs\nbased on combinations of opaque element- processing functions combined using\nfoldl and map, purely from the type transformations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.07680v3", 
    "other_authors": "Joshua Dunfield", 
    "title": "Elaborating Evaluation-Order Polymorphism", 
    "arxiv-id": "1504.07680v3", 
    "author": "Joshua Dunfield", 
    "publish": "2015-04-28T22:50:15Z", 
    "summary": "We classify programming languages according to evaluation order: each\nlanguage fixes one evaluation order as the default, making it transparent to\nprogram in that evaluation order, and troublesome to program in the other.\n  This paper develops a type system that is impartial with respect to\nevaluation order. Evaluation order is implicit in terms, and explicit in types,\nwith by-value and by-name versions of type connectives. A form of intersection\ntype quantifies over evaluation orders, describing code that is agnostic over\n(that is, polymorphic in) evaluation order. By allowing such generic code,\nprograms can express the by-value and by-name versions of a computation without\ncode duplication.\n  We also formulate a type system that only has by-value connectives, plus a\ntype that generalizes the difference between by-value and by-name connectives:\nit is either a suspension (by name) or a \"no-op\" (by value). We show a\nstraightforward encoding of the impartial type system into the more economical\none. Then we define an elaboration from the economical language to a\ncall-by-value semantics, and prove that elaborating a well-typed source\nprogram, where evaluation order is implicit, produces a well-typed target\nprogram where evaluation order is explicit. We also prove a simulation between\nevaluation of the target program and reductions (either by-value or by-name) in\nthe source program.\n  Finally, we prove that typing, elaboration, and evaluation are faithful to\nthe type annotations given in the source program: if the programmer only writes\nby-value types, no by-name reductions can occur at run time."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.07862v1", 
    "other_authors": "Marek Trtik", 
    "title": "Anonymous On-line Communication Between Program Analyses", 
    "arxiv-id": "1504.07862v1", 
    "author": "Marek Trtik", 
    "publish": "2015-04-29T13:58:42Z", 
    "summary": "We propose a light-weight client-server model of communication between\nexisting implementations of different program analyses. The communication is\non-line and anonymous which means that all analyses simultaneously analyse the\nsame program and an analysis does not know what other analyses participate in\nthe communication. The anonymity and model's strong emphasis on independence of\nanalyses allow to preserve almost everything in existing implementations. An\nanalysis only has to add an implementation of a proposed communication\nprotocol, determine places in its code where information from others would\nhelp, and then check whether there is no communication scenario, which would\ncorrupt its result. We demonstrate functionality and effectiveness of the\nproposed communication model in a detailed case study with three analyses: two\nabstract interpreters and the classic symbolic execution. Results of the\nevaluation on SV-COMP benchmarks show impressive improvements in computed\ninvariants and increased counts of successfully analysed benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.08033v1", 
    "other_authors": "James Ian Johnson", 
    "title": "Automating Abstract Interpretation of Abstract Machines", 
    "arxiv-id": "1504.08033v1", 
    "author": "James Ian Johnson", 
    "publish": "2015-04-29T21:46:52Z", 
    "summary": "Static program analysis is a valuable tool for any programming language that\npeople write programs in. The prevalence of scripting languages in the world\nsuggests programming language interpreters are relatively easy to write. Users\nof these languages lament their inability to analyze their code, therefore\nprogramming language analyzers are not easy to write. This thesis investigates\na systematic method of creating abstract interpreters from traditional\ninterpreters, called Abstracting Abstract Machines.\n  Abstract interpreters are difficult to develop due to technical, theoretical,\nand pragmatic problems. Technical problems include engineering data structures\nand algorithms. I show that modest and simple changes to the mathematical\npresentation of abstract machines result in 1000 times better running time -\njust seconds for moderately sized programs.\n  In the theoretical realm, abstraction can make correctness difficult to\nascertain. I provide proof techniques for proving the correctness of regular,\npushdown, and stack-inspecting pushdown models of abstract computation by\nleaving computational power to an external factor: allocation. Even if we don't\ntrust the proof, we can run models concretely against test suites to better\ntrust them.\n  In the pragmatic realm, I show that the systematic process of abstracting\nabstract machines is automatable. I develop a meta-language for expressing\nabstract machines similar to other semantics engineering languages. The\nlanguage's special feature is that it provides an interface to abstract\nallocation. The semantics guarantees that if allocation is finite, then the\nsemantics is a sound and computable approximation of the concrete semantics."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.08039v1", 
    "other_authors": "Panagiotis Vekris, Benjamin Cosman, Ranjit Jhala", 
    "title": "Trust, but Verify: Two-Phase Typing for Dynamic Languages", 
    "arxiv-id": "1504.08039v1", 
    "author": "Ranjit Jhala", 
    "publish": "2015-04-29T22:31:14Z", 
    "summary": "A key challenge when statically typing so-called dynamic languages is the\nubiquity of value-based overloading, where a given function can dynamically\nreflect upon and behave according to the types of its arguments. Thus, to\nestablish basic types, the analysis must reason precisely about values, but in\nthe presence of higher-order functions and polymorphism, this reasoning itself\ncan require basic types. In this paper we address this chicken-and-egg problem\nby introducing the framework of two-phased typing. The first \"trust\" phase\nperforms classical, i.e. flow-, path- and value-insensitive type checking to\nassign basic types to various program expressions. When the check inevitably\nruns into \"errors\" due to value-insensitivity, it wraps problematic expressions\nwith DEAD-casts, which explicate the trust obligations that must be discharged\nby the second phase. The second phase uses refinement typing, a flow- and\npath-sensitive analysis, that decorates the first phase's types with logical\npredicates to track value relationships and thereby verify the casts and\nestablish other correctness properties for dynamically typed languages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.08100v1", 
    "other_authors": "Matthias Keil, Sankha Narayan Guria, Andreas Schlegel, Manuel Geffken, Peter Thiemann", 
    "title": "Transparent Object Proxies for JavaScript", 
    "arxiv-id": "1504.08100v1", 
    "author": "Peter Thiemann", 
    "publish": "2015-04-30T07:20:13Z", 
    "summary": "Proxies are the swiss army knives of object adaptation. They introduce a\nlevel of indirection to intercept select operations on a target object and\ndivert them as method calls to a handler. Proxies have many uses like\nimplementing access control, enforcing contracts, virtualizing resources.\n  One important question in the design of a proxy API is whether a proxy object\nshould inherit the identity of its target. Apparently proxies should have their\nown identity for security-related applications whereas other applications, in\nparticular contract systems, require transparent proxies that compare equal to\ntheir target objects.\n  We examine the issue with transparency in various use cases for proxies,\ndiscuss different approaches to obtain transparency, and propose two designs\nthat require modest modifications in the JavaScript engine and cannot be\nbypassed by the programmer.\n  We implement our designs in the SpiderMonkey JavaScript interpreter and\nbytecode compiler. Our evaluation shows that these modifications of have no\nstatistically significant impact on the benchmark performance of the JavaScript\nengine. Furthermore, we demonstrate that contract systems based on wrappers\nrequire transparent proxies to avoid interference with program execution in\nrealistic settings."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1504.08110v1", 
    "other_authors": "Matthias Keil, Peter Thiemann", 
    "title": "TreatJS: Higher-Order Contracts for JavaScript", 
    "arxiv-id": "1504.08110v1", 
    "author": "Peter Thiemann", 
    "publish": "2015-04-30T08:18:29Z", 
    "summary": "TreatJS is a language embedded, higher-order contract system for JavaScript\nwhich enforces contracts by run-time monitoring. Beyond providing the standard\nabstractions for building higher-order contracts (base, function, and object\ncontracts), TreatJS's novel contributions are its guarantee of non-interfering\ncontract execution, its systematic approach to blame assignment, its support\nfor contracts in the style of union and intersection types, and its notion of a\nparameterized contract scope, which is the building block for composable\nrun-time generated contracts that generalize dependent function contracts.\n  TreatJS is implemented as a library so that all aspects of a contract can be\nspecified using the full JavaScript language. The library relies on JavaScript\nproxies to guarantee full interposition for contracts. It further exploits\nJavaScript's reflective features to run contracts in a sandbox environment,\nwhich guarantees that the execution of contract code does not modify the\napplication state. No source code transformation or change in the JavaScript\nrun-time system is required.\n  The impact of contracts on execution speed is evaluated using the Google\nOctane benchmark."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.00017v1", 
    "other_authors": "Tyler Hannan, Chester Holtz, Jonathan Liao", 
    "title": "Comparative Analysis of Classic Garbage-Collection Algorithms for a   Lisp-like Language", 
    "arxiv-id": "1505.00017v1", 
    "author": "Jonathan Liao", 
    "publish": "2015-04-30T20:16:47Z", 
    "summary": "In this paper, we demonstrate the effectiveness of Cheney's Copy Algorithm\nfor a Lisp-like system and experimentally show the infeasability of developing\nan optimal garbage collector for general use. We summarize and compare several\ngarbage-collection algorithms including Cheney's Algorithm, the canonical Mark\nand Sweep Algorithm, and Knuth's Classical Lisp 2 Algorithm. We implement and\nanalyze these three algorithms in the context of a custom MicroLisp\nenvironment. We conclude and present the core considerations behind the\ndevelopment of a garbage collector---specifically for Lisp---and make an\nattempt to investigate these issues in depth. We also discuss experimental\nresults that imply the effectiveness of Cheney's algorithm over Mark-Sweep for\nLisp-like languages."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.02298v2", 
    "other_authors": "Alexander Bakst, Ranjit Jhala", 
    "title": "Predicate Abstraction for Linked Data Structures", 
    "arxiv-id": "1505.02298v2", 
    "author": "Ranjit Jhala", 
    "publish": "2015-05-09T17:52:27Z", 
    "summary": "We present Alias Refinement Types (ART), a new approach to the verification\nof correctness properties of linked data structures. While there are many\ntechniques for checking that a heap-manipulating program adheres to its\nspecification, they often require that the programmer annotate the behavior of\neach procedure, for example, in the form of loop invariants and pre- and\npost-conditions. Predicate abstraction would be an attractive abstract domain\nfor performing invariant inference, existing techniques are not able to reason\nabout the heap with enough precision to verify functional properties of data\nstructure manipulating programs. In this paper, we propose a technique that\nlifts predicate abstraction to the heap by factoring the analysis of data\nstructures into two orthogonal components: (1) Alias Types, which reason about\nthe physical shape of heap structures, and (2) Refinement Types, which use\nsimple predicates from an SMT decidable theory to capture the logical or\nsemantic properties of the structures. We prove ART sound by translating types\ninto separation logic assertions, thus translating typing derivations in ART\ninto separation logic proofs. We evaluate ART by implementing a tool that\nperforms type inference for an imperative language, and empirically show, using\na suite of data-structure benchmarks, that ART requires only 21% of the\nannotations needed by other state-of-the-art verification techniques."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.02642v1", 
    "other_authors": "Hamid Ebadi, David Sands", 
    "title": "Featherweight PINQ", 
    "arxiv-id": "1505.02642v1", 
    "author": "David Sands", 
    "publish": "2015-05-11T14:40:29Z", 
    "summary": "Differentially private mechanisms enjoy a variety of composition properties.\nLeveraging these, McSherry introduced PINQ (SIGMOD 2009), a system empowering\nnon-experts to construct new differentially private analyses. PINQ is an\nLINQ-like API which provides automatic privacy guarantees for all programs\nwhich use it to mediate sensitive data manipulation. In this work we introduce\nfeatherweight PINQ, a formal model capturing the essence of PINQ. We prove that\nany program interacting with featherweight PINQ's API is differentially\nprivate."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.02878v2", 
    "other_authors": "Kodai Hashimoto, Hiroshi Unno", 
    "title": "Refinement Type Inference via Horn Constraint Optimization", 
    "arxiv-id": "1505.02878v2", 
    "author": "Hiroshi Unno", 
    "publish": "2015-05-12T05:38:53Z", 
    "summary": "We propose a novel method for inferring refinement types of higher-order\nfunctional programs. The main advantage of the proposed method is that it can\ninfer maximally preferred (i.e., Pareto optimal) refinement types with respect\nto a user-specified preference order. The flexible optimization of refinement\ntypes enabled by the proposed method paves the way for interesting\napplications, such as inferring most-general characterization of inputs for\nwhich a given program satisfies (or violates) a given safety (or termination)\nproperty. Our method reduces such a type optimization problem to a Horn\nconstraint optimization problem by using a new refinement type system that can\nflexibly reason about non-determinism in programs. Our method then solves the\nconstraint optimization problem by repeatedly improving a current solution\nuntil convergence via template-based invariant generation. We have implemented\na prototype inference system based on our method, and obtained promising\nresults in preliminary experiments."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.04533v1", 
    "other_authors": "Pavol \u010cern\u00fd, Edmund M. Clarke, Thomas A. Henzinger, Arjun Radhakrishna, Leonid Ryzhyk, Roopsha Samanta, Thorsten Tarrach", 
    "title": "From Non-preemptive to Preemptive Scheduling using Synchronization   Synthesis", 
    "arxiv-id": "1505.04533v1", 
    "author": "Thorsten Tarrach", 
    "publish": "2015-05-18T07:25:17Z", 
    "summary": "We present a computer-aided programming approach to concurrency. The approach\nallows programmers to program assuming a friendly, non-preemptive scheduler,\nand our synthesis procedure inserts synchronization to ensure that the final\nprogram works even with a preemptive scheduler. The correctness specification\nis implicit, inferred from the non-preemptive behavior. Let us consider\nsequences of calls that the program makes to an external interface. The\nspecification requires that any such sequence produced under a preemptive\nscheduler should be included in the set of such sequences produced under a\nnon-preemptive scheduler. The solution is based on a finitary abstraction, an\nalgorithm for bounded language inclusion modulo an independence relation, and\nrules for inserting synchronization. We apply the approach to device-driver\nprogramming, where the driver threads call the software interface of the device\nand the API provided by the operating system. Our experiments demonstrate that\nour synthesis method is precise and efficient, and, since it does not require\nexplicit specifications, is more practical than the conventional approach based\non user-provided assertions."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.05868v1", 
    "other_authors": "Rajeev Alur, Pavol Cerny, Arjun Radhakrishna", 
    "title": "Synthesis through Unification", 
    "arxiv-id": "1505.05868v1", 
    "author": "Arjun Radhakrishna", 
    "publish": "2015-05-21T19:58:46Z", 
    "summary": "Given a specification and a set of candidate programs (program space), the\nprogram synthesis problem is to find a candidate program that satisfies the\nspecification. We present the synthesis through unification (STUN) approach,\nwhich is an extension of the counter-example guided inductive synthesis (CEGIS)\napproach. In CEGIS, the synthesizer maintains a subset S of inputs and a\ncandidate program Prog that is correct for S. The synthesizer repeatedly checks\nif there exists a counter-example input c such that the execution of Prog is\nincorrect on c. If so, the synthesizer enlarges S to include c, and picks a\nprogram from the program space that is correct for the new set S.\n  The STUN approach extends CEGIS with the idea that given a program Prog that\nis correct for a subset of inputs, the synthesizer can try to find a program\nProg' that is correct for the rest of the inputs. If Prog and Prog' can be\nunified into a program in the program space, then a solution has been found. We\npresent a generic synthesis procedure based on the STUN approach and specialize\nit for three different domains by providing the appropriate unification\noperators. We implemented these specializations in prototype tools, and we show\nthat our tools often per- forms significantly better on standard benchmarks\nthan a tool based on a pure CEGIS approach."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.181.4", 
    "link": "http://arxiv.org/pdf/1505.06003v1", 
    "other_authors": "Julien Ponge, Fr\u00e9d\u00e9ric Le Mou\u00ebl, Nicolas Stouls, Yannick Loiseau", 
    "title": "Opportunities for a Truffle-based Golo Interpreter", 
    "arxiv-id": "1505.06003v1", 
    "author": "Yannick Loiseau", 
    "publish": "2015-05-22T09:29:05Z", 
    "summary": "Golo is a simple dynamically-typed language for the Java Virtual Machine.\nInitially implemented as a ahead-of-time compiler to JVM bytecode, it leverages\ninvokedy-namic and JSR 292 method handles to implement a reasonably efficient\nruntime. Truffle is emerging as a framework for building interpreters for JVM\nlanguages with self-specializing AST nodes. Combined with the Graal compiler,\nTruffle offers a simple path towards writing efficient interpreters while\nkeeping the engineering efforts balanced. The Golo project is interested in\nexperimenting with a Truffle interpreter in the future, as it would provides\ninteresting comparison elements between invokedynamic versus Truffle for\nbuilding a language runtime."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2016.01.002", 
    "link": "http://arxiv.org/pdf/1505.07368v1", 
    "other_authors": "Dominik Charousset, Raphael Hiesgen, Thomas C. Schmidt", 
    "title": "Revisiting Actor Programming in C++", 
    "arxiv-id": "1505.07368v1", 
    "author": "Thomas C. Schmidt", 
    "publish": "2015-05-27T15:16:57Z", 
    "summary": "The actor model of computation has gained significant popularity over the\nlast decade. Its high level of abstraction makes it appealing for concurrent\napplications in parallel and distributed systems. However, designing a\nreal-world actor framework that subsumes full scalability, strong reliability,\nand high resource efficiency requires many conceptual and algorithmic additives\nto the original model.\n  In this paper, we report on designing and building CAF, the \"C++ Actor\nFramework\". CAF targets at providing a concurrent and distributed native\nenvironment for scaling up to very large, high-performance applications, and\nequally well down to small constrained systems. We present the key\nspecifications and design concepts---in particular a message-transparent\narchitecture, type-safe message interfaces, and pattern matching\nfacilities---that make native actors a viable approach for many robust,\nelastic, and highly distributed developments. We demonstrate the feasibility of\nCAF in three scenarios: first for elastic, upscaling environments, second for\nincluding heterogeneous hardware like GPGPUs, and third for distributed runtime\nsystems. Extensive performance evaluations indicate ideal runtime behaviour for\nup to 64 cores at very low memory footprint, or in the presence of GPUs. In\nthese tests, CAF continuously outperforms the competing actor environments\nErlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2016.01.002", 
    "link": "http://arxiv.org/pdf/1505.07375v1", 
    "other_authors": "Hong-Yi Dai", 
    "title": "The Mysteries of Lisp -- I: The Way to S-expression Lisp", 
    "arxiv-id": "1505.07375v1", 
    "author": "Hong-Yi Dai", 
    "publish": "2015-05-26T04:16:50Z", 
    "summary": "Despite its old age, Lisp remains mysterious to many of its admirers. The\nmysteries on one hand fascinate the language, on the other hand also obscure\nit. Following Stoyan but paying attention to what he has neglected or omitted,\nin this first essay of a series intended to unravel these mysteries, we trace\nthe development of Lisp back to its origin, revealing how the language has\nevolved into its nowadays look and feel. The insights thus gained will not only\nenhance existent understanding of the language but also inspires further\nimprovement of it."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2016.01.002", 
    "link": "http://arxiv.org/pdf/1505.07383v1", 
    "other_authors": "Brian Anderson, Lars Bergstrom, David Herman, Josh Matthews, Keegan McAllister, Manish Goregaokar, Jack Moffitt, Simon Sapin", 
    "title": "Experience Report: Developing the Servo Web Browser Engine using Rust", 
    "arxiv-id": "1505.07383v1", 
    "author": "Simon Sapin", 
    "publish": "2015-05-26T18:49:02Z", 
    "summary": "All modern web browsers - Internet Explorer, Firefox, Chrome, Opera, and\nSafari - have a core rendering engine written in C++. This language choice was\nmade because it affords the systems programmer complete control of the\nunderlying hardware features and memory in use, and it provides a transparent\ncompilation model.\n  Servo is a project started at Mozilla Research to build a new web browser\nengine that preserves the capabilities of these other browser engines but also\nboth takes advantage of the recent trends in parallel hardware and is more\nmemory-safe. We use a new language, Rust, that provides us a similar level of\ncontrol of the underlying system to C++ but which builds on many concepts\nfamiliar to the functional programming community, forming a novelty - a useful,\nsafe systems programming language.\n  In this paper, we show how a language with an affine type system, regions,\nand many syntactic features familiar to functional language programmers can be\nsuccessfully used to build state-of-the-art systems software. We also outline\nseveral pitfalls encountered along the way and describe some potential areas\nfor future research."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cl.2016.01.002", 
    "link": "http://arxiv.org/pdf/1506.01949v1", 
    "other_authors": "Norman Danner, Daniel R. Licata, Ramyaa Ramyaa", 
    "title": "Denotational cost semantics for functional languages with inductive   types", 
    "arxiv-id": "1506.01949v1", 
    "author": "Ramyaa Ramyaa", 
    "publish": "2015-06-05T15:57:26Z", 
    "summary": "A central method for analyzing the asymptotic complexity of a functional\nprogram is to extract and then solve a recurrence that expresses evaluation\ncost in terms of input size. The relevant notion of input size is often\nspecific to a datatype, with measures including the length of a list, the\nmaximum element in a list, and the height of a tree. In this work, we give a\nformal account of the extraction of cost and size recurrences from higher-order\nfunctional programs over inductive datatypes. Our approach allows a wide range\nof programmer-specified notions of size, and ensures that the extracted\nrecurrences correctly predict evaluation cost. To extract a recurrence from a\nprogram, we first make costs explicit by applying a monadic translation from\nthe source language to a complexity language, and then abstract datatype values\nas sizes. Size abstraction can be done semantically, working in models of the\ncomplexity language, or syntactically, by adding rules to a preorder judgement.\nWe give several different models of the complexity language, which support\ndifferent notions of size. Additionally, we prove by a logical relations\nargument that recurrences extracted by this process are upper bounds for\nevaluation cost; the proof is entirely syntactic and therefore applies to all\nof the models we consider."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2816707.2816710", 
    "link": "http://arxiv.org/pdf/1506.04205v2", 
    "other_authors": "\u00c9ric Tanter, Nicolas Tabareau", 
    "title": "Gradual Certified Programming in Coq", 
    "arxiv-id": "1506.04205v2", 
    "author": "Nicolas Tabareau", 
    "publish": "2015-06-13T00:45:35Z", 
    "summary": "Expressive static typing disciplines are a powerful way to achieve\nhigh-quality software. However, the adoption cost of such techniques should not\nbe under-estimated. Just like gradual typing allows for a smooth transition\nfrom dynamically-typed to statically-typed programs, it seems desirable to\nsupport a gradual path to certified programming. We explore gradual certified\nprogramming in Coq, providing the possibility to postpone the proofs of\nselected properties, and to check \"at runtime\" whether the properties actually\nhold. Casts can be integrated with the implicit coercion mechanism of Coq to\nsupport implicit cast insertion a la gradual typing. Additionally, when\nextracting Coq functions to mainstream languages, our encoding of casts\nsupports lifting assumed properties into runtime checks. Much to our surprise,\nit is not necessary to extend Coq in any way to support gradual certified\nprogramming. A simple mix of type classes and axioms makes it possible to bring\ngradual certified programming to Coq in a straightforward manner."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2816707.2816710", 
    "link": "http://arxiv.org/pdf/1506.04498v1", 
    "other_authors": "Satoshi Egi", 
    "title": "Egison: Non-Linear Pattern-Matching against Non-Free Data Types", 
    "arxiv-id": "1506.04498v1", 
    "author": "Satoshi Egi", 
    "publish": "2015-06-15T07:48:14Z", 
    "summary": "This paper introduces the Egison programming language whose feature is strong\npattern-matching facility against not only algebraic data types but also\nnon-free data types whose data have multiple ways of representation such as\nsets and graphs. Our language supports multiple occurrences of the same\nvariables in a pattern, multiple results of pattern-matching, polymorphism of\npattern-constructors and loop-patterns, patterns that contain \"and-so-forth\"\nwhose repeat count can be changed by the parameter. This paper proposes the way\nto design expressions that have all these features and demonstrates how these\nfeatures are useful to express programs concise. Egison has already implemented\nin Haskell."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2816707.2816710", 
    "link": "http://arxiv.org/pdf/1506.04857v1", 
    "other_authors": "Keehang Kwon", 
    "title": "Mutually Exclusive Modules in Logic Programming", 
    "arxiv-id": "1506.04857v1", 
    "author": "Keehang Kwon", 
    "publish": "2015-06-16T07:09:05Z", 
    "summary": "Logic programming has traditionally lacked devices for expressing mutually\nexclusive modules. We address this limitation by adopting choice-conjunctive\nmodules of the form $D_0 \\& D_1$ where $D_0, D_1$ are a conjunction of Horn\nclauses and $\\&$ is a linear logic connective. Solving a goal $G$ using $D_0 \\&\nD_1$ -- $exec(D_0 \\& D_1,G)$ -- has the following operational semantics:\n$choose$ a successful one between $exec(D_0,G)$ and $exec(D_1,G)$. In other\nwords, if $D_0$ is chosen in the course of solving $G$, then $D_1$ will be\ndiscarded and vice versa. Hence, the class of choice-conjunctive modules can\ncapture the notion of mutually exclusive modules."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2816707.2816710", 
    "link": "http://arxiv.org/pdf/1506.05270v1", 
    "other_authors": "Aggelos Biboudis, George Fourtounis, Yannis Smaragdakis", 
    "title": "jUCM: Universal Class Morphing (position paper)", 
    "arxiv-id": "1506.05270v1", 
    "author": "Yannis Smaragdakis", 
    "publish": "2015-06-17T10:28:41Z", 
    "summary": "We extend prior work on class-morphing to provide a more expressive\npattern-based compile-time reflection language. Our MorphJ language offers a\ndisciplined form of metaprogramming that produces types by statically iterating\nover and pattern-matching on fields and methods of other types. We expand such\ncapabilities with \"universal morphing\", which also allows pattern-matching over\ntypes (e.g., all classes nested in another, all supertypes of a class) while\nmaintaining modular type safety for our meta-programs. We present informal\nexamples of the functionality and discuss a design for adding universal\nmorphing to Java."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2816707.2816710", 
    "link": "http://arxiv.org/pdf/1506.05893v1", 
    "other_authors": "Daniel Bundala, Sanjit A. Seshia", 
    "title": "On Systematic Testing for Execution-Time Analysis", 
    "arxiv-id": "1506.05893v1", 
    "author": "Sanjit A. Seshia", 
    "publish": "2015-06-19T07:37:01Z", 
    "summary": "Given a program and a time deadline, does the program finish before the\ndeadline when executed on a given platform? With the requirement to produce a\ntest case when such a violation can occur, we refer to this problem as the\nworst-case execution-time testing (WCETT) problem.\n  In this paper, we present an approach for solving the WCETT problem for\nloop-free programs by timing the execution of a program on a small number of\ncarefully calculated inputs. We then create a sequence of integer linear\nprograms the solutions of which encode the best timing model consistent with\nthe measurements. By solving the programs we can find the worst-case input as\nwell as estimate execution time of any other input. Our solution is more\naccurate than previous approaches and, unlikely previous work, by increasing\nthe number of measurements we can produce WCETT bounds up to any desired\naccuracy.\n  Timing of a program depends on the properties of the platform it executes on.\nWe further show how our approach can be used to quantify the timing\nrepeatability of the underlying platform."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2784731.2784761", 
    "link": "http://arxiv.org/pdf/1506.06378v2", 
    "other_authors": "Steffen Smolka, Spiridon Eliopoulos, Nate Foster, Arjun Guha", 
    "title": "A Fast Compiler for NetKAT", 
    "arxiv-id": "1506.06378v2", 
    "author": "Arjun Guha", 
    "publish": "2015-06-21T15:37:27Z", 
    "summary": "High-level programming languages play a key role in a growing number of\nnetworking platforms, streamlining application development and enabling precise\nformal reasoning about network behavior. Unfortunately, current compilers only\nhandle \"local\" programs that specify behavior in terms of hop-by-hop forwarding\nbehavior, or modest extensions such as simple paths. To encode richer \"global\"\nbehaviors, programmers must add extra state -- something that is tricky to get\nright and makes programs harder to write and maintain. Making matters worse,\nexisting compilers can take tens of minutes to generate the forwarding state\nfor the network, even on relatively small inputs. This forces programmers to\nwaste time working around performance issues or even revert to using\nhardware-level APIs.\n  This paper presents a new compiler for the NetKAT language that handles rich\nfeatures including regular paths and virtual networks, and yet is several\norders of magnitude faster than previous compilers. The compiler uses symbolic\nautomata to calculate the extra state needed to implement \"global\" programs,\nand an intermediate representation based on binary decision diagrams to\ndramatically improve performance. We describe the design and implementation of\nthree essential compiler stages: from virtual programs (which specify behavior\nin terms of virtual topologies) to global programs (which specify network-wide\nbehavior in terms of physical topologies), from global programs to local\nprograms (which specify behavior in terms of single-switch behavior), and from\nlocal programs to hardware-level forwarding tables. We present results from\nexperiments on real-world benchmarks that quantify performance in terms of\ncompilation time and forwarding table size."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2784731.2784761", 
    "link": "http://arxiv.org/pdf/1506.07813v1", 
    "other_authors": "Joe Gibbs Politz, Spiridon Eliopoulos, Arjun Guha, Shriram Krishnamurthi", 
    "title": "ADsafety: Type-Based Verification of JavaScript Sandboxing", 
    "arxiv-id": "1506.07813v1", 
    "author": "Shriram Krishnamurthi", 
    "publish": "2015-06-25T16:59:52Z", 
    "summary": "Web sites routinely incorporate JavaScript programs from several sources into\na single page. These sources must be protected from one another, which requires\nrobust sandboxing. The many entry-points of sandboxes and the subtleties of\nJavaScript demand robust verification of the actual sandbox source. We use a\nnovel type system for JavaScript to encode and verify sandboxing properties.\nThe resulting verifier is lightweight and efficient, and operates on actual\nsource. We demonstrate the effectiveness of our technique by applying it to\nADsafe, which revealed several bugs and other weaknesses."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2784731.2784761", 
    "link": "http://arxiv.org/pdf/1507.01656v1", 
    "other_authors": "Steve Versteeg", 
    "title": "Languages for Mobile Agents", 
    "arxiv-id": "1507.01656v1", 
    "author": "Steve Versteeg", 
    "publish": "2015-07-07T01:47:31Z", 
    "summary": "Mobile agents represent a new model for network computing. Many different\nlanguages have been used to implement mobile agents. The characteristics that\nmake a language useful for writing mobile agents are: (1) their support of\nagent migration, (2) their support for agent-to-agent communication, (3) how\nthey allow agents to interact with local resources, (4) security mechanisms,\n(5) execution efficiency, (6) language implementation across multiple\nplatforms, and (7) the language's ease of programming of the tasks mobile\nagents perform."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2784731.2784761", 
    "link": "http://arxiv.org/pdf/1507.02437v1", 
    "other_authors": "Maxime Chevalier-Boisvert, Marc Feeley", 
    "title": "Extending Basic Block Versioning with Typed Object Shapes", 
    "arxiv-id": "1507.02437v1", 
    "author": "Marc Feeley", 
    "publish": "2015-07-09T09:58:44Z", 
    "summary": "Typical JavaScript (JS) programs feature a large number of object property\naccesses. Hence, fast property reads and writes are crucial for good\nperformance. Unfortunately, many (often redundant) dynamic checks are implied\nin each property access and the semantic complexity of JS makes it difficult to\noptimize away these tests through program analysis. We introduce two techniques\nto effectively eliminate a large proportion of dynamic checks related to object\nproperty accesses.\n  Typed shapes enable code specialization based on object property types\nwithout potentially complex and expensive analyses. Shape propagation allows\nthe elimination of redundant shape checks in inline caches. These two\ntechniques combine particularly well with Basic Block Versioning (BBV), but\nshould be easily adaptable to tracing Just-In-Time (JIT) compilers and method\nJITs with type feedback.\n  To assess the effectiveness of the techniques presented, we have implemented\nthem in Higgs, a type-specializing JIT compiler for JS. The techniques are\ncompared to a baseline using polymorphic Inline Caches (PICs), as well as\ncommercial JS implementations. Empirical results show that across the 26\nbenchmarks tested, these techniques eliminate on average 48% of type tests,\nreduce code size by 17% and reduce execution time by 25%. On several\nbenchmarks, Higgs performs better than current production JS virtual machines"
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908103", 
    "link": "http://arxiv.org/pdf/1507.02988v3", 
    "other_authors": "Ravi Chugh, Brian Hempel, Mitchell Spradlin, Jacob Albers", 
    "title": "Programmatic and Direct Manipulation, Together at Last", 
    "arxiv-id": "1507.02988v3", 
    "author": "Jacob Albers", 
    "publish": "2015-07-10T18:55:56Z", 
    "summary": "Direct manipulation interfaces and programmatic systems have distinct and\ncomplementary strengths. The former provide intuitive, immediate visual\nfeedback and enable rapid prototyping, whereas the latter enable complex,\nreusable abstractions. Unfortunately, existing systems typically force users\ninto just one of these two interaction modes.\n  We present a system called Sketch-n-Sketch that integrates programmatic and\ndirect manipulation for the particular domain of Scalable Vector Graphics\n(SVG). In Sketch-n-Sketch, the user writes a program to generate an output SVG\ncanvas. Then the user may directly manipulate the canvas while the system\nimmediately infers a program update in order to match the changes to the\noutput, a workflow we call live synchronization. To achieve this, we propose\n(i) a technique called trace-based program synthesis that takes program\nexecution history into account in order to constrain the search space and (ii)\nheuristics for dealing with ambiguities. Based on our experience with examples\nspanning 2,000 lines of code and from the results of a preliminary user study,\nwe believe that Sketch-n-Sketch provides a novel workflow that can augment\ntraditional programming systems. Our approach may serve as the basis for live\nsynchronization in other application domains, as well as a starting point for\nyet more ambitious ways of combining programmatic and direct manipulation."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837631", 
    "link": "http://arxiv.org/pdf/1507.03137v2", 
    "other_authors": "Thomas Gilray, Steven Lyde, Michael D. Adams, Matthew Might, David Van Horn", 
    "title": "Pushdown Control-Flow Analysis for Free", 
    "arxiv-id": "1507.03137v2", 
    "author": "David Van Horn", 
    "publish": "2015-07-11T18:37:48Z", 
    "summary": "Traditional control-flow analysis (CFA) for higher-order languages, whether\nimplemented by constraint-solving or abstract interpretation, introduces\nspurious connections between callers and callees. Two distinct invocations of a\nfunction will necessarily pollute one another's return-flow. Recently, three\ndistinct approaches have been published which provide perfect call-stack\nprecision in a computable manner: CFA2, PDCFA, and AAC. Unfortunately, CFA2 and\nPDCFA are difficult to implement and require significant engineering effort.\nFurthermore, all three are computationally expensive; for a monovariant\nanalysis, CFA2 is in $O(2^n)$, PDCFA is in $O(n^6)$, and AAC is in $O(n^9 log\nn)$.\n  In this paper, we describe a new technique that builds on these but is both\nstraightforward to implement and computationally inexpensive. The crucial\ninsight is an unusual state-dependent allocation strategy for the addresses of\ncontinuation. Our technique imposes only a constant-factor overhead on the\nunderlying analysis and, with monovariance, costs only O(n3) in the worst case.\n  This paper presents the intuitions behind this development, a proof of the\nprecision of this analysis, and benchmarks demonstrating its efficacy."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908098", 
    "link": "http://arxiv.org/pdf/1507.03513v5", 
    "other_authors": "Jean Yang, Travis Hance, Thomas H. Austin, Armando Solar-Lezama, Cormac Flanagan, Stephen Chong", 
    "title": "Precise, Dynamic Information Flow for Database-Backed Applications", 
    "arxiv-id": "1507.03513v5", 
    "author": "Stephen Chong", 
    "publish": "2015-07-13T16:26:09Z", 
    "summary": "We present an approach for dynamic information flow control across the\napplication and database. Our approach reduces the amount of policy code\nrequired, yields formal guarantees across the application and database, works\nwith existing relational database implementations, and scales for realistic\napplications. In this paper, we present a programming model that factors out\ninformation flow policies from application code and database queries, a dynamic\nsemantics for the underlying {\\lambda}^JDB core language, and proofs of\ntermination-insensitive non-interference and policy compliance for the\nsemantics. We implement these ideas in Jacqueline, a Python web framework, and\ndemonstrate feasibility through three application case studies: a course\nmanager, a health record system, and a conference management system used to run\nan academic workshop. We show that in comparison to traditional applications\nwith hand-coded policy checks, Jacqueline applications have 1) a smaller\ntrusted computing base, 2) fewer lines of policy code, and 2) reasonable, often\nnegligible, additional overheads."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908098", 
    "link": "http://arxiv.org/pdf/1507.03559v1", 
    "other_authors": "David Darais, David Van Horn", 
    "title": "Mechanically Verified Calculational Abstract Interpretation", 
    "arxiv-id": "1507.03559v1", 
    "author": "David Van Horn", 
    "publish": "2015-07-13T19:23:25Z", 
    "summary": "Calculational abstract interpretation, long advocated by Cousot, is a\ntechnique for deriving correct-by-construction abstract interpreters from the\nformal semantics of programming languages.\n  This paper addresses the problem of deriving correct-by-verified-construction\nabstract interpreters with the use of a proof assistant. We identify several\ntechnical challenges to overcome with the aim of supporting verified\ncalculational abstract interpretation that is faithful to existing\npencil-and-paper proofs, supports calculation with Galois connections\ngenerally, and enables the extraction of verified static analyzers from these\nproofs. To meet these challenges, we develop a theory of Galois connections in\nmonadic style that include a specification effect. Effectful calculations may\nreason classically, while pure calculations have extractable computational\ncontent. Moving between the worlds of specification and implementation is\nenabled by our metatheory.\n  To validate our approach, we give the first mechanically verified proof of\ncorrectness for Cousot's \"Calculational design of a generic abstract\ninterpreter.\" Our proof \"by calculus\" closely follows the original\npaper-and-pencil proof and supports the extraction of a verified static\nanalyzer."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908098", 
    "link": "http://arxiv.org/pdf/1507.03577v1", 
    "other_authors": "Jinseong Jeon, Xiaokang Qiu, Jeffrey S. Foster, Armando Solar-Lezama", 
    "title": "JSKETCH: Sketching for Java", 
    "arxiv-id": "1507.03577v1", 
    "author": "Armando Solar-Lezama", 
    "publish": "2015-07-13T05:02:53Z", 
    "summary": "Sketch-based synthesis, epitomized by the SKETCH tool, lets developers\nsynthesize software starting from a partial program, also called a sketch or\ntemplate. This paper presents JSKETCH, a tool that brings sketch-based\nsynthesis to Java. JSKETCH's input is a partial Java program that may include\nholes, which are unknown constants, expression generators, which range over\nsets of expressions, and class generators, which are partial classes. JSKETCH\nthen translates the synthesis problem into a SKETCH problem; this translation\nis complex because SKETCH is not object-oriented. Finally, JSKETCH synthesizes\nan executable Java program by interpreting the output of SKETCH."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908098", 
    "link": "http://arxiv.org/pdf/1507.04817v3", 
    "other_authors": "Phuc C. Nguyen, Sam Tobin-Hochstadt, David Van Horn", 
    "title": "Higher-order symbolic execution for contract verification and refutation", 
    "arxiv-id": "1507.04817v3", 
    "author": "David Van Horn", 
    "publish": "2015-07-17T02:21:05Z", 
    "summary": "We present a new approach to automated reasoning about higher-order programs\nby endowing symbolic execution with a notion of higher-order, symbolic values.\nOur approach is sound and relatively complete with respect to a first-order\nsolver for base type values. Therefore, it can form the basis of automated\nverification and bug-finding tools for higher-order programs.\n  To validate our approach, we use it to develop and evaluate a system for\nverifying and refuting behavioral software contracts of components in a\nfunctional language, which we call soft contract verification. In doing so, we\ndiscover a mutually beneficial relation between behavioral contracts and\nhigher-order symbolic execution.\n  Our system uses higher-order symbolic execution, leveraging contracts as a\nsource of symbolic values including unknown behavioral values, and employs an\nupdatable heap of contract invariants to reason about flow-sensitive facts.\nWhenever a contract is refuted, it reports a concrete counterexample\nreproducing the error, which may involve solving for an unknown function. The\napproach is able to analyze first-class contracts, recursive data structures,\nunknown functions, and control-flow-sensitive refinements of values, which are\nall idiomatic in dynamic languages. It makes effective use of an off-the-shelf\nsolver to decide problems without heavy encodings. The approach is competitive\nwith a wide range of existing tools---including type systems, flow analyzers,\nand model checkers---on their own benchmarks. We have built a tool which\nanalyzes programs written in Racket, and report on its effectiveness in\nverifying and refuting contracts."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000332", 
    "link": "http://arxiv.org/pdf/1507.05454v1", 
    "other_authors": "Fred Mesnard, \u00c9tienne Payet, Germ\u00e1n Vidal", 
    "title": "Concolic Testing in Logic Programming", 
    "arxiv-id": "1507.05454v1", 
    "author": "Germ\u00e1n Vidal", 
    "publish": "2015-07-20T11:41:49Z", 
    "summary": "Software testing is one of the most popular validation techniques in the\nsoftware industry. Surprisingly, we can only find a few approaches to testing\nin the context of logic programming. In this paper, we introduce a systematic\napproach for dynamic testing that combines both concrete and symbolic\nexecution. Our approach is fully automatic and guarantees full path coverage\nwhen it terminates. We prove some basic properties of our technique and\nillustrate its practical usefulness through a prototype implementation."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000332", 
    "link": "http://arxiv.org/pdf/1507.05527v2", 
    "other_authors": "Jeevana Priya Inala, Nadia Polikarpova, Xiaokang Qiu, Benjamin S. Lerner, Armando Solar-Lezama", 
    "title": "Synthesis of Recursive ADT Transformations from Reusable Templates", 
    "arxiv-id": "1507.05527v2", 
    "author": "Armando Solar-Lezama", 
    "publish": "2015-07-20T15:09:58Z", 
    "summary": "Recent work has proposed a promising approach to improving scalability of\nprogram synthesis by allowing the user to supply a syntactic template that\nconstrains the space of potential programs. Unfortunately, creating templates\noften requires nontrivial effort from the user, which impedes the usability of\nthe synthesizer. We present a solution to this problem in the context of\nrecursive transformations on algebraic data-types. Our approach relies on\npolymorphic synthesis constructs: a small but powerful extension to the\nlanguage of syntactic templates, which makes it possible to define a program\nspace in a concise and highly reusable manner, while at the same time retains\nthe scalability benefits of conventional templates. This approach enables\nend-users to reuse predefined templates from a library for a wide variety of\nproblems with little effort. The paper also describes a novel optimization that\nfurther improves the performance and scalability of the system. We evaluated\nthe approach on a set of benchmarks that most notably includes desugaring\nfunctions for lambda calculus, which force the synthesizer to discover Church\nencodings for pairs and boolean operations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000204", 
    "link": "http://arxiv.org/pdf/1507.05762v1", 
    "other_authors": "Graeme Gange, Jorge A. Navas, Peter Schachte, Harald Sondergaard, Peter J. Stuckey", 
    "title": "Horn Clauses as an Intermediate Representation for Program Analysis and   Transformation", 
    "arxiv-id": "1507.05762v1", 
    "author": "Peter J. Stuckey", 
    "publish": "2015-07-21T09:39:11Z", 
    "summary": "Many recent analyses for conventional imperative programs begin by\ntransforming programs into logic programs, capitalising on existing LP analyses\nand simple LP semantics. We propose using logic programs as an intermediate\nprogram representation throughout the compilation process. With restrictions\nensuring determinism and single-modedness, a logic program can easily be\ntransformed to machine language or other low-level language, while maintaining\nthe simple semantics that makes it suitable as a language for program analysis\nand transformation. We present a simple LP language that enforces determinism\nand single-modedness, and show that it makes a convenient program\nrepresentation for analysis and transformation."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000344", 
    "link": "http://arxiv.org/pdf/1507.05986v2", 
    "other_authors": "Nataliia Stulova, Jos\u00e9 F. Morales, Manuel V. Hermenegildo", 
    "title": "Practical Run-time Checking via Unobtrusive Property Caching", 
    "arxiv-id": "1507.05986v2", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2015-07-21T21:07:24Z", 
    "summary": "The use of annotations, referred to as assertions or contracts, to describe\nprogram properties for which run-time tests are to be generated, has become\nfrequent in dynamic programing languages. However, the frameworks proposed to\nsupport such run-time testing generally incur high time and/or space overheads\nover standard program execution. We present an approach for reducing this\noverhead that is based on the use of memoization to cache intermediate results\nof check evaluation, avoiding repeated checking of previously verified\nproperties. Compared to approaches that reduce checking frequency, our proposal\nhas the advantage of being exhaustive (i.e., all tests are checked at all\npoints) while still being much more efficient than standard run-time checking.\nCompared to the limited previous work on memoization, it performs the task\nwithout requiring modifications to data structure representation or checking\ncode. While the approach is general and system-independent, we present it for\nconcreteness in the context of the Ciao run-time checking framework, which\nallows us to provide an operational semantics with checks and caching. We also\nreport on a prototype implementation and provide some experimental results that\nsupport that using a relatively small cache leads to significant decreases in\nrun-time checking overhead."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000150", 
    "link": "http://arxiv.org/pdf/1507.06576v1", 
    "other_authors": "Martin Gebser, Amelia Harrison, Roland Kaminski, Vladimir Lifschitz, Torsten Schaub", 
    "title": "Abstract Gringo", 
    "arxiv-id": "1507.06576v1", 
    "author": "Torsten Schaub", 
    "publish": "2015-07-23T17:26:57Z", 
    "summary": "This paper defines the syntax and semantics of the input language of the ASP\ngrounder GRINGO. The definition covers several constructs that were not\ndiscussed in earlier work on the semantics of that language, including\nintervals, pools, division of integers, aggregates with non-numeric values, and\nlparse-style aggregate expressions. The definition is abstract in the sense\nthat it disregards some details related to representing programs by strings of\nASCII characters. It serves as a specification for GRINGO from Version 4.5 on."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000290", 
    "link": "http://arxiv.org/pdf/1507.06852v1", 
    "other_authors": "Maximiliano Cristia, Gianfranco Rossi, Claudia Frydman", 
    "title": "Adding Partial Functions to Constraint Logic Programming with Sets", 
    "arxiv-id": "1507.06852v1", 
    "author": "Claudia Frydman", 
    "publish": "2015-07-24T14:18:59Z", 
    "summary": "Partial functions are common abstractions in formal specification notations\nsuch as Z, B and Alloy. Conversely, executable programming languages usually\nprovide little or no support for them. In this paper we propose to add partial\nfunctions as a primitive feature to a Constraint Logic Programming (CLP)\nlanguage, namely {log}. Although partial functions could be programmed on top\nof {log}, providing them as first-class citizens adds valuable flexibility and\ngenerality to the form of set-theoretic formulas that the language can safely\ndeal with. In particular, the paper shows how the {log} constraint solver is\nnaturally extended in order to accommodate for the new primitive constraints\ndealing with partial functions. Efficiency of the new version is empirically\nassessed by running a number of non-trivial set-theoretical goals involving\npartial functions, obtained from specifications written in Z."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068415000290", 
    "link": "http://arxiv.org/pdf/1507.06988v1", 
    "other_authors": "Lihua Wang, Luz Fernando Capretz", 
    "title": "A Binary Data Stream Scripting Language", 
    "arxiv-id": "1507.06988v1", 
    "author": "Luz Fernando Capretz", 
    "publish": "2015-07-24T18:26:21Z", 
    "summary": "Any file is fundamentally a binary data stream. A practical solution was\nachieved to interpret binary data stream. A new scripting language named Data\nFormat Scripting Language (DFSL) was developed to describe the physical layout\nof the data in a structural, more intelligible way. On the basis of the\nsolution, a generic software application was implemented; it parses various\nbinary data streams according to their respective DFSL scripts and generates\nhuman-readable result and XML document for data sharing. Our solution helps\neliminate the error-prone low-level programming, especially in the hardware\ndevices or network protocol development/debugging processes."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908097", 
    "link": "http://arxiv.org/pdf/1507.07049v3", 
    "other_authors": "Jedidiah McClurg, Hossein Hojjat, Nate Foster, Pavol Cerny", 
    "title": "Event-Driven Network Programming", 
    "arxiv-id": "1507.07049v3", 
    "author": "Pavol Cerny", 
    "publish": "2015-07-25T01:12:57Z", 
    "summary": "Software-defined networking (SDN) programs must simultaneously describe\nstatic forwarding behavior and dynamic updates in response to events.\nEvent-driven updates are critical to get right, but difficult to implement\ncorrectly due to the high degree of concurrency in networks. Existing SDN\nplatforms offer weak guarantees that can break application invariants, leading\nto problems such as dropped packets, degraded performance, security violations,\netc. This paper introduces EVENT-DRIVEN CONSISTENT UPDATES that are guaranteed\nto preserve well-defined behaviors when transitioning between configurations in\nresponse to events. We propose NETWORK EVENT STRUCTURES (NESs) to model\nconstraints on updates, such as which events can be enabled simultaneously and\ncausal dependencies between events. We define an extension of the NetKAT\nlanguage with mutable state, give semantics to stateful programs using NESs,\nand discuss provably-correct strategies for implementing NESs in SDNs. Finally,\nwe evaluate our approach empirically, demonstrating that it gives well-defined\nconsistency guarantees while avoiding expensive synchronization and packet\nbuffering."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908097", 
    "link": "http://arxiv.org/pdf/1507.07264v2", 
    "other_authors": "Shayan Najd, Sam Lindley, Josef Svenningsson, Philip Wadler", 
    "title": "Everything old is new again: Quoted Domain Specific Languages", 
    "arxiv-id": "1507.07264v2", 
    "author": "Philip Wadler", 
    "publish": "2015-07-26T23:11:16Z", 
    "summary": "We describe a new approach to domain specific languages (DSLs), called Quoted\nDSLs (QDSLs), that resurrects two old ideas: quotation, from McCarthy's Lisp of\n1960, and the subformula property, from Gentzen's natural deduction of 1935.\nQuoted terms allow the DSL to share the syntax and type system of the host\nlanguage. Normalising quoted terms ensures the subformula property, which\nguarantees that one can use higher-order types in the source while guaranteeing\nfirst-order types in the target, and enables using types to guide fusion. We\ntest our ideas by re-implementing Feldspar, which was originally implemented as\nan Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908097", 
    "link": "http://arxiv.org/pdf/1507.07719v1", 
    "other_authors": "Silvia Crafa", 
    "title": "The role of concurrency in an evolutionary view of programming   abstractions", 
    "arxiv-id": "1507.07719v1", 
    "author": "Silvia Crafa", 
    "publish": "2015-07-28T10:44:58Z", 
    "summary": "In this paper we examine how concurrency has been embodied in mainstream\nprogramming languages. In particular, we rely on the evolutionary talking\nborrowed from biology to discuss major historical landmarks and crucial\nconcepts that shaped the development of programming languages. We examine the\ngeneral development process, occasionally deepening into some language, trying\nto uncover evolutionary lineages related to specific programming traits. We\nmainly focus on concurrency, discussing the different abstraction levels\ninvolved in present-day concurrent programming and emphasizing the fact that\nthey correspond to different levels of explanation. We then comment on the role\nof theoretical research on the quest for suitable programming abstractions,\nrecalling the importance of changing the working framework and the way of\nlooking every so often. This paper is not meant to be a survey of modern\nmainstream programming languages: it would be very incomplete in that sense. It\naims instead at pointing out a number of remarks and connect them under an\nevolutionary perspective, in order to grasp a unifying, but not simplistic,\nview of the programming languages development process."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908097", 
    "link": "http://arxiv.org/pdf/1507.08087v1", 
    "other_authors": "Benoit Desouter, Tom Schrijvers, Marko van Dooren", 
    "title": "Tabling as a Library with Delimited Control", 
    "arxiv-id": "1507.08087v1", 
    "author": "Marko van Dooren", 
    "publish": "2015-07-29T10:01:22Z", 
    "summary": "Tabling is probably the most widely studied extension of Prolog. But despite\nits importance and practicality, tabling is not implemented by most Prolog\nsystems. Existing approaches require substantial changes to the Prolog engine,\nwhich is an investment out of reach of most systems. To enable more widespread\nadoption, we present a new implementation of tabling in under 600 lines of\nProlog code. Our lightweight approach relies on delimited control and provides\nreasonable performance."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2786545.2786552", 
    "link": "http://arxiv.org/pdf/1507.08398v1", 
    "other_authors": "Baptiste Maingret, Fr\u00e9d\u00e9ric Le Mou\u00ebl, Julien Ponge, Nicolas Stouls, Jian Cao, Yannick Loiseau", 
    "title": "Towards a Decoupled Context-Oriented Programming Language for the   Internet of Things", 
    "arxiv-id": "1507.08398v1", 
    "author": "Yannick Loiseau", 
    "publish": "2015-07-30T06:54:25Z", 
    "summary": "Easily programming behaviors is one major issue of a large and reconfigurable\ndeployment in the Internet of Things. Such kind of devices often requires to\nexternalize part of their behavior such as the sensing, the data aggregation or\nthe code offloading. Most existing context-oriented programming languages\nintegrate in the same class or close layers the whole behavior. We propose to\nabstract and separate the context tracking from the decision process, and to\nuse event-based handlers to interconnect them. We keep a very easy declarative\nand non-layered programming model. We illustrate by defining an extension to\nGolo-a JVM-based dynamic language."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2786545.2786552", 
    "link": "http://arxiv.org/pdf/1507.08610v1", 
    "other_authors": "Kimio Kuramitsu", 
    "title": "Fast, Flexible, and Declarative Construction of Abstract Syntax Trees   with PEGs", 
    "arxiv-id": "1507.08610v1", 
    "author": "Kimio Kuramitsu", 
    "publish": "2015-07-30T18:15:02Z", 
    "summary": "We address a declarative construction of abstract syntax trees with Parsing\nExpression Grammars. AST operators (constructor, connector, and tagging) are\nnewly defined to specify flexible AST constructions. A new challenge coming\nwith PEGs is the consistency management of ASTs in backtracking and packrat\nparsing. We make the transaction AST machine in order to perform AST operations\nin the context of the speculative parsing of PEGs. All the consistency control\nis automated by the analysis of AST operators. The proposed approach is\nimplemented in the Nez parser, written in Java. The performance study shows\nthat the transactional AST machine requires 25\\% approximately more time in\nCSV, XML, and C grammars."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2786545.2786552", 
    "link": "http://arxiv.org/pdf/1508.03263v2", 
    "other_authors": "Keehang Kwon", 
    "title": "Logic Programming with Macro Connectives", 
    "arxiv-id": "1508.03263v2", 
    "author": "Keehang Kwon", 
    "publish": "2015-08-13T16:34:22Z", 
    "summary": "Logic programming such as Prolog is often sequential and slow because each\nexecution step processes only a single, $micro$ connective. To fix this\nproblem, we propose to use $macro$ connectives as the means of improving both\nreadability and performance."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2786545.2786552", 
    "link": "http://arxiv.org/pdf/1508.03536v1", 
    "other_authors": "Tijs van der Storm, Sebastian Erdweg", 
    "title": "Proceedings of the 3rd Workshop on Domain-Specific Language Design and   Implementation (DSLDI 2015)", 
    "arxiv-id": "1508.03536v1", 
    "author": "Sebastian Erdweg", 
    "publish": "2015-08-14T15:05:14Z", 
    "summary": "The goal of the DSLDI workshop is to bring together researchers and\npractitioners interested in sharing ideas on how DSLs should be designed,\nimplemented, supported by tools, and applied in realistic application contexts.\nWe are both interested in discovering how already known domains such as graph\nprocessing or machine learning can be best supported by DSLs, but also in\nexploring new domains that could be targeted by DSLs. More generally, we are\ninterested in building a community that can drive forward the development of\nmodern DSLs. These informal post-proceedings contain the submitted talk\nabstracts to the 3rd DSLDI workshop (DSLDI'15), and a summary of the panel\ndiscussion on Language Composition."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2786545.2786552", 
    "link": "http://arxiv.org/pdf/1508.03837v1", 
    "other_authors": "Keehang Kwon", 
    "title": "Incorporating User Interaction into Imperative Languages", 
    "arxiv-id": "1508.03837v1", 
    "author": "Keehang Kwon", 
    "publish": "2015-08-16T15:36:30Z", 
    "summary": "Adding versatile interactions to imperative programming -- C, Java and\nAndroid -- is an essential task. Unfortunately, existing languages provide\nlittle constructs for user interaction.\n  We propose a computability-logical approach to user interaction. We\nillustrate our idea via C^I, an extension of the core C with a new choice\nstatement."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.189.3", 
    "link": "http://arxiv.org/pdf/1508.04848v1", 
    "other_authors": "Kasper Dokter, Sung-Shik Jongmans, Farhad Arbab, Simon Bliudze", 
    "title": "Relating BIP and Reo", 
    "arxiv-id": "1508.04848v1", 
    "author": "Simon Bliudze", 
    "publish": "2015-08-20T01:40:23Z", 
    "summary": "Coordination languages simplify design and development of concurrent systems.\nParticularly, exogenous coordination languages, like BIP and Reo, enable system\ndesigners to express the interactions among components in a system explicitly.\nIn this paper we establish a formal relation between BI(P) (i.e., BIP without\nthe priority layer) and Reo, by defining transformations between their semantic\nmodels. We show that these transformations preserve all properties expressible\nin a common semantics. This formal relation comprises the basis for a solid\ncomparison and consolidation of the fundamental coordination concepts behind\nthese two languages. Moreover, this basis offers translations that enable users\nof either language to benefit from the toolchains of the other."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.189.3", 
    "link": "http://arxiv.org/pdf/1508.04958v1", 
    "other_authors": "Moritz Sinn, Florian Zuleger, Helmut Veith", 
    "title": "Difference Constraints: An adequate Abstraction for Complexity Analysis   of Imperative Programs", 
    "arxiv-id": "1508.04958v1", 
    "author": "Helmut Veith", 
    "publish": "2015-08-20T11:32:47Z", 
    "summary": "Difference constraints have been used for termination analysis in the\nliterature, where they denote relational inequalities of the form x' <= y + c,\nand describe that the value of x in the current state is at most the value of y\nin the previous state plus some integer constant c. In this paper, we argue\nthat the complexity of imperative programs typically arises from counter\nincrements and resets, which can be modeled naturally by difference\nconstraints. We present the first practical algorithm for the analysis of\ndifference constraint programs and describe how C programs can be abstracted to\ndifference constraint programs. Our approach contributes to the field of\nautomated complexity and (resource) bound analysis by enabling automated\namortized complexity analysis for a new class of programs and providing a\nconceptually simple program model that relates invariant- and bound analysis.\nWe demonstrate the effectiveness of our approach through a thorough\nexperimental comparison on real world C code: our tool Loopus computes the\ncomplexity for considerably more functions in less time than related tools from\nthe literature."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.189.3", 
    "link": "http://arxiv.org/pdf/1508.06526v1", 
    "other_authors": "Keehang Kwon", 
    "title": "A Logical Approach to Event Handling in Imperative Languages", 
    "arxiv-id": "1508.06526v1", 
    "author": "Keehang Kwon", 
    "publish": "2015-08-26T15:09:33Z", 
    "summary": "While event handling is a key element in modern interactive programming, it\nis unfortunate that its theoretical foundation is rather weak. To solve this\nproblem, we propose to adopt a game-logical approach of computability logic\n\\cite{Jap08} to event handling."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.189.3", 
    "link": "http://arxiv.org/pdf/1508.06836v1", 
    "other_authors": "Zvonimir Pavlinovic, Tim King, Thomas Wies", 
    "title": "On Practical SMT-Based Type Error Localization", 
    "arxiv-id": "1508.06836v1", 
    "author": "Thomas Wies", 
    "publish": "2015-08-27T12:52:20Z", 
    "summary": "Compilers for statically typed functional programming languages are notorious\nfor generating confusing type error messages. When the compiler detects a type\nerror, it typically reports the program location where the type checking failed\nas the source of the error. Since other error sources are not even considered,\nthe actual root cause is often missed. A more adequate approach is to consider\nall possible error sources and report the most useful one subject to some\nusefulness criterion. In our previous work, we showed that this approach can be\nformulated as an optimization problem related to satisfiability modulo theories\n(SMT). This formulation cleanly separates the heuristic nature of usefulness\ncriteria from the underlying search problem. Unfortunately, algorithms that\nsearch for an optimal error source cannot directly use principal types which\nare crucial for dealing with the exponential-time complexity of the decision\nproblem of polymorphic type checking. In this paper, we present a new algorithm\nthat efficiently finds an optimal error source in a given ill-typed program.\nOur algorithm uses an improved SMT encoding to cope with the high complexity of\npolymorphic typing by iteratively expanding the typing constraints from which\nprincipal types are derived. The algorithm preserves the clean separation\nbetween the heuristics and the actual search. We have implemented our algorithm\nfor OCaml. In our experimental evaluation, we found that the algorithm reduces\nthe running times for optimal type error localization from minutes to seconds\nand scales better than previous localization algorithms."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.189.3", 
    "link": "http://arxiv.org/pdf/1509.00413v1", 
    "other_authors": "Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, Sailesh R, Subhajit Roy", 
    "title": "Program Synthesis using Natural Language", 
    "arxiv-id": "1509.00413v1", 
    "author": "Subhajit Roy", 
    "publish": "2015-09-01T17:42:49Z", 
    "summary": "Interacting with computers is a ubiquitous activity for millions of people.\nRepetitive or specialized tasks often require creation of small, often one-off,\nprograms. End-users struggle with learning and using the myriad of\ndomain-specific languages (DSLs) to effectively accomplish these tasks.\n  We present a general framework for constructing program synthesizers that\ntake natural language (NL) inputs and produce expressions in a target DSL. The\nframework takes as input a DSL definition and training data consisting of\nNL/DSL pairs. From these it constructs a synthesizer by learning optimal\nweights and classifiers (using NLP features) that rank the outputs of a\nkeyword-programming based translation. We applied our framework to three\ndomains: repetitive text editing, an intelligent tutoring system, and flight\ninformation queries. On 1200+ English descriptions, the respective synthesizers\nrank the desired program as the top-1 and top-3 for 80% and 90% descriptions\nrespectively."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.02439v2", 
    "other_authors": "Nicolas Laurent, Kim Mens", 
    "title": "Parsing Expression Grammars Made Practical", 
    "arxiv-id": "1509.02439v2", 
    "author": "Kim Mens", 
    "publish": "2015-09-08T16:43:53Z", 
    "summary": "Parsing Expression Grammars (PEGs) define languages by specifying\nrecursive-descent parser that recognises them. The PEG formalism exhibits\ndesirable properties, such as closure under composition, built-in\ndisambiguation, unification of syntactic and lexical concerns, and closely\nmatching programmer intuition. Unfortunately, state of the art PEG parsers\nstruggle with left-recursive grammar rules, which are not supported by the\noriginal definition of the formalism and can lead to infinite recursion under\nnaive implementations. Likewise, support for associativity and explicit\nprecedence is spotty. To remedy these issues, we introduce Autumn, a general\npurpose PEG library that supports left-recursion, left and right associativity\nand precedence rules, and does so efficiently. Furthermore, we identify infix\nand postfix operators as a major source of inefficiency in left-recursive PEG\nparsers and show how to tackle this problem. We also explore the extensibility\nof the PEG paradigm by showing how one can easily introduce new parsing\noperators and how our parser accommodates custom memoization and error handling\nstrategies. We compare our parser to both state of the art and battle-tested\nPEG and CFG parsers, such as Rats!, Parboiled and ANTLR."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.04040v1", 
    "other_authors": "J\u00f8rgen Steensgaard-Madsen", 
    "title": "Programs as proofs", 
    "arxiv-id": "1509.04040v1", 
    "author": "J\u00f8rgen Steensgaard-Madsen", 
    "publish": "2015-09-14T11:37:06Z", 
    "summary": "The Curry-Howard correspondence is about a relationship between types and\nprograms on the one hand and propositions and proofs on the other. The\nimplications for programming language design and program verification is an\nactive field of research.\n  Transformer-like semantics of internal definitions that combine a defining\ncomputation and an application will be presented. By specialisation for a given\ndefining computation one can derive inference rules for applications of defined\noperations.\n  With semantics of that kind for every operation, each application identifies\nan axiom in a logic defined by the programming language, so a language can be\nconsidered a theory."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.04315v1", 
    "other_authors": "Robert Webb", 
    "title": "Implementing a teleo-reactive programming system", 
    "arxiv-id": "1509.04315v1", 
    "author": "Robert Webb", 
    "publish": "2015-09-14T20:46:49Z", 
    "summary": "This thesis explores the teleo-reactive programming paradigm for controlling\nautonomous agents, such as robots. Teleo-reactive programming provides a\nrobust, opportunistic method for goal-directed programming that continuously\nreacts to the sensed environment. In particular, the TR and TeleoR systems are\ninvestigated. They influence the design of a teleo-reactive system programming\nin Python, for controlling autonomous agents via the Pedro communications\narchitecture. To demonstrate the system, it is used as a controller in a simple\ngame."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.05100v3", 
    "other_authors": "Rian Shambaugh, Aaron Weiss, Arjun Guha", 
    "title": "Rehearsal: A Configuration Verification Tool for Puppet", 
    "arxiv-id": "1509.05100v3", 
    "author": "Arjun Guha", 
    "publish": "2015-09-17T01:53:49Z", 
    "summary": "Large-scale data centers and cloud computing have turned system configuration\ninto a challenging problem. Several widely-publicized outages have been blamed\nnot on software bugs, but on configuration bugs. To cope, thousands of\norganizations use system configuration languages to manage their computing\ninfrastructure. Of these, Puppet is the most widely used with thousands of\npaying customers and many more open-source users. The heart of Puppet is a\ndomain-specific language that describes the state of a system. Puppet already\nperforms some basic static checks, but they only prevent a narrow range of\nerrors. Furthermore, testing is ineffective because many errors are only\ntriggered under specific machine states that are difficult to predict and\nreproduce. With several examples, we show that a key problem with Puppet is\nthat configurations can be non-deterministic.\n  This paper presents Rehearsal, a verification tool for Puppet configurations.\nRehearsal implements a sound, complete, and scalable determinacy analysis for\nPuppet. To develop it, we (1) present a formal semantics for Puppet, (2) use\nseveral analyses to shrink our models to a tractable size, and (3) frame\ndeterminism-checking as decidable formulas for an SMT solver. Rehearsal then\nleverages the determinacy analysis to check other important properties, such as\nidempotency. Finally, we apply Rehearsal to several real-world Puppet\nconfigurations."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.06503v2", 
    "other_authors": "Arthur Azevedo de Amorim, Nathan Collins, Andr\u00e9 DeHon, Delphine Demange, Catalin Hritcu, David Pichardie, Benjamin C. Pierce, Randy Pollack, Andrew Tolmach", 
    "title": "A Verified Information-Flow Architecture", 
    "arxiv-id": "1509.06503v2", 
    "author": "Andrew Tolmach", 
    "publish": "2015-09-22T08:38:20Z", 
    "summary": "SAFE is a clean-slate design for a highly secure computer system, with\npervasive mechanisms for tracking and limiting information flows. At the lowest\nlevel, the SAFE hardware supports fine-grained programmable tags, with\nefficient and flexible propagation and combination of tags as instructions are\nexecuted. The operating system virtualizes these generic facilities to present\nan information-flow abstract machine that allows user programs to label\nsensitive data with rich confidentiality policies. We present a formal,\nmachine-checked model of the key hardware and software mechanisms used to\ndynamically control information flow in SAFE and an end-to-end proof of\nnoninterference for this model.\n  We use a refinement proof methodology to propagate the noninterference\nproperty of the abstract machine down to the concrete machine level. We use an\nintermediate layer in the refinement chain that factors out the details of the\ninformation-flow control policy and devise a code generator for compiling such\ninformation-flow policies into low-level monitor code. Finally, we verify the\ncorrectness of this generator using a dedicated Hoare logic that abstracts from\nlow-level machine instructions into a reusable set of verified structured code\ngenerators."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.07036v1", 
    "other_authors": "David M. Rogers", 
    "title": "Towards a Direct, By-Need Evaluator for Dependently Typed Languages", 
    "arxiv-id": "1509.07036v1", 
    "author": "David M. Rogers", 
    "publish": "2015-09-23T15:41:56Z", 
    "summary": "We present a C-language implementation of the lambda-pi calculus by extending\nthe (call-by-need) stack machine of Ariola, Chang and Felleisen to hold types,\nusing a typeless- tagless- final interpreter strategy. It has the advantage of\nexpressing all operations as folds over terms, including by-need evaluation,\nrecovery of the initial syntax-tree encoding for any term, and eliminating most\ngarbage-collection tasks. These are made possible by a disciplined approach to\nhandling the spine of each term, along with a robust stack-based API. Type\ninference is not covered in this work, but also derives several advantages from\nthe present stack transformation. Timing and maximum stack space usage results\nfor executing benchmark problems are presented. We discuss how the design\nchoices for this interpreter allow the language to be used as a high-level\nscripting language for automatic distributed parallel execution of common\nscientific computing workflows."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2814251.2814265", 
    "link": "http://arxiv.org/pdf/1509.08068v1", 
    "other_authors": "Abhinav Jangda", 
    "title": "Block-Level Parallelism in Parsing Block Structured Languages", 
    "arxiv-id": "1509.08068v1", 
    "author": "Abhinav Jangda", 
    "publish": "2015-09-27T08:38:37Z", 
    "summary": "Softwares source code is becoming large and complex. Compilation of large\nbase code is a time consuming process. Parallel compilation of code will help\nin reducing the time complexity. Parsing is one of the phases in compiler in\nwhich significant amount of time of compilation is spent. Techniques have\nalready been developed to extract the parallelism available in parser. Current\nLR(k) parallel parsing techniques either face difficulty in creating Abstract\nSyntax Tree or requires modification in the grammar or are specific to less\nexpressive grammars. Most of the programming languages like C, ALGOL are\nblock-structured, and in most languages grammars the grammar of different\nblocks is independent, allowing different blocks to be parsed in parallel. We\nare proposing a block level parallel parser derived from Incremental Jump Shift\nReduce Parser by [13]. Block Parallelized Parser (BPP) can even work as a block\nparallel incremental parser. We define a set of Incremental Categories and\ncreate the partitions of a grammar based on a rule. When parser reaches the\nstart of the block symbol it will check whether the current block is related to\nany incremental category. If block parallel parser find the incremental\ncategory for it, parser will parse the block in parallel. Block parallel parser\nis developed for LR(1) grammar. Without making major changes in Shift Reduce\n(SR) LR(1) parsing algorithm, block parallel parser can create an Abstract\nSyntax tree easily. We believe this parser can be easily extended to LR (k)\ngrammars and also be converted to an LALR (1) parser. We implemented BPP and SR\nLR(1) parsing algorithm for C Programming Language. We evaluated performance of\nboth techniques by parsing 10 random files from Linux Kernel source. BPP showed\n28% and 52% improvement in the case of including header files and excluding\nheader files respectively."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.194.8", 
    "link": "http://arxiv.org/pdf/1509.08566v1", 
    "other_authors": "Mads Rosendahl, Maja H. Kirkeby", 
    "title": "Probabilistic Output Analysis by Program Manipulation", 
    "arxiv-id": "1509.08566v1", 
    "author": "Maja H. Kirkeby", 
    "publish": "2015-09-29T02:11:11Z", 
    "summary": "The aim of a probabilistic output analysis is to derive a probability\ndistribution of possible output values for a program from a probability\ndistribution of its input. We present a method for performing static output\nanalysis, based on program transformation techniques. It generates a\nprobability function as a possibly uncomputable expression in an intermediate\nlanguage. This program is then analyzed, transformed, and approximated. The\nresult is a closed form expression that computes an over approximation of the\noutput probability distribution for the program. We focus on programs where the\npossible input follows a known probability distribution. Tests in programs are\nnot assumed to satisfy the Markov property of having fixed branching\nprobabilities independently of previous history."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.00925v1", 
    "other_authors": "Arjun Guha, Claudiu Saftoiu, Shriram Krishnamurthi", 
    "title": "The Essence of JavaScript", 
    "arxiv-id": "1510.00925v1", 
    "author": "Shriram Krishnamurthi", 
    "publish": "2015-10-04T11:23:12Z", 
    "summary": "We reduce JavaScript to a core calculus structured as a small-step\noperational semantics. We present several peculiarities of the language and\nshow that our calculus models them. We explicate the desugaring process that\nturns JavaScript programs into ones in the core. We demonstrate faithfulness to\nJavaScript using real-world test suites. Finally, we illustrate utility by\ndefining a security property, implementing it as a type system on the core, and\nextending it to the full language."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.01752v2", 
    "other_authors": "Luca Padovani", 
    "title": "Type Reconstruction for the Linear \u03c0-Calculus with Composite Regular   Types", 
    "arxiv-id": "1510.01752v2", 
    "author": "Luca Padovani", 
    "publish": "2015-10-06T20:37:11Z", 
    "summary": "We extend the linear {\\pi}-calculus with composite regular types in such a\nway that data containing linear values can be shared among several processes,\nif there is no overlapping access to such values. We describe a type\nreconstruction algorithm for the extended type system and discuss some\npractical aspects of its implementation."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.03271v2", 
    "other_authors": "Lu\u00eds Cruz-Filipe, Fabrizio Montesi", 
    "title": "Choreographies, Computationally", 
    "arxiv-id": "1510.03271v2", 
    "author": "Fabrizio Montesi", 
    "publish": "2015-10-12T13:09:58Z", 
    "summary": "Choreographic Programming is a programming paradigm for building concurrent\nsoftware that is deadlock-free by construction, by disallowing mismatched I/O\noperations in the language used to write programs (called choreographies).\nPrevious models for choreographic programming are either trivially Turing\ncomplete, because they include arbitrary local computations at each process, or\ntrivially Turing incomplete, e.g., because termination is decidable.\n  In this work, we explore the core expressivity of choreographies, by\nintroducing a minimal language (AC) with restricted local computation (zero,\nsuccessor, and equality). AC is Turing complete purely by virtue of the\ncommunication structures that can be written in it. We show that a\nTuring-complete fragment of AC can be correctly projected to an actor-like\nprocess calculus (AP), thus identifying a process language that is both\ndeadlock-free and Turing-complete. By embedding AC into CC, a standard model\nfor choreographies based on sessions, we also characterise a Turing-complete\nfragment of CC, showing that the local computation primitives found in previous\nworks do not add expressive power. As a corollary, we identify a fragment of\nthe session-based pi-calculus that is both deadlock-free and Turing complete."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.03637v1", 
    "other_authors": "Maurizio Gabbrielli, Saverio Giallorenzo, Fabrizio Montesi", 
    "title": "Applied Choreographies", 
    "arxiv-id": "1510.03637v1", 
    "author": "Fabrizio Montesi", 
    "publish": "2015-10-13T11:41:20Z", 
    "summary": "Choreographic Programming is a methodology for the development of concurrent\nsoftware based on a correctness-by-construction approach which, given a global\ndescription of a system (a choreography), automatically generates deadlock-free\ncommunicating programs via an EndPoint Projection (EPP). Previous works use\ntarget-languages for EPP that, like their source choreography languages, model\ncommunications using channel names (e.g., variants of CCS and {\\pi}-calculus).\nThis leaves a gap between such models and real-world implementations, where\ncommunications are concretely supported by low-level mechanisms for message\nrouting.\n  We bridge this gap by developing Applied Choreographies (AC), a new model for\nchoreographic programming. AC brings the correctness-by-construction\nmethodology of choreographies down to the level of a real executable language.\nThe key feature of AC is that its semantics is based on message correlation ---\na standard technique in Service-Oriented Computing --- while retaining the\nusual simple and intuitive syntax of choreography languages. We provide AC with\na typing discipline that ensures the correct use of the low-level mechanism of\nmessage correlation, thus avoiding communication errors. We also define a\ntwo-step compilation from AC to a low-level Correlation Calculus, which is the\nbasis of a real executable language (Jolie). Finally, we prove an operational\ncorrespondence theorem, which ensures that compiled programs behave as the\noriginal choreography. This is the first result of such correct- ness property\nin the case of a real-world implemented language."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.03726v2", 
    "other_authors": "Simone Martini", 
    "title": "Several types of types in programming languages", 
    "arxiv-id": "1510.03726v2", 
    "author": "Simone Martini", 
    "publish": "2015-10-13T15:05:47Z", 
    "summary": "Types are an important part of any modern programming language, but we often\nforget that the concept of type we understand nowadays is not the same it was\nperceived in the sixties. Moreover, we conflate the concept of \"type\" in\nprogramming languages with the concept of the same name in mathematical logic,\nan identification that is only the result of the convergence of two different\npaths, which started apart with different aims. The paper will present several\nremarks (some historical, some of more conceptual character) on the subject, as\na basis for a further investigation. The thesis we will argue is that there are\nthree different characters at play in programming languages, all of them now\ncalled types: the technical concept used in language design to guide\nimplementation; the general abstraction mechanism used as a modelling tool; the\nclassifying tool inherited from mathematical logic. We will suggest three\npossible dates ad quem for their presence in the programming language\nliterature, suggesting that the emergence of the concept of type in computer\nscience is relatively independent from the logical tradition, until the\nCurry-Howard isomorphism will make an explicit bridge between them."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.03929v4", 
    "other_authors": "Carlo Spaccasassi, Vasileios Koutavas", 
    "title": "Type-Based Analysis for Session Inference", 
    "arxiv-id": "1510.03929v4", 
    "author": "Vasileios Koutavas", 
    "publish": "2015-10-13T23:35:13Z", 
    "summary": "We propose a type-based analysis to infer the session protocols of channels\nin an ML-like concurrent functional language. Combining and extending\nwell-known techniques, we develop a type-checking system that separates the\nunderlying ML type system from the typing of sessions. Without using linearity,\nour system guarantees communication safety and partial lock freedom. It also\nsupports provably complete session inference for finite sessions with no\nprogrammer annotations. We exhibit the usefulness of our system with\ninteresting examples, including one which is not typable in substructural type\nsystems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.04440v1", 
    "other_authors": "Silvia Crafa", 
    "title": "Modelling the Evolution of Programming Languages", 
    "arxiv-id": "1510.04440v1", 
    "author": "Silvia Crafa", 
    "publish": "2015-10-15T08:18:54Z", 
    "summary": "Programming languages are engineered languages that allow to instruct a\nmachine and share algorithmic information; they have a great influence on the\nsociety since they underlie almost every information technology artefact, and\nthey are at the core of the current explosion of software technology. The\nhistory of programming languages is marked by innovations, diversifications,\nlateral transfers and social influences; moreover, it represents an\nintermediate case study between the evolution of human languages and the\nevolution of technology. In this paper we study the application of the\nDarwinian explanation to the programming languages evolution by discussing to\nwhat extent the evolutionary mechanisms distinctive of biology can be applied\nto this area. We show that a number of evolutionary building blocks can be\nrecognised in the realm of computer languages, but we also identify critical\nissues. Far from being crystal clear, this fine-grained study shows to be a\nuseful tool to assess recent results about programming languages phylogenies.\nFinally, we show that rich evolutionary patterns, such as co-evolution,\nmacro-evolutionary trends, niche construction and exaptation, can be\neffectively applied to programming languages and provide for interesting\nexplanatory tools."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-14107-2_7", 
    "link": "http://arxiv.org/pdf/1510.05216v2", 
    "other_authors": "Tiark Rompf, Nada Amin", 
    "title": "From F to DOT: Type Soundness Proofs with Definitional Interpreters", 
    "arxiv-id": "1510.05216v2", 
    "author": "Nada Amin", 
    "publish": "2015-10-18T09:53:06Z", 
    "summary": "Scala's type system unifies ML modules, object-oriented, and functional\nprogramming. The Dependent Object Types (DOT) family of calculi has been\nproposed as a new foundation for Scala and similar languages. Unfortunately, it\nis not clear how DOT relates to any well-known type systems, and type soundness\nhas only been established for very restricted subsets. In fact, important Scala\nfeatures are known to break at least one key metatheoretic property such as\nenvironment narrowing or subtyping transitivity, which are usually required for\na type soundness proof.\n  First, and, perhaps surprisingly, we show how rich DOT calculi can still be\nproved sound. The key insight is that narrowing and subtyping transitivity only\nneed to hold for runtime objects, but not for code that is never executed.\nAlas, the dominant method of proving type soundness, Wright and Felleisen's\nsyntactic approach, is based on term rewriting, which does not a priori make a\ndistinction between runtime and type assignment time.\n  Second, we demonstrate how type soundness can be proved for advanced,\npolymorphic, type systems with respect to high-level, definitional\ninterpreters, implemented in Coq. We present the first mechanized soundness\nproof in this style for System F<: and several extensions, including mutable\nreferences. Our proofs use only simple induction: another surprising result, as\nthe combination of big-step semantics, mutable references, and polymorphism is\ncommonly believed to require co-inductive proof techniques.\n  Third, we show how DOT-like calculi emerge as generalizations of F<:,\nexposing a rich design space of calculi with path-dependent types which we\ncollectively call System D. Armed with insights from the definitional\ninterpreter semantics, we also show how equivalent small-step semantics and\nsoundness proofs in Wright-Felleisen-style can be derived for these systems."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.05527v1", 
    "other_authors": "David A. Naumann", 
    "title": "Towards Patterns for Heaps and Imperative Lambdas", 
    "arxiv-id": "1510.05527v1", 
    "author": "David A. Naumann", 
    "publish": "2015-10-19T15:20:27Z", 
    "summary": "In functional programming, point-free relation calculi have been fruitful for\ngeneral theories of program construction, but for specific applications\npointwise expressions can be more convenient and comprehensible. In imperative\nprogramming, refinement calculi have been tied to pointwise expression in terms\nof state variables, with the curious exception of the ubiquitous but invisible\nheap. To integrate pointwise with point-free, de Moor and Gibbons extended\nlambda calculus with non-injective pattern matching interpreted using\nrelations. This article gives a semantics of that language using ``ideal\nrelations'' between partial orders, and a second semantics using predicate\ntransformers. The second semantics is motivated by its potential use with\nseparation algebra, for pattern matching in programs acting on the heap. Laws\nincluding lax beta and eta are proved in these models and a number of open\nproblems are posed."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.07095v1", 
    "other_authors": "Kyriakos Georgiou, Steve Kerrison, Kerstin Eder", 
    "title": "On the Value and Limits of Multi-level Energy Consumption Static   Analysis for Deeply Embedded Single and Multi-threaded Programs", 
    "arxiv-id": "1510.07095v1", 
    "author": "Kerstin Eder", 
    "publish": "2015-10-24T01:10:26Z", 
    "summary": "There is growing interest in lowering the energy consumption of computation.\nEnergy transparency is a concept that makes a program's energy consumption\nvisible from software to hardware through the different system layers. Such\ntransparency can enable energy optimizations at each layer and between layers,\nand help both programmers and operating systems make energy aware decisions.\nThe common methodology of extracting the energy consumption of a program is\nthrough direct measurement of the target hardware. This usually involves\nspecialized equipment and knowledge most programmers do not have. In this\npaper, we examine how existing methods for static resource analysis and energy\nmodeling can be utilized to perform Energy Consumption Static Analysis (ECSA)\nfor deeply embedded programs. To investigate this, we have developed ECSA\ntechniques that work at the instruction set level and at a higher level, the\nLLVM IR, through a novel mapping technique. We apply our ECSA to a\ncomprehensive set of mainly industrial benchmarks, including single-threaded\nand also multi-threaded embedded programs from two commonly used concurrency\npatterns, task farms and pipelines. We compare our ECSA results to hardware\nmeasurements and predictions obtained based on simulation traces. We discuss a\nnumber of application scenarios for which ECSA results can provide energy\ntransparency and conclude with a set of new research questions for future work."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.07171v1", 
    "other_authors": "Daniel Poetzl, Daniel Kroening", 
    "title": "Formalizing and Checking Thread Refinement for Data-Race-Free Execution   Models (Extended Version)", 
    "arxiv-id": "1510.07171v1", 
    "author": "Daniel Kroening", 
    "publish": "2015-10-24T18:22:05Z", 
    "summary": "When optimizing a thread in a concurrent program (either done manually or by\nthe compiler), it must be guaranteed that the resulting thread is a refinement\nof the original thread. Most theories of valid optimizations are formulated in\nterms of valid syntactic transformations on the program code, or in terms of\nvalid transformations on thread execution traces. We present a new theory\nformulated instead in terms of the state of threads at synchronization\noperations, and show that it provides several advantages: it supports more\noptimizations, and leads to more efficient and simpler procedures for\nrefinement checking. We develop the theory for the SC-for-DRF execution model\n(using locks for synchronization), and show that its application in a compiler\ntesting setting leads to large performance improvements."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.07995v1", 
    "other_authors": "Kamil Dudka, Luk\u00e1\u0161 Hol\u00edk, Petr Peringer, Marek Trt\u00edk, Tom\u00e1\u0161 Vojnar", 
    "title": "From Low-Level Pointers to High-Level Containers", 
    "arxiv-id": "1510.07995v1", 
    "author": "Tom\u00e1\u0161 Vojnar", 
    "publish": "2015-10-27T17:30:22Z", 
    "summary": "We propose a method that transforms a C program manipulating containers using\nlow-level pointer statements into an equivalent program where the containers\nare manipulated via calls of standard high-level container operations like\npush_back or pop_front. The input of our method is a C program annotated by a\nspecial form of shape invariants which can be obtained from current automatic\nshape analysers after a slight modification. The resulting program where the\nlow-level pointer statements are summarized into high-level container\noperations is more understandable and (among other possible benefits) better\nsuitable for program analysis. We have implemented our approach and\nsuccessfully tested it through a number of experiments with list-based\ncontainers, including experiments with simplification of program analysis by\nseparating shape analysis from analysing data-related properties."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.08121v1", 
    "other_authors": "Jonathan Frankle", 
    "title": "Type-Directed Synthesis of Products", 
    "arxiv-id": "1510.08121v1", 
    "author": "Jonathan Frankle", 
    "publish": "2015-10-27T22:40:01Z", 
    "summary": "Software synthesis - the process of generating complete, general-purpose\nprograms from specifications - has become a hot research topic in the past few\nyears. For decades the problem was thought to be insurmountable: the search\nspace of possible programs is far too massive to efficiently traverse. Advances\nin efficient constraint solving have overcome this barrier, enabling a new\ngeneration of effective synthesis systems. Most existing systems compile\nsynthesis tasks down to low-level SMT instances, sacrificing high-level\nsemantic information while solving only first-order problems (i.e., filling\ninteger holes). Recent work takes an alternative approach, using the\nCurry-Howard isomorphism and techniques from automated theorem proving to\nconstruct higher-order programs with algebraic datatypes.\n  My thesis involved extending this type-directed synthesis engine to handle\nproduct types, which required significant modifications to both the underlying\ntheory and the tool itself. Product types streamline other language features,\neliminating variable-arity constructors among other workarounds employed in the\noriginal synthesis system. A form of logical conjunction, products are\ninvertible, making it possible to equip the synthesis system with an efficient\ntheorem-proving technique called focusing that eliminates many of the\nnondeterministic choices inherent in proof search. These theoretical\nenhancements informed a new version of the type-directed synthesis prototype\nimplementation, which remained performance-competitive with the original\nsynthesizer. A significant advantage of the type-directed synthesis framework\nis its extensibility; this thesis is a roadmap for future such efforts to\nincrease the expressive power of the system."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1510.08419v3", 
    "other_authors": "Nadia Polikarpova, Ivan Kuraj, Armando Solar-Lezama", 
    "title": "Program Synthesis from Polymorphic Refinement Types", 
    "arxiv-id": "1510.08419v3", 
    "author": "Armando Solar-Lezama", 
    "publish": "2015-10-28T19:05:11Z", 
    "summary": "We present a method for synthesizing recursive functions that provably\nsatisfy a given specification in the form of a polymorphic refinement type. We\nobserve that such specifications are particularly suitable for program\nsynthesis for two reasons. First, they offer a unique combination of expressive\npower and decidability, which enables automatic verification---and hence\nsynthesis---of nontrivial programs. Second, a type-based specification for a\nprogram can often be effectively decomposed into independent specifications for\nits components, causing the synthesizer to consider fewer component\ncombinations and leading to a combinatorial reduction in the size of the search\nspace. At the core of our synthesis procedure is a new algorithm for refinement\ntype checking, which supports specification decomposition.\n  We have evaluated our prototype implementation on a large set of synthesis\nproblems and found that it exceeds the state of the art in terms of both\nscalability and usability. The tool was able to synthesize more complex\nprograms than those reported in prior work (several sorting algorithms and\noperations on balanced search trees), as well as most of the benchmarks tackled\nby existing synthesizers, often starting from a more concise and intuitive user\ninput."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.00184v3", 
    "other_authors": "Fr\u00e9d\u00e9ric Haziza, Luk\u00e1\u0161 Hol\u00edk, Roland Meyer, Sebastian Wolff", 
    "title": "Pointer Race Freedom", 
    "arxiv-id": "1511.00184v3", 
    "author": "Sebastian Wolff", 
    "publish": "2015-10-31T22:18:40Z", 
    "summary": "We propose a novel notion of pointer race for concurrent programs\nmanipulating a shared heap. A pointer race is an access to a memory address\nwhich was freed, and it is out of the accessor's control whether or not the\ncell has been re-allocated. We establish two results. (1) Under the assumption\nof pointer race freedom, it is sound to verify a program running under explicit\nmemory management as if it was running with garbage collection. (2) Even the\nrequirement of pointer race freedom itself can be verified under the\ngarbage-collected semantics. We then prove analogues of the theorems for a\nstronger notion of pointer race needed to cope with performance-critical code\npurposely using racy comparisons and even racy dereferences of pointers. As a\npractical contribution, we apply our results to optimize a thread-modular\nanalysis under explicit memory management. Our experiments confirm a speed-up\nof up to two orders of magnitude."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.00511v1", 
    "other_authors": "Philipp Haller, Heather Miller", 
    "title": "A Formal Model for Direct-style Asynchronous Observables", 
    "arxiv-id": "1511.00511v1", 
    "author": "Heather Miller", 
    "publish": "2015-11-02T14:15:51Z", 
    "summary": "Languages like F#, C#, and recently also Scala, provide \"Async\" programming\nmodels which aim to make asynchronous programming easier by avoiding an\ninversion of control that is inherent in callback-based programming models.\nThis paper presents a novel approach to integrate the Async model with\nobservable streams of the Reactive Extensions model. Reactive Extensions are\nbest-known from the .NET platform, and widely-used implementations of its\nprogramming model exist also for Java, Ruby, and other languages. This paper\ncontributes a formalization of the unified \"Reactive Async\" model in the\ncontext of an object-based core calculus. Our formal model captures the essence\nof the protocol of asynchronous observables using a heap evolution property. We\nprove a subject reduction theorem; the theorem implies that reduction preserves\nthe heap evolution property. Thus, for well-typed programs our calculus ensures\nthe protocol of asynchronous observables."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.00825v1", 
    "other_authors": "Kengo Kido, Swarat Chaudhuri, Ichiro Hasuo", 
    "title": "Abstract Interpretation with Infinitesimals: Towards Scalability in   Nonstandard Static Analysis (Extended Version)", 
    "arxiv-id": "1511.00825v1", 
    "author": "Ichiro Hasuo", 
    "publish": "2015-11-03T09:22:36Z", 
    "summary": "We extend abstract interpretation for the purpose of verifying hybrid\nsystems. Abstraction has been playing an important role in many verification\nmethodologies for hybrid systems, but some special care is needed for\nabstraction of continuous dynamics defined by ODEs. We apply Cousot and\nCousot's framework of abstract interpretation to hybrid systems, almost as it\nis, by regarding continuous dynamics as an infinite iteration of infinitesimal\ndiscrete jumps. This extension follows the recent line of work by Suenaga,\nHasuo and Sekine, where deductive verification is extended for hybrid systems\nby 1) introducing a constant dt for an infinitesimal value; and 2) employing\nRobinson's nonstandard analysis (NSA) to define mathematically rigorous\nsemantics. Our theoretical results include soundness and termination via\nuniform widening operators; and our prototype implementation successfully\nverifies some benchmark examples."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.01261v1", 
    "other_authors": "Martin Gebser, Phillip Obermeier, Torsten Schaub", 
    "title": "Interactive Answer Set Programming - Preliminary Report", 
    "arxiv-id": "1511.01261v1", 
    "author": "Torsten Schaub", 
    "publish": "2015-11-04T09:42:59Z", 
    "summary": "Traditional Answer Set Programming (ASP) rests upon one-shot solving. A logic\nprogram is fed into an ASP system and its stable models are computed. The high\npractical relevance of dynamic applications led to the development of\nmulti-shot solving systems. An operative system solves continuously changing\nlogic programs. Although this was primarily aiming at dynamic applications in\nassisted living, robotics, or stream reasoning, where solvers interact with an\nenvironment, it also opened up the opportunity of interactive ASP, where a\nsolver interacts with a user. We begin with a formal characterization of\ninteractive ASP in terms of states and operations on them. In turn, we describe\nthe interactive ASP shell aspic along with its basic functionalities."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.01399v3", 
    "other_authors": "Ronald Garcia, \u00c9ric Tanter", 
    "title": "Deriving a Simple Gradual Security Language", 
    "arxiv-id": "1511.01399v3", 
    "author": "\u00c9ric Tanter", 
    "publish": "2015-11-04T17:07:00Z", 
    "summary": "Abstracting Gradual Typing (AGT) is an approach to systematically deriving\ngradual counterparts to static type disciplines. The approach consists of\ndefining the semantics of gradual types by interpreting them as sets of static\ntypes, and then defining an optimal abstraction back to gradual types. These\noperations are used to lift the static discipline to the gradual setting. The\nruntime semantics of the gradual language then arises as reductions on gradual\ntyping derivations.\n  To demonstrate the flexibility of AGT, we gradualize $\\lambda_\\text{SEC}$,\nthe prototypical security-typed language, with respect to only security labels\nrather than entire types, yielding a type system that ranges gradually from\nsimply-typed to securely-typed. We establish noninterference for the gradual\nlanguage, called $\\lambda_{\\widetilde{\\text{SEC}}}$, using Zdancewic's logical\nrelation proof method. Whereas prior work presents gradual security cast\nlanguages, which require explicit security casts, this work yields the first\ngradual security source language, which requires no explicit casts."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.jlamp.2015.10.008", 
    "link": "http://arxiv.org/pdf/1511.01413v1", 
    "other_authors": "Umer Liqat, Kyriakos Georgiou, Steve Kerrison, Pedro Lopez-Garcia, John P. Gallagher, Manuel V. Hermenegildo, Kerstin Eder", 
    "title": "Inferring Parametric Energy Consumption Functions at Different Software   Levels: ISA vs. LLVM IR", 
    "arxiv-id": "1511.01413v1", 
    "author": "Kerstin Eder", 
    "publish": "2015-11-04T17:46:13Z", 
    "summary": "The static estimation of the energy consumed by program executions is an\nimportant challenge, which has applications in program optimization and\nverification, and is instrumental in energy-aware software development. Our\nobjective is to estimate such energy consumption in the form of functions on\nthe input data sizes of programs. We have developed a tool for experimentation\nwith static analysis which infers such energy functions at two levels, the\ninstruction set architecture (ISA) and the intermediate code (LLVM IR) levels,\nand reflects it upwards to the higher source code level. This required the\ndevelopment of a translation from LLVM IR to an intermediate representation and\nits integration with existing components, a translation from ISA to the same\nrepresentation, a resource analyzer, an ISA-level energy model, and a mapping\nfrom this model to LLVM IR. The approach has been applied to programs written\nin the XC language running on XCore architectures, but is general enough to be\napplied to other languages. Experimental results show that our LLVM IR level\nanalysis is reasonably accurate (less than 6.4% average error vs. hardware\nmeasurements) and more powerful than analysis at the ISA level. This paper\nprovides insights into the trade-off of precision versus analyzability at these\nlevels."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.01838v1", 
    "other_authors": "St\u00e9phane Gimenez, Georg Moser", 
    "title": "The Complexity of Interaction (Long Version)", 
    "arxiv-id": "1511.01838v1", 
    "author": "Georg Moser", 
    "publish": "2015-11-05T18:05:10Z", 
    "summary": "In this paper, we analyze the complexity of functional programs written in\nthe interaction-net computation model, an asynchronous, parallel and confluent\nmodel that generalizes linear-logic proof nets. Employing user-defined sized\nand scheduled types, we certify concrete time, space and space-time complexity\nbounds for both sequential and parallel reductions of interaction-net programs\nby suitably assigning complexity potentials to typed nodes. The relevance of\nthis approach is illustrated on archetypal programming examples. The provided\nanalysis is precise, compositional and is, in theory, not restricted to\nparticular complexity classes."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.02415v1", 
    "other_authors": "Marek Trtik", 
    "title": "Anonymous On-line Communication Between Program Analyses", 
    "arxiv-id": "1511.02415v1", 
    "author": "Marek Trtik", 
    "publish": "2015-11-08T00:25:07Z", 
    "summary": "We propose a light-weight client-server model of communication between\nprogram analyses. Clients are individual analyses and the server mediates their\ncommunication. A client cannot see properties of any other and the\ncommunication is anonymous. There is no central algorithm standing above\nclients which would tell them when to communicate what information. Clients\ncommunicate with others spontaneously, according to their actual personal\nneeds. The model is based on our observation that a piece of information\nprovided to an analysis at a right place may (substantially) improve its\nresult. We evaluated the proposed communication model for all possible\ncombinations of three clients on more than 400 benchmarks and the results show\nthat the communication model performs well in practice."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.02597v1", 
    "other_authors": "Larisa Safina, Manuel Mazzara, Fabrizio Montesi", 
    "title": "Data-driven Workflows for Microservices", 
    "arxiv-id": "1511.02597v1", 
    "author": "Fabrizio Montesi", 
    "publish": "2015-11-09T08:31:46Z", 
    "summary": "Microservices is an architectural style inspired by service-oriented\ncomputing that has recently started gaining popularity. Jolie is a programming\nlanguage based on the microservices paradigm: the main building block of Jolie\nsystems are services, in contrast to, e.g., functions or objects. The\nprimitives offered by the Jolie language elicit many of the recurring patterns\nfound in microservices, like load balancers and structured processes. However,\nJolie still lacks some useful constructs for dealing with message types and\ndata manipulation that are present in service-oriented computing. In this\npaper, we focus on the possibility of expressing choices at the level of data\ntypes, a feature well represented in standards for Web Services, e.g., WSDL. We\nextend Jolie to support such type choices and show the impact of our\nimplementation on some of the typical scenarios found in microservice systems.\nThis shows how computation can move from a process-driven to a data-driven\napproach, and leads to the preliminary identification of recurring\ncommunication patterns that can be shaped as design patterns."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.02603v4", 
    "other_authors": "Paschalis Mpeis, Pavlos Petoumenos, Hugh Leather", 
    "title": "Iterative compilation on mobile devices", 
    "arxiv-id": "1511.02603v4", 
    "author": "Hugh Leather", 
    "publish": "2015-11-09T09:14:08Z", 
    "summary": "The abundance of poorly optimized mobile applications coupled with their\nincreasing centrality in our digital lives make a framework for mobile app\noptimization an imperative. While tuning strategies for desktop and server\napplications have a long history, it is difficult to adapt them for use on\nmobile phones.\n  Reference inputs which trigger behavior similar to a mobile application's\ntypical are hard to construct. For many classes of applications the very\nconcept of typical behavior is nonexistent, each user interacting with the\napplication in very different ways. In contexts like this, optimization\nstrategies need to evaluate their effectiveness against real user input, but\ndoing so online runs the risk of user dissatisfaction when suboptimal\noptimizations are evaluated.\n  In this paper we present an iterative compiler which employs a novel capture\nand replay technique in order to collect real user input and use it later to\nevaluate different transformations offline. The proposed mechanism identifies\nand stores only the set of memory pages needed to replay the most heavily used\nfunctions of the application. At idle periods, this minimal state is combined\nwith different binaries of the application, each one build with different\noptimizations enabled. Replaying the targeted functions allows us to evaluate\nthe effectiveness of each set of optimizations for the actual way the user\ninteracts with the application.\n  For the BEEBS benchmark suite, our approach was able to improve performance\nby up to 57%, while keeping the slowdown experienced by the user on average at\n0.8%. By focusing only on heavily used functions, we are able to conserve\nstorage space by between two and three orders of magnitude compared to typical\ncapture and replay implementations."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.02956v1", 
    "other_authors": "Maxime Chevalier-Boisvert, Marc Feeley", 
    "title": "Interprocedural Type Specialization of JavaScript Programs Without Type   Analysis", 
    "arxiv-id": "1511.02956v1", 
    "author": "Marc Feeley", 
    "publish": "2015-11-10T01:29:01Z", 
    "summary": "Dynamically typed programming languages such as Python and JavaScript defer\ntype checking to run time. VM implementations can improve performance by\neliminating redundant dynamic type checks. However, type inference analyses are\noften costly and involve tradeoffs between compilation time and resulting\nprecision. This has lead to the creation of increasingly complex multi-tiered\nVM architectures.\n  Lazy basic block versioning is a simple JIT compilation technique which\neffectively removes redundant type checks from critical code paths. This novel\napproach lazily generates type-specialized versions of basic blocks on-the-fly\nwhile propagating context-dependent type information. This approach does not\nrequire the use of costly program analyses, is not restricted by the precision\nlimitations of traditional type analyses.\n  This paper extends lazy basic block versioning to propagate type information\ninterprocedurally, across function call boundaries. Our implementation in a\nJavaScript JIT compiler shows that across 26 benchmarks, interprocedural basic\nblock versioning eliminates more type tag tests on average than what is\nachievable with static type analysis without resorting to code transformations.\nOn average, 94.3% of type tag tests are eliminated, yielding speedups of up to\n56%. We also show that our implementation is able to outperform Truffle/JS on\nseveral benchmarks, both in terms of execution time and compilation time."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.03213v1", 
    "other_authors": "Pallavi Maiya, Rahul Gupta, Aditya Kanade, Rupak Majumdar", 
    "title": "A Partial Order Reduction Technique for Event-driven Multi-threaded   Programs", 
    "arxiv-id": "1511.03213v1", 
    "author": "Rupak Majumdar", 
    "publish": "2015-11-10T18:22:29Z", 
    "summary": "Event-driven multi-threaded programming is fast becoming a preferred style of\ndeveloping efficient and responsive applications. In this concurrency model,\nmultiple threads execute concurrently, communicating through shared objects as\nwell as by posting asynchronous events that are executed in their order of\narrival. In this work, we consider partial order reduction (POR) for\nevent-driven multi-threaded programs. The existing POR techniques treat event\nqueues associated with threads as shared objects and thereby, reorder every\npair of events handled on the same thread even if reordering them does not lead\nto different states. We do not treat event queues as shared objects and propose\na new POR technique based on a novel backtracking set called the\ndependence-covering set. Events handled by the same thread are reordered by our\nPOR technique only if necessary. We prove that exploring dependence-covering\nsets suffices to detect all deadlock cycles and assertion violations defined\nover local variables. To evaluate effectiveness of our POR scheme, we have\nimplemented a dynamic algorithm to compute dependence-covering sets. On\nexecution traces obtained from a few Android applications, we demonstrate that\nour technique explores many fewer transitions ---often orders of magnitude\nfewer--- compared to exploration based on persistent sets, wherein, event\nqueues are considered as shared objects."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.03406v1", 
    "other_authors": "Shun Honda, Kimio Kuramitsu", 
    "title": "Implementing a Small Parsing Virtual Machine on Embedded Systems", 
    "arxiv-id": "1511.03406v1", 
    "author": "Kimio Kuramitsu", 
    "publish": "2015-11-11T07:44:04Z", 
    "summary": "PEGs are a formal grammar foundation for describing syntax, and are not hard\nto generate parsers with a plain recursive decent parsing. However, the large\namount of C-stack consumption in the recursive parsing is not acceptable\nespecially in resource-restricted embedded systems. Alternatively, we have\nattempted the machine virtualization approach to PEG-based parsing. MiniNez,\nour implemented virtual machine, is presented in this paper with several\ndownsizing techniques, including instruction specialization, inline expansion\nand static flow analysis. As a result, the MiniNez machine achieves both a very\nsmall footprint and competitive performance to generated C parsers. We have\ndemonstrated the experimental results by comparing on two major embedded\nplatforms: Cortex-A7 and Intel Atom processor."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2837614.2837646", 
    "link": "http://arxiv.org/pdf/1511.04846v1", 
    "other_authors": "Yi-Fan Tsai, Devin Coughlin, Bor-Yuh Evan Chang, Xavier Rival", 
    "title": "Synthesizing Short-Circuiting Validation of Data Structure Invariants", 
    "arxiv-id": "1511.04846v1", 
    "author": "Xavier Rival", 
    "publish": "2015-11-16T07:26:32Z", 
    "summary": "This paper presents incremental verification-validation, a novel approach for\nchecking rich data structure invariants expressed as separation logic\nassertions. Incremental verification-validation combines static verification of\nseparation properties with efficient, short-circuiting dynamic validation of\narbitrarily rich data constraints. A data structure invariant checker is an\ninductive predicate in separation logic with an executable interpretation; a\nshort-circuiting checker is an invariant checker that stops checking whenever\nit detects at run time that an assertion for some sub-structure has been fully\nproven statically. At a high level, our approach does two things: it statically\nproves the separation properties of data structure invariants using a static\nshape analysis in a standard way but then leverages this proof in a novel\nmanner to synthesize short-circuiting dynamic validation of the data\nproperties. As a consequence, we enable dynamic validation to make up for\nimprecision in sound static analysis while simultaneously leveraging the static\nverification to make the remaining dynamic validation efficient. We show\nempirically that short-circuiting can yield asymptotic improvements in dynamic\nvalidation, with low overhead over no validation, even in cases where static\nverification is incomplete."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10270-014-0444-y", 
    "link": "http://arxiv.org/pdf/1511.04926v1", 
    "other_authors": "Elena Giachino, Cosimo Laneve, Michael Lienhardt", 
    "title": "A framework for deadlock detection in core ABS", 
    "arxiv-id": "1511.04926v1", 
    "author": "Michael Lienhardt", 
    "publish": "2015-11-16T11:54:10Z", 
    "summary": "We present a framework for statically detecting deadlocks in a concurrent\nobject-oriented language with asynchronous method calls and cooperative\nscheduling of method activations. Since this language features recursion and\ndynamic resource creation, deadlock detection is extremely complex and\nstate-of-the-art solutions either give imprecise answers or do not scale. In\norder to augment precision and scalability we propose a modular framework that\nallows several techniques to be combined. The basic component of the framework\nis a front-end inference algorithm that extracts abstract behavioural\ndescriptions of methods, called contracts, which retain resource dependency\ninformation. This component is integrated with a number of possible different\nback-ends that analyse contracts and derive deadlock information. As a\nproof-of-concept, we discuss two such back-ends: (i) an evaluator that computes\na fixpoint semantics and (ii) an evaluator using abstract model checking."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10270-014-0444-y", 
    "link": "http://arxiv.org/pdf/1511.05104v1", 
    "other_authors": "Elena Giachino, Einar Broch Johnsen, Cosimo Laneve, Ka I Pun", 
    "title": "Time complexity of concurrent programs", 
    "arxiv-id": "1511.05104v1", 
    "author": "Ka I Pun", 
    "publish": "2015-11-16T19:48:48Z", 
    "summary": "We study the problem of automatically computing the time complexity of\nconcurrent object-oriented programs. To determine this complexity we use\nintermediate abstract descriptions that record relevant information for the\ntime analysis (cost of statements, creations of objects, and concurrent\noperations), called behavioural types. Then, we define a translation function\nthat takes behavioural types and makes the parallelism explicit into so-called\ncost equations, which are fed to an automatic off-the-shelf solver for\nobtaining the time complexity."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10270-014-0444-y", 
    "link": "http://arxiv.org/pdf/1511.06459v1", 
    "other_authors": "Patrick Schultz, David I. Spivak, Ryan Wisnesky", 
    "title": "QINL: Query-integrated Languages", 
    "arxiv-id": "1511.06459v1", 
    "author": "Ryan Wisnesky", 
    "publish": "2015-11-20T00:09:02Z", 
    "summary": "We describe an alternative solution to the impedance-mismatch problem between\nprogramming and query languages: rather than embed queries in a programming\nlanguage, as done in LINQ systems, we embed programs in a query language, and\ndub the result QINL."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10270-014-0444-y", 
    "link": "http://arxiv.org/pdf/1511.06965v4", 
    "other_authors": "David Darais, David Van Horn", 
    "title": "Constructive Galois Connections: Taming the Galois Connection Framework   for Mechanized Metatheory", 
    "arxiv-id": "1511.06965v4", 
    "author": "David Van Horn", 
    "publish": "2015-11-22T04:55:17Z", 
    "summary": "Galois connections are a foundational tool for structuring abstraction in\nsemantics and their use lies at the heart of the theory of abstract\ninterpretation. Yet, mechanization of Galois connections remains limited to\nrestricted modes of use, preventing their general application in mechanized\nmetatheory and certified programming.\n  This paper presents constructive Galois connections, a variant of Galois\nconnections that is effective both on paper and in proof assistants; is\ncomplete with respect to a large subset of classical Galois connections; and\nenables more general reasoning principles, including the \"calculational\" style\nadvocated by Cousot.\n  To design constructive Galois connection we identify a restricted mode of use\nof classical ones which is both general and amenable to mechanization in\ndependently-typed functional programming languages. Crucial to our metatheory\nis the addition of monadic structure to Galois connections to control a\n\"specification effect\". Effectful calculations may reason classically, while\npure calculations have extractable computational content. Explicitly moving\nbetween the worlds of specification and implementation is enabled by our\nmetatheory.\n  To validate our approach, we provide two case studies in mechanizing existing\nproofs from the literature: one uses calculational abstract interpretation to\ndesign a static analyzer, the other forms a semantic basis for gradual typing.\nBoth mechanized proofs closely follow their original paper-and-pencil\ncounterparts, employ reasoning principles not captured by previous\nmechanization approaches, support the extraction of verified algorithms, and\nare novel."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2980983.2908091", 
    "link": "http://arxiv.org/pdf/1511.07033v2", 
    "other_authors": "Andrew M. Kent, David Kempe, Sam Tobin-Hochstadt", 
    "title": "Occurrence Typing Modulo Theories", 
    "arxiv-id": "1511.07033v2", 
    "author": "Sam Tobin-Hochstadt", 
    "publish": "2015-11-22T16:54:32Z", 
    "summary": "We present a new type system combining occurrence typing, previously used to\ntype check programs in dynamically-typed languages such as Racket, JavaScript,\nand Ruby, with dependent refinement types. We demonstrate that the addition of\nrefinement types allows the integration of arbitrary solver-backed reasoning\nabout logical propositions from external theories. By building on occurrence\ntyping, we can add our enriched type system as an extension of Typed\nRacket---adding dependency and refinement reuses the existing formalism while\nincreasing its expressiveness.\n  Dependent refinement types allow Typed Racket programmers to express rich\ntype relationships, ranging from data structure invariants such as red-black\ntree balance to preconditions such as vector bounds. Refinements allow\nprogrammers to embed the propositions that occurrence typing in Typed Racket\nalready reasons about into their types. Further, extending occurrence typing to\nrefinements allows us to make the underlying formalism simpler and more\npowerful.\n  In addition to presenting the design of our system, we present a formal model\nof the system, show how to integrate it with theories over both linear\narithmetic and bitvectors, and evaluate the system in the context of the full\nTyped Racket implementation. Specifically, we take safe vector access as a case\nstudy, and examine all vector accesses in a 56,000 line corpus of Typed Racket\nprograms. Our system is able to prove that 50% of these are safe with no new\nannotation, and with a few annotations and modifications, we can capture close\nto 80%."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2980983.2908091", 
    "link": "http://arxiv.org/pdf/1511.07163v1", 
    "other_authors": "Pavol \u010cern\u00fd, Edmund M. Clarke, Thomas A. Henzinger, Arjun Radhakrishna, Leonid Ryzhyk, Roopsha Samanta, Thorsten Tarrach", 
    "title": "Optimizing Solution Quality in Synchronization Synthesis", 
    "arxiv-id": "1511.07163v1", 
    "author": "Thorsten Tarrach", 
    "publish": "2015-11-23T10:24:30Z", 
    "summary": "Given a multithreaded program written assuming a friendly, non-preemptive\nscheduler, the goal of synchronization synthesis is to automatically insert\nsynchronization primitives to ensure that the modified program behaves\ncorrectly, even with a preemptive scheduler. In this work, we focus on the\nquality of the synthesized solution: we aim to infer synchronization placements\nthat not only ensure correctness, but also meet some quantitative objectives\nsuch as optimal program performance on a given computing platform.\n  The key step that enables solution optimization is the construction of a set\nof global constraints over synchronization placements such that each model of\nthe constraints set corresponds to a correctness-ensuring synchronization\nplacement. We extract the global constraints from generalizations of\ncounterexample traces and the control-flow graph of the program. The global\nconstraints enable us to choose from among the encoded synchronization\nsolutions using an objective function. We consider two types of objective\nfunctions: ones that are solely dependent on the program (e.g., minimizing the\nsize of critical sections) and ones that are also dependent on the computing\nplatform. For the latter, given a program and a computing platform, we\nconstruct a performance model based on measuring average contention for\ncritical sections and the average time taken to acquire and release a lock\nunder a given average contention.\n  We empirically evaluated that our approach scales to typical module sizes of\nmany real world concurrent programs such as device drivers and multithreaded\nservers, and that the performance predictions match reality. To the best of our\nknowledge, this is the first comprehensive approach for optimizing the\nplacement of synthesized synchronization."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2980983.2908091", 
    "link": "http://arxiv.org/pdf/1511.07267v2", 
    "other_authors": "Duc-Hiep Chu, Joxan Jaffar", 
    "title": "Local Reasoning with First-Class Heaps, and a New Frame Rule", 
    "arxiv-id": "1511.07267v2", 
    "author": "Joxan Jaffar", 
    "publish": "2015-11-23T15:18:19Z", 
    "summary": "Separation Logic (SL) was a significant advance in program verification of\ndata structures. It used a \"separating\" conjoin operator in data structure\nspecifications to construct heaps from disjoint subheaps, and a \\emph{frame\nrule} to very elegantly realize local reasoning. Consequently, when a program\nis verified in SL, the proof is very natural and succinct. In this paper, we\npresent a new program verification framework whose first motivation is to\nmaintain the essential advantage of SL, that of expressing separation and then\nusing framing to obtain local reasoning. Our framework comprises two new\nfacets. First, we begin with a new domain of discourse of \\emph{explicit\nsubheaps} with \\emph{recursive definitions}. The resulting specification\nlanguage can describe arbitrary data structures, and arbitratry \\emph{sharing}\ntherein. This enables a very precise specification of frames. Second, we\nperform program verification by using a strongest postcondition propagation in\nsymbolic execution, and this provides a basis for \\emph{automation}. Finally,\nwe present an implementation of our verifier, and demonstrate automation on a\nnumber of representative programs. In particular, we present the first\nautomatic proof of a classic graph marking algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2980983.2908091", 
    "link": "http://arxiv.org/pdf/1511.08307v1", 
    "other_authors": "Kimio Kuramitsu", 
    "title": "Nez: practical open grammar language", 
    "arxiv-id": "1511.08307v1", 
    "author": "Kimio Kuramitsu", 
    "publish": "2015-11-26T07:37:10Z", 
    "summary": "Nez is a PEG(Parsing Expressing Grammar)-based open grammar language that\nallows us to describe complex syntax constructs without action code. Since open\ngrammars are declarative and free from a host programming language of parsers,\nsoftware engineering tools and other parser applications can reuse once-defined\ngrammars across programming languages.\n  A key challenge to achieve practical open grammars is the expressiveness of\nsyntax constructs and the resulting parser performance, as the traditional\naction code approach has provided very pragmatic solutions to these two issues.\nIn Nez, we extend the symbol-based state management to recognize\ncontext-sensitive language syntax, which often appears in major programming\nlanguages. In addition, the Abstract Syntax Tree constructor allows us to make\nflexible tree structures, including the left-associative pair of trees. Due to\nthese extensions, we have demonstrated that Nez can parse not all but many\ngrammars.\n  Nez can generate various types of parsers since all Nez operations are\nindependent of a specific parser language. To highlight this feature, we have\nimplemented Nez with dynamic parsing, which allows users to integrate a Nez\nparser as a parser library that loads a grammar at runtime. To achieve its\npractical performance, Nez operators are assembled into low-level virtual\nmachine instructions, including automated state modifications when\nbacktracking, transactional controls of AST construction, and efficient\nmemoization in packrat parsing. We demonstrate that Nez dynamic parsers achieve\nvery competitive performance compared to existing efficient parser generators."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2980983.2908091", 
    "link": "http://arxiv.org/pdf/1511.08414v1", 
    "other_authors": "Tetsuro Matsumura, Kimio Kuramitsu", 
    "title": "A declarative extension of parsing expression grammars for recognizing   most programming languages", 
    "arxiv-id": "1511.08414v1", 
    "author": "Kimio Kuramitsu", 
    "publish": "2015-11-26T15:15:47Z", 
    "summary": "Parsing Expression Grammars are a popular foundation for describing syntax.\nUnfortunately, several syntax of programming languages are still hard to\nrecognize with pure PEGs. Notorious cases appears: typedef-defined names in\nC/C++, indentation-based code layout in Python, and HERE document in many\nscripting languages. To recognize such PEG-hard syntax, we have addressed a\ndeclarative extension to PEGs. The \"declarative\" extension means no programmed\nsemantic actions, which are traditionally used to realize the extended parsing\nbehavior. Nez is our extended PEG language, including symbol tables and\nconditional parsing. This paper demonstrates that the use of Nez Extensions can\nrealize many practical programming languages, such as C, C\\#, Ruby, and Python,\nwhich involve PEG-hard syntax."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.198", 
    "link": "http://arxiv.org/pdf/1512.01438v1", 
    "other_authors": "Oleg Kiselyov, Jacques Garrigue", 
    "title": "Proceedings ML Family/OCaml Users and Developers workshops", 
    "arxiv-id": "1512.01438v1", 
    "author": "Jacques Garrigue", 
    "publish": "2015-12-03T01:00:15Z", 
    "summary": "This volume collects the extended versions of selected papers originally\npresented at the two ACM SIGPLAN workshops: ML Family Workshop 2014 and OCaml\n2014. Both were affiliated with ICFP 2014 and took place on two consecutive\ndays, on September 4 and 5, 2014 in Gothenburg, Sweden.\n  The ML Family workshop aims to recognize the entire extended family of ML and\nML-like languages: languages that are Higher-order, Typed, Inferred, and\nStrict. It provides the forum to discuss common issues, both practical\n(compilation techniques, implementations of concurrency and parallelism,\nprogramming for the Web) and theoretical (fancy types, module systems,\nmetaprogramming). The scope of the workshop includes all aspects of the design,\nsemantics, theory, application, implementation, and teaching of the members of\nthe ML family.\n  The OCaml workshop is more specifically targeted at the OCaml community, with\nan emphasis on new proposals and tools aiming to improve OCaml, its\nenvironment, and the functioning of the community. As such, it is interested in\nworks on the type system, language extensions, compiler and optimizations,\napplications, tools, and experience reports of exciting uses."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.198", 
    "link": "http://arxiv.org/pdf/1512.01690v1", 
    "other_authors": "Dmitri Soshnikov", 
    "title": "Using Functional Programming for Development of Distributed, Cloud and   Web Applications in F#", 
    "arxiv-id": "1512.01690v1", 
    "author": "Dmitri Soshnikov", 
    "publish": "2015-12-05T17:50:21Z", 
    "summary": "In this paper, we argue that modern functional programming languages - in\nparticular, FSharp on the .NET platform - are well suited for the development\nof distributed, web and cloud applications on the Internet. We emphasize that\nFSharp can be successfully used in a range of scenarios - starting from simple\nASP.NET web applications, and including cloud data processing tasks and\ndata-driven web applications. In particular, we show how some of the FSharp\nfeatures (eg. quotations) can be effectively used to develop a distributed web\nsystem using single code-base, and describe the commercial WebSharper project\nby Intellifactory for building distributed client-server web applications, as\nwell as research library that uses Windows Azure for parametric sweep\ncomputational tasks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.198.2", 
    "link": "http://arxiv.org/pdf/1512.01895v1", 
    "other_authors": "Leo White, Fr\u00e9d\u00e9ric Bour, Jeremy Yallop", 
    "title": "Modular implicits", 
    "arxiv-id": "1512.01895v1", 
    "author": "Jeremy Yallop", 
    "publish": "2015-12-07T03:16:36Z", 
    "summary": "We present modular implicits, an extension to the OCaml language for ad-hoc\npolymorphism inspired by Scala implicits and modular type classes. Modular\nimplicits are based on type-directed implicit module parameters, and elaborate\nstraightforwardly into OCaml's first-class functors. Basing the design on\nOCaml's modules leads to a system that naturally supports many features from\nother languages with systematic ad-hoc overloading, including inheritance,\ninstance constraints, constructor classes and associated types."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.198.4", 
    "link": "http://arxiv.org/pdf/1512.01897v1", 
    "other_authors": "Arthur Chargu\u00e9raud", 
    "title": "Improving Type Error Messages in OCaml", 
    "arxiv-id": "1512.01897v1", 
    "author": "Arthur Chargu\u00e9raud", 
    "publish": "2015-12-07T03:17:30Z", 
    "summary": "Cryptic type error messages are a major obstacle to learning OCaml or other\nML-based languages. In many cases, error messages cannot be interpreted without\na sufficiently-precise model of the type inference algorithm. The problem of\nimproving type error messages in ML has received quite a bit of attention over\nthe past two decades, and many different strategies have been considered. The\nchallenge is not only to produce error messages that are both sufficiently\nconcise and systematically useful to the programmer, but also to handle a\nfull-blown programming language and to cope with large-sized programs\nefficiently.\n  In this work, we present a modification to the traditional ML type inference\nalgorithm implemented in OCaml that, by significantly reducing the\nleft-to-right bias, allows us to report error messages that are more helpful to\nthe programmer. Our algorithm remains fully predictable and continues to\nproduce fairly concise error messages that always help making some progress\ntowards fixing the code. We implemented our approach as a patch to the OCaml\ncompiler in just a few hundred lines of code. We believe that this patch should\nbenefit not just to beginners, but also to experienced programs developing\nlarge-scale OCaml programs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.198.1", 
    "link": "http://arxiv.org/pdf/1512.01898v1", 
    "other_authors": "Akinori Abe, Eijiro Sumii", 
    "title": "A Simple and Practical Linear Algebra Library Interface with Static Size   Checking", 
    "arxiv-id": "1512.01898v1", 
    "author": "Eijiro Sumii", 
    "publish": "2015-12-07T03:17:50Z", 
    "summary": "Linear algebra is a major field of numerical computation and is widely\napplied. Most linear algebra libraries (in most programming languages) do not\nstatically guarantee consistency of the dimensions of vectors and matrices,\ncausing runtime errors. While advanced type systems--specifically, dependent\ntypes on natural numbers--can ensure consistency among the sizes of collections\nsuch as lists and arrays, such type systems generally require non-trivial\nchanges to existing languages and application programs, or tricky type-level\nprogramming.\n  We have developed a linear algebra library interface that verifies the\nconsistency (with respect to dimensions) of matrix operations by means of\ngenerative phantom types, implemented via fairly standard ML types and module\nsystem. To evaluate its usability, we ported to it a practical machine learning\nlibrary from a traditional linear algebra library. We found that most of the\nchanges required for the porting could be made mechanically, and changes that\nneeded human thought are minor."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.199.4", 
    "link": "http://arxiv.org/pdf/1512.03861v1", 
    "other_authors": "Martin Lester", 
    "title": "Control Flow Analysis for SF Combinator Calculus", 
    "arxiv-id": "1512.03861v1", 
    "author": "Martin Lester", 
    "publish": "2015-12-12T01:59:18Z", 
    "summary": "Programs that transform other programs often require access to the internal\nstructure of the program to be transformed. This is at odds with the usual\nextensional view of functional programming, as embodied by the lambda calculus\nand SK combinator calculus. The recently-developed SF combinator calculus\noffers an alternative, intensional model of computation that may serve as a\nfoundation for developing principled languages in which to express intensional\ncomputation, including program transformation. Until now there have been no\nstatic analyses for reasoning about or verifying programs written in\nSF-calculus. We take the first step towards remedying this by developing a\nformulation of the popular control flow analysis 0CFA for SK-calculus and\nextending it to support SF-calculus. We prove its correctness and demonstrate\nthat the analysis is invariant under the usual translation from SK-calculus\ninto SF-calculus."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1512.07681v1", 
    "other_authors": "Andrea Canciani, Pierpaolo Degano, Gian-Luigi Ferrari, Letterio Galletta", 
    "title": "A Context-Oriented Extension of F#", 
    "arxiv-id": "1512.07681v1", 
    "author": "Letterio Galletta", 
    "publish": "2015-12-24T01:39:47Z", 
    "summary": "Context-Oriented programming languages provide us with primitive constructs\nto adapt program behaviour depending on the evolution of their operational\nenvironment, namely the context. In previous work we proposed ML_CoDa, a\ncontext-oriented language with two-components: a declarative constituent for\nprogramming the context and a functional one for computing. This paper\ndescribes the implementation of ML_CoDa as an extension of F#."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1512.08990v5", 
    "other_authors": "Johannes Borgstr\u00f6m, Ugo Dal Lago, Andrew D. Gordon, Marcin Szymczak", 
    "title": "A Lambda-Calculus Foundation for Universal Probabilistic Programming", 
    "arxiv-id": "1512.08990v5", 
    "author": "Marcin Szymczak", 
    "publish": "2015-12-30T16:15:30Z", 
    "summary": "We develop the operational semantics of an untyped probabilistic\nlambda-calculus with continuous distributions, as a foundation for universal\nprobabilistic programming languages such as Church, Anglican, and Venture. Our\nfirst contribution is to adapt the classic operational semantics of\nlambda-calculus to a continuous setting via creating a measure space on terms\nand defining step-indexed approximations. We prove equivalence of big-step and\nsmall-step formulations of this distribution-based semantics. To move closer to\ninference techniques, we also define the sampling-based semantics of a term as\na function from a trace of random samples to a value. We show that the\ndistribution induced by integrating over all traces equals the\ndistribution-based semantics. Our second contribution is to formalize the\nimplementation technique of trace Markov chain Monte Carlo (MCMC) for our\ncalculus and to show its correctness. A key step is defining sufficient\nconditions for the distribution induced by trace MCMC to converge to the\ndistribution-based semantics. To the best of our knowledge, this is the first\nrigorous correctness proof for trace MCMC for a higher-order functional\nlanguage."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.00713v1", 
    "other_authors": "Michael Bukatin, Steve Matthews", 
    "title": "Almost Continuous Transformations of Software and Higher-order Dataflow   Programming", 
    "arxiv-id": "1601.00713v1", 
    "author": "Steve Matthews", 
    "publish": "2016-01-05T01:44:35Z", 
    "summary": "We consider two classes of stream-based computations which admit taking\nlinear combinations of execution runs: probabilistic sampling and generalized\nanimation. The dataflow architecture is a natural platform for programming with\nstreams. The presence of linear combinations allows us to introduce the notion\nof almost continuous transformation of dataflow graphs. We introduce a new\napproach to higher-order dataflow programming: a dynamic dataflow program is a\nstream of dataflow graphs evolving by almost continuous transformations. A\ndynamic dataflow program would typically run while it evolves. We introduce\nFluid, an experimental open source system for programming with dataflow graphs\nand almost continuous transformations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.01050v1", 
    "other_authors": "Michael Bukatin, Steve Matthews", 
    "title": "Dataflow Graphs as Matrices and Programming with Higher-order Matrix   Elements", 
    "arxiv-id": "1601.01050v1", 
    "author": "Steve Matthews", 
    "publish": "2016-01-06T02:07:18Z", 
    "summary": "We consider dataflow architecture for two classes of computations which admit\ntaking linear combinations of execution runs: probabilistic sampling and\ngeneralized animation. We improve the earlier technique of almost continuous\nprogram transformations by adopting a discipline of bipartite graphs linking\nnodes obtained via general transformations and nodes obtained via linear\ntransformations which makes it possible to develop and evolve dataflow programs\nover these classes of computations by continuous program transformations. The\nuse of bipartite graphs allows us to represent the dataflow programs from this\nclass as matrices of real numbers and evolve and modify programs by continuous\nchange of these numbers.\n  We develop a formalism for higher-order dataflow programming for this class\nof dataflow graphs based on the higher-order matrix elements. Some of our\nsoftware experiments are briefly discussed."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.02059v1", 
    "other_authors": "Andrew P. Black, Kim B. Bruce, James Noble", 
    "title": "The Essence of Inheritance", 
    "arxiv-id": "1601.02059v1", 
    "author": "James Noble", 
    "publish": "2016-01-09T01:05:03Z", 
    "summary": "Programming languages serve a dual purpose: to communicate programs to\ncomputers, and to communicate programs to humans. Indeed, it is this dual\npurpose that makes programming language design a constrained and challenging\nproblem. Inheritance is an essential aspect of that second purpose: it is a\ntool to improve communication. Humans understand new concepts most readily by\nfirst looking at a number of concrete examples, and later abstracting over\nthose examples. The essence of inheritance is that it mirrors this process: it\nprovides a formal mechanism for moving from the concrete to the abstract."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.02484v1", 
    "other_authors": "Faris Abou-Saleh, James Cheney, Jeremy Gibbons, James McKinna, Perdita Stevens", 
    "title": "Reflections on Monadic Lenses", 
    "arxiv-id": "1601.02484v1", 
    "author": "Perdita Stevens", 
    "publish": "2016-01-11T15:47:08Z", 
    "summary": "Bidirectional transformations (bx) have primarily been modeled as pure\nfunctions, and do not account for the possibility of the side-effects that are\navailable in most programming languages. Recently several formulations of bx\nthat use monads to account for effects have been proposed, both among\npractitioners and in academic research. The combination of bx with effects\nturns out to be surprisingly subtle, leading to problems with some of these\nproposals and increasing the complexity of others. This paper reviews the\nproposals for monadic lenses to date, and offers some improved definitions,\npaying particular attention to the obstacles to naively adding monadic effects\nto existing definitions of pure bx such as lenses and symmetric lenses, and the\nsubtleties of equivalence of symmetric bidirectional transformations in the\npresence of effects."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.02536v1", 
    "other_authors": "Pradeeban Kathiravelu, Xiao Chen, Dipesh Dugar Mitthalal, Lu\u00eds Veiga", 
    "title": "JikesRVM: Internal Mechanisms Study and Garbage Collection with MMTk", 
    "arxiv-id": "1601.02536v1", 
    "author": "Lu\u00eds Veiga", 
    "publish": "2016-01-11T17:42:06Z", 
    "summary": "High Level Language Virtual Machines is a core topic of interest for the\nresearchers who are into virtual execution environments. As an open source\nvirtual machine released to 16 universities, as early as 2001, Jikes RVM has\nbeen a major drive for many researches. While working on this project, we\nstudied the JIT compilation of Jikes RVM as well as the Garbage Collection (GC)\nwhich is handled by the Memory Management Toolkit (MMTk), a part of the Jikes\nRVM. We also studied the Compressor Mark-Compact Collector algorithm and\nimplemented it for MMTk. We have also implemented a micro-benchmark for the GC\nalgorithms in Java, named \"XPDBench\", for benchmarking the implementations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.03116v1", 
    "other_authors": "Muyuan Li, Daniel E McArdle, Jeffrey C Murphy, Bhargav Shivkumar, Lukasz Ziarek", 
    "title": "Adding Real-time Capabilities to a SML Compiler", 
    "arxiv-id": "1601.03116v1", 
    "author": "Lukasz Ziarek", 
    "publish": "2016-01-13T02:44:24Z", 
    "summary": "There has been much recent interest in adopting functional and reactive\nprogramming for use in real-time system design. Moving toward a more\ndeclarative methodology for developing real-time systems purports to improve\nthe fidelity of software. To study the benefits of functional and reactive\nprogramming for real-time systems, real-time aware functional compilers and\nlanguage runtimes are required. In this paper we examine the necessary changes\nto a modern Standard ML compiler, MLton, to provide basic support for real-time\nexecution. We detail our current progress in modifying MLton with a threading\nmodel that supports priorities, a chunked object model to support real-time\ngarbage collection, and low level modification to execute on top of a real-time\noperating system. We present preliminary numbers and our work in progress\nprototype, which is able to boot ML programs compiled with MLton on x86\nmachines."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.06517v2", 
    "other_authors": "Yong Lin, Martin Henz", 
    "title": "Programmable Restoration Granularity in Constraint Programming", 
    "arxiv-id": "1601.06517v2", 
    "author": "Martin Henz", 
    "publish": "2016-01-25T08:56:27Z", 
    "summary": "In most constraint programming systems, a limited number of search engines is\noffered while the programming of user-customized search algorithms requires\nlow-level efforts, which complicates the deployment of such algorithms. To\nalleviate this limitation, concepts such as computation spaces have been\ndeveloped. Computation spaces provide a coarse-grained restoration mechanism,\nbecause they store all information contained in a search tree node. Other\ngranularities are possible, and in this paper we make the case for dynamically\nadapting the restoration granularity during search. In order to elucidate\nprogrammable restoration granularity, we present restoration as an aspect of a\nconstraint programming system, using the model of aspect-oriented programming.\nA proof-of-concept implementation using Gecode shows promising results."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1601.07300v2", 
    "other_authors": "Yong Lin, Martin Henz", 
    "title": "Recollection: an Alternative Restoration Technique for Constraint   Programming Systems", 
    "arxiv-id": "1601.07300v2", 
    "author": "Martin Henz", 
    "publish": "2016-01-27T09:34:36Z", 
    "summary": "Search is a key service within constraint programming systems, and it demands\nthe restoration of previously accessed states during the exploration of a\nsearch tree. Restoration proceeds either bottom-up within the tree to roll back\npreviously performed operations using a trail, or top-down to redo them,\nstarting from a previously stored state and using suitable information stored\nalong the way. In this paper, we elucidate existing restoration techniques\nusing a pair of abstract methods and employ them to present a new technique\nthat we call recollection. The proposed technique stores the variables that\nwere affected by constraint propagation during fix points reasoning steps, and\nit conducts neither operation roll-back nor recomputation, while consuming much\nless memory than storing previous visited states. We implemented this idea as a\nprototype within the Gecode solver. An empirical evaluation reveals that\nconstraint problems with expensive propagation and frequent failures can\nbenefit from recollection with respect to runtime at the expense of a marginal\nincrease in memory consumption, comparing with the most competitive variant of\nrecomputation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1602.00602v2", 
    "other_authors": "Edd Barrett, Carl Friedrich Bolz, Rebecca Killick, Sarah Mount, Laurence Tratt", 
    "title": "Virtual Machine Warmup Blows Hot and Cold", 
    "arxiv-id": "1602.00602v2", 
    "author": "Laurence Tratt", 
    "publish": "2016-02-01T17:27:06Z", 
    "summary": "Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally\nthought to execute programs in two phases: first the warmup phase determines\nwhich parts of a program would most benefit from dynamic compilation and JIT\ncompiles them into machine code; after compilation has occurred, the program is\nsaid to be at peak performance. When measuring the performance of JIT compiling\nVMs, data collected during the warmup phase is generally discarded, placing the\nfocus on peak performance. In this paper we first run a number of small,\ndeterministic benchmarks on a variety of well known VMs, before introducing a\nrigorous statistical model for determining when warmup has occurred. Across 3\nbenchmarking machines only 43.3% to 56.5% of (VM, benchmark) pairs conform to\nthe traditional view of warmup and none of the VMs consistently warms up."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1602.00981v2", 
    "other_authors": "Oliver Bra\u010devac, Sebastian Erdweg, Guido Salvaneschi, Mira Mezini", 
    "title": "CPL: A Core Language for Cloud Computing -- Technical Report", 
    "arxiv-id": "1602.00981v2", 
    "author": "Mira Mezini", 
    "publish": "2016-02-02T15:48:50Z", 
    "summary": "Running distributed applications in the cloud involves deployment. That is,\ndistribution and configuration of application services and middleware\ninfrastructure. The considerable complexity of these tasks resulted in the\nemergence of declarative JSON-based domain-specific deployment languages to\ndevelop deployment programs. However, existing deployment programs unsafely\ncompose artifacts written in different languages, leading to bugs that are hard\nto detect before run time. Furthermore, deployment languages do not provide\nextension points for custom implementations of existing cloud services such as\napplication-specific load balancing policies.\n  To address these shortcomings, we propose CPL (Cloud Platform Language), a\nstatically-typed core language for programming both distributed applications as\nwell as their deployment on a cloud platform. In CPL, application services and\ndeployment programs interact through statically typed, extensible interfaces,\nand an application can trigger further deployment at run time. We provide a\nformal semantics of CPL and demonstrate that it enables type-safe, composable\nand extensible libraries of service combinators, such as load balancing and\nfault tolerance."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.201.2", 
    "link": "http://arxiv.org/pdf/1602.02715v2", 
    "other_authors": "Stanislaw Ambroszkiewicz", 
    "title": "On the notion of \"von Neumann vicious circle\" coined by John Backus", 
    "arxiv-id": "1602.02715v2", 
    "author": "Stanislaw Ambroszkiewicz", 
    "publish": "2016-02-03T19:36:23Z", 
    "summary": "\"The von Neumann vicious circle\" means that non-von Neumann computer\narchitectures cannot be developed because of the lack of widely available and\neffective non-von Neumann languages. New languages cannot be created because of\nlack of conceptual foundations for non-von Neumann architectures. The reason is\nthat programming languages are high-level abstract isomorphic copies of von\nNeumann computer architectures. This constitutes the current paradigm in\nComputer Science. The paradigm is equivalent to the predominant view that\ncomputations on higher order objects (functionals) can be done only\nsymbolically, i.e. by term rewriting. The paper is a short introduction to the\npapers arXiv:1501.03043 and arXiv:1510.02787 trying to break the paradigm by\nintroducing a framework that may be seen as a higher order functional HDL\n(Hardware Description Language)."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.1", 
    "link": "http://arxiv.org/pdf/1602.03591v1", 
    "other_authors": "Dominic Orchard, Nobuko Yoshida", 
    "title": "Using session types as an effect system", 
    "arxiv-id": "1602.03591v1", 
    "author": "Nobuko Yoshida", 
    "publish": "2016-02-11T01:19:51Z", 
    "summary": "Side effects are a core part of practical programming. However, they are\noften hard to reason about, particularly in a concurrent setting. We propose a\nfoundation for reasoning about concurrent side effects using sessions.\nPrimarily, we show that session types are expressive enough to encode an effect\nsystem for stateful processes. This is formalised via an effect-preserving\nencoding of a simple imperative language with an effect system into the\npi-calculus with session primitives and session types (into which we encode\neffect specifications). This result goes towards showing a connection between\nthe expressivity of session types and effect systems. We briefly discuss how\nthe encoding could be extended and applied to reason about and control\nconcurrent side effects."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.03599v1", 
    "other_authors": "Juliana Franco, Sophia Drossopoulou", 
    "title": "Behavioural types for non-uniform memory accesses", 
    "arxiv-id": "1602.03599v1", 
    "author": "Sophia Drossopoulou", 
    "publish": "2016-02-11T01:21:09Z", 
    "summary": "Concurrent programs executing on NUMA architectures consist of concurrent\nentities (e.g. threads, actors) and data placed on different nodes. Execution\nof these concurrent entities often reads or updates states from remote nodes.\nThe performance of such systems depends on the extent to which the concurrent\nentities can be executing in parallel, and on the amount of the remote reads\nand writes.\n  We consider an actor-based object oriented language, and propose a type\nsystem which expresses the topology of the program (the placement of the actors\nand data on the nodes), and an effect system which characterises remote reads\nand writes (in terms of which node reads/writes from which other nodes). We use\na variant of ownership types for the topology, and a combination of behavioural\nand ownership types for the effect system."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.03729v2", 
    "other_authors": "Lu\u00eds Cruz-Filipe, Fabrizio Montesi", 
    "title": "A Language for the Declarative Composition of Concurrent Protocols", 
    "arxiv-id": "1602.03729v2", 
    "author": "Fabrizio Montesi", 
    "publish": "2016-02-11T14:02:09Z", 
    "summary": "A recent study of bugs in real-world concurrent and distributed systems found\nthat, while implementations of individual protocols tend to be robust, the\ncomposition of multiple protocols and its interplay with internal computation\nis the culprit for most errors. Multiparty Session Types and Choreographic\nProgramming are methodologies for developing correct-by-construction concurrent\nand distributed software, based on global descriptions of communication flows.\nHowever, protocol composition is either limited or left unchecked. Inspired by\nthese two methodologies, in this work we present a new language model for the\nsafe composition of protocols, called Procedural Choreographies (PC). Protocols\nin PC are procedures, parameterised on the processes that enact them.\nProcedures define communications declaratively using global descriptions, and\nprograms are written by invoking and composing these procedures. An\nimplementation in terms of a process model is then mechanically synthesised,\nguaranteeing correctness and deadlock-freedom. We study PC in the settings of\nsynchronous and asynchronous communications, and illustrate its expressivity\nwith some representative examples."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.05365v1", 
    "other_authors": "Marino Miculan, Marco Peressotti", 
    "title": "A Specification of Open Transactional Memory for Haskell", 
    "arxiv-id": "1602.05365v1", 
    "author": "Marco Peressotti", 
    "publish": "2016-02-17T10:46:45Z", 
    "summary": "Transactional memory (TM) has emerged as a promising abstraction for\nconcurrent programming alternative to lock-based synchronizations. However,\nmost TM models admit only isolated transactions, which are not adequate in\nmulti-threaded programming where transactions have to interact via shared data\nbefore committing. In this paper, we present Open Transactional Memory (OTM), a\nprogramming abstraction supporting safe, data-driven interactions between\ncomposable memory transactions. This is achieved by relaxing isolation between\ntransactions, still ensuring atomicity: threads of different transactions can\ninteract by accessing shared variables, but then their transactions have to\ncommit together-actually, these transactions are transparently merged. This\nmodel allows for loosely-coupled interactions since transaction merging is\ndriven only by accesses to shared data, with no need to specify participants\nbeforehand. In this paper we provide a specification of the OTM in the setting\nof Concurrent Haskell, showing that it is a conservative extension of current\nSTM abstraction. In particular, we provide a formal semantics, which allows us\nto prove that OTM satisfies the \\emph{opacity} criterion."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.06568v1", 
    "other_authors": "Martin Berger, Laurence Tratt, Christian Urban", 
    "title": "Modelling homogeneous generative meta-programming", 
    "arxiv-id": "1602.06568v1", 
    "author": "Christian Urban", 
    "publish": "2016-02-21T19:18:58Z", 
    "summary": "Homogeneous generative meta-programming (HGMP) enables the generation of\nprogram fragments at compile-time or run-time. We present the first\nfoundational calculus which can model powerful HGMP languages such as Template\nHaskell. The calculus is designed such that we can gradually enhance it with\nthe features needed to model many of the advanced features of real languages.\nAs a demonstration of the flexibility of our approach, we also provide a simple\ntype system for the calculus."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.07285v1", 
    "other_authors": "Rohit Singh, Armando Solar-Lezama", 
    "title": "Automatic Generation of Formula Simplifiers based on Conditional Rewrite   Rules", 
    "arxiv-id": "1602.07285v1", 
    "author": "Armando Solar-Lezama", 
    "publish": "2016-02-23T20:09:33Z", 
    "summary": "This paper addresses the problem of creating simplifiers for logic formulas\nbased on conditional term rewriting. In particular, the paper focuses on a\nprogram synthesis application where formula simplifications have been shown to\nhave a significant impact. We show that by combining machine learning\ntechniques with constraint-based synthesis, it is possible to synthesize a\nformula simplifier fully automatically from a corpus of representative\nproblems, making it possible to create formula simplifiers tailored to specific\nproblem domains. We demonstrate the benefits of our approach for synthesis\nbenchmarks from the SyGuS competition and automated grading."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.203.9", 
    "link": "http://arxiv.org/pdf/1602.08406v2", 
    "other_authors": "Guilhem Jaber, Nikos Tzevelekos", 
    "title": "Trace semantics for polymorphic references", 
    "arxiv-id": "1602.08406v2", 
    "author": "Nikos Tzevelekos", 
    "publish": "2016-02-26T17:14:18Z", 
    "summary": "We introduce a trace semantics for a call-by-value language with full\npolymorphism and higher-order references. This is an operational game semantics\nmodel based on a nominal interpretation of parametricity whereby polymorphic\nvalues are abstracted with special kinds of names. The use of polymorphic\nreferences leads to violations of parametricity which we counter by closely\nrecoding the disclosure of typing information in the semantics. We prove the\nmodel sound for the full language and strengthen our result to full abstraction\nfor a large fragment where polymorphic references obey specific inhabitation\nconditions."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1602.08863v1", 
    "other_authors": "Lu\u00eds Cruz-Filipe, Fabrizio Montesi", 
    "title": "Choreographies in Practice", 
    "arxiv-id": "1602.08863v1", 
    "author": "Fabrizio Montesi", 
    "publish": "2016-02-29T08:49:49Z", 
    "summary": "Choreographic Programming is a development methodology for concurrent\nsoftware that guarantees correctness by construction. The key to this paradigm\nis to disallow mismatched I/O operations in programs, called choreographies,\nand then mechanically synthesise distributed implementations in terms of\nstandard process models via a mechanism known as EndPoint Projection (EPP).\n  Despite the promise of choreographic programming, there is still a lack of\npractical evaluations that illustrate the applicability of choreographies to\nconcrete computational problems with standard concurrent solutions. In this\nwork, we explore the potential of choreographies by using Procedural\nChoreographies (PC), a model that we recently proposed, to write distributed\nalgorithms for sorting (Quicksort), solving linear equations (Gaussian\nelimination), and computing Fast Fourier Transform. We discuss the lessons\nlearned from this experiment, giving possible directions for the usage and\nfuture improvements of choreography languages."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.00431v1", 
    "other_authors": "Amirali Sanatinia, Guevara Noubir", 
    "title": "On GitHub's Programming Languages", 
    "arxiv-id": "1603.00431v1", 
    "author": "Guevara Noubir", 
    "publish": "2016-03-01T20:03:44Z", 
    "summary": "GitHub is the most widely used social, distributed version control system. It\nhas around 10 million registered users and hosts over 16 million public\nrepositories. Its user base is also very active as GitHub ranks in the top 100\nAlexa most popular websites. In this study, we collect GitHub's state in its\nentirety. Doing so, allows us to study new aspects of the ecosystem. Although\nGitHub is the home to millions of users and repositories, the analysis of\nusers' activity time-series reveals that only around 10% of them can be\nconsidered active. The collected dataset allows us to investigate the\npopularity of programming languages and existence of pattens in the relations\nbetween users, repositories, and programming languages.\n  By, applying a k-means clustering method to the users-repositories commits\nmatrix, we find that two clear clusters of programming languages separate from\nthe remaining. One cluster forms for \"web programming\" languages (Java Script,\nRuby, PHP, CSS), and a second for \"system oriented programming\" languages (C,\nC++, Python). Further classification, allow us to build a phylogenetic tree of\nthe use of programming languages in GitHub. Additionally, we study the main and\nthe auxiliary programming languages of the top 1000 repositories in more\ndetail. We provide a ranking of these auxiliary programming languages using\nvarious metrics, such as percentage of lines of code, and PageRank."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03011v2", 
    "other_authors": "Salvador Tamarit, Julio Mari\u00f1o, Guillermo Vigueras, Manuel Carro", 
    "title": "Towards a Semantics-Aware Transformation Toolchain for Heterogeneous   Systems", 
    "arxiv-id": "1603.03011v2", 
    "author": "Manuel Carro", 
    "publish": "2016-03-09T19:42:59Z", 
    "summary": "Obtaining good performance when programming heterogeneous computing platforms\nposes significant challenges for the programmer. We present a program\ntransformation environment, implemented in Haskell, where architecture-agnostic\nscientific C code with semantic annotations is transformed into functionally\nequivalent code better suited for a given platform. The transformation steps\nare formalized (and implemented) as rules which can be fired when certain\nsyntactic and semantic conditions are met. These conditions are to be fulfilled\nby program properties which can be automatically inferred or, alternatively,\nstated as annotations in the source code. Rule selection can be guided by\nheuristics derived from a machine learning procedure which tries to capture how\nrun-time characteristics (e.g., resource consumption or performance) are\naffected by the transformation steps."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03022v2", 
    "other_authors": "Guillermo Vigueras, Manuel Carro, Salvador Tamarit, Julio Mari\u00f1o", 
    "title": "Towards Automatic Learning of Heuristics for Mechanical Transformations   of Procedural Code", 
    "arxiv-id": "1603.03022v2", 
    "author": "Julio Mari\u00f1o", 
    "publish": "2016-03-09T20:32:00Z", 
    "summary": "The current trend in next-generation exascale systems goes towards\nintegrating a wide range of specialized (co-)processors into traditional\nsupercomputers. However, the integration of different specialized devices\nincreases the degree of heterogeneity and the complexity in programming such\ntype of systems. Due to the efficiency of heterogeneous systems in terms of\nWatt and FLOPS per surface unit, opening the access of heterogeneous platforms\nto a wider range of users is an important problem to be tackled. In order to\nbridge the gap between heterogeneous systems and programmers, in this paper we\npropose a machine learning-based approach to learn heuristics for defining\ntransformation strategies of a program transformation system. Our approach\nproposes a novel combination of reinforcement learning and classification\nmethods to efficiently tackle the problems inherent to this type of systems.\nPreliminary results demonstrate the suitability of the approach for easing the\nprogrammability of heterogeneous systems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03165v3", 
    "other_authors": "Sumit Gulwani, Ivan Radi\u010dek, Florian Zuleger", 
    "title": "Automated Clustering and Program Repair for Introductory Programming   Assignments", 
    "arxiv-id": "1603.03165v3", 
    "author": "Florian Zuleger", 
    "publish": "2016-03-10T07:00:06Z", 
    "summary": "Providing feedback on programming assignments is a tedious task for the\ninstructor, and even impossible in large Massive Open Online Courses with\nthousands of students. In this paper, we present a novel technique for\nautomatic feedback generation: (1) For a given programming assignment, we\nautomatically cluster the correct student attempts using a dynamic program\nanalysis. From each cluster we select one student attempt as a specification.\n(2) Given an incorrect student attempt we run a repair procedure against all\nspecifications, and automatically generate the minimal repair based on one of\nthem. We implemented the proposed approach in a publicly available tool and\nevaluated it on a large number of existing student attempts, and additionally\nperformed a user study about usefulness of provided feedback, and established\nthat: our tool can completely automatically, in real time, repair a large\nnumber of student attempts, including complicated and larger repairs, while\npreliminary results show feedback to be useful."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03533v1", 
    "other_authors": "Elaheh Ghassabani, Mohammad Abdollahi Azgomi", 
    "title": "Stateless Code Model Checking of Information Flow Security", 
    "arxiv-id": "1603.03533v1", 
    "author": "Mohammad Abdollahi Azgomi", 
    "publish": "2016-03-11T06:28:50Z", 
    "summary": "Observational determinism is a security property that characterizes secure\ninformation flow for multithreaded programs. Most of the methods that have been\nused to verify observational determinism are based on either type systems or\nconventional model checking techniques. A conventional model checker is\nstateful and often verifies a system model usually constructed manually. As\nthese methods are based on stateful model checking, they are confronted with\nthe state space explosion problem. In order to verify and test computer\nprograms, stateless code model checking is more appropriate than conventional\ntechniques. It is an effective method for systematic testing of large and\ncomplicated concurrent programs, and for exploring the state space of such\nprograms. In this paper, we propose a new method for verifying information flow\nsecurity in concurrent programs. For the first time, we use stateless code\nmodel checking to verify observational determinism."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03535v1", 
    "other_authors": "Elaheh Ghassabani, Mohammad Abdollahi Azgomi", 
    "title": "A New Approach to Stateless Model Checking of LTL Properties", 
    "arxiv-id": "1603.03535v1", 
    "author": "Mohammad Abdollahi Azgomi", 
    "publish": "2016-03-11T06:37:28Z", 
    "summary": "Verification of large and complicated concurrent programs is an important\nissue in the software world. Stateless model checking is an appropriate method\nfor systematically and automatically testing of large programs, which has\nproved its power in verifying code of large programs. Another well-known method\nin this area is runtime verification. Both stateless model checking and runtime\nverification are similar in some ways. One common approach in runtime\nverification is to construct runtime monitors for properties expressed in\nlinear temporal logic. Currently, there are some semantics to check linear\ntemporal logic formulae on finite paths proposed in the field of runtime\nverification, which can also be applied to stateless model checking. However,\nexisting stateless model checkers do not support LTL formulae. In some\nsettings, it is more advantageous to make use of stateless model checking\ninstead of runtime verification. This paper proposes a novel encoding of one of\nthe recent LTL semantics on finite paths into an actor-based system. We take a\ntruly parallel approach without saving any program states or traces, which not\nonly addresses important problems in runtime verification, but can also be\napplied to stateless model checking."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.03536v1", 
    "other_authors": "Elaheh Ghassabani, Mohammad Abdollahi Azgomi", 
    "title": "DSCMC: Distributed Stateless Code Model Checker", 
    "arxiv-id": "1603.03536v1", 
    "author": "Mohammad Abdollahi Azgomi", 
    "publish": "2016-03-11T06:49:43Z", 
    "summary": "Stateless code model checking is an effective verification technique, which\nis more applicable than stateful model checking to the software world. Existing\nstateless model checkers support the verification of neither LTL formulae nor\nthe information flow security properties. This paper proposes a distributed\nstateless code model checker (DSCMC) designed based on the Actor model, and has\nthe capability of verifying code written in different programming languages.\nThis tool is implemented using Erlang, which is an actor-based programming\nlanguage. DSCMC is able to detect deadlocks, livelocks, and data races\nautomatically. In addition, the tool can verify information flow security and\nthe properties specified in LTL. Thanks to its actor-based architecture, DSCMC\nprovides a wide range of capabilities. The parallel architecture of the tool\nexploiting the rich concurrency model of Erlang is suited to the time-intensive\nprocess of stateless code model checking."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.04298v1", 
    "other_authors": "Matthijs V\u00e1k\u00e1r", 
    "title": "An Effectful Treatment of Dependent Types", 
    "arxiv-id": "1603.04298v1", 
    "author": "Matthijs V\u00e1k\u00e1r", 
    "publish": "2016-03-14T15:09:42Z", 
    "summary": "We extend Levy's call-by-push-value (CBPV) analysis from simple to dependent\ntype theory (DTT) in order to study the interaction between computational\neffects and dependent types. We define the naive system of dependently typed\nCBPV, dCBPV-, and its extension with a principle of Kleisli extensions for\ndependent functions, dCBPV+. We investigate these systems from the points of\nview of syntax, categorical semantics, concrete models and operational\nsemantics, in presence of a range of effects. We observe that, while the\nexpressive power of dCBPV+ is needed if we want well-defined call-by-value\n(CBV) and call-by-name (CBN) translations of DTT, it is a less straightforward\nsystem than dCBPV-, in presence of some effects. Indeed, to be able to\nconstruct specific models and to retain the subject reduction property in the\noperational semantics, we are required to impose certain subtyping conditions,\nthe idea being that the type of a computation may only become more (not less)\nspecified as certain effects are executed."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.05197v1", 
    "other_authors": "Shayan Najd, Sam Lindley, Josef Svenningsson, Philip Wadler", 
    "title": "Embedding by Normalisation", 
    "arxiv-id": "1603.05197v1", 
    "author": "Philip Wadler", 
    "publish": "2016-03-16T18:04:04Z", 
    "summary": "This paper presents the insight that practical embedding techniques, commonly\nused for implementing Domain-Specific Languages, correspond to theoretical\nNormalisation-By-Evaluation (NBE) techniques, commonly used for deriving\ncanonical form of terms with respect to an equational theory.\n  NBE constitutes of four components: a syntactic domain, a semantic domain,\nand a pair of translations between the two. Embedding also often constitutes of\nfour components: an object language, a host language, encoding of object terms\nin the host, and extraction of object code from the host.\n  The correspondence is deep in that all four components in embedding and NBE\ncorrespond to each other. Based on this correspondence, this paper introduces\nEmbedding-By-Normalisation (EBN) as a principled approach to study and\nstructure embedding.\n  The correspondence is useful in that solutions from NBE can be borrowed to\nsolve problems in embedding. In particular, based on NBE techniques, such as\nType-Directed Partial Evaluation, this paper presents a solution to the problem\nof extracting object code from embedded programs involving sum types, such as\nconditional expressions, and primitives, such as literals and operations on\nthem."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.08059v1", 
    "other_authors": "Steven Meyer", 
    "title": "CVC Verilog Compiler -- Fast Complex Language Compilers Can be Simple", 
    "arxiv-id": "1603.08059v1", 
    "author": "Steven Meyer", 
    "publish": "2016-03-26T00:25:37Z", 
    "summary": "This paper explains why the CVC Verilog hardware description language (HDL)\noptimized flow graph compiled simulator is fast. CVC is arguably the fastest\nfull IEEE 1364 2005 standard compiled Verilog simulator available yet consists\nof only 95,000 lines of C code and was developed by only two people. The paper\nexplains how CVC validates the anti-formalism computer science methodology best\nexpressed by Peter Naur's datalogy and provides specific guidelines for\napplying the method. CVC development history from a slow interpreter into a\nfast flow graph based machine code compiled simulator is described. The failure\nof initial efforts that tried to convert CVC into interpreted execution of\npossibly auto generated virtual machines is discussed. The paper presents\nevidence showing CVC's speed by comparing CVC against the open source Icarus\nsimulator. CVC is normally 35 to 45 times faster than Icarus, but can be as\nfast as 111 times or as slow as 30 times. The paper then criticizes the\ncompeting Allen and Kennedy theory from their \"Optimizing Compilers\" book that\nargues fast Verilog simulation requires detail removing high level abstraction.\nCVC speed comes from efficient low level usage of microprocessor instruction\nparallelism. The paper concludes with a discussion of why the author believes\nspecial purpose full accurate delay 1364 standard hardware Verilog simulators\nand parallel Verilog simulation distributed over many processors can not be\nfaster. CVC is available as open source software."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.08648v2", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "A Comparison of NOOP to Structural Domain-Theoretic Models of   Object-Oriented Programming", 
    "arxiv-id": "1603.08648v2", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2016-03-29T05:35:34Z", 
    "summary": "Mainstream object-oriented programming languages such as Java, C#, C++ and\nScala are all almost entirely nominally-typed. NOOP is a recently developed\ndomain-theoretic model of OOP that was designed to include full nominal\ninformation found in nominally-typed OOP. This paper compares NOOP to the most\nwidely known domain-theoretic models of OOP, namely, the models developed by\nCardelli and Cook, which were structurally-typed models. Leveraging the\ndevelopment of NOOP, the comparison presented in this paper provides a clear\nand precise mathematical account for the relation between nominal and\nstructural OO type systems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.08949v2", 
    "other_authors": "Cl\u00e1udio Vasconcelos, Ant\u00f3nio Ravara", 
    "title": "The While language", 
    "arxiv-id": "1603.08949v2", 
    "author": "Ant\u00f3nio Ravara", 
    "publish": "2016-03-29T20:30:27Z", 
    "summary": "This article presents a formalisation of a simple imperative programming\nlanguage. The objective is to study and develop \"hands-on\" a formal\nspecifcation of a programming language, namely its syntax, operational\nsemantics and type system. To have an executable version of the language, we\nimplemented in Racket its operational semantics and type system."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.09274v3", 
    "other_authors": "Somnath Mazumdar, Roberto Giorgi", 
    "title": "A Survey on Hardware and Software Support for Thread Level Parallelism", 
    "arxiv-id": "1603.09274v3", 
    "author": "Roberto Giorgi", 
    "publish": "2016-03-30T16:49:31Z", 
    "summary": "To support growing massive parallelism, functional components and also the\ncapabilities of current processors are changing and continue to do so. Todays\ncomputers are built upon multiple processing cores and run applications\nconsisting of a large number of threads, making runtime thread management a\ncomplex process. Further, each core can support multiple, concurrent thread\nexecution. Hence, hardware and software support for threads is more and more\nneeded to improve peak-performance capacity, overall system throughput, and has\ntherefore been the subject of much research. This paper surveys, many of the\nproposed or currently available solutions for executing, distributing and\nmanaging threads both in hardware and software. The nature of current\napplications is diverse. To increase the system performance, all programming\nmodels may not be suitable to harness the built-in massive parallelism of\nmulticore processors. Due to the heterogeneity in hardware, hybrid programming\nmodel (which combines the features of shared and distributed model) currently\nhas become very promising. In this paper, first, we have given an overview of\nthreads, threading mechanisms and its management issues during execution. Next,\nwe discuss about different parallel programming models considering to their\nexplicit thread support. We also review the programming models with respect to\ntheir support to shared-memory, distributed-memory and heterogeneity. Hardware\nsupport at execution time is very crucial to the performance of the system,\nthus different types of hardware support for threads also exist or have been\nproposed, primarily based on widely used programming models. We also further\ndiscuss on software support for threads, to mainly increase the deterministic\nbehavior during runtime. Finally, we conclude the paper by discussing some\ncommon issues related to the thread management."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.09290v1", 
    "other_authors": "Andres N\u00f6tzli, Fraser Brown", 
    "title": "LifeJacket: Verifying precise floating-point optimizations in LLVM", 
    "arxiv-id": "1603.09290v1", 
    "author": "Fraser Brown", 
    "publish": "2016-03-30T17:46:57Z", 
    "summary": "Optimizing floating-point arithmetic is vital because it is ubiquitous,\ncostly, and used in compute-heavy workloads. Implementing precise optimizations\ncorrectly, however, is difficult, since developers must account for all the\nesoteric properties of floating-point arithmetic to ensure that their\ntransformations do not alter the output of a program. Manual reasoning is error\nprone and stifles incorporation of new optimizations. We present an approach to\nautomate reasoning about floating-point optimizations using satisfiability\nmodulo theories (SMT) solvers. We implement the approach in LifeJacket, a\nsystem for automatically verifying precise floating-point optimizations for the\nLLVM assembly language. We have used LifeJacket to verify 43 LLVM optimizations\nand to discover eight incorrect ones, including three previously unreported\nproblems. LifeJacket is an open source extension of the Alive system for\noptimization verification."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1603.09597v6", 
    "other_authors": "Pritam M. Gharat, Uday P. Khedker, Alan Mycroft", 
    "title": "Flow- and Context-Sensitive Points-to Analysis using Generalized   Points-to Graphs", 
    "arxiv-id": "1603.09597v6", 
    "author": "Alan Mycroft", 
    "publish": "2016-03-31T14:05:30Z", 
    "summary": "Computing precise (fully flow-sensitive and context-sensitive) and exhaustive\npoints-to information is computationally expensive. Many practical tools\napproximate the points-to information trading precision for efficiency. This\nhas adverse impact on computationally intensive analyses such as model\nchecking. Past explorations in top-down approaches of fully flow- and\ncontext-sensitive points-to analysis (FCPA) have not scaled. We explore the\nalternative of bottom-up interprocedural approach which constructs summary flow\nfunctions for procedures to represent the effect of their calls. This approach\nhas been effectively used for many analyses. However, it is computationally\nexpensive for FCPA which requires modelling unknown locations accessed\nindirectly through pointers. Such accesses are commonly handled by using\nplaceholders to explicate unknown locations or by using multiple call-specific\nsummary flow functions.\n  We generalize the concept of points-to relations by using the counts of\nindirection levels leaving the unknown locations implicit. This allows us to\ncreate summary flow functions in the form of generalized points-to graphs\n(GPGs) without the need of placeholders. By design, GPGs represent both memory\n(in terms of classical points-to facts) and memory transformers (in terms of\ngeneralized points-to facts). We perform FCPA by progressively reducing\ngeneralized points-to facts to classical points-to facts. GPGs distinguish\nbetween may and must pointer updates thereby facilitating strong updates within\ncalling contexts.\n  The size of GPG is linearly bounded by the number of variables and is\nindependent of the number of statements in the procedure. Empirical\nmeasurements on SPEC benchmarks show that GPGs are indeed compact in spite of\nlarge procedure sizes. This allows us to scale FCPA to 158 kLoC using GPGs\n(compared to 35 kLoC reported by liveness-based FCPA)."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1604.01290v1", 
    "other_authors": "Vladimir N. Makarov", 
    "title": "Implementation of the Programming Language Dino -- A Case Study in   Dynamic Language Performance", 
    "arxiv-id": "1604.01290v1", 
    "author": "Vladimir N. Makarov", 
    "publish": "2016-04-05T15:12:06Z", 
    "summary": "The article gives a brief overview of the current state of programming\nlanguage Dino in order to see where its stands between other dynamic\nprogramming languages. Then it describes the current implementation, used tools\nand major implementation decisions including how to implement a stable,\nportable and simple JIT compiler.\n  We study the effect of major implementation decisions on the performance of\nDino on x86-64, AARCH64, and Powerpc64. In brief, the performance of some model\nbenchmark on x86-64 was improved by $\\textbf{3.1}$ times after moving from a\nstack based virtual machine to a register-transfer architecture, a further\n$\\textbf{1.5}$ times by adding byte code combining, a further $\\textbf{2.3}$\ntimes through the use of JIT, and a further $\\textbf{4.4}$ times by performing\ntype inference with byte code specialization, with a resulting overall\nperformance improvement of about $\\textbf{47}$ times. To put these results in\ncontext, we include performance comparisons of Dino with widely used\nimplementations of Ruby, Python 3, PyPy and JavaScript on the three platforms\nmentioned above.\n  The goal of this article is to share the experience of Dino implementation\nwith other dynamic language implementors in hope that it can help them to\nimprove implementation of popular dynamic languages to make them probably\nfaster and more portable, using less developer resources, and may be to avoid\nsome mistakes and wrong directions which were experienced during Dino\ndevelopment."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1604.02474v3", 
    "other_authors": "Michael Greenberg", 
    "title": "Space-Efficient Latent Contracts", 
    "arxiv-id": "1604.02474v3", 
    "author": "Michael Greenberg", 
    "publish": "2016-04-08T20:30:12Z", 
    "summary": "Standard higher-order contract monitoring breaks tail recursion and leads to\nspace leaks that can change a program's asymptotic complexity. Space efficient\nsemantics restore tail recursion and bound the amount of space used by\ncontacts. Space efficient contract monitoring for contracts enforcing simple\ntypes are well studied. Prior work establishes a space-efficient semantics in a\nmanifest setting (Greenberg 2015); we adapt that work to a latent calculus and\nextend it work with dependent contracts in contract PCF (CPCF; see Dimoulas et\nal. 2011)."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1604.02480v1", 
    "other_authors": "Panagiotis Vekris, Benjamin Cosman, Ranjit Jhala", 
    "title": "Refinement Types for TypeScript", 
    "arxiv-id": "1604.02480v1", 
    "author": "Ranjit Jhala", 
    "publish": "2016-04-08T20:50:15Z", 
    "summary": "We present Refined TypeScript (RSC), a lightweight refinement type system for\nTypeScript, that enables static verification of higher-order, imperative\nprograms. We develop a formal core of RSC that delineates the interaction\nbetween refinement types and mutability. Next, we extend the core to account\nfor the imperative and dynamic features of TypeScript. Finally, we evaluate RSC\non a set of real world benchmarks, including parts of the Octane benchmarks,\nD3, Transducers, and the TypeScript compiler."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-39570-8_8", 
    "link": "http://arxiv.org/pdf/1604.03020v1", 
    "other_authors": "Hongwei Xi, Hanwen Wu", 
    "title": "Linearly Typed Dyadic Group Sessions for Building Multiparty Sessions", 
    "arxiv-id": "1604.03020v1", 
    "author": "Hanwen Wu", 
    "publish": "2016-04-11T16:23:24Z", 
    "summary": "Traditionally, each party in a (dyadic or multiparty) session implements\nexactly one role specified in the type of the session. We refer to this kind of\nsession as an individual session (i-session). As a generalization of i-session,\na group session (g-session) is one in which each party may implement a group of\nroles based on one channel. In particular, each of the two parties involved in\na dyadic g-session implements either a group of roles or its complement. In\nthis paper, we present a formalization of g-sessions in a multi-threaded\nlambda-calculus (MTLC) equipped with a linear type system, establishing for the\nMTLC both type preservation and global progress. As this formulated MTLC can be\nreadily embedded into ATS, a full-fledged language with a functional\nprogramming core that supports both dependent types (of DML-style) and linear\ntypes, we obtain a direct implementation of linearly typed g-sessions in ATS.\nThe primary contribution of the paper lies in both of the identification of\ng-sessions as a fundamental building block for multiparty sessions and the\ntheoretical development in support of this identification."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10766-016-0426-5", 
    "link": "http://arxiv.org/pdf/1604.03211v1", 
    "other_authors": "Alcides Fonseca, Bruno Cabral, Jo\u00e3o Rafael, Ivo Correia", 
    "title": "Automatic Parallelization: Executing Sequential Programs on a Task-Based   Parallel Runtime", 
    "arxiv-id": "1604.03211v1", 
    "author": "Ivo Correia", 
    "publish": "2016-04-11T18:22:03Z", 
    "summary": "There are billions of lines of sequential code inside nowadays' software\nwhich do not benefit from the parallelism available in modern multicore\narchitectures. Automatically parallelizing sequential code, to promote an\nefficient use of the available parallelism, has been a research goal for some\ntime now. This work proposes a new approach for achieving such goal. We created\na new parallelizing compiler that analyses the read and write instructions, and\ncontrol-flow modifications in programs to identify a set of dependencies\nbetween the instructions in the program. Afterwards, the compiler, based on the\ngenerated dependencies graph, rewrites and organizes the program in a\ntask-oriented structure. Parallel tasks are composed by instructions that\ncannot be executed in parallel. A work-stealing-based parallel runtime is\nresponsible for scheduling and managing the granularity of the generated tasks.\nFurthermore, a compile-time granularity control mechanism also avoids creating\nunnecessary data-structures. This work focuses on the Java language, but the\ntechniques are general enough to be applied to other programming languages. We\nhave evaluated our approach on 8 benchmark programs against OoOJava, achieving\nhigher speedups. In some cases, values were close to those of a manual\nparallelization. The resulting parallel code also has the advantage of being\nreadable and easily configured to improve further its performance manually."
},{
    "category": "cs.PL", 
    "doi": "10.1007/s10766-016-0426-5", 
    "link": "http://arxiv.org/pdf/1604.03641v1", 
    "other_authors": "Brianna M. Ren, Jeffrey S. Foster", 
    "title": "Just-in-Time Static Type Checking for Dynamic Languages", 
    "arxiv-id": "1604.03641v1", 
    "author": "Jeffrey S. Foster", 
    "publish": "2016-04-13T03:04:17Z", 
    "summary": "Dynamic languages such as Ruby, Python, and JavaScript have many compelling\nbenefits, but the lack of static types means subtle errors can remain latent in\ncode for a long time. While many researchers have developed various systems to\nbring some of the benefits of static types to dynamic languages, prior\napproaches have trouble dealing with metaprogramming, which generates code as\nthe program executes. In this paper, we propose Hummingbird, a new system that\nuses a novel technique, just-in-time static type checking, to type check Ruby\ncode even in the presence of metaprogramming. In Hummingbird, method type\nsignatures are gathered dynamically at run-time, as those methods are created.\nWhen a method is called, Hummingbird statically type checks the method body\nagainst current type signatures. Thus, Hummingbird provides thorough static\nchecks on a per-method basis, while also allowing arbitrarily complex\nmetaprogramming. For performance, Hummingbird memoizes the static type checking\npass, invalidating cached checks only if necessary. We formalize Hummingbird\nusing a core, Ruby-like language and prove it sound. To evaluate Hummingbird,\nwe applied it to six apps, including three that use Ruby on Rails, a powerful\nframework that relies heavily on metaprogramming. We found that all apps\ntypecheck successfully using Hummingbird, and that Hummingbird's performance\noverhead is reasonable. We applied Hummingbird to earlier versions of one Rails\napp and found several type errors that had been introduced and then fixed.\nLastly, we demonstrate using Hummingbird in Rails development mode to typecheck\nan app as live updates are applied to it."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.04695v1", 
    "other_authors": "Michael D. Adams, Celeste Hollenbeck, Matthew Might", 
    "title": "On the Complexity and Performance of Parsing with Derivatives", 
    "arxiv-id": "1604.04695v1", 
    "author": "Matthew Might", 
    "publish": "2016-04-16T05:20:44Z", 
    "summary": "Current algorithms for context-free parsing inflict a trade-off between ease\nof understanding, ease of implementation, theoretical complexity, and practical\nperformance. No algorithm achieves all of these properties simultaneously.\n  Might et al. (2011) introduced parsing with derivatives, which handles\narbitrary context-free grammars while being both easy to understand and simple\nto implement. Despite much initial enthusiasm and a multitude of independent\nimplementations, its worst-case complexity has never been proven to be better\nthan exponential. In fact, high-level arguments claiming it is fundamentally\nexponential have been advanced and even accepted as part of the folklore.\nPerformance ended up being sluggish in practice, and this sluggishness was\ntaken as informal evidence of exponentiality.\n  In this paper, we reexamine the performance of parsing with derivatives. We\nhave discovered that it is not exponential but, in fact, cubic. Moreover,\nsimple (though perhaps not obvious) modifications to the implementation by\nMight et al. (2011) lead to an implementation that is not only easy to\nunderstand but also highly performant in practice."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.04729v1", 
    "other_authors": "Rohin Shah, Emina Torlak, Rastislav Bodik", 
    "title": "SIMPL: A DSL for Automatic Specialization of Inference Algorithms", 
    "arxiv-id": "1604.04729v1", 
    "author": "Rastislav Bodik", 
    "publish": "2016-04-16T11:45:03Z", 
    "summary": "Inference algorithms in probabilistic programming languages (PPLs) can be\nthought of as interpreters, since an inference algorithm traverses a model\ngiven evidence to answer a query. As with interpreters, we can improve the\nefficiency of inference algorithms by compiling them once the model, evidence\nand query are known. We present SIMPL, a domain specific language for inference\nalgorithms, which uses this idea in order to automatically specialize annotated\ninference algorithms. Due to the approach of specialization, unlike a\ntraditional compiler, with SIMPL new inference algorithms can be added easily,\nand still be optimized using domain-specific information. We evaluate SIMPL and\nshow that partial evaluation gives a 2-6x speedup, caching provides an\nadditional 1-1.5x speedup, and generating C code yields an additional 13-20x\nspeedup, for an overall speedup of 30-150x for several inference algorithms and\nmodels."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.05044v1", 
    "other_authors": "Marco Patrignani, Dominique Devriese, Frank Piessens", 
    "title": "On Modular and Fully-Abstract Compilation -- Technical Appendix", 
    "arxiv-id": "1604.05044v1", 
    "author": "Frank Piessens", 
    "publish": "2016-04-18T08:54:33Z", 
    "summary": "Secure compilation studies compilers that generate target-level components\nthat are as secure as their source-level counterparts. Full abstraction is the\nmost widely-proven property when defining a secure compiler. A compiler is\nmodular if it allows different components to be compiled independently and then\nto be linked together to form a whole program. Unfortunately, many existing\nfully-abstract compilers to untyped machine code are not modular. So, while\nfully-abstractly compiled components are secure from malicious attackers, if\nthey are linked against each other the resulting component may become\nvulnerable to attacks. This paper studies how to devise modular, fully-abstract\ncompilers. It first analyses the attacks arising when compiled programs are\nlinked together, identifying security threats that are due to linking. Then, it\ndefines a compiler from an object-based language with method calls and dynamic\nmemory allocation to untyped assembly language extended with a memory isolation\nmechanism. The paper provides a proof sketch that the defined compiler is\nfully-abstract and modular, so its output can be linked together without\nintroducing security violations."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.05841v2", 
    "other_authors": "Prasanna Kumar. K, Amitabha Sanyal, Amey Karkare", 
    "title": "Liveness-Based Garbage Collection for Lazy Languages", 
    "arxiv-id": "1604.05841v2", 
    "author": "Amey Karkare", 
    "publish": "2016-04-20T06:40:55Z", 
    "summary": "We consider the problem of reducing the memory required to run lazy\nfirst-order functional programs. Our approach is to analyze programs for\nliveness of heap-allocated data. The result of the analysis is used to preserve\nonly live data---a subset of reachable data---during garbage collection. The\nresult is an increase in the garbage reclaimed and a reduction in the peak\nmemory requirement of programs. While this technique has already been shown to\nyield benefits for eager first-order languages, the lack of a statically\ndeterminable execution order and the presence of closures pose new challenges\nfor lazy languages. These require changes both in the liveness analysis itself\nand in the design of the garbage collector.\n  To show the effectiveness of our method, we implemented a copying collector\nthat uses the results of the liveness analysis to preserve live objects, both\nevaluated (i.e., in WHNF) and closures. Our experiments confirm that for\nprograms running with a liveness-based garbage collector, there is a\nsignificant decrease in peak memory requirements. In addition, a sizable\nreduction in the number of collections ensures that in spite of using a more\ncomplex garbage collector, the execution times of programs running with\nliveness and reachability-based collectors remain comparable."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.06245v4", 
    "other_authors": "Cl\u00e1udio Vasconcelos, Ant\u00f3nio Ravara", 
    "title": "A Revision of the Mool Language", 
    "arxiv-id": "1604.06245v4", 
    "author": "Ant\u00f3nio Ravara", 
    "publish": "2016-04-21T10:24:17Z", 
    "summary": "We present here in a thorough analysis of the Mool language, covering not\nonly its implementation but also the formalisation (syntax, operational\nsemantics, and type system). The objective is to detect glitches in both the\nimplementation and in the formal definitions, proposing as well new features\nand added expressiveness. To test our proposals we implemented the revision\ndeveloped in the Racket platform."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1604.07169v1", 
    "other_authors": "Krishnendu Chatterjee, Hongfei Fu, Amir Kafshdar Goharshady", 
    "title": "Termination Analysis of Probabilistic Programs through   Positivstellensatz's", 
    "arxiv-id": "1604.07169v1", 
    "author": "Amir Kafshdar Goharshady", 
    "publish": "2016-04-25T08:57:28Z", 
    "summary": "We consider nondeterministic probabilistic programs with the most basic\nliveness property of termination. We present efficient methods for termination\nanalysis of nondeterministic probabilistic programs with polynomial guards and\nassignments. Our approach is through synthesis of polynomial ranking\nsupermartingales, that on one hand significantly generalizes linear ranking\nsupermartingales and on the other hand is a counterpart of polynomial\nranking-functions for proving termination of nonprobabilistic programs. The\napproach synthesizes polynomial ranking-supermartingales through\nPositivstellensatz's, yielding an efficient method which is not only sound, but\nalso semi-complete over a large subclass of programs. We show experimental\nresults to demonstrate that our approach can handle several classical programs\nwith complex polynomial guards and assignments, and can synthesize efficient\nquadratic ranking-supermartingales when a linear one does not exist even for\nsimple affine programs."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2908080.2908128", 
    "link": "http://arxiv.org/pdf/1605.00283v2", 
    "other_authors": "Gilles Barthe, Gian Pietro Farina, Marco Gaboardi, Emilio Jes\u00f9s Gallego Arias, Andy Gordon, Justin Hsu, Pierre-Yves Strub", 
    "title": "Differentially Private Bayesian Programming", 
    "arxiv-id": "1605.00283v2", 
    "author": "Pierre-Yves Strub", 
    "publish": "2016-05-01T17:49:30Z", 
    "summary": "We present PrivInfer, an expressive framework for writing and verifying\ndifferentially private Bayesian machine learning algorithms. Programs in\nPrivInfer are written in a rich functional probabilistic programming language\nwith constructs for performing Bayesian inference. Then, differential privacy\nof programs is established using a relational refinement type system, in which\nrefinements on probability types are indexed by a metric on distributions. Our\nframework leverages recent developments in Bayesian inference, probabilistic\nprogramming languages, and in relational refinement types. We demonstrate the\nexpressiveness of PrivInfer by verifying privacy for several examples of\nprivate Bayesian inference."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.01352v1", 
    "other_authors": "Sergio Antoy, Michael Hanus", 
    "title": "Default Rules for Curry", 
    "arxiv-id": "1605.01352v1", 
    "author": "Michael Hanus", 
    "publish": "2016-05-04T17:18:05Z", 
    "summary": "In functional logic programs, rules are applicable independently of textual\norder, i.e., any rule can potentially be used to evaluate an expression. This\nis similar to logic languages and contrary to functional languages, e.g.,\nHaskell enforces a strict sequential interpretation of rules. However, in some\nsituations it is convenient to express alternatives by means of compact default\nrules. Although default rules are often used in functional programs, the\nnon-deterministic nature of functional logic programs does not allow to\ndirectly transfer this concept from functional to functional logic languages in\na meaningful way. In this paper we propose a new concept of default rules for\nCurry that supports a programming style similar to functional programming while\npreserving the core properties of functional logic programming, i.e.,\ncompleteness, non-determinism, and logic-oriented use of functions. We discuss\nthe basic concept and propose an implementation which exploits advanced\nfeatures of functional logic languages."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.01362v1", 
    "other_authors": "Esben Andreasen, Colin S. Gordon, Satish Chandra, Manu Sridharan, Frank Tip, Koushik Sen", 
    "title": "Trace Typing: An Approach for Evaluating Retrofitted Type Systems   (Extended Version)", 
    "arxiv-id": "1605.01362v1", 
    "author": "Koushik Sen", 
    "publish": "2016-05-04T17:59:58Z", 
    "summary": "Recent years have seen growing interest in the retrofitting of type systems\nonto dynamically-typed programming languages, in order to improve type safety,\nprogrammer productivity, or performance. In such cases, type system developers\nmust strike a delicate balance between disallowing certain coding patterns to\nkeep the type system simple, or including them at the expense of additional\ncomplexity and effort. Thus far, the process for designing retrofitted type\nsystems has been largely ad hoc, because evaluating multiple variations of a\ntype system on large bodies of existing code is a significant undertaking.\n  We present trace typing: a framework for automatically and quantitatively\nevaluating variations of a retrofitted type system on large code bases. The\ntrace typing approach involves gathering traces of program executions,\ninferring types for instances of variables and expressions occurring in a\ntrace, and merging types according to merge strategies that reflect specific\n(combinations of) choices in the source-level type system design space.\n  We evaluated trace typing through several experiments. We compared several\nvariations of type systems retrofitted onto JavaScript, measuring the number of\nprogram locations with type errors in each case on a suite of over fifty\nthousand lines of JavaScript code. We also used trace typing to validate and\nguide the design of a new retrofitted type system that enforces fixed object\nlayout for JavaScript objects. Finally, we leveraged the types computed by\ntrace typing to automatically identify tag tests --- dynamic checks that refine\na type --- and examined the variety of tests identified."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.01480v1", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "Towards Understanding Generics", 
    "arxiv-id": "1605.01480v1", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2016-05-05T03:19:11Z", 
    "summary": "This article reports on steps towards building a simple and accurate\ndomain-theoretic model of generic nominally-typed OOP."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.02033v1", 
    "other_authors": "Ramkumar Lakshminarayanan, Balaji Dhanasekaran, Ben George Ephre", 
    "title": "A Study on Features and Limitations of On-line C Compilers", 
    "arxiv-id": "1605.02033v1", 
    "author": "Ben George Ephre", 
    "publish": "2016-04-30T09:41:07Z", 
    "summary": "Compilers are used to run programs that are written in a range of designs\nfrom text to executable formats. With the advent of the internet, studies\nrelated to the development of cloud based compilers are being carried out.\nThere is a considerable increase of on-line compilers enabling on-line\ncompilation of user programs without any mandate to. This study is specific to\non-line C compilers to investigate the correctness, issues and limitations."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.02142v2", 
    "other_authors": "Jonathan Sterling", 
    "title": "Nominal LCF: A Language for Generic Proof", 
    "arxiv-id": "1605.02142v2", 
    "author": "Jonathan Sterling", 
    "publish": "2016-05-07T05:05:40Z", 
    "summary": "The syntax and semantics of user-supplied hypothesis names in tactic\nlanguages is a thorny problem, because the binding structure of a proof is a\nfunction of the goal at which a tactic script is executed. We contribute a new\nlanguage to deal with the dynamic and interactive character of names in tactic\nscripts called Nominal LCF, and endow it with a denotational semantics in\ndI-domains. A large fragment of Nominal LCF has already been implemented and\nused to great effect in the new RedPRL proof assistant."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.02935v1", 
    "other_authors": "Casper Bach Poulsen, Peter D. Mosses", 
    "title": "Flag-Based Big-Step Semantics", 
    "arxiv-id": "1605.02935v1", 
    "author": "Peter D. Mosses", 
    "publish": "2016-05-10T10:47:02Z", 
    "summary": "Structural operational semantic specifications come in different styles:\nsmall-step and big-step. A problem with the big-step style is that specifying\ndivergence and abrupt termination gives rise to annoying duplication. We\npresent a novel approach to representing divergence and abrupt termination in\nbig-step semantics using status flags. This avoids the duplication problem, and\nuses fewer rules and premises for representing divergence than previous\napproaches in the literature."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.02941v1", 
    "other_authors": "Tomas Petricek, Gustavo Guerra, Don Syme", 
    "title": "Types from data: Making structured data first-class citizens in F#", 
    "arxiv-id": "1605.02941v1", 
    "author": "Don Syme", 
    "publish": "2016-05-10T11:09:13Z", 
    "summary": "Most modern applications interact with external services and access data in\nstructured formats such as XML, JSON and CSV. Static type systems do not\nunderstand such formats, often making data access more cumbersome. Should we\ngive up and leave the messy world of external data to dynamic typing and\nruntime checks? Of course, not!\n  We present F# Data, a library that integrates external structured data into\nF#. As most real-world data does not come with an explicit schema, we develop a\nshape inference algorithm that infers a shape from representative sample\ndocuments. We then integrate the inferred shape into the F# type system using\ntype providers. We formalize the process and prove a relative type soundness\ntheorem.\n  Our library significantly reduces the amount of data access code and it\nprovides additional safety guarantees when contrasted with the widely used\nweakly typed techniques."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.03636v1", 
    "other_authors": "Pavel \u010cadek, Jan Strej\u010dek, Marek Trt\u00edk", 
    "title": "Tighter Loop Bound Analysis (Technical report)", 
    "arxiv-id": "1605.03636v1", 
    "author": "Marek Trt\u00edk", 
    "publish": "2016-05-11T23:01:53Z", 
    "summary": "We present a new algorithm for computing upper bounds on the number of\nexecutions of each program instruction during any single program run. The upper\nbounds are expressed as functions of program input values. The algorithm is\nprimarily designed to produce bounds that are relatively tight, i.e. not\nunnecessarily blown up. The upper bounds for instructions allow us to infer\nloop bounds, i.e.~upper bounds on the number of loop iterations. Experimental\nresults show that the algorithm implemented in a prototype tool Looperman often\nproduces tighter bounds than current tools for loop bound analysis."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.04477v2", 
    "other_authors": "Nils Jansen, Christian Dehnert, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Lukas Westhofen", 
    "title": "Bounded Model Checking for Probabilistic Programs", 
    "arxiv-id": "1605.04477v2", 
    "author": "Lukas Westhofen", 
    "publish": "2016-05-14T22:34:59Z", 
    "summary": "In this paper we investigate the applicability of standard model checking\napproaches to verifying properties in probabilistic programming. As the\noperational model for a standard probabilistic program is a potentially\ninfinite parametric Markov decision process, no direct adaption of existing\ntechniques is possible. Therefore, we propose an on-the-fly approach where the\noperational model is successively created and verified via a step-wise\nexecution of the program. This approach enables to take key features of many\nprobabilistic programs into account: nondeterminism and conditioning. We\ndiscuss the restrictions and demonstrate the scalability on several benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.05274v2", 
    "other_authors": "Radu Grigore", 
    "title": "Java Generics are Turing Complete", 
    "arxiv-id": "1605.05274v2", 
    "author": "Radu Grigore", 
    "publish": "2016-05-17T18:13:37Z", 
    "summary": "This paper describes a reduction from the halting problem of Turing machines\nto subtype checking in Java. It follows that subtype checking in Java is\nundecidable, which answers a question posed by Kennedy and Pierce in 2007. It\nalso follows that Java's type checker can recognize any recursive language,\nwhich improves a result of Gil and Levy from 2016. The latter point is\nillustrated by a parser generator for fluent interfaces."
},{
    "category": "cs.PL", 
    "doi": "10.1017/S1471068416000168", 
    "link": "http://arxiv.org/pdf/1605.08475v1", 
    "other_authors": "Nirvan Tyagi, Jayson Lynch, Erik D. Demaine", 
    "title": "Toward an Energy Efficient Language and Compiler for (Partially)   Reversible Algorithms", 
    "arxiv-id": "1605.08475v1", 
    "author": "Erik D. Demaine", 
    "publish": "2016-05-26T23:25:24Z", 
    "summary": "We introduce a new programming language for expressing reversibility,\nEnergy-Efficient Language (Eel), geared toward algorithm design and\nimplementation. Eel is the first language to take advantage of a partially\nreversible computation model, where programs can be composed of both reversible\nand irreversible operations. In this model, irreversible operations cost energy\nfor every bit of information created or destroyed. To handle programs of\nvarying degrees of reversibility, Eel supports a log stack to automatically\ntrade energy costs for space costs, and introduces many powerful control logic\noperators including protected conditional, general conditional, protected\nloops, and general loops. In this paper, we present the design and compiler for\nthe three language levels of Eel along with an interpreter to simulate and\nannotate incurred energy costs of a program."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2951913.2951928", 
    "link": "http://arxiv.org/pdf/1606.01106v2", 
    "other_authors": "Giuseppe Castagna, Tommaso Petrucciani, Kim Nguyen", 
    "title": "Set-Theoretic Types for Polymorphic Variants", 
    "arxiv-id": "1606.01106v2", 
    "author": "Kim Nguyen", 
    "publish": "2016-06-03T14:34:55Z", 
    "summary": "Polymorphic variants are a useful feature of the OCaml language whose current\ndefinition and implementation rely on kinding constraints to simulate a\nsubtyping relation via unification. This yields an awkward formalization and\nresults in a type system whose behaviour is in some cases unintuitive and/or\nunduly restrictive. In this work, we present an alternative formalization of\npoly-morphic variants, based on set-theoretic types and subtyping, that yields\na cleaner and more streamlined system. Our formalization is more expressive\nthan the current one (it types more programs while preserving type safety), it\ncan internalize some meta-theoretic properties, and it removes some\npathological cases of the current implementation resulting in a more intuitive\nand, thus, predictable type system. More generally, this work shows how to add\nfull-fledged union types to functional languages of the ML family that usually\nrely on the Hindley-Milner type system. As an aside, our system also improves\nthe theory of semantic subtyping, notably by proving completeness for the type\nreconstruction algorithm."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2951913.2951928", 
    "link": "http://arxiv.org/pdf/1606.01400v2", 
    "other_authors": "Anton Podkopaev, Ilya Sergey, Aleksandar Nanevski", 
    "title": "Operational Aspects of C/C++ Concurrency", 
    "arxiv-id": "1606.01400v2", 
    "author": "Aleksandar Nanevski", 
    "publish": "2016-06-04T17:40:40Z", 
    "summary": "In this work, we present a family of operational semantics that gradually\napproximates the realistic program behaviors in the C/C++11 memory model. Each\nsemantics in our framework is built by elaborating and combining two simple\ningredients: viewfronts and operation buffers. Viewfronts allow us to express\nthe spatial aspect of thread interaction, i.e., which values a thread can read,\nwhile operation buffers enable manipulation with the temporal execution aspect,\ni.e., determining the order in which the results of certain operations can be\nobserved by concurrently running threads.\n  Starting from a simple abstract state machine, through a series of gradual\nrefinements of the abstract state, we capture such language aspects and\nsynchronization primitives as release/acquire atomics, sequentially-consistent\nand non-atomic memory accesses, also providing a semantics for relaxed atomics,\nwhile avoiding the Out-of-Thin-Air problem. To the best of our knowledge, this\nis the first formal and executable operational semantics of C11 capable of\nexpressing all essential concurrent aspects of the standard.\n  We illustrate our approach via a number of characteristic examples, relating\nthe observed behaviors to those of standard litmus test programs from the\nliterature. We provide an executable implementation of the semantics in PLT\nRedex, along with a number of implemented litmus tests and examples, and\nshowcase our prototype on a large case study: randomized testing and debugging\nof a realistic Read-Copy-Update data structure."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2951913.2951928", 
    "link": "http://arxiv.org/pdf/1606.03644v1", 
    "other_authors": "Matthias Springer", 
    "title": "Inter-language Collaboration in an Object-oriented Virtual Machine", 
    "arxiv-id": "1606.03644v1", 
    "author": "Matthias Springer", 
    "publish": "2016-06-11T23:52:54Z", 
    "summary": "Multi-language virtual machines have a number of advantages. They allow\nsoftware developers to use software libraries that were written for different\nprogramming languages. Furthermore, language implementors do not have to bother\nwith low-level VM functionality and their implementation can benefit from\noptimizations in existing virtual machines. MagLev is an implementation of the\nRuby programming language on top of the GemStone/S virtual machine for the\nSmalltalk programming language. In this work, we present how software\ncomponents written in both languages can interact. We show how MagLev unifies\nthe Smalltalk and the Ruby object model, taking into account Smalltalk meta\nclasses, Ruby singleton classes, and Ruby modules. Besides, we show how we can\ncall Ruby methods from Smalltalk and vice versa. We also present MagLev's\nconcept of bridge methods for implementing Ruby method calling conventions.\nFinally, we compare our solution to other language implementations and virtual\nmachines."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2951913.2951928", 
    "link": "http://arxiv.org/pdf/1606.03809v2", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "Why Nominal-Typing Matters in Object-Oriented Programming", 
    "arxiv-id": "1606.03809v2", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2016-06-13T04:37:13Z", 
    "summary": "The statements `inheritance is not subtyping' and `mainstream OO languages\nunnecessarily place restrictions over inheritance' have rippled as mantras\nthrough the PL research community for years. Many mainstream OO developers and\nOO language designers however do not accept these statements. In\n\\emph{nominally-typed} OO languages that these developers and language\ndesigners are dearly familiar with, inheritance simply is subtyping; and they\nbelieve OO type inheritance is an inherently nominal notion not a structural\none.\n  Nominally-typed OO languages are among the most used programming languages\ntoday. However, the value of nominal typing to mainstream OO developers, as a\nmeans for designing robust OO software, seems to be in wait for full\nappreciation among PL researchers--thereby perpetuating an unnecessary schism\nbetween many OO developers and language designers and many OO PL researchers,\nwith each side discounting, if not even disregarding, the views of the other.\n  In this essay we strengthen earlier efforts to demonstrate the semantic value\nof nominal typing by presenting a technical comparison between nominal OO type\nsystems and structural OO type systems. Recently, a domain-theoretic model of\nnominally-typed OOP was compared to well-known models of structurally-typed\nOOP. Combined, these comparisons provide a clear and deep account for the\nrelation between nominal and structural OO type systems that has not been\npresented before, and they help demonstrate the key value of nominal typing and\nnominal subtyping to OO developers and language designers.\n  We believe a clearer understanding of the key semantic advantage of pure\nnominal OO typing over pure structural OO typing can help remedy the existing\nschism. We believe future foundational OO PL research, to further its relevance\nto mainstream OOP, should be based less on structural models of OOP and more on\nnominal ones instead."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2951913.2951928", 
    "link": "http://arxiv.org/pdf/1606.04240v1", 
    "other_authors": "Keehang Kwon", 
    "title": "For-loops in Logic Programming", 
    "arxiv-id": "1606.04240v1", 
    "author": "Keehang Kwon", 
    "publish": "2016-06-14T08:06:01Z", 
    "summary": "Logic programming has traditiLogic programming has traditionally lacked\ndevices for expressing iterative tasks. To overcome this problem, this paper\nproposes iterative goal formulas of the form $\\seqandq{x}{L} G$ where $G$ is a\ngoal, $x$ is a variable, and $L$ is a list. $\\seqandq{x}{L}$ is called a\nparallel bounded quantifier. These goals allow us to specify the following\ntask: iterate $G$ with $x$ ranging over all the elements of $L$. onally lacked\ndevices for expressing iterative tasks. To overcome this problem, this paper\nproposes iterative goal formulas of the form $\\seqandq{x}{L} G$ where $G$ is a\ngoal, $x$ is a variable, and $L$ is a list. $\\seqandq{x}{L}$ is called a\nparallel bounded quantifier. These goals allow us to specify the following\ntask: iterate $G$ with $x$ ranging over all the elements of $L$."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.211.4", 
    "link": "http://arxiv.org/pdf/1606.05939v1", 
    "other_authors": "Pierpaolo Degano, Gian-Luigi Ferrari, Letterio Galletta", 
    "title": "Event-driven Adaptation in COP", 
    "arxiv-id": "1606.05939v1", 
    "author": "Letterio Galletta", 
    "publish": "2016-06-20T01:09:07Z", 
    "summary": "Context-Oriented Programming languages provide us with primitive constructs\nto adapt program behaviour depending on the evolution of their operational\nenvironment, namely the context. In previous work we proposed ML_CoDa, a\ncontext-oriented language with two-components: a declarative constituent for\nprogramming the context and a functional one for computing. This paper\ndescribes an extension of ML_CoDa to deal with adaptation to unpredictable\ncontext changes notified by asynchronous events."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.211.5", 
    "link": "http://arxiv.org/pdf/1606.05940v1", 
    "other_authors": "Tony Garnock-Jones", 
    "title": "From Events to Reactions: A Progress Report", 
    "arxiv-id": "1606.05940v1", 
    "author": "Tony Garnock-Jones", 
    "publish": "2016-06-20T01:09:16Z", 
    "summary": "Syndicate is a new coordinated, concurrent programming language. It occupies\na novel point on the spectrum between the shared-everything paradigm of threads\nand the shared-nothing approach of actors. Syndicate actors exchange messages\nand share common knowledge via a carefully controlled database that clearly\nscopes conversations. This approach clearly simplifies coordination of\nconcurrent activities. Experience in programming with Syndicate, however,\nsuggests a need to raise the level of linguistic abstraction. In addition to\nwriting event handlers and managing event subscriptions directly, the language\nwill have to support a reactive style of programming. This paper presents\nevent-oriented Syndicate programming and then describes a preliminary design\nfor augmenting it with new reactive programming constructs."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.212.2", 
    "link": "http://arxiv.org/pdf/1606.06378v1", 
    "other_authors": "Philip Johnson-Freyd, Paul Downen, Zena M. Ariola", 
    "title": "First Class Call Stacks: Exploring Head Reduction", 
    "arxiv-id": "1606.06378v1", 
    "author": "Zena M. Ariola", 
    "publish": "2016-06-21T00:45:41Z", 
    "summary": "Weak-head normalization is inconsistent with functional extensionality in the\ncall-by-name $\\lambda$-calculus. We explore this problem from a new angle via\nthe conflict between extensionality and effects. Leveraging ideas from work on\nthe $\\lambda$-calculus with control, we derive and justify alternative\noperational semantics and a sequence of abstract machines for performing head\nreduction. Head reduction avoids the problems with weak-head reduction and\nextensionality, while our operational semantics and associated abstract\nmachines show us how to retain weak-head reduction's ease of implementation."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.212.4", 
    "link": "http://arxiv.org/pdf/1606.06380v1", 
    "other_authors": "Maciej Pir\u00f3g, Jeremy Gibbons", 
    "title": "From Push/Enter to Eval/Apply by Program Transformation", 
    "arxiv-id": "1606.06380v1", 
    "author": "Jeremy Gibbons", 
    "publish": "2016-06-21T00:46:03Z", 
    "summary": "Push/enter and eval/apply are two calling conventions used in implementations\nof functional languages. In this paper, we explore the following observation:\nwhen considering functions with multiple arguments, the stack under the\npush/enter and eval/apply conventions behaves similarly to two particular\nimplementations of the list datatype: the regular cons-list and a form of lists\nwith lazy concatenation respectively. Along the lines of Danvy et al.'s\nfunctional correspondence between definitional interpreters and abstract\nmachines, we use this observation to transform an abstract machine that\nimplements push/enter into an abstract machine that implements eval/apply. We\nshow that our method is flexible enough to transform the push/enter Spineless\nTagless G-machine (which is the semantic core of the GHC Haskell compiler) into\nits eval/apply variant."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.212.5", 
    "link": "http://arxiv.org/pdf/1606.06381v1", 
    "other_authors": "Neil Sculthorpe, Paolo Torrini, Peter D. Mosses", 
    "title": "A Modular Structural Operational Semantics for Delimited Continuations", 
    "arxiv-id": "1606.06381v1", 
    "author": "Peter D. Mosses", 
    "publish": "2016-06-21T00:46:13Z", 
    "summary": "It has been an open question as to whether the Modular Structural Operational\nSemantics framework can express the dynamic semantics of call/cc. This paper\nshows that it can, and furthermore, demonstrates that it can express the more\ngeneral delimited control operators control and shift."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1606.06726v1", 
    "other_authors": "Tobias Pape, Carl Friedrich Bolz, Robert Hirschfeld", 
    "title": "Adaptive Just-in-time Value Class Optimization for Lowering Memory   Consumption and Improving Execution Time Performance", 
    "arxiv-id": "1606.06726v1", 
    "author": "Robert Hirschfeld", 
    "publish": "2016-06-21T19:57:15Z", 
    "summary": "The performance of value classes is highly dependent on how they are\nrepresented in the virtual machine. Value class instances are immutable, have\nno identity, and can only refer to other value objects or primitive values and\nsince they should be very lightweight and fast, it is important to optimize\nthem carefully. In this paper we present a technique to detect and compress\ncommon patterns of value class usage to improve memory usage and performance.\nThe technique identifies patterns of frequent value object references and\nintroduces abbreviated forms for them. This allows to store multiple\ninter-referenced value objects in an inlined memory representation, reducing\nthe overhead stemming from meta-data and object references. Applied to a small\nprototype and an implementation of the Racket language, we found improvements\nin memory usage and execution time for several micro-benchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1606.07557v1", 
    "other_authors": "Eric L Seidel, Ranjit Jhala, Westley Weimer", 
    "title": "Dynamic Witnesses for Static Type Errors", 
    "arxiv-id": "1606.07557v1", 
    "author": "Westley Weimer", 
    "publish": "2016-06-24T03:42:28Z", 
    "summary": "Static type errors are a common stumbling block for newcomers to typed\nfunctional languages. We present a dynamic approach to explaining type errors\nby generating counterexample witness inputs that illustrate how an ill-typed\nprogram goes wrong. First, given an ill-typed function, we symbolically execute\nthe body to dynamically synthesize witness values that can make the program go\nwrong. We prove that our procedure synthesizes general witnesses in that if a\nwitness is found, then for all inhabited input types, there exist values that\ncan make the function go wrong. Second, we show how to extend the above\nprocedure to produce a reduction graph that can be used to interactively\nvisualize and debug witness executions. Third, we evaluate the coverage of our\napproach on two data sets comprising over 4,500 ill-typed student programs. Our\ntechnique is able to generate witnesses for 88% of the programs, and our\nreduction graph yields small counterexamples for 81% of the witnesses. Finally,\nwe evaluate the utility of our witnesses in helping students understand and fix\ntype errors, and find that students presented with our witnesses consistently\nshow a greater understanding of type errors than those presented with a\nstandard error message."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1606.07687v1", 
    "other_authors": "Stefan Schulze Frielinghaus, Helmut Seidl, Ralf Vogler", 
    "title": "Enforcing Termination of Interprocedural Analysis", 
    "arxiv-id": "1606.07687v1", 
    "author": "Ralf Vogler", 
    "publish": "2016-06-24T13:57:35Z", 
    "summary": "Interprocedural analysis by means of partial tabulation of summary functions\nmay not terminate when the same procedure is analyzed for infinitely many\nabstract calling contexts or when the abstract domain has infinite strictly\nascending chains. As a remedy, we present a novel local solver for general\nabstract equation systems, be they monotonic or not, and prove that this solver\nfails to terminate only when infinitely many variables are encountered. We\nclarify in which sense the computed results are sound. Moreover, we show that\ninterprocedural analysis performed by this novel local solver, is guaranteed to\nterminate for all non-recursive programs --- irrespective of whether the\ncomplete lattice is infinite or has infinite strictly ascending or descending\nchains."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1606.08708v1", 
    "other_authors": "Chelsea Battell", 
    "title": "Domain Specific Language for Modular Knitting Pattern Definitions: Purl", 
    "arxiv-id": "1606.08708v1", 
    "author": "Chelsea Battell", 
    "publish": "2016-06-25T14:15:34Z", 
    "summary": "Purl is a language to be used for modular definition and verification of\nknitting patterns. The syntax is similar to the standard knitting pattern\nnotation provided by the Craft Yarn Council. Purl provides constructs not\navailable in the standard notation to allow reuse of segments of patterns. This\nreport describes the basics of knitting and hand-knitting patterns. A knitting\npattern language more terse than the standard notation is presented with the\nimplementation of a compiler to this standard."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1607.00825v1", 
    "other_authors": "Stefan Richthofer", 
    "title": "Garbage Collection in JyNI - How to bridge Mark/Sweep and Reference   Counting GC", 
    "arxiv-id": "1607.00825v1", 
    "author": "Stefan Richthofer", 
    "publish": "2016-07-01T19:11:01Z", 
    "summary": "Jython is a Java-based Python implementation and the most seamless way to\nintegrate Python and Java. It achieves high efficiency by compiling Python code\nto Java bytecode and thus letting Java's JIT optimize it - an approach that\nenables Python code to call Java functions or to subclass Java classes. It\nenables Python code to leverage Java's multithreading features and utilizes\nJava's built-in garbage collection (GC). However, it currently does not support\nCPython's C-API and thus does not support native extensions like NumPy and\nSciPy. Since most scientific code depends on such extensions, it is not\nrunnable with Jython. Jython Native Interface (JyNI) is a compatibility layer\nthat aims to provide CPython's native C extension API on top of Jython. JyNI is\nimplemented using the Java Native Interface (JNI) and its native part is\ndesigned to be binary compatible with existing extension builds [...]."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.scico.2016.08.003", 
    "link": "http://arxiv.org/pdf/1607.01590v1", 
    "other_authors": "Ulrich Neumerkel, Stefan Kral", 
    "title": "Indexing dif/2", 
    "arxiv-id": "1607.01590v1", 
    "author": "Stefan Kral", 
    "publish": "2016-07-06T12:27:45Z", 
    "summary": "Many Prolog programs are unnecessarily impure because of inadequate means to\nexpress syntactic inequality. While the frequently provided built-in `dif/2` is\nable to correctly describe expected answers, its direct use in programs often\nleads to overly complex and inefficient definitions --- mainly due to the lack\nof adequate indexing mechanisms. We propose to overcome these problems by using\na new predicate that subsumes both equality and inequality via reification.\nCode complexity is reduced with a monotonic, higher-order if-then-else\nconstruct based on `call/N`. For comparable correct uses of impure definitions,\nour approach is as determinate and similarly efficient as its impure\ncounterparts."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.216.3", 
    "link": "http://arxiv.org/pdf/1607.02226v1", 
    "other_authors": "Julien Cohen", 
    "title": "Renaming Global Variables in C Mechanically Proved Correct", 
    "arxiv-id": "1607.02226v1", 
    "author": "Julien Cohen", 
    "publish": "2016-07-08T05:30:43Z", 
    "summary": "Most integrated development environments are shipped with refactoring tools.\nHowever, their refactoring operations are often known to be unreliable. As a\nconsequence, developers have to test their code after applying an automatic\nrefactoring. In this article, we consider a refactoring operation (renaming of\nglobal variables in C), and we prove that its core implementation preserves the\nset of possible behaviors of transformed programs. That proof of correctness\nrelies on the operational semantics of C provided by CompCert C in Coq."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.216.7", 
    "link": "http://arxiv.org/pdf/1607.02229v1", 
    "other_authors": "Venkatesh Kannan, G. W. Hamilton", 
    "title": "Program Transformation to Identify List-Based Parallel Skeletons", 
    "arxiv-id": "1607.02229v1", 
    "author": "G. W. Hamilton", 
    "publish": "2016-07-08T05:31:26Z", 
    "summary": "Algorithmic skeletons are used as building-blocks to ease the task of\nparallel programming by abstracting the details of parallel implementation from\nthe developer. Most existing libraries provide implementations of skeletons\nthat are defined over flat data types such as lists or arrays. However,\nskeleton-based parallel programming is still very challenging as it requires\nintricate analysis of the underlying algorithm and often uses inefficient\nintermediate data structures. Further, the algorithmic structure of a given\nprogram may not match those of list-based skeletons. In this paper, we present\na method to automatically transform any given program to one that is defined\nover a list and is more likely to contain instances of list-based skeletons.\nThis facilitates the parallel execution of a transformed program using existing\nimplementations of list-based parallel skeletons. Further, by using an existing\ntransformation called distillation in conjunction with our method, we produce\ntransformed programs that contain fewer inefficient intermediate data\nstructures."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.02231v1", 
    "other_authors": "Mirko Viroli, Jacob Beal", 
    "title": "Resiliency with Aggregate Computing: State of the Art and Roadmap", 
    "arxiv-id": "1607.02231v1", 
    "author": "Jacob Beal", 
    "publish": "2016-07-08T05:35:59Z", 
    "summary": "One of the difficulties in developing collective adaptive systems is the\nchallenge of simultaneously engineering both the desired resilient behaviour of\nthe collective and the details of its implementation on individual devices.\nAggregate computing simplifies this problem by separating these aspects into\ndifferent layers of abstraction by means of a unifying notion of computational\nfield and a functional computational model. We review the state of the art in\naggregate computing, discuss the various resiliency properties it supports, and\ndevelop a roadmap of foundational problems still needing to be addressed in the\ncontinued development of this emerging discipline."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.02238v1", 
    "other_authors": "Duc-Hiep Chu, Joxan Jaffar, Vijayaraghavan Murali", 
    "title": "Incremental Quantitative Analysis on Dynamic Costs", 
    "arxiv-id": "1607.02238v1", 
    "author": "Vijayaraghavan Murali", 
    "publish": "2016-07-08T05:48:14Z", 
    "summary": "In quantitative program analysis, values are assigned to execution traces to\nrepresent a quality measure. Such analyses cover important applications, e.g.\nresource usage. Examining all traces is well known to be intractable and\ntherefore traditional algorithms reason over an over-approximated set.\nTypically, inaccuracy arises due to inclusion of infeasible paths in this set.\nThus path-sensitivity is one cure. However, there is another reason for the\ninaccuracy: that the cost model, i.e., the way in which the analysis of each\ntrace is quantified, is dynamic. That is, the cost of a trace is dependent on\nthe context in which the trace is executed. Thus the goal of accurate analysis,\nalready challenged by path-sensitivity, is now further challenged by\ncontext-sensitivity.\n  In this paper, we address the problem of quantitative analysis defined over a\ndynamic cost model. Our algorithm is an \"anytime\" algorithm: it generates an\nanswer quickly, but if the analysis resource budget allows, it progressively\nproduces better solutions via refinement iterations. The result of each\niteration remains sound, but importantly, must converge to an exact analysis\nwhen given an unlimited resource budget. In order to be scalable, our algorithm\nis designed to be incremental. We finally give evidence that a new level of\npracticality is achieved by an evaluation on a realistic collection of\nbenchmarks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.02927v1", 
    "other_authors": "Silvia Crafa, Luca Padovani", 
    "title": "On the chemistry of typestate-oriented actors", 
    "arxiv-id": "1607.02927v1", 
    "author": "Luca Padovani", 
    "publish": "2016-07-11T12:49:23Z", 
    "summary": "Typestate-oriented programming is an extension of the OO paradigm in which\nobjects are modeled not just in terms of interfaces but also in terms of their\nusage protocols, describing legal sequences of method calls, possibly depending\non the object's internal state. We argue that the Actor Model allows\ntypestate-OOP in an inherently distributed setting, whereby objects/actors can\nbe accessed concurrently by several processes, and local entities cooperate to\ncarry out a communication protocol. In this article we illustrate the approach\nby means of a number of examples written in Scala Akka. We show that Scala's\nabstractions support clean and natural typestate-oriented actor programming\nwith the usual asynchronous and non-blocking semantics. We also show that the\nstandard type system of Scala and a typed wrapping of usual (untyped) Akka's\nActorRef are enough to provide rich forms of type safety so that well-typed\nactors respect their intended communication protocols. This approach draws on a\nsolid theoretical background, consisting of a sound behavioral type system for\nthe Join Calculus, that is a foundational calculus of distributed asynchronous\nprocesses whose semantics is based on the Chemical Abstract Machine, that\nunveiled its strong connections with typestate-oriented programming of both\nconcurrent objects and actors."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.03445v1", 
    "other_authors": "Nadia Polikarpova, Jean Yang, Shachar Itzhaky, Armando Solar-Lezama", 
    "title": "Type-Driven Repair for Information Flow Security", 
    "arxiv-id": "1607.03445v1", 
    "author": "Armando Solar-Lezama", 
    "publish": "2016-07-12T17:43:54Z", 
    "summary": "We present Lifty, a language that uses type-driven program repair to enforce\ninformation flow policies. In Lifty, the programmer specifies a policy by\nannotating the source of sensitive data with a refinement type, and the system\nautomatically inserts access checks necessary to enforce this policy across the\ncode. This is a significant improvement over current practice, where\nprogrammers manually implement access checks, and any missing check can cause\nan information leak. To support this programming model, we have developed (1)\nan encoding of information flow security in terms of decidable refinement types\nthat enables fully automatic verification and (2) a program repair algorithm\nthat localizes unsafe accesses to sensitive data and replaces them with\nprovably secure alternatives. We formalize the encoding and prove its\nnoninterference guarantee. Our experience using Lifty to implement a conference\nmanagement system shows that it decreases policy burden and is able to\nefficiently synthesize all necessary access checks, including those required to\nprevent a set of reported real-world information leaks."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.04180v4", 
    "other_authors": "Cyrus Omar, Ian Voysey, Michael Hilton, Jonathan Aldrich, Matthew A. Hammer", 
    "title": "Hazelnut: A Bidirectionally Typed Structure Editor Calculus", 
    "arxiv-id": "1607.04180v4", 
    "author": "Matthew A. Hammer", 
    "publish": "2016-07-14T16:07:29Z", 
    "summary": "Structure editors allow programmers to edit the tree structure of a program\ndirectly. This can have cognitive benefits, particularly for novice and\nend-user programmers (as evidenced by the popularity of structure editors like\nScratch.) It also simplifies matters for tool designers, because they do not\nneed to contend with malformed program text.\n  This paper defines Hazelnut, a structure editor based on a small\nbidirectionally typed lambda calculus extended with holes and a cursor (a la\nHuet's zipper.) Hazelnut goes one step beyond syntactic well-formedness: it's\nedit actions operate over statically meaningful (i.e. well-typed) terms.\nNaively, this prohibition on ill-typed edit states would force the programmer\nto construct terms in a rigid \"outside-in\" manner. To avoid this problem, the\naction semantics automatically places terms assigned a type that is\ninconsistent with the expected type inside a hole. This safely defers the type\nconsistency check until the term inside the hole is finished.\n  Hazelnut is a foundational type-theoretic account of typed structure editing,\nrather than an end-user tool itself. To that end, we describe how Hazelnut's\nrich metatheory, which we have mechanized in Agda, guides the definition of an\nextension to the calculus. We also discuss various plausible evaluation\nstrategies for terms with holes, and in so doing reveal connections with\ngradual typing and contextual modal type theory (the Curry-Howard\ninterpretation of contextual modal logic.) Finally, we discuss how Hazelnut's\nsemantics lends itself to implementation as a functional reactive program. Our\nreference implementation is written using js_of_ocaml."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.217.3", 
    "link": "http://arxiv.org/pdf/1607.04197v1", 
    "other_authors": "Romain Vernoux", 
    "title": "Design of an intermediate representation for query languages", 
    "arxiv-id": "1607.04197v1", 
    "author": "Romain Vernoux", 
    "publish": "2016-07-14T16:37:17Z", 
    "summary": "Data oriented applications, usually written in a high-level, general-purpose\nprogramming language (such as Java) interact with database through a coarse\ninterface. Informally, the text of a query is built on the application side\n(either via plain string concatenation or through an abstract notion of\nstatement) and shipped to the database over the wire where it is executed. The\nresults are then serialized and sent back to the \"client-code\" where they are\ntranslated in the language's native datatypes. This round trip is detrimental\nto performances but, worse, such a programming model prevents one from having\nricher queries, namely queries containing user-defined functions (that is\nfunctions defined by the programmer and used e.g. in the filter condition of a\nSQL query). While some databases also possess a \"server-side\" language (e.g.\nPL/SQL in Oracle database), its integration with the very-optimized query\nexecution engine is still minimal and queries containing (PL/SQL) user-defined\nfunctions remain notoriously inefficient. In this setting, we reviewed existing\nlanguage-integrated query frameworks, highlighting that existing database query\nlanguages (including SQL) share high-level querying primitives (e.g.,\nfiltering, joins, aggregation) that can be represented by operators, but differ\nwidely regarding the semantics of their expression language. In order to\nrepresent queries in an application language- and database-agnostic manner, we\ndesigned a small calculus, dubbed \"QIR\" for Query Intermediate Representation.\nQIR contains expressions, corresponding to a small extension of the pure\nlambda-calculus, and operators to represent usual querying primitives. In the\neffort to send efficient queries to the database, we abstracted the idea of\n\"good\" query representations in a measure on QIR terms. Then, we designed an\nevaluation strategy rewriting QIR query representations into \"better\" ones."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.219.6", 
    "link": "http://arxiv.org/pdf/1607.04461v1", 
    "other_authors": "Gabriele Paganelli", 
    "title": "Horn Binary Serialization Analysis", 
    "arxiv-id": "1607.04461v1", 
    "author": "Gabriele Paganelli", 
    "publish": "2016-07-15T11:05:26Z", 
    "summary": "A bit layout is a sequence of fields of certain bit lengths that specifies\nhow to interpret a serial stream, e.g., the MP3 audio format. A layout with\nvariable length fields needs to include meta-information to help the parser\ninterpret unambiguously the rest of the stream; e.g. a field providing the\nlength of a following variable length field. If no such information is\navailable, then the layout is ambiguous. I present a linear-time algorithm to\ndetermine whether a layout is ambiguous or not by modelling the behaviour of a\nserial parser reading the stream as forward chaining reasoning on a collection\nof Horn clauses."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.219.6", 
    "link": "http://arxiv.org/pdf/1607.05443v2", 
    "other_authors": "Leonidas Lampropoulos, Diane Gallois-Wong, Catalin Hritcu, John Hughes, Benjamin C. Pierce, Li-yao Xia", 
    "title": "Beginner's Luck: A Language for Property-Based Generators", 
    "arxiv-id": "1607.05443v2", 
    "author": "Li-yao Xia", 
    "publish": "2016-07-19T08:01:17Z", 
    "summary": "Property-based random testing a la QuickCheck requires building efficient\ngenerators for well-distributed random data satisfying complex logical\npredicates, but writing these generators can be difficult and error prone. We\npropose a domain-specific language in which generators are conveniently\nexpressed by decorating predicates with lightweight annotations to control both\nthe distribution of generated values and the amount of constraint solving that\nhappens before each variable is instantiated. This language, called Luck, makes\ngenerators easier to write, read, and maintain.\n  We give Luck a formal semantics and prove several fundamental properties,\nincluding the soundness and completeness of random generation with respect to a\nstandard predicate semantics. We evaluate Luck on common examples from the\nproperty-based testing literature and on two significant case studies, showing\nthat it can be used in complex domains with comparable bug-finding\neffectiveness and a significant reduction in testing code size compared to\nhandwritten generators."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.219.6", 
    "link": "http://arxiv.org/pdf/1607.05609v2", 
    "other_authors": "Philipp Haller, Alexandre Loiko", 
    "title": "Object Capabilities and Lightweight Affinity in Scala: Implementation,   Formalization, and Soundness", 
    "arxiv-id": "1607.05609v2", 
    "author": "Alexandre Loiko", 
    "publish": "2016-07-19T14:37:11Z", 
    "summary": "Aliasing is a known source of challenges in the context of imperative\nobject-oriented languages, which have led to important advances in type systems\nfor aliasing control. However, their large-scale adoption has turned out to be\na surprisingly difficult challenge. While new language designs show promise,\nthey do not address the need of aliasing control in existing languages.\n  This paper presents a new approach to isolation and uniqueness in an\nexisting, widely-used language, Scala. The approach is unique in the way it\naddresses some of the most important obstacles to the adoption of type system\nextensions for aliasing control. First, adaptation of existing code requires\nonly a minimal set of annotations. Only a single bit of information is required\nper class. Surprisingly, the paper shows that this information can be provided\nby the object-capability discipline, widely-used in program security. We\nformalize our approach as a type system and prove key soundness theorems. The\ntype system is implemented for the full Scala language, providing, for the\nfirst time, a sound integration with Scala's local type inference. Finally, we\nempirically evaluate the conformity of existing Scala open-source code on a\ncorpus of over 75,000 LOC."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.219.6", 
    "link": "http://arxiv.org/pdf/1607.05707v1", 
    "other_authors": "Sreepathi Pai, Keshav Pingali", 
    "title": "Lowering IrGL to CUDA", 
    "arxiv-id": "1607.05707v1", 
    "author": "Keshav Pingali", 
    "publish": "2016-07-19T19:38:39Z", 
    "summary": "The IrGL intermediate representation is an explicitly parallel representation\nfor irregular programs that targets GPUs. In this report, we describe IrGL\nconstructs, examples of their use and how IrGL is compiled to CUDA by the\nGalois GPU compiler."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1607.05830v5", 
    "other_authors": "Steffen Smolka, Praveen Kumar, Nate Foster, Dexter Kozen, Alexandra Silva", 
    "title": "Cantor meets Scott: Semantic Foundations for Probabilistic Networks", 
    "arxiv-id": "1607.05830v5", 
    "author": "Alexandra Silva", 
    "publish": "2016-07-20T06:05:07Z", 
    "summary": "ProbNetKAT is a probabilistic extension of NetKAT with a denotational\nsemantics based on Markov kernels. The language is expressive enough to\ngenerate continuous distributions, which raises the question of how to compute\neffectively in the language. This paper gives an new characterization of\nProbNetKAT's semantics using domain theory, which provides the foundation\nneeded to build a practical implementation. We show how to use the semantics to\napproximate the behavior of arbitrary ProbNetKAT programs using distributions\nwith finite support. We develop a prototype implementation and show how to use\nit to solve a variety of problems including characterizing the expected\ncongestion induced by different routing schemes and reasoning probabilistically\nabout reachability in a network."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1607.06927v1", 
    "other_authors": "Daniel Kroening, Daniel Poetzl, Peter Schrammel, Bj\u00f6rn Wachter", 
    "title": "Sound Static Deadlock Analysis for C/Pthreads (Extended Version)", 
    "arxiv-id": "1607.06927v1", 
    "author": "Bj\u00f6rn Wachter", 
    "publish": "2016-07-23T12:25:32Z", 
    "summary": "We present a static deadlock analysis approach for C/pthreads. The design of\nour method has been guided by the requirement to analyse real-world code. Our\napproach is sound (i.e., misses no deadlocks) for programs that have defined\nbehaviour according to the C standard, and precise enough to prove\ndeadlock-freedom for a large number of programs. The method consists of a\npipeline of several analyses that build on a new context- and thread-sensitive\nabstract interpretation framework. We further present a lightweight dependency\nanalysis to identify statements relevant to deadlock analysis and thus speed up\nthe overall analysis. In our experimental evaluation, we succeeded to prove\ndeadlock-freedom for 262 programs from the Debian GNU/Linux distribution with\nin total 2.6 MLOC in less than 11 hours."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1607.07727v1", 
    "other_authors": "Navid Khoshavi, Mohammad Maghsoudloo, Hamid R. Zarandi", 
    "title": "Leveraging the Potential of Control-Flow Error Resilient Techniques in   Multithreaded Programs", 
    "arxiv-id": "1607.07727v1", 
    "author": "Hamid R. Zarandi", 
    "publish": "2016-07-20T15:39:55Z", 
    "summary": "This paper presents a software-based technique to recover control-flow errors\nin multithreaded programs. Control-flow error recovery is achieved through\ninserting additional instructions into multithreaded program at compile time\nregarding to two dependency graphs. These graphs are extracted to model\ncontrol-flow and data dependencies among basic blocks and thread interactions\nbetween different threads of a program. In order to evaluate the proposed\ntechnique, three multithreaded benchmarks quick sort, matrix multiplication and\nlinked list utilized to run on a multi-core processor, and a total of 5000\ntransient faults has been injected into several executable points of each\nprogram. The results show that this technique detects and corrects between\n91.9% and 93.8% of the injected faults with acceptable performance and memory\noverheads."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1608.00089v1", 
    "other_authors": "Dana Drachsler-Cohen, Martin Vechev, Eran Yahav", 
    "title": "Optimal Learning of Specifications from Examples", 
    "arxiv-id": "1608.00089v1", 
    "author": "Eran Yahav", 
    "publish": "2016-07-30T08:12:32Z", 
    "summary": "A fundamental challenge in synthesis from examples is designing a learning\nalgorithm that poses the minimal number of questions to an end user while\nguaranteeing that the target hypothesis is discovered. Such guarantees are\npractically important because they ensure that end users will not be\noverburdened with unnecessary questions.\n  We present SPEX -- a learning algorithm that addresses the above challenge.\nSPEX considers the hypothesis space of formulas over first-order predicates and\nlearns the correct hypothesis by only asking the user simple membership queries\nfor concrete examples. Thus, SPEX is directly applicable to any learning\nproblem that fits its hypothesis space and uses membership queries.\n  SPEX works by iteratively eliminating candidate hypotheses from the space\nuntil converging to the target hypothesis. The main idea is to use the\nimplication order between hypotheses to guarantee that in each step the\nquestion presented to the user obtains maximal pruning of the space. This\nproblem is particularly challenging when predicates are potentially correlated.\n  To show that SPEX is practically useful, we expressed two rather different\napplications domains in its framework: learning programs for the domain of\ntechnical analysts (stock trading) and learning data structure specifications.\nThe experimental results show that SPEX's optimality guarantee is effective: it\ndrastically reduces the number of questions posed to the user while\nsuccessfully learning the exact hypothesis."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1608.00787v1", 
    "other_authors": "Alexander Vandenbroucke, Maciej Pir\u00f3g, Benoit Desouter, Tom Schrijvers", 
    "title": "Tabling with Sound Answer Subsumption", 
    "arxiv-id": "1608.00787v1", 
    "author": "Tom Schrijvers", 
    "publish": "2016-08-02T12:23:16Z", 
    "summary": "Tabling is a powerful resolution mechanism for logic programs that captures\ntheir least fixed point semantics more faithfully than plain Prolog. In many\ntabling applications, we are not interested in the set of all answers to a\ngoal, but only require an aggregation of those answers. Several works have\nstudied efficient techniques, such as lattice-based answer subsumption and\nmode-directed tabling, to do so for various forms of aggregation.\n  While much attention has been paid to expressivity and efficient\nimplementation of the different approaches, soundness has not been considered.\nThis paper shows that the different implementations indeed fail to produce\nleast fixed points for some programs. As a remedy, we provide a formal\nframework that generalises the existing approaches and we establish a soundness\ncriterion that explains for which programs the approach is sound.\n  This article is under consideration for acceptance in TPLP."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1608.00816v1", 
    "other_authors": "Amr Hany Saleh, Tom Schrijvers", 
    "title": "Efficient Algebraic Effect Handlers for Prolog", 
    "arxiv-id": "1608.00816v1", 
    "author": "Tom Schrijvers", 
    "publish": "2016-08-02T13:38:00Z", 
    "summary": "Recent work has provided delimited control for Prolog to dynamically\nmanipulate the program control-flow, and to implement a wide range of\ncontrol-flow and dataflow effects on top of. Unfortunately, delimited control\nis a rather primitive language feature that is not easy to use.\n  As a remedy, this work introduces algebraic effect handlers for Prolog, as a\nhigh-level and structured way of defining new side-effects in a modular\nfashion. We illustrate the expressive power of the feature and provide an\nimplementation by means of elaboration into the delimited control primitives.\n  The latter add a non-negligible performance overhead when used extensively.\nTo address this issue, we present an optimised compilation approach that\ncombines partial evaluation with dedicated rewrite rules. The rewrite rules are\ndriven by a lightweight effect inference that analyses what effect operations\nmay be called by a goal. We illustrate the effectiveness of this approach on a\nrange of benchmarks. This article is under consideration for acceptance in\nTPLP."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3009837.3009843", 
    "link": "http://arxiv.org/pdf/1608.00989v1", 
    "other_authors": "Jan Wielemaker, Keri Harris", 
    "title": "Lock-free atom garbage collection for multithreaded Prolog", 
    "arxiv-id": "1608.00989v1", 
    "author": "Keri Harris", 
    "publish": "2016-08-02T20:11:56Z", 
    "summary": "The runtime system of dynamic languages such as Prolog or Lisp and their\nderivatives contain a symbol table, in Prolog often called the atom table. A\nsimple dynamically resizing hash-table used to be an adequate way to implement\nthis table. As Prolog becomes fashionable for 24x7 server processes we need to\ndeal with atom garbage collection and concurrent access to the atom table.\nClassical lock-based implementations to ensure consistency of the atom table\nscale poorly and a stop-the-world approach to implement atom garbage collection\nquickly becomes a bottle-neck, making Prolog unsuitable for soft real-time\napplications. In this article we describe a novel implementation for the atom\ntable using lock-free techniques where the atom-table remains accessible even\nduring atom garbage collection. Relying only on CAS (Compare And Swap) and not\non external libraries, the implementation is straightforward and portable.\n  Under consideration for acceptance in TPLP."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.01106v2", 
    "other_authors": "Maja H. Kirkeby, Mads Rosendahl", 
    "title": "Probabilistic Resource Analysis by Program Transformation", 
    "arxiv-id": "1608.01106v2", 
    "author": "Mads Rosendahl", 
    "publish": "2016-08-03T08:23:34Z", 
    "summary": "The aim of a probabilistic resource analysis is to derive a probability\ndistribution of possible resource usage for a program from a probability\ndistribution of its input. We present an automated multi- phase rewriting based\nmethod to analyze programs written in a subset of C. It generates a probability\ndistribution of the resource usage as a possibly uncomputable expression and\nthen transforms it into a closed form expression using over-approximations. We\npresent the technique, outline the implementation and show results from\nexperiments with the system."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.01723v1", 
    "other_authors": "Osbert Bastani, Rahul Sharma, Alex Aiken, Percy Liang", 
    "title": "Synthesizing Program Input Grammars", 
    "arxiv-id": "1608.01723v1", 
    "author": "Percy Liang", 
    "publish": "2016-08-05T00:13:42Z", 
    "summary": "We present an algorithm for synthesizing a context-free grammar encoding the\nlanguage of valid program inputs from a set of input examples and blackbox\naccess to the program. Our algorithm addresses shortcomings of existing grammar\ninference algorithms, which both severely overgeneralize and are prohibitively\nslow. Our implementation, Glade, leverages the grammar synthesized by our\nalgorithm to fuzz programs with highly structured inputs. We show that Glade\nconsistently increases the incremental coverage compared to two baseline\nfuzzers."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.02565v1", 
    "other_authors": "Isabel Garcia-Contreras, Jose F. Morales, Manuel V. Hermenegildo", 
    "title": "Semantic Code Browsing", 
    "arxiv-id": "1608.02565v1", 
    "author": "Manuel V. Hermenegildo", 
    "publish": "2016-08-08T19:25:10Z", 
    "summary": "Programmers currently enjoy access to a very high number of code repositories\nand libraries of ever increasing size. The ensuing potential for reuse is\nhowever hampered by the fact that searching within all this code becomes an\nincreasingly difficult task. Most code search engines are based on syntactic\ntechniques such as signature matching or keyword extraction. However, these\ntechniques are inaccurate (because they basically rely on documentation) and at\nthe same time do not offer very expressive code query languages. We propose a\nnovel approach that focuses on querying for semantic characteristics of code\nobtained automatically from the code itself. Program units are pre-processed\nusing static analysis techniques, based on abstract interpretation, obtaining\nsafe semantic approximations. A novel, assertion-based code query language is\nused to express desired semantic characteristics of the code as partial\nspecifications. Relevant code is found by comparing such partial specifications\nwith the inferred semantics for program elements. Our approach is fully\nautomatic and does not rely on user annotations or documentation. It is more\npowerful and flexible than signature matching because it is parametric on the\nabstract domain and properties, and does not require type definitions. Also, it\nreasons with relations between properties, such as implication and abstraction,\nrather than just equality. It is also more resilient to syntactic code\ndifferences. We describe the approach and report on a prototype implementation\nwithin the Ciao system.\n  Under consideration for acceptance in TPLP."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.03350v1", 
    "other_authors": "Yutaka Nagashima, Liam O'Connor", 
    "title": "Close Encounters of the Higher Kind Emulating Constructor Classes in   Standard ML", 
    "arxiv-id": "1608.03350v1", 
    "author": "Liam O'Connor", 
    "publish": "2016-08-11T02:17:09Z", 
    "summary": "We implement a library for encoding constructor classes in Standard ML,\nincluding elaboration from minimal definitions, and automatic instantiation of\nsuperclasses."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.03424v1", 
    "other_authors": "Maria Alpuente, Angel Cuenca, Santiago Escobar, Jose Meseguer", 
    "title": "Partial Evaluation of Order-sorted Equational Programs modulo Axioms", 
    "arxiv-id": "1608.03424v1", 
    "author": "Jose Meseguer", 
    "publish": "2016-08-11T11:51:04Z", 
    "summary": "Partial evaluation (PE) is a powerful and general program optimization\ntechnique with many successful applications. However, it has never been\ninvestigated in the context of expressive rule-based languages like Maude,\nCafeOBJ, OBJ, ASF+SDF, and ELAN, which support: 1) rich type structures with\nsorts, subsorts and overloading; 2) equational rewriting modulo axioms such as\ncommutativity, associativity-commutativity, and\nassociativity-commutativity-identity. In this extended abstract, we illustrate\nthe key concepts by showing how they apply to partial evaluation of expressive\nrule-based programs written in Maude. Our partial evaluation scheme is based on\nan automatic unfolding algorithm that computes term variants and relies on\nequational least general generalization for ensuring global termination. We\ndemonstrate the use of the resulting partial evaluator for program optimization\non several examples where it shows significant speed-ups."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.03650v1", 
    "other_authors": "Roberto Amadini, Pierre Flener, Justin Pearson, Joseph D. Scott, Peter J. Stuckey, Guido Tack", 
    "title": "MiniZinc with Strings", 
    "arxiv-id": "1608.03650v1", 
    "author": "Guido Tack", 
    "publish": "2016-08-12T01:17:09Z", 
    "summary": "Strings are extensively used in modern programming languages and constraints\nover strings of unknown length occur in a wide range of real-world applications\nsuch as software analysis and verification, testing, model checking, and web\nsecurity. Nevertheless, practically no CP solver natively supports string\nconstraints. We introduce string variables and a suitable set of string\nconstraints as builtin features of the MiniZinc modelling language.\nFurthermore, we define an interpreter for converting a MiniZinc model with\nstrings into a FlatZinc instance relying on only integer variables. This\nprovides a user-friendly interface for modelling combinatorial problems with\nstrings, and enables both string and non-string solvers to actually solve such\nproblems."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.04016v1", 
    "other_authors": "Sergio Antoy, Andy Jost", 
    "title": "A New Functional-Logic Compiler for Curry: Sprite", 
    "arxiv-id": "1608.04016v1", 
    "author": "Andy Jost", 
    "publish": "2016-08-13T18:41:11Z", 
    "summary": "We introduce a new native code compiler for Curry codenamed Sprite. Sprite is\nbased on the Fair Scheme, a compilation strategy that provides instructions for\ntransforming declarative, non-deterministic programs of a certain class into\nimperative, deterministic code. We outline salient features of Sprite, discuss\nits implementation of Curry programs, and present benchmarking results. Sprite\nis the first-to-date operationally complete implementation of Curry.\nPreliminary results show that ensuring this property does not incur a\nsignificant penalty."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.04415v3", 
    "other_authors": "E. Komendantskaya, P. Johann, M. Schmidt", 
    "title": "A Productivity Checker for Logic Programming", 
    "arxiv-id": "1608.04415v3", 
    "author": "M. Schmidt", 
    "publish": "2016-08-15T21:35:32Z", 
    "summary": "Automated analysis of recursive derivations in logic programming is known to\nbe a hard problem. Both termination and non-termination are undecidable\nproblems in Turing-complete languages. However, some declarative languages\noffer a practical work-around for this problem, by making a clear distinction\nbetween whether a program is meant to be understood inductively or\ncoinductively. For programs meant to be understood inductively, termination\nmust be guaranteed, whereas for programs meant to be understood coinductively,\nproductive non-termination (or \"productivity\") must be ensured. In practice,\nsuch classification helps to better understand and implement some\nnon-terminating computations.\n  Logic programming was one of the first declarative languages to make this\ndistinction: in the 1980's, Lloyd and van Emden's \"computations at infinity\"\ncaptured the big-step operational semantics of derivations that produce\ninfinite terms as answers. In modern terms, computations at infinity describe\n\"global productivity\" of computations in logic programming. Most programming\nlanguages featuring coinduction also provide an observational, or small-step,\nnotion of productivity as a computational counterpart to global productivity.\nThis kind of productivity is ensured by checking that finite initial fragments\nof infinite computations can always be observed to produce finite portions of\ntheir infinite answer terms.\n  In this paper we introduce a notion of observational productivity for logic\nprogramming as an algorithmic approximation of global productivity, give an\neffective procedure for semi-deciding observational productivity, and offer an\nimplemented automated observational productivity checker for logic programs."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.04592v2", 
    "other_authors": "Sung-Shik T. Q. Jongmans, Farhad Arbab", 
    "title": "Data optimizations for constraint automata", 
    "arxiv-id": "1608.04592v2", 
    "author": "Farhad Arbab", 
    "publish": "2016-08-16T13:45:53Z", 
    "summary": "Constraint automata (CA) constitute a coordination model based on finite\nautomata on infinite words. Originally introduced for modeling of coordinators,\nan interesting new application of CAs is implementing coordinators (i.e.,\ncompiling CAs into executable code). Such an approach guarantees\ncorrectness-by-construction and can even yield code that outperforms\nhand-crafted code. The extent to which these two potential advantages\nmaterialize depends on the smartness of CA-compilers and the existence of\nproofs of their correctness.\n  Every transition in a CA is labeled by a \"data constraint\" that specifies an\natomic data-flow between coordinated processes as a first-order formula. At\nrun-time, compiler-generated code must handle data constraints as efficiently\nas possible. In this paper, we present, and prove the correctness of two\noptimization techniques for CA-compilers related to handling of data\nconstraints: a reduction to eliminate redundant variables and a translation\nfrom (declarative) data constraints to (imperative) data commands expressed in\na small sequential language. Through experiments, we show that these\noptimization techniques can have a positive impact on performance of generated\nexecutable code."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.04999v3", 
    "other_authors": "Paul Anderson, James Cheney, Weili Fu, Roly Perera", 
    "title": "$\u03bc$Puppet: A Declarative Subset of the Puppet Configuration Language", 
    "arxiv-id": "1608.04999v3", 
    "author": "Roly Perera", 
    "publish": "2016-08-17T15:26:48Z", 
    "summary": "Puppet is a popular declarative framework for specifying and managing complex\nsystem configurations. The Puppet framework includes a domain-specific language\nwith several advanced features inspired by object-oriented programming,\nincluding user-defined resource types, 'classes' with a form of inheritance,\nand dependency management. Like most real-world languages, the language has\nevolved in an ad hoc fashion, resulting in a design with numerous features,\nsome of which are complex, hard to understand, and difficult to use correctly.\n  We present an operational semantics for $\\mu$Puppet, a representative subset\nof the Puppet language that covers the distinctive features of Puppet, while\nexcluding features that are either deprecated or work-in-progress. Formalising\nthe semantics sheds light on difficult parts of the language, identifies\nopportunities for future improvements, and provides a foundation for future\nanalysis or debugging techniques, such as static typechecking or provenance\ntracking. Our semantics leads straightforwardly to a reference implementation\nin Haskell. We also discuss some of Puppet's idiosyncrasies, particularly its\nhandling of classes and scope, and present an initial corpus of test cases\nsupported by our formal semantics."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.05233v2", 
    "other_authors": "Franti\u0161ek Farka, Ekaterina Komendantskaya, Kevin Hammond", 
    "title": "Coinductive Soundness of Corecursive Type Class Resolution", 
    "arxiv-id": "1608.05233v2", 
    "author": "Kevin Hammond", 
    "publish": "2016-08-18T10:37:22Z", 
    "summary": "Horn clauses and first-order resolution are commonly used to implement type\nclasses in Haskell. Several corecursive extensions to type class resolution\nhave recently been proposed, with the goal of allowing (co)recursive dictionary\nconstruction where resolution does not termi- nate. This paper shows, for the\nfirst time, that corecursive type class resolution and its extensions are\ncoinductively sound with respect to the greatest Herbrand models of logic\nprograms and that they are induc- tively unsound with respect to the least\nHerbrand models. We establish incompleteness results for various fragments of\nthe proof system."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.05263v2", 
    "other_authors": "David Tolpin, Jan Willem van de Meent, Hongseok Yang, Frank Wood", 
    "title": "Design and Implementation of Probabilistic Programming Language Anglican", 
    "arxiv-id": "1608.05263v2", 
    "author": "Frank Wood", 
    "publish": "2016-08-18T14:00:35Z", 
    "summary": "Anglican is a probabilistic programming system designed to interoperate with\nClojure and other JVM languages. We introduce the programming language\nAnglican, outline our design choices, and discuss in depth the implementation\nof the Anglican language and runtime, including macro-based compilation,\nextended CPS-based evaluation model, and functional representations for\nprobabilistic paradigms, such as a distribution, a random process, and an\ninference algorithm.\n  We show that a probabilistic functional language can be implemented\nefficiently and integrated tightly with a conventional functional language with\nonly moderate computational overhead. We also demonstrate how advanced\nprobabilistic modeling concepts are mapped naturally to the functional\nfoundation."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.06012v1", 
    "other_authors": "Matthew A. Hammer, Bor-Yuh Evan Chang, David Van Horn", 
    "title": "A Vision for Online Verification-Validation", 
    "arxiv-id": "1608.06012v1", 
    "author": "David Van Horn", 
    "publish": "2016-08-21T23:42:54Z", 
    "summary": "Today's programmers face a false choice between creating software that is\nextensible and software that is correct. Specifically, dynamic languages permit\nsoftware that is richly extensible (via dynamic code loading, dynamic object\nextension, and various forms of reflection), and today's programmers exploit\nthis flexibility to \"bring their own language features\" to enrich extensible\nlanguages (e.g., by using common JavaScript libraries). Meanwhile, such\nlibrary-based language extensions generally lack enforcement of their\nabstractions, leading to programming errors that are complex to avoid and\npredict.\n  To offer verification for this extensible world, we propose online\nverification-validation (OVV), which consists of language and VM design that\nenables a \"phaseless\" approach to program analysis, in contrast to the standard\nstatic-dynamic phase distinction. Phaseless analysis freely interposes abstract\ninterpretation with concrete execution, allowing analyses to use dynamic\n(concrete) information to prove universal (abstract) properties about future\nexecution.\n  In this paper, we present a conceptual overview of OVV through a motivating\nexample program that uses a hypothetical database library. We present a generic\nsemantics for OVV, and an extension to this semantics that offers a simple\ngradual type system for the database library primitives. The result of\ninstantiating this gradual type system in an OVV setting is a checker that can\nprogressively type successive continuations of the program until a continuation\nis fully verified. To evaluate the proposed vision of OVV for this example, we\nimplement the VM semantics (in Rust), and show that this design permits\nprogressive typing in this manner."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.06499v2", 
    "other_authors": "Danel Ahman, Catalin Hritcu, Kenji Maillard, Guido Martinez, Gordon Plotkin, Jonathan Protzenko, Aseem Rastogi, Nikhil Swamy", 
    "title": "Dijkstra Monads for Free", 
    "arxiv-id": "1608.06499v2", 
    "author": "Nikhil Swamy", 
    "publish": "2016-08-23T13:18:01Z", 
    "summary": "Dijkstra monads enable a dependent type theory to be enhanced with support\nfor specifying and verifying effectful code via weakest preconditions. Together\nwith their closely related counterparts, Hoare monads, they provide the basis\non which verification tools like F*, Hoare Type Theory (HTT), and Ynot are\nbuilt.\n  We show that Dijkstra monads can be derived \"for free\" by applying a\ncontinuation-passing style (CPS) translation to the standard monadic\ndefinitions of the underlying computational effects. Automatically deriving\nDijkstra monads in this way provides a correct-by-construction and efficient\nway of reasoning about user-defined effects in dependent type theories.\n  We demonstrate these ideas in EMF*, a new dependently typed calculus,\nvalidating it via both formal proof and a prototype implementation within F*.\nBesides equipping F* with a more uniform and extensible effect system, EMF*\nenables a novel mixture of intrinsic and extrinsic proofs within F*."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.06583v1", 
    "other_authors": "Jade ALglave, Patrick Cousot", 
    "title": "Syntax and analytic semantics of LISA", 
    "arxiv-id": "1608.06583v1", 
    "author": "Patrick Cousot", 
    "publish": "2016-08-23T17:17:08Z", 
    "summary": "We provide the syntax and semantics of the LISA (for \"Litmus Instruction Set\nArchitecture\") language. The parallel assembly language LISA is implemented in\nthe herd7 tool (http://virginia.cs.ucl.ac.uk/herd/) for simulating weak\nconsistency models."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.07161v1", 
    "other_authors": "Nicholas Tierney", 
    "title": "A Simple Guide to S3 Methods", 
    "arxiv-id": "1608.07161v1", 
    "author": "Nicholas Tierney", 
    "publish": "2016-08-23T06:51:27Z", 
    "summary": "Writing functions in R is an important skill for anyone using R. S3 methods\nallow for functions to be generalised across different classes and are easy to\nimplement. Whilst many R users are be adept at creating their own functions, it\nseems that there is room for many more to take advantage of R's S3 methods.\nThis paper provides a simple and targeted guide to explain what S3 methods are,\nwhy people should them, and how they can do it."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.07206v1", 
    "other_authors": "Jeffrey Murphy, Bhargav Shivkumar, Lukasz Ziarek", 
    "title": "Embedded SML using the MLton compiler", 
    "arxiv-id": "1608.07206v1", 
    "author": "Lukasz Ziarek", 
    "publish": "2016-08-25T16:17:29Z", 
    "summary": "In this extended abstract we present our current work on leveraging Standard\nML for developing embedded and real-time systems. Specifically we detail our\nexperiences in modifying MLton, a whole program, optimizing compiler for\nStandard ML, for use in such contexts. We focus primarily on the language\nruntime, re-working the threading subsystem and garbage collector, as well as\nnecessary changes for integrating MLton generated programs into a light weight\noperating system kernel. We compare and contrast these changes to our previous\nwork on extending MLton for multicore systems, which focused around acheiving\nscalability."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.07261v3", 
    "other_authors": "Satish Chandra, Colin S. Gordon, Jean-Baptiste Jeannin, Cole Schlesinger, Manu Sridharan, Frank Tip, Youngil Choi", 
    "title": "Type Inference for Static Compilation of JavaScript (Extended Version)", 
    "arxiv-id": "1608.07261v3", 
    "author": "Youngil Choi", 
    "publish": "2016-08-25T19:26:22Z", 
    "summary": "We present a type system and inference algorithm for a rich subset of\nJavaScript equipped with objects, structural subtyping, prototype inheritance,\nand first-class methods. The type system supports abstract and recursive\nobjects, and is expressive enough to accommodate several standard benchmarks\nwith only minor workarounds. The invariants enforced by the types enable an\nahead-of-time compiler to carry out optimizations typically beyond the reach of\nstatic compilers for dynamic languages. Unlike previous inference techniques\nfor prototype inheritance, our algorithm uses a combination of lower and upper\nbound propagation to infer types and discover type errors in all code,\nincluding uninvoked functions. The inference is expressed in a simple\nconstraint language, designed to leverage off-the-shelf fixed point solvers. We\nprove soundness for both the type system and inference algorithm. An\nexperimental evaluation showed that the inference is powerful, handling the\naforementioned benchmarks with no manual type annotation, and that the inferred\ntypes enable effective static compilation."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.07531v2", 
    "other_authors": "Jade Alglave, Patrick Cousot, Luc Maranget", 
    "title": "Syntax and semantics of the weak consistency model specification   language cat", 
    "arxiv-id": "1608.07531v2", 
    "author": "Luc Maranget", 
    "publish": "2016-08-26T17:28:06Z", 
    "summary": "We provide the syntax and semantics of the cat language, a domain specific\nlanguage to describe consistency properties of parallel/distributed programs.\nThe language is implemented in the herd7 too\n(http://diy.inria.fr/doc/herd.html)l."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.08219v1", 
    "other_authors": "Loris D'Antoni, Rishabh Singh, Michael Vaughn", 
    "title": "NoFAQ: Synthesizing Command Repairs from Examples", 
    "arxiv-id": "1608.08219v1", 
    "author": "Michael Vaughn", 
    "publish": "2016-08-29T20:00:02Z", 
    "summary": "Command-line tools are confusing and hard to use for novice programmers due\nto their cryptic error messages and lack of documentation. Novice users often\nresort to online help-forums for finding corrections to their buggy commands,\nbut have a hard time in searching precisely for posts that are relevant to\ntheir problem and then applying the suggested solutions to their buggy command.\n  We present a tool, NoFAQ, that uses a set of rules to suggest possible fixes\nwhen users write buggy commands that trigger commonly occurring errors. The\nrules are expressed in a language called FixIt and each rule pattern-matches\nagainst the user's buggy command and the corresponding error message, and uses\nthese inputs to produce a possible fixed command. Our main contribution is an\nalgorithm based on lazy VSA for synthesizing FixIt rules from examples of buggy\nand repaired commands. The algorithm allows users to add new rules in NoFAQ\nwithout having to manually encode them. We present the evaluation of NoFAQ on\n92 benchmark problems and show that NoFAQ is able to instantly synthesize rules\nfor 81 benchmark problems in real time using just 2 to 5 input-output examples\nfor each rule."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1608.08330v2", 
    "other_authors": "Kai Stadtm\u00fcller, Martin Sulzmann, Peter Thiemann", 
    "title": "Static Trace-Based Deadlock Analysis for Synchronous Mini-Go", 
    "arxiv-id": "1608.08330v2", 
    "author": "Peter Thiemann", 
    "publish": "2016-08-30T05:18:07Z", 
    "summary": "We consider the problem of static deadlock detection for programs in the Go\nprogramming language which make use of synchronous channel communications. In\nour analysis, regular expressions extended with a fork operator capture the\ncommunication behavior of a program. Starting from a simple criterion that\ncharacterizes traces of deadlock-free programs, we develop automata-based\nmethods to check for deadlock-freedom. The approach is implemented and\nevaluated with a series of examples."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.01985v1", 
    "other_authors": "Roly Perera, Simon J. Gay", 
    "title": "Behavioural Prototypes", 
    "arxiv-id": "1609.01985v1", 
    "author": "Simon J. Gay", 
    "publish": "2016-09-03T11:14:51Z", 
    "summary": "We sketch a simple language of concurrent objects which explores the design\nspace between type systems and continuous testing. In our language, programs\nare collections of communicating automata checked automatically for multiparty\ncompatibility. This property, taken from the session types literature but here\napplied to terms rather than types, guarantees that no state-related errors\narise during execution: no object gets stuck because it was sent the wrong\nmessage, and every message is processed."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.03146v1", 
    "other_authors": "Mathieu Guillame-Bert", 
    "title": "Honey: A dataflow programming language for the processing, featurization   and analysis of multivariate, asynchronous and non-uniformly sampled scalar   symbolic time sequences", 
    "arxiv-id": "1609.03146v1", 
    "author": "Mathieu Guillame-Bert", 
    "publish": "2016-09-11T10:18:29Z", 
    "summary": "We introduce HONEY; a new specialized programming language designed to\nfacilitate the processing of multivariate, asynchronous and non-uniformly\nsampled symbolic and scalar time sequences. When compiled, a Honey program is\ntransformed into a static process flow diagram, which is then executed by a\nvirtual machine. Honey's most notable features are: (1) Honey introduces a new,\nefficient and non-prone to error paradigm for defining recursive process flow\ndiagrams from text input with the mindset of imperative programming. Honey's\nspecialized, high level and concise syntax allows fast and easy writing,\nreading and maintenance of complex processing of large scalar symbolic time\nsequence datasets. (2) Honey guarantees programs will be executed similarly on\nstatic or real-time streaming datasets. (3) Honey's IDE includes an interactive\nvisualization tool which allows for an interactive exploration of the\nintermediate and final outputs. This combination enables fast incremental\nprototyping, debugging, monitoring and maintenance of complex programs. (4) In\ncase of large datasets (larger than the available memory), Honey programs can\nbe executed to process input greedily. (5) The graphical structure of a\ncompiled program provides several desirable properties, including distributed\nand/or paralleled execution, memory optimization, and program structure\nvisualization. (6) Honey contains a large library of both common and novel\noperators developed through various research projects. An open source C++\nimplementation of Honey as well as the Honey IDE and the interactive data\nvisualizer are publicly available."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.04233v1", 
    "other_authors": "Roly Perera, Simon J. Gay", 
    "title": "Liveness for Verification", 
    "arxiv-id": "1609.04233v1", 
    "author": "Simon J. Gay", 
    "publish": "2016-09-14T12:16:24Z", 
    "summary": "We explore the use of liveness for interactive program verification for a\nsimple concurrent object language. Our experimental IDE integrates two\n(formally dual) kinds of continuous testing into the development environment:\ncompatibility-checking, which verifies an object's use of other objects, and\ncompliance-checking, which verifies an object's claim to refine the behaviour\nof another object. Source code errors highlighted by the IDE are not static\ntype errors but the reflection back to the source of runtime errors that occur\nin some execution of the system. We demonstrate our approach, and discuss\nopportunities and challenges."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.05337v1", 
    "other_authors": "Dakota Fisher, Matthew A. Hammer, William Byrd, Matthew Might", 
    "title": "miniAdapton: A Minimal Implementation of Incremental Computation in   Scheme", 
    "arxiv-id": "1609.05337v1", 
    "author": "Matthew Might", 
    "publish": "2016-09-17T13:53:10Z", 
    "summary": "We describe a complete Scheme implementation of miniAdapton, which implements\nthe core functionality of the Adapton system for incremental computation (also\nknown as self-adjusting computation). Like Adapton, miniAdapton allows\nprogrammers to safely combine mutation and memoization. miniAdapton is built on\ntop of an even simpler system, microAdapton. Both miniAdapton and microAdapton\nare designed to be easy to understand, extend, and port to host languages other\nthan Scheme. We also present adapton variables, a new interface in Adapton for\nvariables intended to represent expressions."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.05365v1", 
    "other_authors": "Nicolas Laurent, Kim Mens", 
    "title": "Taming Context-Sensitive Languages with Principled Stateful Parsing", 
    "arxiv-id": "1609.05365v1", 
    "author": "Kim Mens", 
    "publish": "2016-09-17T17:09:42Z", 
    "summary": "Historically, true context-sensitive parsing has seldom been applied to\nprogramming languages, due to its inherent complexity. However, many mainstream\nprogramming and markup languages (C, Haskell, Python, XML, and more) possess\ncontext-sensitive features. These features are traditionally handled with\nad-hoc code (e.g., custom lexers), outside of the scope of parsing theory.\n  Current grammar formalisms struggle to express context-sensitive features.\nMost solutions lack context transparency: they make grammars hard to write,\nmaintain and compose by hardwiring context through the entire grammar. Instead,\nwe approach context-sensitive parsing through the idea that parsers may recall\npreviously matched input (or data derived therefrom) in order to make parsing\ndecisions. We make use of mutable parse state to enable this form of recall.\n  We introduce principled stateful parsing as a new transactional discipline\nthat makes state changes transparent to parsing mechanisms such as backtracking\nand memoization. To enforce this discipline, users specify parsers using\nformally specified primitive state manipulation operations.\n  Our solution is available as a parsing library named Autumn. We illustrate\nour solution by implementing some practical context-sensitive grammar features\nsuch as significant whitespace handling and namespace classification."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.05687v1", 
    "other_authors": "Rumyana Neykova, Nobuko Yoshida", 
    "title": "Multiparty Session Actors", 
    "arxiv-id": "1609.05687v1", 
    "author": "Nobuko Yoshida", 
    "publish": "2016-09-19T12:27:05Z", 
    "summary": "Actor coordination armoured with a suitable protocol description language has\nbeen a pressing problem in the actors community. We study the applicability of\nmultiparty session type (MPST) protocols for verification of actor programs. We\nincorporate sessions to actors by introducing minimum additions to the model\nsuch as the notion of actor roles and protocol mailboxes. The framework uses\nScribble, which is a protocol description language based on multiparty session\ntypes. Our programming model supports actor-like syntax and runtime\nverification mechanism guaranteeing communication safety of the participating\nentities. An actor can implement multiple roles in a similar way as an object\ncan implement multiple interfaces. Multiple roles allow for cooperative\ninter-concurrency in a single actor. We demonstrate our framework by designing\nand implementing a session actor library in Python and its runtime verification\nmechanism. Benchmark results demonstrate that the runtime checks induce\nnegligible overhead. We evaluate the applicability of our verification\nframework to specify actor interactions by implementing twelve examples from an\nactor benchmark suit."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.09709v1", 
    "other_authors": "Francesco Mazzoli, Andreas Abel", 
    "title": "Type checking through unification", 
    "arxiv-id": "1609.09709v1", 
    "author": "Andreas Abel", 
    "publish": "2016-09-30T13:03:25Z", 
    "summary": "In this paper we describe how to leverage higher-order unification to type\ncheck a dependently typed language with meta-variables. The literature usually\npresents the unification algorithm as a standalone component, however the need\nto check definitional equality of terms while type checking gives rise to a\ntight interplay between type checking and unification. This interplay is a\nmajor source of complexity in the type-checking algorithm for existing\ndependently typed programming languages. We propose an algorithm that encodes a\ntype-checking problem entirely in the form of unification constraints, reducing\nthe complexity of the type-checking code by taking advantage of higher order\nunification, which is already part of the implementation of many dependently\ntyped languages."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1609.09718v1", 
    "other_authors": "Alexey Bandura, Nikita Kurilenko, Manuel Mazzara, Victor Rivera, Larisa Safina, Alexander Tchitchigin", 
    "title": "Jolie Community on the Rise", 
    "arxiv-id": "1609.09718v1", 
    "author": "Alexander Tchitchigin", 
    "publish": "2016-09-30T13:25:05Z", 
    "summary": "Jolie is a programming language that follows the microservices paradigm. As\nan open source project, it has built a community of developers worldwide - both\nin the industry as well as in academia - taken care of the development,\ncontinuously improved its usability, and therefore broadened the adoption. In\nthis paper, we present some of the most recent results and work in progress\nthat has been made within our research team."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1610.00097v3", 
    "other_authors": "Matthew A. Hammer, Joshua Dunfield, Dimitrios J. Economou, Monal Narasimhamurthy", 
    "title": "Typed Adapton: Refinement types for incremental computations with   precise names", 
    "arxiv-id": "1610.00097v3", 
    "author": "Monal Narasimhamurthy", 
    "publish": "2016-10-01T07:01:43Z", 
    "summary": "Over the past decade, programming language techniques for incremental\ncomputation have demonstrated that incremental programs with precise, carefully\nchosen dynamic names for data and sub-computations can dramatically outperform\nnon-incremental programs, as well as those using traditional memoization\n(without such names). However, prior work lacks a verification mechanism to\nsolve the ambiguous name problem, the problem of statically enforcing precise\nnames. We say that an allocated pointer name is precise for an evaluation\nderivation when it identifies at most one value or subcomputation, and\nambiguous otherwise.\n  In this work, we define a refinement type system that gives practical static\napproximations to enforce precise, deterministic allocation names in otherwise\nfunctional programs. We show that this type system permits expressing familiar\nfunctional programs, and generic, composable library components. We prove that\nour type system enforces that well-typed programs name their values and\nsub-computations precisely, without ambiguity. Drawing closer to an\nimplementation, we derive a bidirectional version of the type system, and prove\nthat it corresponds to our declarative type system. A key challenge in\nimplementing the bidirectional system is handling constraints over names, name\nterms and name sets; toward this goal, we give decidable, syntactic rules to\nguide these checks."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1610.00831v1", 
    "other_authors": "Michael Bukatin, Steve Matthews, Andrey Radul", 
    "title": "Notes on Pure Dataflow Matrix Machines: Programming with   Self-referential Matrix Transformations", 
    "arxiv-id": "1610.00831v1", 
    "author": "Andrey Radul", 
    "publish": "2016-10-04T03:10:55Z", 
    "summary": "Dataflow matrix machines are self-referential generalized recurrent neural\nnets. The self-referential mechanism is provided via a stream of matrices\ndefining the connectivity and weights of the network in question. A natural\nquestion is: what should play the role of untyped lambda-calculus for this\nprogramming architecture? The proposed answer is a discipline of programming\nwith only one kind of streams, namely the streams of appropriately shaped\nmatrices. This yields Pure Dataflow Matrix Machines which are networks of\ntransformers of streams of matrices capable of defining a pure dataflow matrix\nmachine."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1610.02144v1", 
    "other_authors": "Robert O'Callahan, Chris Jones, Nathan Froyd, Kyle Huey, Albert Noll, Nimrod Partush", 
    "title": "Lightweight User-Space Record And Replay", 
    "arxiv-id": "1610.02144v1", 
    "author": "Nimrod Partush", 
    "publish": "2016-10-07T05:11:32Z", 
    "summary": "The ability to record and replay program executions with low overhead enables\nmany applications, such as reverse-execution debugging, debugging of\nhard-to-reproduce test failures, and \"black box\" forensic analysis of failures\nin deployed systems. Existing record-and-replay approaches rely on recording an\nentire virtual machine (which is heavyweight), modifying the OS kernel (which\nadds deployment and maintenance costs), or pervasive code instrumentation\n(which imposes significant performance and complexity overhead). We\ninvestigated whether it is possible to build a practical record-and-replay\nsystem avoiding all these issues. The answer turns out to be yes --- if the CPU\nand operating system meet certain non-obvious constraints. Fortunately modern\nIntel CPUs, Linux kernels and user-space frameworks meet these constraints,\nalthough this has only become true recently. With some novel optimizations, our\nsystem RR records and replays real-world workloads with low overhead with an\nentirely user-space implementation running on stock hardware and operating\nsystems. RR forms the basis of an open-source reverse-execution debugger seeing\nsignificant use in practice. We present the design and implementation of RR,\ndescribe its performance on a variety of workloads, and identify constraints on\nhardware and operating system design required to support our approach."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1610.02952v1", 
    "other_authors": "Aziem Chawdhary, Ed Robbins, Andy King", 
    "title": "Incrementally Closing Octagons", 
    "arxiv-id": "1610.02952v1", 
    "author": "Andy King", 
    "publish": "2016-10-10T15:10:01Z", 
    "summary": "The octagon abstract domain is a widely used numeric abstract domain\nexpressing relational information between variables whilst being both\ncomputationally efficient and simple to implement. Each element of the domain\nis a system of constraints where each constraint takes the restricted form $\\pm\nx_i \\pm x_j \\leq d$. A key family of operations for the octagon domain are\nclosure algorithms, which check satisfiability and provide a normal form for\noctagonal constraint systems. We present new quadratic incremental algorithms\nfor closure, strong closure and integer closure and proofs of their\ncorrectness. We highlight the benefits and measure the performance of these new\nalgorithms."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-46559-3_4", 
    "link": "http://arxiv.org/pdf/1610.03148v1", 
    "other_authors": "Qirun Zhang, Chengnian Sun, Zhendong Su", 
    "title": "Skeletal Program Enumeration for Rigorous Compiler Testing", 
    "arxiv-id": "1610.03148v1", 
    "author": "Zhendong Su", 
    "publish": "2016-10-11T01:02:44Z", 
    "summary": "A program can be viewed as a syntactic structure P (syntactic skeleton)\nparameterized by a collection of the identifiers V (variable names). This paper\nintroduces the skeletal program enumeration (SPE) problem: Given a fixed\nsyntactic skeleton P and a set of variables V , enumerate a set of programs P\nexhibiting all possible variable usage patterns within P. It proposes an\neffective realization of SPE for systematic, rigorous compiler testing by\nleveraging three important observations: (1) Programs with different variable\nusage patterns exhibit diverse control- and data-dependence information, and\nhelp exploit different compiler optimizations and stress-test compilers; (2)\nmost real compiler bugs were revealed by small tests (i.e., small-sized P) ---\nthis \"small-scope\" observation opens up SPE for practical compiler validation;\nand (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set,\nand thus can offer a level of guarantee that is absent from all existing\ncompiler testing techniques.\n  The key challenge of SPE is how to eliminate the enormous amount of\nequivalent programs w.r.t. {\\alpha}-conversion. Our main technical contribution\nis a novel algorithm for computing the canonical (and smallest) set of all\nnon-{\\alpha}-equivalent programs. We have realized our SPE technique and\nevaluated it using syntactic skeletons derived from GCC's testsuite. Our\nevaluation results on testing GCC and Clang are extremely promising. In less\nthan six months, our approach has led to 217 confirmed bug reports, 104 of\nwhich have already been fixed, and the majority are long latent bugs despite\nthe extensive prior efforts of automatically testing both compilers (e.g.,\nCsmith and EMI). The results also show that our algorithm for enumerating\nnon-{\\alpha}-equivalent programs provides six orders of magnitude reduction,\nenabling processing the GCC test-suite in under a month."
},{
    "category": "cs.PL", 
    "doi": "10.1145/3001854.3001855", 
    "link": "http://arxiv.org/pdf/1610.04461v1", 
    "other_authors": "Markus Raab", 
    "title": "Persistent Contextual Values as Inter-Process Layers", 
    "arxiv-id": "1610.04461v1", 
    "author": "Markus Raab", 
    "publish": "2016-10-14T13:47:49Z", 
    "summary": "Mobile applications today often fail to be context aware when they also need\nto be customizable and efficient at run-time. Context-oriented programming\nallows programmers to develop applications that are more context aware. Its\ncentral construct, the so-called layer, however, is not customizable. We\npropose to use novel persistent contextual values for mobile development.\nPersistent contextual values automatically adapt their value to the context.\nFurthermore they provide access without overhead. Key-value configuration files\ncontain the specification of contextual values and the persisted contextual\nvalues themselves. By modifying the configuration files, the contextual values\ncan easily be customized for every context. From the specification, we generate\ncode to simplify development. Our implementation, called Elektra, permits\ndevelopment in several languages including C++ and Java. In a benchmark we\ncompare layer activations between threads and between applications. In a case\nstudy involving a web-server on a mobile embedded device the performance\noverhead is minimal, even with many context switches."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.04790v1", 
    "other_authors": "Diogenes Nunez, Samuel Z. Guyer, Emery D. Berger", 
    "title": "Prioritized Garbage Collection: Explicit GC Support for Software Caches", 
    "arxiv-id": "1610.04790v1", 
    "author": "Emery D. Berger", 
    "publish": "2016-10-15T22:30:15Z", 
    "summary": "Programmers routinely trade space for time to increase performance, often in\nthe form of caching or memoization. In managed languages like Java or\nJavaScript, however, this space-time tradeoff is complex. Using more space\ntranslates into higher garbage collection costs, especially at the limit of\navailable memory. Existing runtime systems provide limited support for\nspace-sensitive algorithms, forcing programmers into difficult and often\nbrittle choices about provisioning.\n  This paper presents prioritized garbage collection, a cooperative programming\nlanguage and runtime solution to this problem. Prioritized GC provides an\ninterface similar to soft references, called priority references, which\nidentify objects that the collector can reclaim eagerly if necessary. The key\ndifference is an API for defining the policy that governs when priority\nreferences are cleared and in what order. Application code specifies a priority\nvalue for each reference and a target memory bound. The collector reclaims\nreferences, lowest priority first, until the total memory footprint of the\ncache fits within the bound. We use this API to implement a space-aware\nleast-recently-used (LRU) cache, called a Sache, that is a drop-in replacement\nfor existing caches, such as Google's Guava library. The garbage collector\nautomatically grows and shrinks the Sache in response to available memory and\nworkload with minimal provisioning information from the programmer. Using a\nSache, it is almost impossible for an application to experience a memory leak,\nmemory pressure, or an out-of-memory crash caused by software caching."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.04799v1", 
    "other_authors": "Shayan Najd, Simon Peyton Jones", 
    "title": "Trees That Grow", 
    "arxiv-id": "1610.04799v1", 
    "author": "Simon Peyton Jones", 
    "publish": "2016-10-15T23:43:48Z", 
    "summary": "We study the notion of extensibility in functional data types, as a new\napproach to the problem of decorating abstract syntax trees with additional\nsets of information. We observed the need for such extensibility while\nredesigning the data types representing Haskell abstract syntax inside GHC.\n  Specifically, we describe our approach to the tree-decoration problem using a\nnovel syntactic machinery in Haskell for expressing extensible data types. We\nshow that the syntactic machinery is complete in that it can express all the\nsyntactically possible forms of extensions to algebraic data type declarations.\nThen, we describe an encoding of the syntactic machinery based on the existing\nfeatures in Glasgow Haskell Compiler(GHC)."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.05114v3", 
    "other_authors": "Moez A. AbdelGawad", 
    "title": "Towards an Accurate Mathematical Model of Generic Nominally-Typed OOP", 
    "arxiv-id": "1610.05114v3", 
    "author": "Moez A. AbdelGawad", 
    "publish": "2016-10-14T09:07:50Z", 
    "summary": "The construction of GNOOP as a domain-theoretic model of generic\nnominally-typed OOP is currently underway. This extended abstract presents the\nconcepts of `nominal intervals' and `full generication' that are likely to help\nin building GNOOP as an accurate mathematical model of generic nominally-typed\nOOP. The abstract also presents few related category-theoretic suggestions. The\npresented concepts and suggestions are particularly geared towards enabling\nGNOOP to offer a precise and simple view of so-far-hard-to-analyze features of\ngeneric OOP such as variance annotations (e.g., Java wildcard types) and erased\ngenerics (e.g., Java type erasure)."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.05954v1", 
    "other_authors": "Jan Rochel", 
    "title": "Unfolding Semantics of the Untyped \u03bb-Calculus with letrec", 
    "arxiv-id": "1610.05954v1", 
    "author": "Jan Rochel", 
    "publish": "2016-10-19T10:41:32Z", 
    "summary": "We investigate the relationship between finite terms in {\\lambda}-letrec, the\n{\\lambda}-calculus with letrec, and the infinite {\\lambda}-terms they express.\nWe say that a lambda-letrec term expresses a lambda-term if the latter can be\nobtained as an infinite unfolding of the former. Unfolding is the process of\nsubstituting occurrences of function variables by the right-hand side of their\ndefinition.\n  We consider the following questions: (i) How can we characterise those\ninfinite {\\lambda}-terms that are {\\lambda}-letrec-expressible? (ii) Given two\n{\\lambda}-letrec terms, how can we determine whether they have the same\nunfolding? (iii) Given a {\\lambda}-letrec term, can we find a more compact\nversion of the term with the same unfolding? To tackle these questions we\nintroduce and study the following formalisms: (i) a rewriting system for\nunfolding {\\lambda}-letrec terms into {\\lambda}-terms (ii) a rewriting system\nfor `observing' {\\lambda}-terms by dissecting their term structure (iii)\nhigher-order and first-order graph formalisms together with translations\nbetween them as well as translations from and to {\\lambda}-letrec.\n  We identify a first-order term graph formalism on which bisimulation\npreserves and reflects the unfolding semantics of {\\lambda}-letrec and which is\nclosed under functional bisimulation. From this we derive efficient methods to\ndetermine whether two terms are equivalent under infinite unfolding and to\ncompute the maximally shared form of a given {\\lambda}-letrec term."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.06768v1", 
    "other_authors": "Hiroshi Unno, Sho Torii", 
    "title": "Automating Induction for Solving Horn Clauses", 
    "arxiv-id": "1610.06768v1", 
    "author": "Sho Torii", 
    "publish": "2016-10-21T12:44:23Z", 
    "summary": "Verification problems of programs written in various paradigms (such as\nimperative, logic, concurrent, functional, and object-oriented ones) can be\nreduced to problems of solving Horn clause constraints on predicate variables\nthat represent unknown inductive invariants. This paper presents a novel Horn\nconstraint solving method based on inductive theorem proving: the method\nreduces Horn constraint solving to validity checking of first-order formulas\nwith inductively defined predicates, which are then checked by induction on the\nderivation of the predicates. To automate inductive proofs, we introduce a\nnovel proof system tailored to Horn constraint solving and use an SMT solver to\ndischarge proof obligations arising in the proof search. The main advantage of\nthe proposed method is that it can verify relational specifications across\nprograms in various paradigms where multiple function calls need to be analyzed\nsimultaneously. The class of specifications includes practically important ones\nsuch as functional equivalence, associativity, commutativity, distributivity,\nmonotonicity, idempotency, and non-interference. Furthermore, our novel\ncombination of Horn clause constraints with inductive theorem proving enables\nus to naturally and automatically axiomatize recursive functions that are\npossibly non-terminating, non-deterministic, higher-order, exception-raising,\nand over non-inductively defined data types. We have implemented a relational\nverification tool for the OCaml functional language based on the proposed\nmethod and obtained promising results in preliminary experiments."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.07033v1", 
    "other_authors": "Martin Leinberger, Ralf L\u00e4mmel, Steffen Staab", 
    "title": "LambdaDL: Syntax and Semantics (Preliminary Report)", 
    "arxiv-id": "1610.07033v1", 
    "author": "Steffen Staab", 
    "publish": "2016-10-22T11:07:59Z", 
    "summary": "Semantic data fuels many different applications, but is still lacking proper\nintegration into programming languages. Untyped access is error-prone while\nmapping approaches cannot fully capture the conceptualization of semantic data.\nIn this paper, we present $\\lambda_{DL}$,a $\\lambda$-calculus with a modified\ntype system to provide type-safe integration of semantic data. This is achieved\nby the integration of description logics into the $\\lambda$-calculus for typing\nand data access. It is centered around several key design principles. Among\nthese are (1) the usage of semantic conceptualizations as types, (2) subtype\ninference for these types, and (3) type-checked query access to the data by\nboth ensuring the satisfiability of queries as well as typing query results\nprecisely in $\\lambda_{DL}$. The paper motivates the use of a modified type\nsystem for semantic data and it provides the theoretic foundation for the\nintegration of description logics as well as the core formal specifications of\n$\\lambda_{DL}$ including a proof of type safety."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2983990.2984028", 
    "link": "http://arxiv.org/pdf/1610.07118v1", 
    "other_authors": "Niki Vazou, Jeff Polakow", 
    "title": "Verified Parallel String Matching in Haskell", 
    "arxiv-id": "1610.07118v1", 
    "author": "Jeff Polakow", 
    "publish": "2016-10-23T03:15:04Z", 
    "summary": "In this paper, we prove correctness of parallelizing a string matcher using\nHaskell as a theorem prover. We use refinement types to specify correctness\nproperties, Haskell terms to express proofs and Liquid Haskell to check\ncorrectness of proofs. First, we specify and prove that a class of monoid\nmorphisms can be parallelized via parallel monoid concatenation. Then, we\nencode string matching as a morphism to get a provably correct parallel\ntransformation. Our 1839LoC prototype proof shows that Liquid Haskell can be\nused as a fully expressive theorem prover on realistic Haskell implementations."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.227", 
    "link": "http://arxiv.org/pdf/1610.07696v1", 
    "other_authors": "Mirco Tribastone, Herbert Wiklicky", 
    "title": "Proceedings 14th International Workshop Quantitative Aspects of   Programming Languages and Systems", 
    "arxiv-id": "1610.07696v1", 
    "author": "Herbert Wiklicky", 
    "publish": "2016-10-25T01:00:47Z", 
    "summary": "This volume contains the post-proceedings of the 14th International Workshop\non Quantitative Aspects of Programming Languages and Systems (QAPL), held as a\nsatellite workshop of ETAPS 2016 in Eindhoven, The Netherlands, on 2-3 April\n2016."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.227", 
    "link": "http://arxiv.org/pdf/1610.07965v1", 
    "other_authors": "Andrzej S. Murawski, Nikos Tzevelekos", 
    "title": "Higher-Order Linearisability", 
    "arxiv-id": "1610.07965v1", 
    "author": "Nikos Tzevelekos", 
    "publish": "2016-10-25T17:01:29Z", 
    "summary": "Linearisability is a central notion for verifying concurrent libraries: a\ngiven library is proven safe if its operational history can be rearranged into\na new sequential one which, in addition, satisfies a given specification.\nLinearisability has been examined for libraries in which method arguments and\nmethod results are of ground type, including libraries parameterised with such\nmethods. In this paper we extend linearisability to the general higher-order\nsetting: methods can be passed as arguments and returned as values. A library\nmay also depend on abstract methods of any order. We use this generalised\nnotion to show correctness of several higher-order example libraries."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.227", 
    "link": "http://arxiv.org/pdf/1610.07978v1", 
    "other_authors": "Richard A. Eisenberg", 
    "title": "Dependent Types in Haskell: Theory and Practice", 
    "arxiv-id": "1610.07978v1", 
    "author": "Richard A. Eisenberg", 
    "publish": "2016-10-25T17:28:46Z", 
    "summary": "Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been\nadding new type-level programming features for some time. Many of these\nfeatures---chiefly: generalized algebraic datatypes (GADTs), type families,\nkind polymorphism, and promoted datatypes---have brought Haskell to the\ndoorstep of dependent types. Many dependently typed programs can even currently\nbe encoded, but often the constructions are painful.\n  In this dissertation, I describe Dependent Haskell, which supports full\ndependent types via a backward-compatible extension to today's Haskell. An\nimportant contribution of this work is an implementation, in GHC, of a portion\nof Dependent Haskell, with the rest to follow. The features I have implemented\nare already released, in GHC 8.0. This dissertation contains several practical\nexamples of Dependent Haskell code, a full description of the differences\nbetween Dependent Haskell and today's Haskell, a novel type-safe dependently\ntyped lambda-calculus (called Pico) suitable for use as an intermediate\nlanguage for compiling Dependent Haskell, and a type inference and elaboration\nalgorithm, Bake, that translates Dependent Haskell to type-correct Pico."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.227", 
    "link": "http://arxiv.org/pdf/1610.08476v1", 
    "other_authors": "Michael M. Vitousek, Jeremy G. Siek", 
    "title": "Gradual Typing in an Open World", 
    "arxiv-id": "1610.08476v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2016-10-26T19:36:31Z", 
    "summary": "Gradual typing combines static and dynamic typing in the same language,\noffering the benefits of both to programmers. Static typing provides error\ndetection and strong guarantees while dynamic typing enables rapid prototyping\nand flexible programming idioms. For programmers to fully take advantage of a\ngradual type system, however, they must be able to trust their type\nannotations, and so runtime checks must be performed at the boundaries of\nstatic and dynamic code to ensure that static types are respected. Higher order\nand mutable values cannot be completely checked at these boundaries, and so\nadditional checks must be performed at their use sites. Traditionally, this has\nbeen achieved by installing wrappers or proxies on such values that moderate\nthe flow of data between static and dynamic, but these can cause problems if\nthe language supports comparison of object identity or has a foreign function\ninterface.\n  Reticulated Python is a gradually typed variant of Python implemented via a\nsource-to-source translator for Python 3. It implements a proxy-free\nalternative design named transient casts. This paper presents a formal\nsemantics for transient casts and shows that not only are they sound, but they\nwork in an open-world setting in which the Reticulated translator has only been\napplied to some of the program; the rest is untranslated Python. We formalize\nthis open world soundness property and use Coq to prove that it holds for\nAnthill Python, a calculus that models Reticulated Python."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.227", 
    "link": "http://arxiv.org/pdf/1610.09183v1", 
    "other_authors": "Ludovic Henrio, Justine Rochas", 
    "title": "Multiactive objects and their applications", 
    "arxiv-id": "1610.09183v1", 
    "author": "Justine Rochas", 
    "publish": "2016-10-28T12:15:59Z", 
    "summary": "In order to tackle the development of concurrent and distributed systems, the\nactive object programming model provides a high-level abstraction to program\nconcurrent behaviours. There exists already a variety of active object\nframeworks targeted at a large range of application domains: modelling,\nverification, efficient execution. However, among these frameworks, very few of\nthem consider a multi-threaded execution of active objects. Introducing a\ncontrolled parallelism within active objects enables overcoming some of their\nlimitations. In this paper, we present a complete framework around the\nmultiactive object programming model. We present it through ProActive, the Java\nlibrary that offers multiactive objects, and through MultiASP, the programming\nlanguage that allows the formalisation of our developments. We then use\nmultiactive objects to compile a cooperative active object language with\ndifferent synchronisation primitives into our programming model. This paper\nalso presents different use cases and the development support to illustrate the\npractical usability of our language. Formalisation of our work provides the\nprogrammer with guarantees on the behaviour of the multiactive object\nprogramming model and of the compiler."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1610.10050v2", 
    "other_authors": "Lu\u00eds Cruz-Filipe, Kim S. Larsen, Fabrizio Montesi", 
    "title": "The Paths to Choreography Extraction", 
    "arxiv-id": "1610.10050v2", 
    "author": "Fabrizio Montesi", 
    "publish": "2016-10-31T18:22:40Z", 
    "summary": "Choreographies are global descriptions of interactions among concurrent\ncomponents, most notably used in the settings of verification (e.g., Multiparty\nSession Types) and synthesis of correct-by-construction software (Choreographic\nProgramming). They require a top-down approach: programmers first write\nchoreographies, and then use them to verify or synthesize their programs.\nHowever, most existing software does not come with choreographies yet, which\nprevents their application.\n  To attack this problem, we propose a novel methodology (called choreography\nextraction) that, given a set of programs or protocol specifications,\nautomatically constructs a choreography that describes their behavior. The key\nto our extraction is identifying a set of paths in a graph that represents the\nsymbolic execution of the programs of interest. Our method improves on previous\nwork in several directions: we can now deal with programs that are equipped\nwith a state and internal computation capabilities; time complexity is\ndramatically better; we capture programs that are correct but not necessarily\nsynchronizable, i.e., they work because they exploit asynchronous\ncommunication."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.00467v1", 
    "other_authors": "Ruijie Fang, Siqi Liu", 
    "title": "A Performance Survey on Stack-based and Register-based Virtual Machines", 
    "arxiv-id": "1611.00467v1", 
    "author": "Siqi Liu", 
    "publish": "2016-11-02T04:37:13Z", 
    "summary": "Virtual machines have been widely adapted for high-level programming language\nimplementations and for providing a degree of platform neutrality. As the\noverall use and adaptation of virtual machines grow, the overall performance of\nvirtual machines has become a widely-discussed topic. In this paper, we present\na survey on the performance differences of the two most widely adapted types of\nvirtual machines - the stack-based virtual machine and the register-based\nvirtual machine - using various benchmark programs. Additionally, we adopted a\nnew approach of measuring performance by measuring the overall dispatch time,\namount of dispatches, fetch time, and execution time while running benchmarks\non custom-implemented, lightweight virtual machines. Finally, we present two\nlightweight, custom-designed, Turing-equivalent virtual machines that are\nspecifically designed in benchmarking virtual machine performance - the\n\"Conceptum\" stack-based virtual machine, and the \"Inertia\" register-based\nvirtual machine. Our result showed that while on average the register machine\nspends 20.39% less time in executing benchmarks than the stack machine, the\nstack-based virtual machine is still faster than the virtual machine regarding\nthe instruction fetch time."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.00602v1", 
    "other_authors": "Ruslan Shevchenko", 
    "title": "Scala-gopher: CSP-style programming techniques with idiomatic Scala", 
    "arxiv-id": "1611.00602v1", 
    "author": "Ruslan Shevchenko", 
    "publish": "2016-11-02T13:38:36Z", 
    "summary": "Cala-gopher is a library-level Scala implementation of communication sequence\nprocess constructs: channels, selectors (similar to analogical constructs in\nLimbo or Go) transputers (similar to Occam proc) and a set of high-level\noperations on top of akka and SIP-22 async. The framework integrates CSP-style\nprogramming into standard Scala concurrency environment via idiomatic API. This\nallows usage of communication patterns, well known in Go world, but not easy\nexpressable in mainstream scala concurrency frameworks, along with algebraic\napproach for composing computation builders. Besides, we want to discuss\ncurrent implementation issues and future directions in the context of evolving\nof compiler and libraries ecosystem."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.00692v1", 
    "other_authors": "Jan Hoffmann, Ankush Das, Shu-Chun Weng", 
    "title": "Towards Automatic Resource Bound Analysis for OCaml", 
    "arxiv-id": "1611.00692v1", 
    "author": "Shu-Chun Weng", 
    "publish": "2016-11-02T17:36:19Z", 
    "summary": "This article presents a resource analysis system for OCaml programs. This\nsystem automatically derives worst-case resource bounds for higher-order\npolymorphic programs with user-defined inductive types. The technique is\nparametric in the resource and can derive bounds for time, memory allocations\nand energy usage. The derived bounds are multivariate resource polynomials\nwhich are functions of different size parameters that depend on the standard\nOCaml types. Bound inference is fully automatic and reduced to a linear\noptimization problem that is passed to an off-the-shelf LP solver. Technically,\nthe analysis system is based on a novel multivariate automatic amortized\nresource analysis (AARA). It builds on existing work on linear AARA for\nhigher-order programs with user-defined inductive types and on multivariate\nAARA for first-order programs with built-in lists and binary trees. For the\nfirst time, it is possible to automatically derive polynomial bounds for\nhigher-order functions and polynomial bounds that depend on user-defined\ninductive types. Moreover, the analysis handles programs with side effects and\neven outperforms the linear bound inference of previous systems. At the same\ntime, it preserves the expressivity and efficiency of existing AARA techniques.\nThe practicality of the analysis system is demonstrated with an implementation\nand integration with Inria's OCaml compiler. The implementation is used to\nautomatically derive resource bounds for 411 functions and 6018 lines of code\nderived from OCaml libraries, the CompCert compiler, and implementations of\ntextbook algorithms. In a case study, the system infers bounds on the number of\nqueries that are sent by OCaml programs to DynamoDB, a commercial NoSQL cloud\ndatabase service."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.00860v1", 
    "other_authors": "Prakalp Srivastava, Maria Kotsifakou, Vikram Adve", 
    "title": "HPVM: A Portable Virtual Instruction Set for Heterogeneous Parallel   Systems", 
    "arxiv-id": "1611.00860v1", 
    "author": "Vikram Adve", 
    "publish": "2016-11-03T01:53:20Z", 
    "summary": "We describe a programming abstraction for heterogeneous parallel hardware,\ndesigned to capture a wide range of popular parallel hardware, including GPUs,\nvector instruction sets and multicore CPUs. Our abstraction, which we call\nHPVM, is a hierarchical dataflow graph with shared memory and vector\ninstructions. We use HPVM to define both a virtual instruction set (ISA) and\nalso a compiler intermediate representation (IR). The virtual ISA aims to\nachieve both functional portability and performance portability across\nheterogeneous systems, while the compiler IR aims to enable effective code\ngeneration and optimization for such systems.\n  HPVM effectively supports all forms of parallelism used to achieve\ncomputational speedups (as opposed to concurrency), including task parallelism,\ncoarse-grain data parallelism, fine-grain data parallelism, and pipelined\nparallelism. HPVM also enables flexible scheduling and tiling: different nodes\nin the dataflow graph can be mapped flexibly to different combinations of\ncompute units, and the graph hierarchy expresses memory tiling, essential for\nachieving high performance on GPU and CPU targets."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.01063v2", 
    "other_authors": "Krishnendu Chatterjee, Petr Novotn\u00fd, \u0110or\u0111e \u017dikeli\u0107", 
    "title": "Stochastic Invariants for Probabilistic Termination", 
    "arxiv-id": "1611.01063v2", 
    "author": "\u0110or\u0111e \u017dikeli\u0107", 
    "publish": "2016-11-03T15:28:36Z", 
    "summary": "Termination is one of the basic liveness properties, and we study the\ntermination problem for probabilistic programs with real-valued variables.\nPrevious works focused on the qualitative problem that asks whether an input\nprogram terminates with probability~1 (almost-sure termination). A powerful\napproach for this qualitative problem is the notion of ranking supermartingales\nwith respect to a given set of invariants. The quantitative problem\n(probabilistic termination) asks for bounds on the termination probability. A\nfundamental and conceptual drawback of the existing approaches to address\nprobabilistic termination is that even though the supermartingales consider the\nprobabilistic behavior of the programs, the invariants are obtained completely\nignoring the probabilistic aspect.\n  In this work we address the probabilistic termination problem for\nlinear-arithmetic probabilistic programs with nondeterminism. We define the\nnotion of {\\em stochastic invariants}, which are constraints along with a\nprobability bound that the constraints hold. We introduce a concept of {\\em\nrepulsing supermartingales}. First, we show that repulsing supermartingales can\nbe used to obtain bounds on the probability of the stochastic invariants.\nSecond, we show the effectiveness of repulsing supermartingales in the\nfollowing three ways: (1)~With a combination of ranking and repulsing\nsupermartingales we can compute lower bounds on the probability of termination;\n(2)~repulsing supermartingales provide witnesses for refutation of almost-sure\ntermination; and (3)~with a combination of ranking and repulsing\nsupermartingales we can establish persistence properties of probabilistic\nprograms.\n  We also present results on related computational problems and an experimental\nevaluation of our approach on academic examples."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.01507v2", 
    "other_authors": "Yatin A. Manerkar, Caroline Trippel, Daniel Lustig, Michael Pellauer, Margaret Martonosi", 
    "title": "Counterexamples and Proof Loophole for the C/C++ to POWER and ARMv7   Trailing-Sync Compiler Mappings", 
    "arxiv-id": "1611.01507v2", 
    "author": "Margaret Martonosi", 
    "publish": "2016-11-04T19:52:35Z", 
    "summary": "The C and C++ high-level languages provide programmers with atomic operations\nfor writing high-performance concurrent code. At the assembly language level, C\nand C++ atomics get mapped down to individual instructions or combinations of\ninstructions by compilers, depending on the ordering guarantees and\nsynchronization instructions provided by the underlying architecture. These\ncompiler mappings must uphold the ordering guarantees provided by C/C++ atomics\nor the compiled program will not behave according to the C/C++ memory model. In\nthis paper we discuss two counterexamples to the well-known trailing-sync\ncompiler mappings for the Power and ARMv7 architectures that were previously\nthought to be proven correct. In addition to the counterexamples, we discuss\nthe loophole in the proof of the mappings that allowed the incorrect mappings\nto be proven correct. We also discuss the current state of compilers and\narchitectures in relation to the bug."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.01667v2", 
    "other_authors": "Christian Schuessler, Roland Gruber", 
    "title": "A Traversable Fixed Size Small Object Allocator in C++", 
    "arxiv-id": "1611.01667v2", 
    "author": "Roland Gruber", 
    "publish": "2016-11-05T16:33:02Z", 
    "summary": "At the allocation and deallocation of small objects with fixed size, the\nstandard allocator of the runtime system has commonly a worse time performance\ncompared to allocators adapted for a special application field. We propose a\nmemory allocator, originally developed for mesh primitives but also usable for\nany other small equally sized objects. For a large amount of objects it leads\nto better results than allocating data with the C ++new instruction and behaves\nnowhere worse. The proposed synchronization approach for this allocator behaves\nlock-free in practical scenarios without using machine instructions, such as\ncompare-and-swap. A traversal structure is integrated requiring less memory\nthan using containers such as STL-vectors or lists, but with comparable time\nperformance."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.02392v1", 
    "other_authors": "Khurram A. Jafery, Joshua Dunfield", 
    "title": "Sums of Uncertainty: Refinements Go Gradual", 
    "arxiv-id": "1611.02392v1", 
    "author": "Joshua Dunfield", 
    "publish": "2016-11-08T05:18:52Z", 
    "summary": "A long-standing shortcoming of statically typed functional languages is that\ntype checking does not rule out pattern-matching failures (run-time match\nexceptions). Refinement types distinguish different values of datatypes; if a\nprogram annotated with refinements passes type checking, pattern-matching\nfailures become impossible. Unfortunately, refinement is a monolithic property\nof a type, exacerbating the difficulty of adding refinement types to nontrivial\nprograms.\n  Gradual typing has explored how to incrementally move between static typing\nand dynamic typing. We develop a type system of gradual sums that combines\nrefinement with imprecision. Then, we develop a bidirectional version of the\ntype system, which rules out excessive imprecision, and give a type-directed\ntranslation to a target language with explicit casts. We prove that the static\nsublanguage cannot have match failures, that a well-typed program remains\nwell-typed if its type annotations are made less precise, and that making\nannotations less precise causes target programs to fail later. Several of these\nresults correspond to criteria for gradual typing given by Siek et al. (2015)."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.05026v2", 
    "other_authors": "Mario Bravetti, Marco Carbone, Gianluigi Zavattaro", 
    "title": "Undecidability of Asynchronous Session Subtyping", 
    "arxiv-id": "1611.05026v2", 
    "author": "Gianluigi Zavattaro", 
    "publish": "2016-11-15T20:52:17Z", 
    "summary": "The most prominent proposals of subtyping for asynchronous session types are\nby Mostrous and Yoshida for binary sessions, by Chen et al. for binary sessions\nunder the assumption that every message emitted is eventually consumed, and by\nMostrous et al. for multiparty session types. We prove that, differently from\nwhat stated or conjectured in above proposals, all of these three subtyping\nrelations are undecidable. Additionally, we identify two subclasses of types\nwhere the relation becomes decidable."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.05105v1", 
    "other_authors": "Matteo Cimini, Dale Miller, Jeremy G. Siek", 
    "title": "Well-Typed Languages are Sound", 
    "arxiv-id": "1611.05105v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2016-11-16T00:56:59Z", 
    "summary": "Type soundness is an important property of modern programming languages. In\nthis paper we explore the idea that \"well-typed languages are sound\": the idea\nthat the appropriate typing discipline over language specifications guarantees\nthat the language is type sound. We instantiate this idea for a certain class\nof languages defined using small step operational semantics by ensuring the\nprogress and preservation theorems. Our first contribution is a syntactic\ndiscipline for organizing and restricting language specifications so that they\nautomatically satisfy the progress theorem. This discipline is not novel but\nmakes explicit the way expert language designers have been organizing a certain\nclass of languages for long time. We give a formal account of this discipline\nby representing language specifications as (higher-order) logic programs and by\ngiving a meta type system over that collection of formulas. Our second\ncontribution is a methodology and meta type system for guaranteeing that\nlanguages satisfy the preservation theorem. Ultimately, we proved that language\nspecifications that conform to our meta type systems are guaranteed to be type\nsound. We have implemented these ideas in the TypeSoundnessCertifier, a tool\nthat takes language specifications in the form of logic programs and type\nchecks them according to our meta type systems. For those languages that pass\nour type checker, our tool automatically produces a proof of type soundness\nthat can be machine-checked by the Abella proof assistant. For those languages\nthat fail our type checker, the tool pinpoints the design mistakes that hinder\ntype soundness. We have applied the TypeSoundnessCertifier to a large number of\nprogramming languages, including those with recursive types, polymorphism,\nletrec, exceptions, lists and other common types and operators."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.05651v1", 
    "other_authors": "Hugo A. L\u00f3pez, Flemming Nielson, Hanne Riis Nielson", 
    "title": "A Theory of Available-by-Design Communicating Systems", 
    "arxiv-id": "1611.05651v1", 
    "author": "Hanne Riis Nielson", 
    "publish": "2016-11-17T12:19:57Z", 
    "summary": "Choreographic programming is a programming-language design approach that\ndrives error-safe protocol development in distributed systems. Starting from a\nglobal specification (choreography) one can generate distributed\nimplementations. The advantages of this top-down approach lie in the\ncorrectness-by-design principle, where implementations (endpoints) generated\nfrom a choreography behave according to the strict control flow described in\nthe choreography, and do not deadlock. Motivated by challenging scenarios in\nCyber-Physical Systems (CPS), we study how choreographic programming can cater\nfor dynamic infrastructures where not all endpoints are always available. We\nintroduce the Global Quality Calculus ($GC_q$), a variant of choreographic\nprogramming for the description of communication systems where some of the\ncomponents involved in a communication might fail. GCq features novel operators\nfor multiparty, partial and collective communications. This paper studies the\nnature of failure-aware communication: First, we introduce $GC_q$ syntax,\nsemantics and examples of its use. The interplay between failures and\ncollective communications in a choreography can lead to choreographies that\ncannot progress due to absence of resources. In our second contribution, we\nprovide a type system that ensures that choreographies can be realized despite\nchanging availability conditions. A specification in $GC_q$ guides the\nimplementation of distributed endpoints when paired with global (session)\ntypes. Our third contribution provides an endpoint-projection based methodology\nfor the generation of failure-aware distributed processes. We show the\ncorrectness of the projection, and that well-typed choreographies with\navailability considerations enjoy progress."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.05980v2", 
    "other_authors": "David Menendez, Santosh Nagarakatte", 
    "title": "Precondition Inference for Peephole Optimizations in LLVM", 
    "arxiv-id": "1611.05980v2", 
    "author": "Santosh Nagarakatte", 
    "publish": "2016-11-18T05:21:00Z", 
    "summary": "Peephole optimizations are a common source of compiler bugs. Compiler\ndevelopers typically transform an incorrect peephole optimization into a valid\none by strengthening the precondition. This process is challenging and tedious.\nThis paper proposes PInfer, a data-driven approach for inferring preconditions\nfor peephole optimizations expressed in Alive. PInfer generates positive and\nnegatives examples for an optimization, enumerates predicates on-demand, and\nlearns a set of predicates that separate the positive and negative examples.\nPInfer repeats this process until it finds a precondition that ensures the\nvalidity of the optimization. PInfer reports both the weakest precondition and\na set of succinct partial preconditions to the developer. The PInfer prototype\nsuccessfully generates either the partial precondition or the weakest\nprecondition for 164 out of 174 peephole optimizations in the Alive suite. It\nalso generates preconditions that are weaker than LLVM's precondition for 73\noptimizations. We also demonstrate the applicability of this technique to\ngeneralize 54 concrete expression directed acyclic graphs generated by an LLVM\nIR-based super optimizer."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.06276v2", 
    "other_authors": "Simon Fowler, Sam Lindley, Philip Wadler", 
    "title": "Mixing Metaphors: Actors as Channels and Channels as Actors", 
    "arxiv-id": "1611.06276v2", 
    "author": "Philip Wadler", 
    "publish": "2016-11-18T23:32:34Z", 
    "summary": "Channel- and actor-based programming languages are both used in practice, but\nthe two are often confused. Languages such as Go provide anonymous processes\nwhich communicate using typed buffers---known as channels---while languages\nsuch as Erlang provide addressable processes each with a single incoming\nmessage queue---known as actors.\n  The lack of a common representation makes it difficult to reason about the\ntranslations that exist in the folklore. We define a calculus lambda-ch for\ntyped asynchronous channels, and a calculus lambda-act for typed actors. We\nshow translations from lambda-act into lambda-ch and lambda-ch into lambda-act\nand prove that both translations are type- and semantics-preserving."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.07502v1", 
    "other_authors": "Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, Swarat Chaudhuri", 
    "title": "Component-based Synthesis of Table Consolidation and Transformation   Tasks from Examples", 
    "arxiv-id": "1611.07502v1", 
    "author": "Swarat Chaudhuri", 
    "publish": "2016-11-22T20:31:18Z", 
    "summary": "This paper presents an example-driven synthesis technique for automating a\nlarge class of data preparation tasks that arise in data science. Given a set\nof input tables and an out- put table, our approach synthesizes a table\ntransformation program that performs the desired task. Our approach is not\nrestricted to a fixed set of DSL constructs and can synthesize programs from an\narbitrary set of components, including higher-order combinators. At a\nhigh-level, our approach performs type-directed enumerative search over partial\npro- grams but incorporates two key innovations that allow it to scale: First,\nour technique can utilize any first-order specification of the components and\nuses SMT-based deduction to reject partial programs. Second, our algorithm uses\npartial evaluation to increase the power of deduction and drive enumerative\nsearch. We have evaluated our synthesis algorithm on dozens of data preparation\ntasks obtained from on-line forums, and we show that our approach can\nautomatically solve a large class of problems encountered by R users."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.07610v1", 
    "other_authors": "Marianna Rapoport, Ond\u0159ej Lhot\u00e1k", 
    "title": "Mutable WadlerFest DOT", 
    "arxiv-id": "1611.07610v1", 
    "author": "Ond\u0159ej Lhot\u00e1k", 
    "publish": "2016-11-23T02:37:51Z", 
    "summary": "The Dependent Object Types (DOT) calculus aims to model the essence of Scala,\nwith a focus on abstract type members, path-dependent types, and subtyping.\nOther Scala features could be defined by translation to DOT. Mutation is a\nfundamental feature of Scala currently missing in DOT. Mutation in DOT is\nneeded not only to model effectful computation and mutation in Scala programs,\nbut even to precisely specify how Scala initializes immutable variables and\nfields (vals). We present an extension to DOT that adds typed mutable reference\ncells. We have proven the extension sound with a mechanized proof in Coq. We\npresent the key features of our extended calculus and its soundness proof, and\ndiscuss the challenges that we encountered in our search for a sound design and\nthe alternative solutions that we considered."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.09259v1", 
    "other_authors": "Sam Lindley, Conor McBride, Craig McLaughlin", 
    "title": "Do be do be do", 
    "arxiv-id": "1611.09259v1", 
    "author": "Craig McLaughlin", 
    "publish": "2016-11-28T17:44:19Z", 
    "summary": "We explore the design and implementation of Frank, a strict functional\nprogramming language with a bidirectional effect type system designed from the\nground up around a novel variant of Plotkin and Pretnar's effect handler\nabstraction.\n  Effect handlers provide an abstraction for modular effectful programming: a\nhandler acts as an interpreter for a collection of commands whose interfaces\nare statically tracked by the type system. However, Frank eliminates the need\nfor an additional effect handling construct by generalising the basic mechanism\nof functional abstraction itself. A function is simply the special case of a\nFrank operator that interprets no commands. Moreover, Frank's operators can be\nmultihandlers which simultaneously interpret commands from several sources at\nonce, without disturbing the direct style of functional programming with\nvalues.\n  Effect typing in Frank employs a novel form of effect polymorphism which\navoid mentioning effect variables in source code. This is achieved by\npropagating an ambient ability inwards, rather than accumulating unions of\npotential effects outwards.\n  We introduce Frank by example, and then give a formal account of the Frank\ntype system and its semantics. We introduce Core Frank by elaborating Frank\noperators into functions, case expressions, and unary handlers, and then give a\nsound small-step operational semantics for Core Frank.\n  Programming with effects and handlers is in its infancy. We contribute an\nexploration of future possibilities, particularly in combination with other\nforms of rich type system."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.09606v1", 
    "other_authors": "Sigurd Schneider, Gert Smolka, Sebastian Hack", 
    "title": "An Inductive Proof Method for Simulation-based Compiler Correctness", 
    "arxiv-id": "1611.09606v1", 
    "author": "Sebastian Hack", 
    "publish": "2016-11-29T12:47:15Z", 
    "summary": "We study induction on the program structure as a proof method for\nbisimulation-based compiler correctness. We consider a first-order language\nwith mutually recursive function definitions, system calls, and an environment\nsemantics. The proof method relies on a generalization of compatibility of\nfunction definition with the bisimulation. We use the inductive method to show\ncorrectness of a form of dead code elimination. This is an interesting case\nstudy because the transformation removes function, variable, and parameter\ndefinitions from the program. While such transformations require modification\nof the simulation in a coinductive proof, the inductive method deals with them\nnaturally. All our results are formalized in Coq."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1611.09626v1", 
    "other_authors": "Andr\u00e9s Aristiz\u00e1bal, Dariusz Biernacki, Sergue\u00ef Lenglet, Piotr Polesiuk", 
    "title": "Environmental Bisimulations for Delimited-Control Operators with Dynamic   Prompt Generation", 
    "arxiv-id": "1611.09626v1", 
    "author": "Piotr Polesiuk", 
    "publish": "2016-11-29T13:39:44Z", 
    "summary": "We present sound and complete environmental bisimilarities for a variant of\nDybvig et al.'s calculus of multi-prompted delimited-control operators with\ndynamic prompt generation. The reasoning principles that we obtain generalize\nand advance the existing techniques for establishing program equivalence in\ncalculi with single-prompted delimited control.\n  The basic theory that we develop is presented using Madiot et al.'s framework\nthat allows for smooth integration and composition of up-to techniques\nfacilitating bisimulation proofs. We also generalize the framework in order to\nexpress environmental bisimulations that support equivalence proofs of\nevaluation contexts representing continuations. This change leads to a novel\nand powerful up-to technique enhancing bisimulation proofs in the presence of\ncontrol operators."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1612.00666v1", 
    "other_authors": "Christian Johansen, Olaf Owe", 
    "title": "Dynamic Structural Operational Semantics", 
    "arxiv-id": "1612.00666v1", 
    "author": "Olaf Owe", 
    "publish": "2016-12-02T12:56:10Z", 
    "summary": "We introduce Dynamic SOS as a framework for describing semantics of\nprogramming languages that include dynamic software upgrades. Dynamic SOS\n(DSOS) is built on top of the Modular SOS of P. Mosses, with an underlying\ncategory theory formalization. The idea of Dynamic SOS is to bring out the\nessential differences between dynamic upgrade constructs and program execution\nconstructs. The important feature of Modular SOS (MSOS) that we exploit in DSOS\nis the sharp separation of the program execution code from the additional\n(data) structures needed at run-time. In DSOS we aim to achieve the same\nmodularity and decoupling for dynamic software upgrades. This is partly\nmotivated by the long term goal of having machine-checkable proofs for general\nresults like type safety. We exemplify Dynamic SOS on two languages supporting\ndynamic software upgrades, namely the C-like PROTEUS, which supports updating\nof variables, functions, records, or types at specific program points, and\nCREOL, which supports dynamic class upgrades in the setting of concurrent\nobjects. Existing type analyses for software upgrades can be done on top of\nDSOS too, as we illustrate for PROTEUS. A second contribution is the definition\nof a general encapsulating construction on Modular SOS useful in situations\nwhere a form of encapsulation of the execution is needed. We show how to apply\nthis in the setting of concurrent object-oriented programming with active\nobjects and asynchronous method calls."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-662-54458-7_25", 
    "link": "http://arxiv.org/pdf/1612.00669v2", 
    "other_authors": "Matthias Keil, Peter Thiemann", 
    "title": "Transaction-based Sandboxing for JavaScript", 
    "arxiv-id": "1612.00669v2", 
    "author": "Peter Thiemann", 
    "publish": "2016-12-02T13:11:41Z", 
    "summary": "Today's JavaScript applications are composed of scripts from different\norigins that are loaded at run time. As not all of these origins are equally\ntrusted, the execution of these scripts should be isolated from one another.\nHowever, some scripts must access the application state and some may be allowed\nto change it, while preserving the confidentiality and integrity constraints of\nthe application.\n  This paper presents design and implementation of DecentJS, a\nlanguage-embedded sandbox for full JavaScript. It enables scripts to run in a\nconfigurable degree of isolation with fine-grained access control. It provides\na transactional scope in which effects are logged for review by the access\ncontrol policy. After inspection of the log, effects can be committed to the\napplication state or rolled back.\n  The implementation relies on JavaScript proxies to guarantee full\ninterposition for the full language and for all code, including dynamically\nloaded scripts and code injected via eval. Its only restriction is that scripts\nmust be compliant with JavaScript's strict mode."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2986012.2986030", 
    "link": "http://arxiv.org/pdf/1612.00670v1", 
    "other_authors": "Johannes Emerich", 
    "title": "How Are Programs Found? Speculating About Language Ergonomics With   Curry-Howard", 
    "arxiv-id": "1612.00670v1", 
    "author": "Johannes Emerich", 
    "publish": "2016-12-02T13:18:58Z", 
    "summary": "Functional languages with strong static type systems have beneficial\nproperties to help ensure program correctness and reliability. Surprisingly,\ntheir practical significance in applications is low relative to other languages\nlacking in those dimensions. In this paper, the programs-as-proofs analogy is\ntaken seriously to gain speculative insights by analysis of creation habits in\nthe proof-centric discipline of mathematics. Viewed in light of this analogy, a\nsampling of mathematicians' attitudes towards formal proof suggests that the\ncrucial role of intuition and experimentation in programming tasks may be under\nappreciated, hinting at a possible explanation of the challenges rigorously\ndisciplined languages face in practical applications."
}]