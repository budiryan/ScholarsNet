[{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9810022v1", 
    "title": "Broy-Lamport Specification Problem: A Gurevich Abstract State Machine   Solution", 
    "arxiv-id": "cs/9810022v1", 
    "author": "James K. Huggins", 
    "publish": "1998-10-26T16:52:34Z", 
    "summary": "We apply the Gurevich Abstract State Machine methodology to a benchmark\nspecification problem of Broy and Lamport."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9810023v1", 
    "title": "Equivalence is in the Eye of the Beholder", 
    "arxiv-id": "cs/9810023v1", 
    "author": "James K. Huggins", 
    "publish": "1998-10-26T18:46:06Z", 
    "summary": "In a recent provocative paper, Lamport points out \"the insubstantiality of\nprocesses\" by proving the equivalence of two different decompositions of the\nsame intuitive algorithm by means of temporal formulas. We point out that the\ncorrect equivalence of algorithms is itself in the eye of the beholder. We\ndiscuss a number of related issues and, in particular, whether algorithms can\nbe proved equivalent directly."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9810024v1", 
    "title": "Evolving Algebras and Partial Evaluation", 
    "arxiv-id": "cs/9810024v1", 
    "author": "James K. Huggins", 
    "publish": "1998-10-26T19:13:12Z", 
    "summary": "We describe an automated partial evaluator for evolving algebras implemented\nat the University of Michigan."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9810025v1", 
    "title": "An Offline Partial Evaluator for Evolving Algebras", 
    "arxiv-id": "cs/9810025v1", 
    "author": "James K. Huggins", 
    "publish": "1998-10-26T19:19:04Z", 
    "summary": "We describe the architecture of an evolving algebra partial evaluator, a\nprogram which specializes an evolving algebra with respect to a portion of its\ninput. We discuss the particular analysis, specialization, and optimization\ntechniques used and show an example of its use."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9810026v1", 
    "title": "The Railroad Crossing Problem: An Experiment with Instantaneous Actions   and Immediate Reactions", 
    "arxiv-id": "cs/9810026v1", 
    "author": "James K. Huggins", 
    "publish": "1998-10-26T19:24:37Z", 
    "summary": "We give an evolving algebra solution for the well-known railroad crossing\nproblem and use the occasion to experiment with agents that perform\ninstantaneous actions in continuous time and in particular with agents that\nfire at the moment they are enabled."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9811007v1", 
    "title": "Second Product Line Practice Workshop Report", 
    "arxiv-id": "cs/9811007v1", 
    "author": "J. Withey", 
    "publish": "1998-11-02T19:34:17Z", 
    "summary": "The second Software Engineering Institute Product Line Practice Workshop was\na hands-on meeting held in November 1997 to share industry practices in\nsoftware product lines and to explore the technical and non-technical issues\ninvolved. This report synthesizes the workshop presentations and discussions,\nwhich identified factors involved in product line practices and analyzed issues\nin the areas of software engineering, technical management, and enterprise\nmanagement."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9811011v1", 
    "title": "Case Study in Survivable Network System Analysis", 
    "arxiv-id": "cs/9811011v1", 
    "author": "Nancy Mead", 
    "publish": "1998-11-04T14:32:22Z", 
    "summary": "This paper presents a method for analyzing the survivability of distributed\nnetwork systems and an example of its application."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9811014v1", 
    "title": "Abstract State Machines 1988-1998: Commented ASM Bibliography", 
    "arxiv-id": "cs/9811014v1", 
    "author": "James K. Huggins", 
    "publish": "1998-11-09T19:32:38Z", 
    "summary": "An annotated bibliography of papers which deal with or use Abstract State\nMachines (ASMs), as of January 1998."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9902008v1", 
    "title": "Managing Object-Oriented Integration and Regression Testing", 
    "arxiv-id": "cs/9902008v1", 
    "author": "Mario Winter", 
    "publish": "1999-02-05T12:01:56Z", 
    "summary": "Systematic testing of object-oriented software turned out to be much more\ncomplex than testing conventional software. Especially the highly incremental\nand iterative development cycle demands both many more changes and partially\nimplemented resp. re-implemented classes. Much more integration and regression\ntesting has to be done to reach stable stages during the development. In this\npresentation we propose a diagram capturing all possible dependencies and\ninteractions in an object-oriented program. Then we give algorithms and\ncoverage criteria to identify integration resp. regression test strategys and\nall test cases to be executed after some implementation resp. modification\nactivities. Finally, we summarize some practical experiences and heuristics."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9903018v1", 
    "title": "LuaJava - A Scripting Tool for Java", 
    "arxiv-id": "cs/9903018v1", 
    "author": "Noemi Rodriguez", 
    "publish": "1999-03-30T11:28:44Z", 
    "summary": "Scripting languages are becoming more and more important as a tool for\nsoftware development, as they provide great flexibility for rapid prototyping\nand for configuring componentware applications. In this paper we present\nLuaJava, a scripting tool for Java. LuaJava adopts Lua, a dynamically typed\ninterpreted language, as its script language. Great emphasis is given to the\ntransparency of the integration between the two languages, so that objects from\none language can be used inside the other like native objects. The final result\nof this integration is a tool that allows the construction of configurable Java\napplications, using off-the-shelf components, in a high abstraction level."
},{
    "category": "cs.SE", 
    "doi": "10.1109/HUMANOIDS.2016.7803255", 
    "link": "http://arxiv.org/pdf/cs/9906030v1", 
    "title": "SCR3: towards usability of formal methods", 
    "arxiv-id": "cs/9906030v1", 
    "author": "M. Chechik", 
    "publish": "1999-06-28T17:30:59Z", 
    "summary": "This paper gives an overview of SCR3 -- a toolset designed to increase the\nusability of formal methods for software development. Formal requirements are\nspecified in SCR3 in an easy to use and review format, and then used in\nchecking requirements for correctness and in verifying consistency between\nannotated code and requirements.\n  In this paper we discuss motivations behind this work, describe several tools\nwhich are part of SCR3, and illustrate their operation on an example of a\nCruise Control system."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/9906031v1", 
    "title": "Events in Linear-Time Properties", 
    "arxiv-id": "cs/9906031v1", 
    "author": "M. Chechik", 
    "publish": "1999-06-28T18:06:27Z", 
    "summary": "For over a decade, researchers in formal methods tried to create formalisms\nthat permit natural specification of systems and allow mathematical reasoning\nabout their correctness. The availability of fully-automated reasoning tools\nenables more non-specialists to use formal methods effectively --- their\nresponsibility reduces to just specifying the model and expressing the desired\nproperties. Thus, it is essential that these properties be represented in a\nlanguage that is easy to use and sufficiently expressive.\n  Linear-time temporal logic is a formalism that has been extensively used by\nresearchers for specifying properties of systems. When such properties are\nclosed under stuttering, i.e. their interpretation is not modified by\ntransitions that leave the system in the same state, verification tools can\nutilize a partial-order reduction technique to reduce the size of the model and\nthus analyze larger systems. If LTL formulas do not contain the ``next''\noperator, the formulas are closed under stuttering, but the resulting language\nis not expressive enough to capture many important properties, e.g., properties\ninvolving events. Determining if an arbitrary LTL formula is closed under\nstuttering is hard --- it has been proven to be PSPACE-complete.\n  In this paper we relax the restriction on LTL that guarantees closure under\nstuttering, introduce the notion of edges in the context of LTL, and provide\ntheorems that enable syntactic reasoning about closure under stuttering of LTL\nformulas."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/9906032v1", 
    "title": "Formal Modeling in a Commercial Setting: A Case Study", 
    "arxiv-id": "cs/9906032v1", 
    "author": "M. Chechik", 
    "publish": "1999-06-29T15:29:42Z", 
    "summary": "This paper describes a case study conducted in collaboration with Nortel to\ndemonstrate the feasibility of applying formal modeling techniques to\ntelecommunication systems. A formal description language, SDL, was chosen by\nour qualitative CASE tool evaluation to model a multimedia-messaging system\ndescribed by an 80-page natural language specification. Our model was used to\nidentify errors in the software requirements document and to derive test\nsuites, shadowing the existing development process and keeping track of a\nvariety of productivity data."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/9907019v1", 
    "title": "A Reasonable C++ Wrappered Java Native Interface", 
    "arxiv-id": "cs/9907019v1", 
    "author": "Craig Bordelon", 
    "publish": "1999-07-12T13:34:21Z", 
    "summary": "A reasonable C++ Java Native Interface (JNI) technique termed C++ Wrappered\nJNI (C++WJ) is presented. The technique simplifies current error-prone JNI\ndevelopment by wrappering JNI calls. Provided development is done with the aid\nof a C++ compiler, C++WJ offers type checking and behind the scenes caching. A\ntool (jH) patterned on javah automates the creation of C++WJ classes.\n  The paper presents the rationale behind the choices that led to C++WJ.\nHandling of Java class and interface hierarchy including Java type downcasts is\ndiscussed. Efficiency considerations in the C++WJ lead to two flavors of C++\nclasses: jtypes and Jtypes. A jtype is a lightweight less than full wrapper of\na JNI object reference. A Jtype is a heavyweight full wrapper of a JNI object\nreference."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/9912018v1", 
    "title": "Computation in an algebra of test selection criteria", 
    "arxiv-id": "cs/9912018v1", 
    "author": "Shmuel Zaks", 
    "publish": "1999-12-24T15:59:23Z", 
    "summary": "One of the key concepts in testing is that of adequate test sets. A test\nselection criterion decides which test sets are adequate. In this paper, a\nlanguage schema for specifying a large class of test selection criteria is\ndeveloped; the schema is based on two operations for building complex criteria\nfrom simple ones. Basic algebraic properties of the two operations are derived.\n  In the second part of the paper, a simple language-an instance of the general\nschema-is studied in detail, with the goal of generating small adequate test\nsets automatically. It is shown that one version of the problem is intractable,\nwhile another is solvable by an efficient algorithm. An implementation of the\nalgorithm is described."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0011029v1", 
    "title": "Systematic Debugging of Attribute Grammars", 
    "arxiv-id": "cs/0011029v1", 
    "author": "Masataka Sassa", 
    "publish": "2000-11-21T08:09:10Z", 
    "summary": "Although attribute grammars are commonly used for compiler construction,\nlittle investigation has been conducted on debugging attribute grammars. The\npaper proposes two types of systematic debugging methods, an algorithmic\ndebugging and slice-based debugging, both tailored for attribute grammars. By\nmeans of query-based interaction with the developer, our debugging methods\neffectively narrow the potential bug space in the attribute grammar description\nand eventually identify the incorrect attribution rule. We have incorporated\nthis technology in our visual debugging tool called Aki."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0012009v1", 
    "title": "Finding Failure Causes through Automated Testing", 
    "arxiv-id": "cs/0012009v1", 
    "author": "Andreas Zeller", 
    "publish": "2000-12-14T13:49:52Z", 
    "summary": "A program fails. Under which circumstances does this failure occur? One\nsingle algorithm, the delta debugging algorithm, suffices to determine these\nfailure-inducing circumstances. Delta debugging tests a program systematically\nand automatically to isolate failure-inducing circumstances such as the program\ninput, changes to the program code, or executed statements."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0012014v1", 
    "title": "Slicing of Constraint Logic Programs", 
    "arxiv-id": "cs/0012014v1", 
    "author": "Jan Maluszynski", 
    "publish": "2000-12-18T11:59:31Z", 
    "summary": "Slicing is a program analysis technique originally developed for imperative\nlanguages. It facilitates understanding of data flow and debugging.\n  This paper discusses slicing of Constraint Logic Programs. Constraint Logic\nProgramming (CLP) is an emerging software technology with a growing number of\napplications. Data flow in constraint programs is not explicit, and for this\nreason the concepts of slice and the slicing techniques of imperative languages\nare not directly applicable.\n  This paper formulates declarative notions of slice suitable for CLP. They\nprovide a basis for defining slicing techniques (both dynamic and static) based\non variable sharing. The techniques are further extended by using groundness\ninformation.\n  A prototype dynamic slicer of CLP programs implementing the presented ideas\nis briefly described together with the results of some slicing experiments."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0105008v1", 
    "title": "Applying Slicing Technique to Software Architectures", 
    "arxiv-id": "cs/0105008v1", 
    "author": "Jianjun Zhao", 
    "publish": "2001-05-05T08:09:08Z", 
    "summary": "Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural specifications) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintenance, and reuse will become an important issue.\nThis paper introduces a new form of slicing, named architectural slicing, to\naid architectural understanding and reuse. In contrast to traditional slicing,\narchitectural slicing is designed to operate on the architectural specification\nof a software system, rather than the source code of a program. Architectural\nslicing provides knowledge about the high-level structure of a software system,\nrather than the low-level implementation details of a program. In order to\ncompute an architectural slice, we present the architecture information flow\ngraph which can be used to represent information flows in a software\narchitecture. Based on the graph, we give a two-phase algorithm to compute an\narchitectural slice."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0105009v1", 
    "title": "Using Dependence Analysis to Support Software Architecture Understanding", 
    "arxiv-id": "cs/0105009v1", 
    "author": "Jianjun Zhao", 
    "publish": "2001-05-05T08:41:43Z", 
    "summary": "Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural descriptions) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintaining, and reusing will become an important\nissue. In this paper we introduce a new dependence analysis technique, named\narchitectural dependence analysis to support software architecture development.\nIn contrast to traditional dependence analysis, architectural dependence\nanalysis is designed to operate on an architectural description of a software\nsystem, rather than the source code of a conventional program. Architectural\ndependence analysis provides knowledge of dependences for the high-level\narchitecture of a software system, rather than the low-level implementation\ndetails of a conventional program."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISRE.1999.777992", 
    "link": "http://arxiv.org/pdf/cs/0105010v1", 
    "title": "On Assessing the Complexity of Software Architectures", 
    "arxiv-id": "cs/0105010v1", 
    "author": "Jianjun Zhao", 
    "publish": "2001-05-05T09:18:11Z", 
    "summary": "This paper proposes some new architectural metrics which are appropriate for\nevaluating the architectural attributes of a software system. The main feature\nof our approach is to assess the complexity of a software architecture by\nanalyzing various types of architectural dependences in the architecture."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0109019v1", 
    "title": "Tracing Execution of Software for Design Coverage", 
    "arxiv-id": "cs/0109019v1", 
    "author": "Alexander Ran", 
    "publish": "2001-09-14T15:12:51Z", 
    "summary": "Test suites are designed to validate the operation of a system against\nrequirements. One important aspect of a test suite design is to ensure that\nsystem operation logic is tested completely. A test suite should drive a system\nthrough all abstract states to exercise all possible cases of its operation.\nThis is a difficult task. Code coverage tools support test suite designers by\nproviding the information about which parts of source code are covered during\nsystem execution. Unfortunately, code coverage tools produce only source code\ncoverage information. For a test engineer it is often hard to understand what\nthe noncovered parts of the source code do and how they relate to requirements.\nWe propose a generic approach that provides design coverage of the executed\nsoftware simplifying the development of new test suites. We demonstrate our\napproach on common design abstractions such as statecharts, activity diagrams,\nmessage sequence charts and structure diagrams. We implement the design\ncoverage using Third Eye tracing and trace analysis framework. Using design\ncoverage, test suites could be created faster by focussing on untested design\nelements."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111013v1", 
    "title": "Quality Control, Testing and Deployment Results in NIF ICCS", 
    "arxiv-id": "cs/0111013v1", 
    "author": "Michael R. Gorvad", 
    "publish": "2001-11-06T22:12:14Z", 
    "summary": "The strategy used to develop the NIF Integrated Computer Control System\n(ICCS) calls for incremental cycles of construction and formal test to deliver\na total of 1 million lines of code. Each incremental release takes four to six\nmonths to implement specific functionality and culminates when offline tests\nconducted in the ICCS Integration and Test Facility verify functional,\nperformance, and interface requirements. Tests are then repeated on line to\nconfirm integrated operation in dedicated laser laboratories or ultimately in\nthe NIF. Test incidents along with other change requests are recorded and\ntracked to closure by the software change control board (SCCB). Annual\nindependent audits advise management on software process improvements.\nExtensive experience has been gained by integrating controls in the prototype\nlaser preamplifier laboratory. The control system installed in the preamplifier\nlab contains five of the ten planned supervisory subsystems and seven of\nsixteen planned front-end processors (FEPs). Beam alignment, timing, diagnosis\nand laser pulse amplification up to 20 joules was tested through an automated\nseries of shots. Other laboratories have provided integrated testing of six\nadditional FEPs. Process measurements including earned-value, product size, and\ndefect densities provide software project controls and generate confidence that\nthe control system will be successfully deployed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111014v1", 
    "title": "Visual DCT - Visual EPICS Database Configuration Tool", 
    "arxiv-id": "cs/0111014v1", 
    "author": "A. Luedeke", 
    "publish": "2001-11-07T10:01:57Z", 
    "summary": "Visual DCT is an EPICS configuration tool completely written in Java and\ntherefore supported in various systems. It was developed to provide features\nmissing in existing configuration tools as Capfast and GDCT. Visually Visual\nDCT resembles GDCT - records can be created, moved and linked, fields and links\ncan be easily modified. But Visual DCT offers more: using groups, records can\nbe grouped together in a logical block, which allows a hierarchical design.\nAdditionally indication of data flow direction using arrows makes the design\neasier to understand. Visual DCT has a powerful DB parser, which allows\nimporting existing DB and DBD files. Output file is also DB file, all comments\nand record order is preserved and visual data saved as comment, which allows\nDBs to be edited in other tools or manually. Great effort has been taken and\nmany tricks used to optimize the performance in order to compensate for the\nfact that Java is an interpreted language."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111016v1", 
    "title": "Application Software Structure Enables Nif Operations Kirby W. Fong", 
    "arxiv-id": "cs/0111016v1", 
    "author": "Randy T. Shelton", 
    "publish": "2001-11-07T21:50:43Z", 
    "summary": "The NIF Integrated Computer Control System (ICCS) application software uses a\nset of service frameworks that assures uniform behavior spanning the front-end\nprocessors (FEPs) and supervisor programs. This uniformity is visible both in\nthe way each program employs shared services and in the flexibility it affords\nfor attaching graphical user interfaces (GUIs). Uniformity of structure across\napplications is desired for the benefit of programmers who will be maintaining\nthe many programs that constitute the ICCS. In this paper, the framework\ncomponents that have the greatest impact on the application structure are\ndiscussed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111019v2", 
    "title": "Application of digital regulated Power Supplies for Magnet Control at   the Swiss Light Source", 
    "arxiv-id": "cs/0111019v2", 
    "author": "A. Luedeke", 
    "publish": "2001-11-08T19:00:26Z", 
    "summary": "The Swiss Light Source (SLS) has in the order of 500 magnet power supplies\n(PS) installed, ranging from from 3 A/20 V four-quadrant PS to a 950 A/1000 V\ntwo-quadrant 3 Hz PS. All magnet PS have a local digital controller for a\ndigital regulation loop and a 5 MHz optical point-to-point link to the VME\nlevel. The PS controller is running a pulse width/pulse repetition regulation\nscheme, optional with multiple slave regulation loops. Many internal regulation\nparameters and controller diagnostics are readable by the control system.\nIndustry Pack modules with standard VME carrier cards are used as VME hardware\ninterface with the high control density of eight links per VME card. The low\nlevel EPICS interface is identical for all 500 magnet PS, including insertion\ndevices. The digital PS have proven to be very stable and reliable during\ncommissioning of the light source. All specifications were met for all PS. The\nadvanced diagnostic for the magnet PS turned out to be very useful not only for\nthe diagnostic of the PS but also to identify problems on the magnets."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111021v3", 
    "title": "System Integration of High Level Applications during the Commissioning   of the Swiss Light Source", 
    "arxiv-id": "cs/0111021v3", 
    "author": "A. Luedeke", 
    "publish": "2001-11-09T14:35:51Z", 
    "summary": "The commissioning of the Swiss Light Source (SLS) started in Feb. 2000 with\nthe Linac, continued in May 2000 with the booster synchrotron and by Dec. 2000\nfirst light in the storage ring were produced. The first four beam lines had to\nbe operational by August 2001. The thorough integration of all subsystems to\nthe control system and a high level of automation was prerequisite to meet the\ntight time schedule. A careful balanced distribution of functionality into high\nlevel and low level applications allowed an optimization of short development\ncycles and high reliability of the applications. High level applications were\nimplemented as CORBA based client/server applications (tcl/tk and Java based\nclients, C++ based servers), IDL applications using EZCA, medm/dm2k screens and\ntcl/tk applications using CDEV. Low level applications were mainly built as\nEPICS process databases, SNL state machines and customized drivers.\nFunctionality of the high level application was encapsulated and pushed to\nlower levels whenever it has proven to be adequate. That enabled to reduce\nmachine setups to a handful of physical parameters and allow the usage of\nstandard EPICS tools for display, archiving and processing of complex physical\nvalues. High reliability and reproducibility were achieved with that approach."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0111045v1", 
    "title": "The Overview of the National Ignition Facility Distributed Computer   Control System", 
    "arxiv-id": "cs/0111045v1", 
    "author": "J. P. Woodruff", 
    "publish": "2001-11-16T22:04:52Z", 
    "summary": "The Integrated Computer Control System (ICCS) for the National Ignition\nFacility (NIF) is a layered architecture of 300 front-end processors (FEP)\ncoordinated by supervisor subsystems including automatic beam alignment and\nwavefront control, laser and target diagnostics, pulse power, and shot control\ntimed to 30 ps. FEP computers incorporate either VxWorks on PowerPC or Solaris\non UltraSPARC processors that interface to over 45,000 control points attached\nto VME-bus or PCI-bus crates respectively. Typical devices are stepping motors,\ntransient digitizers, calorimeters, and photodiodes. The front-end layer is\ndivided into another segment comprised of an additional 14,000 control points\nfor industrial controls including vacuum, argon, synthetic air, and safety\ninterlocks implemented with Allen-Bradley programmable logic controllers\n(PLCs). The computer network is augmented asynchronous transfer mode (ATM) that\ndelivers video streams from 500 sensor cameras monitoring the 192 laser beams\nto operator workstations. Software is based on an object-oriented framework\nusing CORBA distribution that incorporates services for archiving, machine\nconfiguration, graphical user interface, monitoring, event logging, scripting,\nalert management, and access control. Software coding using a mixed language\nenvironment of Ada95 and Java is one-third complete at over 300 thousand source\nlines. Control system installation is currently under way for the first 8\nbeams, with project completion scheduled for 2008."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0201023v1", 
    "title": "Model-Based Software Engineering and Ada: Synergy for the Development of   Safety-Critical Systems", 
    "arxiv-id": "cs/0201023v1", 
    "author": "Hans-Peter Zaengerl", 
    "publish": "2002-01-26T11:32:31Z", 
    "summary": "In this paper we outline a software development process for safety-critical\nsystems that aims at combining some of the specific strengths of model-based\ndevelopment with those of programming language based development using\nsafety-critical subsets of Ada. Model-based software development and\nmodel-based test case generation techniques are combined with code generation\ntechniques and tools providing a transition from model to code both for a\nsystem itself and for its test cases. This allows developers to combine\ndomain-oriented, model-based techniques with source code based validation\ntechniques, as required for conformity with standards for the development of\nsafety-critical software, such as the avionics standard RTCA/DO-178B. We\nintroduce the AutoFocus and Validator modeling and validation toolset and\nsketch its usage for modeling, test case generation, and code generation in a\ncombined approach, which is further illustrated by a simplified leading edge\naerospace model with built-in fault tolerance."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0201028v1", 
    "title": "Software Validation using Power Profiles", 
    "arxiv-id": "cs/0201028v1", 
    "author": "Alexander Ran", 
    "publish": "2002-01-30T22:28:52Z", 
    "summary": "The validation of modern software systems incorporates both functional and\nquality requirements. This paper proposes a validation approach for software\nquality requirement - its power consumption. This approach validates whether\nthe software produces the desired results with a minimum expenditure of energy.\nWe present energy requirements and an approach for their validation using a\npower consumption model, test-case specification, software traces, and power\nmeasurements. Three different approaches for power data gathering are\ndescribed. The power consumption of mobile phone applications is obtained and\nmatched against the power consumption model."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0204014v1", 
    "title": "An Assessment of the Consistency for Software Measurement Methods", 
    "arxiv-id": "cs/0204014v1", 
    "author": "F. Torre Cervigon", 
    "publish": "2002-04-09T09:33:44Z", 
    "summary": "Consistency, defined as the requirement that a series of measurements of the\nsame project carried out by different raters using the same method should\nproduce similar results, is one of the most important aspects to be taken into\naccount in the measurement methods of the software. In spite of this, there is\na widespread view that many measurement methods introduce an undesirable amount\nof subjectivity in the measurement process. This perception has made several\norganizations develop revisions of the standard methods whose main aim is to\nimprove their consistency by introducing some suitable modifications of those\naspects which are believed to introduce a greater degree of subjectivity.Each\nrevision of a method must be empirically evaluated to determine to what extent\nis the aim of improving its consistency achieved. In this article we will\ndefine an homogeneous statistic intended to describe the consistency level of a\nmethod, and we will develop the statistical analysis which should be carried\nout in order to conclude whether or not a measurement method is more consistent\nthan other one."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0204034v1", 
    "title": "Monitoring and Debugging Concurrent and Distributed Object-Oriented   Systems", 
    "arxiv-id": "cs/0204034v1", 
    "author": "Joseph R. Kiniry", 
    "publish": "2002-04-15T23:33:24Z", 
    "summary": "A major part of debugging, testing, and analyzing a complex software system\nis understanding what is happening within the system at run-time. Some\ndevelopers advocate running within a debugger to better understand the system\nat this level. Others embed logging statements, even in the form of hard-coded\ncalls to print functions, throughout the code. These techniques are all\ngeneral, rough forms of what we call system monitoring, and, while they have\nlimited usefulness in simple, sequential systems, they are nearly useless in\ncomplex, concurrent ones. We propose a set of new mechanisms, collectively\nknown as a monitoring system, for understanding such complex systems, and we\ndescribe an example implementation of such a system, called IDebug, for the\nJava programming language."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0204035v1", 
    "title": "Semantic Properties for Lightweight Specification in Knowledgeable   Development Environments", 
    "arxiv-id": "cs/0204035v1", 
    "author": "Joseph R. Kiniry", 
    "publish": "2002-04-15T23:40:52Z", 
    "summary": "Semantic properties are domain-specific specification constructs used to\naugment an existing language with richer semantics. These properties are taken\nadvantage of in system analysis, design, implementation, testing, and\nmaintenance through the use of documentation and source-code transformation\ntools. Semantic properties are themselves specified at two levels: loosely with\nprecise natural language, and formally within the problem domain. The\nrefinement relationships between these specification levels, as well as between\na semantic property's use and its realization in program code via tools, is\nspecified with a new formal method for reuse called kind theory."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0204036v1", 
    "title": "Semantic Component Composition", 
    "arxiv-id": "cs/0204036v1", 
    "author": "Joseph R. Kiniry", 
    "publish": "2002-04-15T23:50:42Z", 
    "summary": "Building complex software systems necessitates the use of component-based\narchitectures. In theory, of the set of components needed for a design, only\nsome small portion of them are \"custom\"; the rest are reused or refactored\nexisting pieces of software. Unfortunately, this is an idealized situation.\nJust because two components should work together does not mean that they will\nwork together.\n  The \"glue\" that holds components together is not just technology. The\ncontracts that bind complex systems together implicitly define more than their\nexplicit type. These \"conceptual contracts\" describe essential aspects of\nextra-system semantics: e.g., object models, type systems, data representation,\ninterface action semantics, legal and contractual obligations, and more.\n  Designers and developers spend inordinate amounts of time technologically\nduct-taping systems to fulfill these conceptual contracts because system-wide\nsemantics have not been rigorously characterized or codified. This paper\ndescribes a formal characterization of the problem and discusses an initial\nimplementation of the resulting theoretical system."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0206021v1", 
    "title": "The analysis of the IFPUG method sensitivity", 
    "arxiv-id": "cs/0206021v1", 
    "author": "F. Torre Cervigon", 
    "publish": "2002-06-14T09:24:53Z", 
    "summary": "J. Albrecht`s Function Point Analysis (FPA) is a method to determine the\nfunctional size of software products. An organization called International\nFunction Point Users Group (IPFUG), considers the FPA as a standard in the\nsoftware functional size measurement. The Albrechts method is followed by IPFUG\nmethod which includes some modifications in order to improve it. A limitation\nof the method refers to the fact that FPA is not sensitive enough to\ndifferentiate the functional size in small enhancements. That affects the\nproductivity analysis, where the software product functional size is required.\nTo provide more power to the functional size measurement, A. Abran, M. Maya and\nH. Nguyeckim have proposed some modifications to improve it. The IPFUG v 4.1\nmethod which includes these modifications is named IFPUG v 4.1 extended. In\nthis work we set the conditions to delimiting granular from non granular\nfunctions and we calculate the static calibration and sensitivity graphs for\nthe measurements of a set of projects with a high percentage of granular\nfunctions, all of then measured with the IFPUG v 4.1 method and the IFPUG v 4.1\nextended. Finally, we introduce a statistic analysis in order to determine\nwhether significant differences exist between both methods or not."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207044v3", 
    "title": "Declarative program development in Prolog with GUPU", 
    "arxiv-id": "cs/0207044v3", 
    "author": "Stefan Kral", 
    "publish": "2002-07-11T16:57:59Z", 
    "summary": "We present GUPU, a side-effect free environment specialized for programming\ncourses. It seamlessly guides and supports students during all phases of\nprogram development, covering specification, implementation, and program\ndebugging. GUPU features several innovations in this area. The specification\nphase is supported by reference implementations augmented with diagnostic\nfacilities. During implementation, immediate feedback from test cases and from\nvisualization tools helps the programmer's program understanding. A set of\nslicing techniques narrows down programming errors. The whole process is guided\nby a marking system."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207046v2", 
    "title": "COINS: a constraint-based interactive solving system", 
    "arxiv-id": "cs/0207046v2", 
    "author": "Patrice Boizumault", 
    "publish": "2002-07-11T16:25:20Z", 
    "summary": "This paper describes the COINS (COnstraint-based INteractive Solving) system:\na conflict-based constraint solver. It helps understanding inconsistencies,\nsimulates constraint additions and/or retractions (without any propagation),\ndetermines if a given constraint belongs to a conflict and provides diagnosis\ntools (e.g. why variable v cannot take value val). COINS also uses\nuser-friendly representation of conflicts and explanations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207047v2", 
    "title": "Tracing and Explaining Execution of CLP(FD) Programs", 
    "arxiv-id": "cs/0207047v2", 
    "author": "Mats Carlsson", 
    "publish": "2002-07-11T16:43:30Z", 
    "summary": "Previous work in the area of tracing CLP(FD) programs mainly focuses on\nproviding information about control of execution and domain modification. In\nthis paper, we present a trace structure that provides information about\nadditional important aspects. We incorporate explanations in the trace\nstructure, i.e. reasons for why certain solver actions occur. Furthermore, we\ncome up with a format for describing the execution of the filtering algorithms\nof global constraints. Some new ideas about the design of the trace are also\npresented. For example, we have modeled our trace as a nested block structure\nin order to achieve a hierarchical view. Also, new ways about how to represent\nand identify different entities such as constraints and domain variables are\npresented."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207048v2", 
    "title": "CLPGUI: a generic graphical user interface for constraint logic   programming over finite domains", 
    "arxiv-id": "cs/0207048v2", 
    "author": "Francois Fages", 
    "publish": "2002-07-11T18:05:59Z", 
    "summary": "CLPGUI is a graphical user interface for visualizing and interacting with\nconstraint logic programs over finite domains. In CLPGUI, the user can control\nthe execution of a CLP program through several views of constraints, of finite\ndomain variables and of the search tree. CLPGUI is intended to be used both for\nteaching purposes, and for debugging and improving complex programs of\nrealworld scale. It is based on a client-server architecture for connecting the\nCLP process to a Java-based GUI process. Communication by message passing\nprovides an open architecture which facilitates the reuse of graphical\ncomponents and the porting to different constraint programming systems.\nArbitrary constraints and goals can be posted incrementally from the GUI. We\npropose several dynamic 2D and 3D visualizations of the search tree and of the\nevolution of finite domain variables. We argue that the 3D representation of\nsearch trees proposed in this paper provides the most appropriate visualization\nof large search trees. We describe the current implementation of the\nannotations and of the interactive execution model in GNU-Prolog, and report\nsome evaluation results."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207049v2", 
    "title": "More Precise Yet Efficient Type Inference for Logic Programs", 
    "arxiv-id": "cs/0207049v2", 
    "author": "Francisco Bueno", 
    "publish": "2002-07-11T16:50:45Z", 
    "summary": "Type analyses of logic programs which aim at inferring the types of the\nprogram being analyzed are presented in a unified abstract interpretation-based\nframework. This covers most classical abstract interpretation-based type\nanalyzers for logic programs, built on either top-down or bottom-up\ninterpretation of the program. In this setting, we discuss the widening\noperator, arguably a crucial one. We present a new widening which is more\nprecise than those previously proposed. Practical results with our analysis\ndomain are also presented, showing that it also allows for efficient analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207050v2", 
    "title": "Value withdrawal explanations: a theoretical tool for programming   environments", 
    "arxiv-id": "cs/0207050v2", 
    "author": "Willy Lesaint", 
    "publish": "2002-07-11T16:52:49Z", 
    "summary": "Constraint logic programming combines declarativity and efficiency thanks to\nconstraint solvers implemented for specific domains. Value withdrawal\nexplanations have been efficiently used in several constraints programming\nenvironments but there does not exist any formalization of them. This paper is\nan attempt to fill this lack. Furthermore, we hope that this theoretical tool\ncould help to validate some programming environments. A value withdrawal\nexplanation is a tree describing the withdrawal of a value during a domain\nreduction by local consistency notions and labeling. Domain reduction is\nformalized by a search tree using two kinds of operators: operators for local\nconsistency notions and operators for labeling. These operators are defined by\nsets of rules. Proof trees are built with respect to these rules. For each\nremoved value, there exists such a proof tree which is the withdrawal\nexplanation of this value."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207051v2", 
    "title": "Exporting Prolog source code", 
    "arxiv-id": "cs/0207051v2", 
    "author": "Nicos Angelopoulos", 
    "publish": "2002-07-11T17:17:17Z", 
    "summary": "In this paper we present a simple source code configuration tool. ExLibris\noperates on libraries and can be used to extract from local libraries all code\nrelevant to a particular project. Our approach is not designed to address\nproblems arising in code production lines, but rather, to support the needs of\nindividual or small teams of researchers who wish to communicate their Prolog\nprograms. In the process, we also wish to accommodate and encourage the writing\nof reusable code. Moreover, we support and propose ways of dealing with issues\narising in the development of code that can be run on a variety of like-minded\nProlog systems. With consideration to these aims we have made the following\ndecisions: (i) support file-based source development, (ii) require minimal\nprogram transformation, (iii) target simplicity of usage, and (iv) introduce\nminimum number of new primitives."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207052v1", 
    "title": "Proceedings of the 12th International Workshop on Logic Programming   Environments", 
    "arxiv-id": "cs/0207052v1", 
    "author": "Alexandre Tessier", 
    "publish": "2002-07-12T01:23:51Z", 
    "summary": "The twelfth Workshop on Logic Programming Environments, WLPE 2002, is one in\na series of international workshops held in the topic area. The workshops\nfacilitate the exchange ideas and results among researchers and system\ndevelopers on all aspects of environments for logic programming. Relevant\ntopics for these workshops include user interfaces, human engineering,\nexecution visualization, development tools, providing for new paradigms, and\ninterfacing to language system tools and external systems. This twelfth\nworkshop held in Copenhaguen. It follows the successful eleventh Workshop on\nLogic Programming Environments held in Cyprus in December, 2001.\n  WLPE 2002 features ten presentations. The presentations involve, in some way,\nconstraint logic programming, object-oriented programming and abstract\ninterpretation. Topics areas addressed include tools for software development,\nexecution visualization, software maintenance, instructional aids.\n  This workshop was a post-conference workshop at ICLP 2002.\n  Alexandre Tessier, Program Chair, WLPE 2002, June 2002."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207053v1", 
    "title": "An Architecture for Making Object-Oriented Systems Available from Prolog", 
    "arxiv-id": "cs/0207053v1", 
    "author": "Anjo Anjewierden", 
    "publish": "2002-07-12T01:22:20Z", 
    "summary": "It is next to impossible to develop real-life applications in just pure\nProlog. With XPCE we realised a mechanism for integrating Prolog with an\nexternal object-oriented system that turns this OO system into a natural\nextension to Prolog. We describe the design and how it can be applied to other\nexternal OO systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0207054v1", 
    "title": "Enhancing Usefulness of Declarative Programming Frameworks through   Complete Integration", 
    "arxiv-id": "cs/0207054v1", 
    "author": "Olof Torgersson", 
    "publish": "2002-07-12T01:17:13Z", 
    "summary": "The Gisela framework for declarative programming was developed with the\nspecific aim of providing a tool that would be useful for knowledge\nrepresentation and reasoning within real-world applications. To achieve this, a\ncomplete integration into an object-oriented application development\nenvironment was used. The framework and methodology developed provide two\nalternative application programming interfaces (APIs): Programming using\nobjects or programming using a traditional equational declarative style. In\naddition to providing complete integration, Gisela also allows extensions and\nmodifications due to the general computation model and well-defined APIs. We\ngive a brief overview of the declarative model underlying Gisela and we present\nthe methodology proposed for building applications together with some real\nexamples."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0303013v2", 
    "title": "Extending the code generation capabilities of the Together CASE tool to   support Data Definition languages", 
    "arxiv-id": "cs/0303013v2", 
    "author": "Massimo Marino", 
    "publish": "2003-03-18T15:51:06Z", 
    "summary": "Together is the recommended software development tool in the Atlas\ncollaboration. The programmatic API, which provides the capability to use and\naugment Together's internal functionality, is comprised of three major\ncomponents - IDE, RWI and SCI. IDE is a read-only interface used to generate\ncustom outputs based on the information contained in a Together model. RWI\nallows to both extract and write information to a Together model. SCI is the\nSource Code Interface, as the name implies it allows to work at the level of\nthe source code. Together is extended by writing modules (java classes)\nextensively making use of the relevant API. We exploited Together extensibility\nto add support for the Atlas Dictionary Language. ADL is an extended subset of\nOMG IDL. The implemented module (ADLModule) makes Together to support ADL\nkeywords, enables options and generate ADL object descriptions directly from\nUML Class diagrams. The module thoroughly accesses a Together reverse\nengineered C++ project - and/or design only class diagrams - and it is general\nenough to allow for possibly additional HEP-specific Together tool tailoring."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0305037v2", 
    "title": "Power Law Distributions in Class Relationships", 
    "arxiv-id": "cs/0305037v2", 
    "author": "Steve Counsell", 
    "publish": "2003-05-20T10:21:17Z", 
    "summary": "Power law distributions have been found in many natural and social phenomena,\nand more recently in the source code and run-time characteristics of\nObject-Oriented (OO) systems. A power law implies that small values are\nextremely common, whereas large values are extremely rare. In this paper, we\nidentify twelve new power laws relating to the static graph structures of Java\nprograms. The graph structures analyzed represented different forms of OO\ncoupling, namely, inheritance, aggregation, interface, parameter type and\nreturn type. Identification of these new laws provide the basis for predicting\nlikely features of classes in future developments. The research in this paper\nties together work in object-based coupling and World Wide Web structures."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0305049v1", 
    "title": "The Athena Data Dictionary and Description Language", 
    "arxiv-id": "cs/0305049v1", 
    "author": "Craig Tull", 
    "publish": "2003-05-28T15:16:08Z", 
    "summary": "Athena is the ATLAS off-line software framework, based upon the GAUDI\narchitecture from LHCb. As part of ATLAS' continuing efforts to enhance and\ncustomise the architecture to meet our needs, we have developed a data object\ndescription tool suite and service for Athena. The aim is to provide a set of\ntools to describe, manage, integrate and use the Event Data Model at a design\nlevel according to the concepts of the Athena framework (use of patterns,\nrelationships, ...). Moreover, to ensure stability and reusability this must be\nfully independent from the implementation details. After an extensive\ninvestigation into the many options, we have developed a language grammar based\nupon a description language (IDL, ODL) to provide support for object\nintegration in Athena. We have then developed a compiler front end based upon\nthis language grammar, JavaCC, and a Java Reflection API-like interface. We\nhave then used these tools to develop several compiler back ends which meet\nspecific needs in ATLAS such as automatic generation of object converters, and\ndata object scripting interfaces. We present here details of our work and\nexperience to date on the Athena Definition Language and Athena Data\nDictionary."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306005v1", 
    "title": "The Virtual Monte Carlo", 
    "arxiv-id": "cs/0306005v1", 
    "author": "for the ALICE Collaboration", 
    "publish": "2003-05-30T21:59:14Z", 
    "summary": "The concept of Virtual Monte Carlo (VMC) has been developed by the ALICE\nSoftware Project to allow different Monte Carlo simulation programs to run\nwithout changing the user code, such as the geometry definition, the detector\nresponse simulation or input and output formats. Recently, the VMC classes have\nbeen integrated into the ROOT framework, and the other relevant packages have\nbeen separated from the AliRoot framework and can be used individually by any\nother HEP project. The general concept of the VMC and its set of base classes\nprovided in ROOT will be presented. Existing implementations for Geant3, Geant4\nand FLUKA and simple examples of usage will be described."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306054v1", 
    "title": "OVAL: the CMS Testing Robot", 
    "arxiv-id": "cs/0306054v1", 
    "author": "C. Charlot", 
    "publish": "2003-06-12T17:08:34Z", 
    "summary": "Oval is a testing tool which help developers to detect unexpected changes in\nthe behavior of their software. It is able to automatically compile some test\nprograms, to prepare on the fly the needed configuration files, to run the\ntests within a specified Unix environment, and finally to analyze the output\nand check expectations. Oval does not provide utility code to help writing the\ntests, therefore it is quite independant of the programming/scripting language\nof the software to be tested. It can be seen as a kind of robot which apply the\ntests and warn about any unexpected change in the output. Oval was developed by\nthe LLR laboratory for the needs of the CMS experiment, and it is now\nrecommended by the CERN LCG project."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306057v1", 
    "title": "IceCube's Development Environment", 
    "arxiv-id": "cs/0306057v1", 
    "author": "D. Glowacki", 
    "publish": "2003-06-12T17:31:32Z", 
    "summary": "When the IceCube experiment started serious software development it needed a\ndevelopment environment in which both its developers and clients could work and\nthat would encourage and support a good software development process. Some of\nthe key features that IceCube wanted in such a environment were: the separation\nof the configuration and build tools; inclusion of an issue tracking system;\nsupport for the Unified Change Model; support for unit testing; and support for\ncontinuous building. No single, affordable, off the shelf, environment offered\nall these features. However there are many open source tools that address\nsubsets of these feature, therefore IceCube set about selecting those tools\nwhich it could use in developing its own environment and adding its own tools\nwhere no suitable tools were found. This paper outlines the tools that where\nchosen, what are their responsibilities in the development environment and how\nthey fit together. The complete environment will be demonstrated with a walk\nthrough of single cycle of the development process."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306076v1", 
    "title": "FAYE: A Java Implement of the Frame/Stream/Stop Analysis Model", 
    "arxiv-id": "cs/0306076v1", 
    "author": "S. Patton", 
    "publish": "2003-06-13T21:36:06Z", 
    "summary": "FAYE, The Frame AnalYsis Executable, is a Java based implementation of the\nFrame/Stream/Stop model for analyzing data. Unlike traditional Event based\nanalysis models, the Frame/Stream/Stop model has no preference as to which part\nof any data is to be analyzed, and an Event get as equal treatment as a change\nin the high voltage. This model means that FAYE is a suitable analysis\nframework for many different type of data analysis, such as detector trends or\nas a visualization core. During the design of FAYE the emphasis has been on\nclearly delineating each of the executable's responsibilities and on keeping\ntheir implementations as completely independent as possible. This leads to the\nlarge part of FAYE being a generic core which is experiment independent, and\nsmaller section that customizes this core to an experiments own data\nstructures. This customization can even be done in C++, using JNI, while the\nexecutable's control remains in Java. This paper reviews the Frame/Stream/Stop\nmodel and then looks at how FAYE has approached its implementation, with an\nemphasis on which responsibilities are handled by the generic core, and which\nparts an experiment must provide as part of the customization portion of the\nexecutable."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306078v1", 
    "title": "ROOT Status and Future Developments", 
    "arxiv-id": "cs/0306078v1", 
    "author": "Rene Brun", 
    "publish": "2003-06-16T17:22:59Z", 
    "summary": "In this talk we will review the major additions and improvements made to the\nROOT system in the last 18 months and present our plans for future\ndevelopments. The additons and improvements range from modifications to the I/O\nsub-system to allow users to save and restore objects of classes that have not\nbeen instrumented by special ROOT macros, to the addition of a geometry package\ndesigned for building, browsing, tracking and visualizing detector geometries.\nOther improvements include enhancements to the quick analysis sub-system\n(TTree::Draw()), the addition of classes that allow inter-file object\nreferences (TRef, TRefArray), better support for templated and STL classes,\namelioration of the Automatic Script Compiler and the incorporation of new\nfitting and mathematical tools. Efforts have also been made to increase the\nmodularity of the ROOT system with the introduction of more abstract interfaces\nand the development of a plug-in manager. In the near future, we intend to\ncontinue the development of PROOF and its interfacing with GRID environments.\nWe plan on providing an interface between Geant3, Geant4 and Fluka and the new\ngeometry package. The ROOT GUI classes will finally be available on Windows and\nwe plan to release a GUI inspector and builder. In the last year, ROOT has\ndrawn the endorsement of additional experiments and institutions. It is now\nofficially supported by CERN and used as key I/O component by the LCG project."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306080v1", 
    "title": "BOA: Framework for Automated Builds", 
    "arxiv-id": "cs/0306080v1", 
    "author": "N. Ratnikova", 
    "publish": "2003-06-14T00:07:06Z", 
    "summary": "Managing large-scale software products is a complex software engineering\ntask. The automation of the software development, release and distribution\nprocess is most beneficial in the large collaborations, where the big number of\ndevelopers, multiple platforms and distributed environment are typical factors.\nThis paper describes Build and Output Analyzer framework and its components\nthat have been developed in CMS to facilitate software maintenance and improve\nsoftware quality. The system allows to generate, control and analyze various\ntypes of automated software builds and tests, such as regular rebuilds of the\ndevelopment code, software integration for releases and installation of the\nexisting versions."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306082v1", 
    "title": "The Community Authorization Service: Status and Future", 
    "arxiv-id": "cs/0306082v1", 
    "author": "S. Tuecke", 
    "publish": "2003-06-14T01:16:51Z", 
    "summary": "Virtual organizations (VOs) are communities of resource providers and users\ndistributed over multiple policy domains. These VOs often wish to define and\nenforce consistent policies in addition to the policies of their underlying\ndomains. This is challenging, not only because of the problems in distributing\nthe policy to the domains, but also because of the fact that those domains may\neach have different capabilities for enforcing the policy. The Community\nAuthorization Service (CAS) solves this problem by allowing resource providers\nto delegate some policy authority to the VO while maintaining ultimate control\nover their resources. In this paper we describe CAS and our past and current\nimplementations of CAS, and we discuss our plans for CAS-related research."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306083v1", 
    "title": "The Athena Startup Kit", 
    "arxiv-id": "cs/0306083v1", 
    "author": "W. T. L. P. Lavrijsen", 
    "publish": "2003-06-14T03:03:11Z", 
    "summary": "The Athena Startup Kit (ASK), is an interactive front-end to the Atlas\nsoftware framework (ATHENA). Written in python, a very effective \"glue\"\nlanguage, it is build on top of the, in principle unrelated, code repository,\nbuild, configuration, debug, binding, and analysis tools. ASK automates many\nerror-prone tasks that are otherwise left to the end-user, thereby pre-empting\na whole category of potential problems. Through the existing tools, which ASK\nwill setup for the user if and as needed, it locates available resources,\nmaintains job coherency, manages the run-time environment, allows for\ninteractivity and debugging, and provides standalone execution scripts. An\nend-user who wants to run her own analysis algorithms within the standard\nenvironment can let ASK generate the appropriate skeleton package, the needed\ndependencies and run-time, as well as a default job options script. For new and\ncasual users, ASK comes with a graphical user interface; for advanced users,\nASK has a scriptable command line interface. Both are built on top of the same\nset of libraries. ASK does not need to be, and isn't, experiment neutral. Thus\nit has built-in workarounds for known gotcha's, that would otherwise be a major\ntime-sink for each and every new user. ASK minimizes the overhead for those\nphysicists in Atlas who just want to write and run their analysis code."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306085v1", 
    "title": "GANGA: a user-Grid interface for Atlas and LHCb", 
    "arxiv-id": "cs/0306085v1", 
    "author": "R. W. L. Jones", 
    "publish": "2003-06-14T02:57:32Z", 
    "summary": "The Gaudi/Athena and Grid Alliance (GANGA) is a front-end for the\nconfiguration, submission, monitoring, bookkeeping, output collection, and\nreporting of computing jobs run on a local batch system or on the grid. In\nparticular, GANGA handles jobs that use applications written for the Gaudi\nsoftware framework shared by the Atlas and LHCb experiments. GANGA exploits the\ncommonality of Gaudi-based computing jobs, while insulating against grid-,\nbatch- and framework-specific technicalities, to maximize end-user productivity\nin defining, configuring, and executing jobs. Designed for a python-based\ncomponent architecture, GANGA has a modular underpinning and is therefore well\nplaced for contributing to, and benefiting from, work in related projects. Its\nfunctionality is accessible both from a scriptable command-line interface, for\nexpert users and automated tasks, and through a graphical interface, which\nsimplifies the interaction with GANGA for beginning and c1asual users.\n  This paper presents the GANGA design and implementation, the development of\nthe underlying software bus architecture, and the functionality of the first\npublic GANGA release."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306089v1", 
    "title": "The StoreGate: a Data Model for the Atlas Software Architecture", 
    "arxiv-id": "cs/0306089v1", 
    "author": "S. Rajagopalan", 
    "publish": "2003-06-14T06:56:24Z", 
    "summary": "The Atlas collaboration at CERN has adopted the Gaudi software architecture\nwhich belongs to the blackboard family: data objects produced by knowledge\nsources (e.g. reconstruction modules) are posted to a common in-memory data\nbase from where other modules can access them and produce new data objects. The\nStoreGate has been designed, based on the Atlas requirements and the experience\nof other HENP systems such as Babar, CDF, CLEO, D0 and LHCB, to identify in a\nsimple and efficient fashion (collections of) data objects based on their type\nand/or the modules which posted them to the Transient Data Store (the\nblackboard). The developer also has the freedom to use her preferred key class\nto uniquely identify a data object according to any other criterion. Besides\nthis core functionality, the StoreGate provides the developers with a powerful\ninterface to handle in a coherent fashion persistable references, object\nlifetimes, memory management and access control policy for the data objects in\nthe Store. It also provides a Handle/Proxy mechanism to define and hide the\ncache fault mechanism: upon request, a missing Data Object can be transparently\ncreated and added to the Transient Store presumably retrieving it from a\npersistent data-base, or even reconstructing it on demand."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306098v1", 
    "title": "Making refactoring decisions in large-scale Java systems: an empirical   stance", 
    "arxiv-id": "cs/0306098v1", 
    "author": "Steve Counsell", 
    "publish": "2003-06-16T12:19:18Z", 
    "summary": "Decisions on which classes to refactor are fraught with difficulty. The\nproblem of identifying candidate classes becomes acute when confronted with\nlarge systems comprising hundreds or thousands of classes. In this paper, we\ndescribe a metric by which key classes, and hence candidates for refactoring,\ncan be identified. Measures quantifying the usage of two forms of coupling,\ninheritance and aggregation, together with two other class features (number of\nmethods and attributes) were extracted from the source code of three large Java\nsystems. Our research shows that metrics from other research domains can be\nadapted to the software engineering process. Substantial differences were found\nbetween each of the systems in terms of the key classes identified and hence\nopportunities for refactoring those classes varied between those systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306101v1", 
    "title": "The DataFlow System of the ATLAS Trigger and DAQ", 
    "arxiv-id": "cs/0306101v1", 
    "author": "G. Lehmann", 
    "publish": "2003-06-16T17:25:53Z", 
    "summary": "The baseline design and implementation of the DataFlow system, to be\ndocumented in the ATLAS DAQ/HLT Technical Design Report in summer 2003, will be\npresented. Empahsis will be placed on the system performance and scalability\nbased on the results from prototyping studies which have maximised the use of\ncommercially available hardware."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0306108v1", 
    "title": "Web Engineering", 
    "arxiv-id": "cs/0306108v1", 
    "author": "Bebo White", 
    "publish": "2003-06-18T03:13:44Z", 
    "summary": "Web Engineering is the application of systematic, disciplined and\nquantifiable approaches to development, operation, and maintenance of Web-based\napplications. It is both a pro-active approach and a growing collection of\ntheoretical and empirical research in Web application development. This paper\ngives an overview of Web Engineering by addressing the questions: a) why is it\nneeded? b) what is its domain of operation? c) how does it help and what should\nit do to improve Web application development? and d) how should it be\nincorporated in education and training? The paper discusses the significant\ndifferences that exist between Web applications and conventional software, the\ntaxonomy of Web applications, the progress made so far and the research issues\nand experience of creating a specialisation at the master's level. The paper\nreaches a conclusion that Web Engineering at this stage is a moving target\nsince Web technologies are constantly evolving, making new types of\napplications possible, which in turn may require innovations in how they are\nbuilt, deployed and maintained."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309029v1", 
    "title": "Instrumenting self-modifying code", 
    "arxiv-id": "cs/0309029v1", 
    "author": "K. De Bosschere", 
    "publish": "2003-09-16T13:55:32Z", 
    "summary": "Adding small code snippets at key points to existing code fragments is called\ninstrumentation. It is an established technique to debug certain otherwise hard\nto solve faults, such as memory management issues and data races. Dynamic\ninstrumentation can already be used to analyse code which is loaded or even\ngenerated at run time.With the advent of environments such as the Java Virtual\nMachine with optimizing Just-In-Time compilers, a new obstacle arises:\nself-modifying code. In order to instrument this kind of code correctly, one\nmust be able to detect modifications and adapt the instrumentation code\naccordingly, preferably without incurring a high penalty speedwise. In this\npaper we propose an innovative technique that uses the hardware page protection\nmechanism of modern processors to detect such modifications. We also show how\nan instrumentor can adapt the instrumented version depending on the kind of\nmodificiations as well as an experimental evaluation of said techniques."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309031v1", 
    "title": "Timestamp Based Execution Control for C and Java Programs", 
    "arxiv-id": "cs/0309031v1", 
    "author": "Minoru Terada", 
    "publish": "2003-09-17T06:35:44Z", 
    "summary": "Many programmers have had to deal with an overwritten variable resulting for\nexample from an aliasing problem. The culprit is obviously the last\nwrite-access to that memory location before the manifestation of the bug. The\nusual technique for removing such bugs starts with the debugger by (1) finding\nthe last write and (2) moving the control point of execution back to that time\nby re-executing the program from the beginning. We wish to automate this. Step\n(2) is easy if we can somehow mark the last write found in step (1) and control\nthe execution-point to move it back to this time.\n  In this paper we propose a new concept, position, that is, a point in the\nprogram execution trace, as needed for step (2) above. The position enables\ndebuggers to automate the control of program execution to support common\ndebugging activities. We have implemented position in C by modifying GCC and in\nJava with a bytecode transformer. Measurements show that position can be\nprovided with an acceptable amount of overhead."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309032v1", 
    "title": "Towards declarative diagnosis of constraint programs over finite domains", 
    "arxiv-id": "cs/0309032v1", 
    "author": "Alexandre Tessier", 
    "publish": "2003-09-17T12:42:58Z", 
    "summary": "The paper proposes a theoretical approach of the debugging of constraint\nprograms based on a notion of explanation tree. The proposed approach is an\nattempt to adapt algorithmic debugging to constraint programming. In this\ntheoretical framework for domain reduction, explanations are proof trees\nexplaining value removals. These proof trees are defined by inductive\ndefinitions which express the removals of values as consequences of other value\nremovals. Explanations may be considered as the essence of constraint\nprogramming. They are a declarative view of the computation trace. The\ndiagnosis consists in locating an error in an explanation rooted by a symptom."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309037v1", 
    "title": "Postmortem Object Type Identification", 
    "arxiv-id": "cs/0309037v1", 
    "author": "Bryan M. Cantrill", 
    "publish": "2003-09-22T00:52:59Z", 
    "summary": "This paper presents a novel technique for the automatic type identification\nof arbitrary memory objects from a memory dump. Our motivating application is\ndebugging memory corruption problems in optimized, production systems -- a\nproblem domain largely unserved by extant methodologies. We describe our\nalgorithm as applicable to any typed language, and we discuss it with respect\nto the formidable obstacles posed by C. We describe the heuristics that we have\ndeveloped to overcome these difficulties and achieve effective type\nidentification on C-based systems. We further describe the implementation of\nour heuristics on one C-based system -- the Solaris operating system kernel --\nand describe the extensions that we have added to the Solaris postmortem\ndebugger to allow for postmortem type identification. We show that our\nimplementation yields a sufficiently high rate of type identification to be\nuseful for debugging memory corruption problems. Finally, we discuss some of\nthe novel automated debugging mechanisms that can be layered upon postmortem\ntype identification."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309047v1", 
    "title": "Causes and Effects in Computer Programs", 
    "arxiv-id": "cs/0309047v1", 
    "author": "Andreas Zeller", 
    "publish": "2003-09-24T12:51:18Z", 
    "summary": "Debugging is commonly understood as finding and fixing the cause of a\nproblem. But what does ``cause'' mean? How can we find causes? How can we prove\nthat a cause is a cause--or even ``the'' cause? This paper defines common terms\nin debugging, highlights the principal techniques, their capabilities and\nlimitations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0309055v1", 
    "title": "A mathematical framework for automated bug localization", 
    "arxiv-id": "cs/0309055v1", 
    "author": "Tadanori Mizuno", 
    "publish": "2003-09-30T01:19:11Z", 
    "summary": "In this paper, we propose a mathematical framework for automated bug\nlocalization. This framework can be briefly summarized as follows. A program\nexecution can be represented as a rooted acyclic directed graph. We define an\nexecution snapshot by a cut-set on the graph. A program state can be regarded\nas a conjunction of labels on edges in a cut-set. Then we argue that a\ndebugging task is a pruning process of the execution graph by using cut-sets. A\npruning algorithm, i.e., a debugging task, is also presented."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310007v1", 
    "title": "Event-based Program Analysis with DeWiz", 
    "arxiv-id": "cs/0310007v1", 
    "author": "J. Volkert", 
    "publish": "2003-10-06T11:51:15Z", 
    "summary": "Due to the increased complexity of parallel and distributed programs,\ndebugging of them is considered to be the most difficult and time consuming\npart of the software lifecycle. Tool support is hence a crucial necessity to\nhide complexity from the user. However, most existing tools seem inadequate as\nsoon as the program under consideration exploits more than a few processors\nover a long execution time. This problem is addressed by the novel debugging\ntool DeWiz (Debugging Wizard), whose focus lies on scalability. DeWiz has a\nmodular, scalable architecture, and uses the event graph model as a\nrepresentation of the investigated program. DeWiz provides a set of modules,\nwhich can be combined to generate, analyze, and visualize event graph data.\nWithin this processing pipeline the toolset tries to extract useful\ninformation, which is presented to the user at an arbitrary level of\nabstraction. Additionally, DeWiz is a framework, which can be used to easily\nimplement arbitrary user-defined modules."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310015v1", 
    "title": "Debugging Tool for Localizing Faulty Processes in Message Passing   Programs", 
    "arxiv-id": "cs/0310015v1", 
    "author": "Kenichi Hagihara", 
    "publish": "2003-10-09T07:00:11Z", 
    "summary": "In message passing programs, once a process terminates with an unexpected\nerror, the terminated process can propagate the error to the rest of processes\nthrough communication dependencies, resulting in a program failure. Therefore,\nto locate faults, developers must identify the group of processes involved in\nthe original error and faulty processes that activate faults. This paper\npresents a novel debugging tool, named MPI-PreDebugger (MPI-PD), for localizing\nfaulty processes in message passing programs. MPI-PD automatically\ndistinguishes the original and the propagated errors by checking communication\nerrors during program execution. If MPI-PD observes any communication errors,\nit backtraces communication dependencies and points out potential faulty\nprocesses in a timeline view. We also introduce three case studies, in which\nMPI-PD has been shown to play the key role in their debugging. From these\nstudies, we believe that MPI-PD helps developers to locate faults and allows\nthem to concentrate in correcting their programs."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310016v1", 
    "title": "Debugging Backwards in Time", 
    "arxiv-id": "cs/0310016v1", 
    "author": "Bil Lewis", 
    "publish": "2003-10-09T14:51:57Z", 
    "summary": "By recording every state change in the run of a program, it is possible to\npresent the programmer every bit of information that might be desired.\nEssentially, it becomes possible to debug the program by going ``backwards in\ntime,'' vastly simplifying the process of debugging. An implementation of this\nidea, the ``Omniscient Debugger,'' is used to demonstrate its viability and has\nbeen used successfully on a number of large programs. Integration with an event\nanalysis engine for searching and control is presented. Several small-scale\nuser studies provide encouraging results. Finally performance issues and\nimplementation are discussed along with possible optimizations.\n  This paper makes three contributions of interest: the concept and technique\nof ``going backwards in time,'' the GUI which presents a global view of the\nprogram state and has a formal notion of ``navigation through time,'' and the\nintegration with an event analyzer."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310024v1", 
    "title": "Availability Guarantee for Deterministic Replay Starting Points in   Real-Time Systems", 
    "arxiv-id": "cs/0310024v1", 
    "author": "Daniel Sundmark", 
    "publish": "2003-10-14T08:42:16Z", 
    "summary": "Cyclic debugging requires repeatable executions. As non-deterministic or\nreal-time systems typically do not have the potential to provide this, special\nmethods are required. One such method is replay, a process that requires\nmonitoring of a running system and logging of the data produced by that\nmonitoring. We shall discuss the process of preparing the replay, a part of the\nprocess that has not been very well described before."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310026v1", 
    "title": "Generalized Systematic Debugging for Attribute Grammars", 
    "arxiv-id": "cs/0310026v1", 
    "author": "Masataka Sassa", 
    "publish": "2003-10-15T08:26:22Z", 
    "summary": "Attribute grammars (AGs) are known to be a useful formalism for semantic\nanalysis and translation. However, debugging AGs is complex owing to inherent\ndifficulties of AGs, such as recursive grammar structure and attribute\ndependency. In this paper, a new systematic method of debugging AGs is\nproposed. Our approach is, in principle, based on previously proposed\nalgorithmic debugging of AGs, but is more general. This easily enables\nintegration of various query-based systematic debugging methods, including the\nslice-based method. The proposed method has been implemented in Aki, a debugger\nfor AG description. We evaluated our new approach experimentally using Aki,\nwhich demonstrates the usability of our debugging method."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310040v1", 
    "title": "Automated Fault Localization Using Potential Invariants", 
    "arxiv-id": "cs/0310040v1", 
    "author": "Steven P. Reiss", 
    "publish": "2003-10-18T22:44:47Z", 
    "summary": "We present a general method for fault localization based on abstracting over\nprogram traces, and a tool that implements the method using Ernst's notion of\npotential invariants. Our experiments so far have been unsatisfactory,\nsuggesting that further research is needed before invariants can be used to\nlocate faults."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0310042v1", 
    "title": "Rigorous design of tracers: an experiment for constraint logic   programming", 
    "arxiv-id": "cs/0310042v1", 
    "author": "Pierre Deransart", 
    "publish": "2003-10-22T09:31:18Z", 
    "summary": "In order to design and implement tracers, one must decide what exactly to\ntrace and how to produce this trace. On the one hand, trace designs are too\noften guided by implementation concerns and are not as useful as they should\nbe. On the other hand, an interesting trace which cannot be produced\nefficiently, is not very useful either. In this article we propose a\nmethodology which helps to efficiently produce accurate traces. Firstly, design\na formal specification of the trace model. Secondly, derive a prototype tracer\nfrom this specification. Thirdly, analyze the produced traces. Fourthly,\nimplement an efficient tracer. Lastly, compare the traces of the two tracers.\nAt each step, problems can be found. In that case one has to iterate the\nprocess. We have successfully applied the proposed methodology to the design\nand implementation of a real tracer for constraint logic programming which is\nable to efficiently generate information required to build interesting\ngraphical views of executions."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0311037v1", 
    "title": "DUCT: An Interactive Define-Use Chain Navigation Tool for Relative   Debugging", 
    "arxiv-id": "cs/0311037v1", 
    "author": "David Abramson", 
    "publish": "2003-11-25T01:38:12Z", 
    "summary": "This paper describes an interactive tool that facilitates following\ndefine-use chains in large codes. The motivation for the work is to support\nrelative debugging, where it is necessary to iteratively refine a set of\nasser-tions between different versions of a program. DUCT is novel because it\nexploits the Microsoft Intermediate Language (MSIL) that underpins the .NET\nFramework. Accordingly, it works on a wide range of programming languages\nwithout any modification. The paper describes the design and implementation of\nDUCT, and then illustrates its use with a small case study."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0402006v1", 
    "title": "MammoGrid: Large-Scale Distributed Mammogram Analysis", 
    "arxiv-id": "cs/0402006v1", 
    "author": "Tony Solomonides", 
    "publish": "2004-02-02T19:34:53Z", 
    "summary": "Breast cancer as a medical condition and mammograms as images exhibit many\ndimensions of variability across the population. Similarly, the way diagnostic\nsystems are used and maintained by clinicians varies between imaging centres\nand breast screening programmes, and so does the appearance of the mammograms\ngenerated. A distributed database that reflects the spread of pathologies\nacross the population is an invaluable tool for the epidemiologist and the\nunderstanding of the variation in image acquisition protocols is essential to a\nradiologist in a screening programme. Exploiting emerging grid technology, the\naim of the MammoGrid [1] project is to develop a Europe-wide database of\nmammograms that will be used to investigate a set of important healthcare\napplications and to explore the potential of the grid to support effective\nco-working between healthcare professionals."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0402015v1", 
    "title": "A Preliminary Study for the development of an Early Method for the   Measurement in Function Points of a Software Product", 
    "arxiv-id": "cs/0402015v1", 
    "author": "Gustavo Uria Paino", 
    "publish": "2004-02-09T12:43:13Z", 
    "summary": "The Function Points Analysis (FPA) of A.J. Albrecht is a method to determine\nthe functional size of software products. The International Function Point\nUsers Group, (IFPUG), establishes the FPA like a standard in the software\nfunctional size measurement. The IFPUG [3] [4] method follows the Albrecht's\nmethod and incorporates in its succesive versions modifications to the rules\nand hints with the intention of improving it [7]. The required documentation\nlevel to apply the method is the functional specification which corresponds to\nlevel I in the Rudolph's clasification [8]. This documentation is avalaible\nwith some difficulty for those companies which are dedicated to develop\nsoftware for third parties when they have to prepare the appropiate budget for\nthis development. Then, we face the need of developing an early method [6] [9]\nfor measuring the functional size of a software product that we will name to\nabbreviate it Early Method or EFPM (Early Function Point Method). The required\ndocumentation to apply the EFPM would be the User Requirements or some\nanalogous documentations. This is a part of a research work now in process in\nOviedo University. In this article we only show the following, results:\n  From the measurements of a set of projects using the IFPUG method v. 4.1 we\nobtain the linear correlation coefficients between the total number of Function\nPoints for each project and the counters of the ILFs number, ILFs+EIFs number\nand EIs+EOs+EQs number.\n  Using the preliminary results we compute the regression functions. This\nresults we will allow us to determine the factors to be considered in the\ndevelopment of EFPM and to estimate the function points."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0407038v1", 
    "title": "Model Checking of Statechart Models: Survey and Research Directions", 
    "arxiv-id": "cs/0407038v1", 
    "author": "S. Ramesh", 
    "publish": "2004-07-16T10:18:42Z", 
    "summary": "We survey existing approaches to the formal verification of statecharts using\nmodel checking. Although the semantics and subset of statecharts used in each\napproach varies considerably, along with the model checkers and their\nspecification languages, most approaches rely on translating the hierarchical\nstructure into the flat representation of the input language of the model\nchecker. This makes model checking difficult to scale to industrial models, as\nthe state space grows exponentially with flattening. We look at current\napproaches to model checking hierarchical structures and find that their\nsemantics is significantly different from statecharts. We propose to address\nthe problem of state space explosion using a combination of techniques, which\nare proposed as directions for further research."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0407050v1", 
    "title": "Modeling and Validating Hybrid Systems Using VDM and Mathematica", 
    "arxiv-id": "cs/0407050v1", 
    "author": "Reinhold Kainhofer", 
    "publish": "2004-07-20T15:15:48Z", 
    "summary": "Hybrid systems are characterized by the hybrid evolution of their state: A\npart of the state changes discretely, the other part changes continuously over\ntime. Typically, modern control applications belong to this class of systems,\nwhere a digital controller interacts with a physical environment. In this\narticle we illustrate how a combination of the formal method VDM and the\ncomputer algebra system Mathematica can be used to model and simulate both\naspects: the control logic and the physics involved. A new Mathematica package\nemulating VDM-SL has been developed that allows the integration of differential\nequation systems into formal specifications. The SAFER example from Kelly\n(1997) serves to demonstrate the new simulation capabilities Mathematica adds:\nAfter the thruster selection process, the astronaut's actual position and\nvelocity is calculated by numerically solving Euler's and Newton's equations\nfor rotation and translation. Furthermore, interactive validation is supported\nby a graphical user interface and data animation."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0407051v2", 
    "title": "Bug shallowness in open-source, Macintosh software", 
    "arxiv-id": "cs/0407051v2", 
    "author": "G Gordon Worley III", 
    "publish": "2004-07-20T13:04:36Z", 
    "summary": "Central to the power of open-source software is bug shallowness, the relative\nease of finding and fixing bugs. The open-source movement began with Unix\nsoftware, so many users were also programmers capable of finding and fixing\nbugs given the source code. But as the open-source movement reaches the\nMacintosh platform, bugs may not be shallow because few Macintosh users are\nprogrammers. Based on reports from open-source developers, I, however, conclude\nthat that bugs are as shallow in open-source, Macintosh software as in any\nother open-source software."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0411096v1", 
    "title": "Inter-Package Dependency Networks in Open-Source Software", 
    "arxiv-id": "cs/0411096v1", 
    "author": "Eugene Wallingford", 
    "publish": "2004-11-29T03:38:48Z", 
    "summary": "This research analyzes complex networks in open-source software at the\ninter-package level, where package dependencies often span across projects and\nbetween development groups. We review complex networks identified at ``lower''\nlevels of abstraction, and then formulate a description of interacting software\ncomponents at the package level, a relatively ``high'' level of abstraction. By\nmining open-source software repositories from two sources, we empirically show\nthat the coupling of modules at this granularity creates a small-world and\nscale-free network in both instances."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0502040v1", 
    "title": "Testing Systems of Concurrent Black-boxes--an Automata-Theoretic and   Decompositional Approach", 
    "arxiv-id": "cs/0502040v1", 
    "author": "Zhe Dang", 
    "publish": "2005-02-08T01:15:47Z", 
    "summary": "The global testing problem studied in this paper is to seek a definite answer\nto whether a system of concurrent black-boxes has an observable behavior in a\ngiven finite (but could be huge) set \"Bad\". We introduce a novel approach to\nsolve the problem that does not require integration testing. Instead, in our\napproach, the global testing problem is reduced to testing individual\nblack-boxes in the system one by one in some given order. Using an\nautomata-theoretic approach, test sequences for each individual black-box are\ngenerated from the system's description as well as the test results of\nblack-boxes prior to this black-box in the given order. In contrast to the\nconventional compositional/modular verification/testing approaches, our\napproach is essentially decompositional.\n  Also, our technique is complete, sound, and can be carried out automatically.\nOur experiment results show that the total number of tests needed to solve the\nglobal testing problem is substantially small even for an extremely large\n\"Bad\"."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2001.989822", 
    "link": "http://arxiv.org/pdf/cs/0503002v1", 
    "title": "Local and Global Analysis: Complementary Activities for Increasing the   Effectiveness of Requirements Verification and Validation", 
    "arxiv-id": "cs/0503002v1", 
    "author": "James D. Arthur", 
    "publish": "2005-03-02T00:21:11Z", 
    "summary": "This paper presents a unique approach to connecting requirements engineering\n(RE) activities into a process framework that can be employed to obtain quality\nrequirements with reduced expenditures of effort and cost. We propose a\ntwo-phase model that is novel in that it introduces the concept of verification\nand validation (V&V) early in the requirements life cycle. In the first phase,\nwe perform V&V immediately following the elicitation of requirements for each\nindividually distinct system function. Because the first phase focuses on\ncapturing smaller sets of related requirements iteratively, each corresponding\nV&V activity is better focused for detecting and correcting errors in each\nrequirement set. In the second phase, a complementary verification activity is\ninitiated; the corresponding focus is on the quality of linkages between\nrequirements sets rather than on the requirements within the sets.\nConsequently, this approach reduces the effort in verification and enhances the\nfocus on the verification task. Our approach, unlike other models, has a\nminimal time delay between the elicitation of requirements and the execution of\nthe V&V activities. Because of this short time gap, the stakeholders have a\nclearer recollection of the requirements, their context and rationale; this\nenhances the stakeholder feedback. Furthermore, our model includes activities\nthat closely align with the effective RE processes employed in the software\nindustry. Thus, our approach facilitates a better understanding of the flow of\nrequirements, and provides guidance for the implementation of the RE process."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0503003v1", 
    "title": "An Objectives-Driven Process for Selecting Methods to Support   Requirements Engineering Activities", 
    "arxiv-id": "cs/0503003v1", 
    "author": "James D. Arthur", 
    "publish": "2005-03-02T00:46:46Z", 
    "summary": "This paper presents a framework that guides the requirements engineer in the\nimplementation and execution of an effective requirements generation process.\nWe achieve this goal by providing a well-defined requirements engineering model\nand a criteria based process for optimizing method selection for attendant\nactivities. Our model, unlike other models, addresses the complete requirements\ngeneration process and consists of activities defined at more adequate levels\nof abstraction. Additionally, activity objectives are identified and explicitly\nstated - not implied as in the current models. Activity objectives are crucial\nas they drive the selection of methods for each activity. Our model also\nincorporates a unique approach to verification and validation that enhances\nquality and reduces the cost of generating requirements. To assist in the\nselection of methods, we have mapped commonly used methods to activities based\non their objectives. In addition, we have identified method selection criteria\nand prescribed a reduced set of methods that optimize these criteria for each\nactivity defined by our requirements generation process. Thus, the defined\napproach assists in the task of selecting methods by using selection criteria\nto reduce a large collection of potential methods to a smaller, manageable set.\nThe model and the set of methods, taken together, provide the much needed\nguidance for the effective implementation and execution of the requirements\ngeneration process."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0503004v1", 
    "title": "Effective Requirements Generation: Synchronizing Early Verification &   Validation, Methods and Method Selection Criteria", 
    "arxiv-id": "cs/0503004v1", 
    "author": "James D. Arthur", 
    "publish": "2005-03-02T01:05:32Z", 
    "summary": "This paper presents an approach for the implementation and execution of an\neffective requirements generation process. We achieve this goal by providing a\nwell-defined requirements engineering model that includes verification and\nvalidation (V&V), and analysis. In addition, we identify focused activity\nobjectives and map popular methods to lower-level activities, and define a\ncriterion based process for optimizing method selection for attendant\nactivities. Our model, unlike other models, addresses the complete requirements\ngeneration process and consists of activities defined at more adequate levels\nof abstraction. Furthermore, our model also incorporates a unique approach to\nV&V that enhances quality and reduces the cost of generating requirements.\nAdditionally, activity objectives are identified and explicitly stated - not\nimplied as in the current models. To assist in the selection of an appropriate\nset of methods, we have mapped commonly used methods to activities based on\ntheir objectives. Finally, we have identified method selection criteria and\nprescribed a reduced set of methods that optimize these criteria for each\nactivity in our model. Thus, our approach assists in the task of selecting\nmethods by using selection criteria to reduce a large collection of potential\nmethods to a smaller, manageable set. The model, clear mapping of methods to\nactivity objectives, and the criteria based process, taken together, provide\nthe much needed guidance for the effective implementation and execution of the\nrequirements generation process."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0503050v1", 
    "title": "Systematic Method for Path-Complete White Box Testing", 
    "arxiv-id": "cs/0503050v1", 
    "author": "Nikita Sakhanenko", 
    "publish": "2005-03-21T21:26:48Z", 
    "summary": "A systematic, language-independent method of finding a minimal set of paths\ncovering the code of a sequential program is proposed for application in White\nBox testing. Execution of all paths from the set ensures also statement\ncoverage. Execution fault marks problematic areas of the code. The method\nstarts from a UML activity diagram of a program. The diagram is transformed\ninto a directed graph: graph's nodes substitute decision and action points;\ngraph's directed edges substitute action arrows.\n  The number of independent paths equals easy-to-compute cyclomatic complexity\nof the graph. Association of a vector to each path creates a path vector space.\nIndependence of the paths is equivalent to linear independence of the vectors.\nIt is sufficient to test any base of the path space to complete the procedure.\nAn effective algorithm for choosing the base paths is presented."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0503068v1", 
    "title": "A Survey of Reverse Engineering and Program Comprehension", 
    "arxiv-id": "cs/0503068v1", 
    "author": "Michael L. Nelson", 
    "publish": "2005-03-24T13:55:53Z", 
    "summary": "Reverse engineering has been a standard practice in the hardware community\nfor some time. It has only been within the last ten years that reverse\nengineering, or \"program comprehension\", has grown into the current\nsub-discipline of software engineering. Traditional software engineering is\nprimarily focused on the development and design of new software. However, most\nprogrammers work on software that other people have designed and developed. Up\nto 50% of a software maintainers time can be spent determining the intent of\nsource code. The growing demand to reevaluate and reimplement legacy software\nsystems, brought on by the proliferation of clientserver and World Wide Web\ntechnologies, has underscored the need for reverse engineering tools and\ntechniques. This paper introduces the terminology of reverse engineering and\ngives some of the obstacles that make reverse engineering difficult. Although\nreverse engineering remains heavily dependent on the human component, a number\nof automated tools are presented that aid the reverse engineer."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0504109v1", 
    "title": "Prototype of Fault Adaptive Embedded Software for Large-Scale Real-Time   Systems", 
    "arxiv-id": "cs/0504109v1", 
    "author": "Michael Haney", 
    "publish": "2005-04-29T14:59:23Z", 
    "summary": "This paper describes a comprehensive prototype of large-scale fault adaptive\nembedded software developed for the proposed Fermilab BTeV high energy physics\nexperiment. Lightweight self-optimizing agents embedded within Level 1 of the\nprototype are responsible for proactive and reactive monitoring and mitigation\nbased on specified layers of competence. The agents are self-protecting,\ndetecting cascading failures using a distributed approach. Adaptive,\nreconfigurable, and mobile objects for reliablility are designed to be\nself-configuring to adapt automatically to dynamically changing environments.\nThese objects provide a self-healing layer with the ability to discover,\ndiagnose, and react to discontinuities in real-time processing. A generic\nmodeling environment was developed to facilitate design and implementation of\nhardware resource specifications, application data flow, and failure mitigation\nstrategies. Level 1 of the planned BTeV trigger system alone will consist of\n2500 DSPs, so the number of components and intractable fault scenarios involved\nmake it impossible to design an `expert system' that applies traditional\ncentralized mitigative strategies based on rules capturing every possible\nsystem state. Instead, a distributed reactive approach is implemented using the\ntools and methodologies developed by the Real-Time Embedded Systems group."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0505029v2", 
    "title": "Automated Improvement for Component Reuse", 
    "arxiv-id": "cs/0505029v2", 
    "author": "Muthu Ramachandran", 
    "publish": "2005-05-11T18:23:19Z", 
    "summary": "Software component reuse is the key to significant gains in productivity.\nHowever, the major problem is the lack of identifying and developing\npotentially reusable components. This paper concentrates on our approach to the\ndevelopment of reusable software components. A prototype tool has been\ndeveloped, known as the Reuse Assessor and Improver System (RAIS) which can\ninteractively identify, analyse, assess, and modify abstractions, attributes\nand architectures that support reuse. Practical and objective reuse guidelines\nare used to represent reuse knowledge and to do domain analysis. It takes\nexisting components, provides systematic reuse assessment which is based on\nreuse advice and analysis, and produces components that are improved for reuse.\nOur work on guidelines has been extended to a large scale industrial\napplication."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0506050v1", 
    "title": "The Workshop - Implementing Well Structured Enterprise Applications", 
    "arxiv-id": "cs/0506050v1", 
    "author": "J. Buchmann", 
    "publish": "2005-06-13T13:25:14Z", 
    "summary": "We specify an abstraction layer to be used between an enterprise application\nand the utilized enterprise framework (like J2EE or .NET). This specification\nis called the Workshop. It provides an intuitive metaphor supporting the\nprogrammer in designing easy understandable code. We present an implementation\nof this specification. It is based upon the J2EE framework and is called the\nJWorkshop. As a proof of concept we implement a special certification authority\ncalled the Key Authority based upon the JWorkshop. The mentioned certification\nauthority runs very successfully in a variety of different real world projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0506067v1", 
    "title": "Measuring Woody: The Size of Debian 3.0", 
    "arxiv-id": "cs/0506067v1", 
    "author": "Jesus Gonzalez-Barahona", 
    "publish": "2005-06-15T21:48:06Z", 
    "summary": "Debian is possibly the largest free software distribution, with well over\n4,500 source packages in the latest stable release (Debian 3.0) and more than\n8,000 source packages in the release currently in preparation. However, we wish\nto know what these numbers mean. In this paper, we use David A. Wheeler's\nSLOCCount system to determine the number of physical source lines of code\n(SLOC) of Debian 3.0 (aka woody). We show that Debian 3.0 includes more than\n105,000,000 physical SLOC (almost twice than Red Hat 9, released about 8 months\nlater), showing that the Debian development model (based on the work of a large\ngroup of voluntary developers spread around the world) is at least as capable\nas other development methods (like the more centralized one, based on the work\nof employees, used by Red Hat or Microsoft) to manage distributions of this\nsize.\n  It is also shown that if Debian had been developed using traditional\nproprietary methods, the COCOMO model estimates that its cost would be close to\n$6.1 billion USD to develop Debian 3.0. In addition, we offer both an analysis\nof the programming languages used in the distribution (C amounts for about 65%,\nC++ for about 12%, Shell for about 8% and LISP is around 4%, with many others\nto follow), and the largest packages (The Linux kernel, Mozilla, XFree86, PM3,\netc.)"
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0507061v1", 
    "title": "Software Architecture Overview", 
    "arxiv-id": "cs/0507061v1", 
    "author": "Andre Adrian", 
    "publish": "2005-07-24T09:43:27Z", 
    "summary": "What is Software Architecture? The rules, paradigmen, pattern that help to\nconstruct, build and test a serious piece of software. It is the practical\nexperience boiled down to abstract level. Software Architecture builds on\nSystem Engineering and the scientific method as established by Galileo Galilei:\nMeasure what you can and make measureable what you can not. The experiment\n(test) is more important then the deduction. Pieces of information about\nsoftware architecture are all over the internet. This paper uses citation as\nmuch as possible. The aim is to bring together an overview, not to rephrase the\nwording."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0508105v1", 
    "title": "A Tracer Driver for Versatile Dynamic Analyses of Constraint Logic   Programs", 
    "arxiv-id": "cs/0508105v1", 
    "author": "Mireille Ducasse", 
    "publish": "2005-08-24T11:30:26Z", 
    "summary": "Programs with constraints are hard to debug. In this paper, we describe a\ngeneral architecture to help develop new debugging tools for constraint\nprogramming. The possible tools are fed by a single general-purpose tracer. A\ntracer-driver is used to adapt the actual content of the trace, according to\nthe needs of the tool. This enables the tools and the tracer to communicate in\na client-server scheme. Each tool describes its needs of execution data thanks\nto event patterns. The tracer driver scrutinizes the execution according to\nthese event patterns and sends only the data that are relevant to the connected\ntools. Experimental measures show that this approach leads to good performance\nin the context of constraint logic programming, where a large variety of tools\nexists and the trace is potentially huge."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0511018v2", 
    "title": "From General Systems to Soft Systems to Soft Computing: Applications for   Large and Complex Real World Systems", 
    "arxiv-id": "cs/0511018v2", 
    "author": "Prashant", 
    "publish": "2005-11-03T23:29:55Z", 
    "summary": "This is article is taken out."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0601035v1", 
    "title": "Deductive Object Programming", 
    "arxiv-id": "cs/0601035v1", 
    "author": "Francois Colonna", 
    "publish": "2006-01-10T09:50:45Z", 
    "summary": "We propose some slight additions to O-O languages to implement the necessary\nfeatures for using Deductive Object Programming (DOP). This way of programming\nbased upon the manipulation of the Production Tree of the Objects of Interest,\nresult in making Persistent these Objects and in sensibly lowering the code\ncomplexity."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0601118v1", 
    "title": "A Formal Architecture-Centric Model-Driven Approach for the Automatic   Generation of Grid Applications", 
    "arxiv-id": "cs/0601118v1", 
    "author": "Flavio Oquendo", 
    "publish": "2006-01-28T00:08:30Z", 
    "summary": "This paper discusses the concept of model-driven software engineering applied\nto the Grid application domain. As an extension to this concept, the approach\ndescribed here, attempts to combine both formal architecture-centric and\nmodel-driven paradigms. It is a commonly recognized statement that Grid systems\nhave seldom been designed using formal techniques although from past experience\nsuch techniques have shown advantages. This paper advocates a formal\nengineering approach to Grid system developments in an effort to contribute to\nthe rigorous development of Grids software architectures. This approach\naddresses quality of service and cross-platform developments by applying the\nmodel-driven paradigm to a formal architecture-centric engineering method. This\ncombination benefits from a formal semantic description power in addition to\nmodel-based transformations. The result of such a novel combined concept\npromotes the re-use of design models and facilitates developments in Grid\ncomputing."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0601119v1", 
    "title": "Engineering Conceptual Data Models from Domain Ontologies: A Critical   Evaluation", 
    "arxiv-id": "cs/0601119v1", 
    "author": "Richard McClatchey", 
    "publish": "2006-01-28T17:40:44Z", 
    "summary": "This paper studies the differences and similarities between domain ontologies\nand conceptual data models and the role that ontologies can play in\nestablishing conceptual data models during the process of information systems\ndevelopment. A mapping algorithm has been proposed and embedded in a special\npurpose Transformation Engine to generate a conceptual data model from a given\ndomain ontology. Both quantitative and qualitative methods have been adopted to\ncritically evaluate this new approach. In addition, this paper focuses on\nevaluating the quality of the generated conceptual data model elements using\nBunge-Wand-Weber and OntoClean ontologies. The results of this evaluation\nindicate that the generated conceptual data model provides a high degree of\naccuracy in identifying the substantial domain entities along with their\nattributes and relationships being derived from the consensual semantics of\ndomain knowledge. The results are encouraging and support the potential role\nthat this approach can take part in process of information system development."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0603033v1", 
    "title": "Inter-component communication methods in object-oriented frameworks", 
    "arxiv-id": "cs/0603033v1", 
    "author": "Vaghinak Petrosyan", 
    "publish": "2006-03-09T08:44:43Z", 
    "summary": "Modern frameworks for development of graphical interfaces are using the\nnative controls of the operating system. Because of that they are using\noperating system events model for inter-component communication. We consider a\nmethod to increase inter-component communication speed by sending messages from\none component to the other passing over the operating system. Besides the\nmessages subscription helps to avoid receiving of unnecessary messages."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0603037v1", 
    "title": "Deriving Conceptual Data Models from Domain Ontologies for   Bioinformatics", 
    "arxiv-id": "cs/0603037v1", 
    "author": "Richard McClatchey", 
    "publish": "2006-03-09T13:59:18Z", 
    "summary": "This paper studies the role that ontologies can play in establishing\nconceptual data models during the process of information systems development. A\nmapping algorithm has been proposed and embedded in a special purpose\nTransformation-Engine to generate a conceptual data model from a given domain\nontology. In addition, this paper focuses on applying the proposed approach to\na bioinformatics context as the nature of biological data is considered a\nbarrier in representing biological conceptual data models. Both quantitative\nand qualitative methods have been adopted to critically evaluate this new\napproach. The results of this evaluation indicate that the quality of the\ngenerated conceptual data models can reflect the problem domain entities and\nthe associations between them. The results are encouraging and support the\npotential role that this approach can play in providing a suitable starting\npoint for conceptual data model development."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0604073v2", 
    "title": "Octave-GTK: A GTK binding for GNU Octave", 
    "arxiv-id": "cs/0604073v2", 
    "author": "Leela Velusamy", 
    "publish": "2006-04-19T16:46:23Z", 
    "summary": "This paper discusses the problems faced with interoperability between two\nprogramming languages, with respect to GNU Octave, and GTK API written in C, to\nprovide the GTK API on Octave.Octave-GTK is the fusion of two different API's:\none exported by GNU Octave [scientific computing tool] and the other GTK [GUI\ntoolkit]; this enables one to use GTK primitives within GNU Octave, to build\ngraphical front ends,at the same time using octave engine for number crunching\npower. This paper illustrates our implementation of binding logic, and shows\nresults extended to various other libraries using the same base code generator.\nAlso shown, are methods of code generation, binding automation, and the niche\nwe plan to fill in the absence of GUI in Octave. Canonical discussion of\nadvantages, feasibility and problems faced in the process are elucidated."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0605020v1", 
    "title": "Applied MVC Patterns. A pattern language", 
    "arxiv-id": "cs/0605020v1", 
    "author": "Sergey Alpaev", 
    "publish": "2006-05-05T10:25:26Z", 
    "summary": "How to get advantages of MVC model without making applications unnecessarily\ncomplex? The full-featured MVC implementation is on the top end of ladder of\ncomplexity. The other end is meant for simple cases that do not call for such\ncomplex designs, however still in need of the advantages of MVC patterns, such\nas ability to change the look-and-feel. This paper presents patterns of MVC\nimplementation that help to benefit from the paradigm and keep the right\nbalance between flexibility and implementation complexity."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0606092v2", 
    "title": "Static Analysis using Parameterised Boolean Equation Systems", 
    "arxiv-id": "cs/0606092v2", 
    "author": "Pedro Merino", 
    "publish": "2006-06-21T07:23:25Z", 
    "summary": "The well-known problem of state space explosion in model checking is even\nmore critical when applying this technique to programming languages, mainly due\nto the presence of complex data structures. One recent and promising approach\nto deal with this problem is the construction of an abstract and correct\nrepresentation of the global program state allowing to match visited states\nduring program model exploration. In particular, one powerful method to\nimplement abstract matching is to fill the state vector with a minimal amount\nof relevant variables for each program point. In this paper, we combine the\non-the-fly model-checking approach (incremental construction of the program\nstate space) and the static analysis method called influence analysis\n(extraction of significant variables for each program point) in order to\nautomatically construct an abstract matching function. Firstly, we describe the\nproblem as an alternation-free value-based mu-calculus formula, whose validity\ncan be checked on the program model expressed as a labeled transition system\n(LTS). Secondly, we translate the analysis into the local resolution of a\nparameterised boolean equation system (PBES), whose representation enables a\nmore efficient construction of the resulting abstract matching function.\nFinally, we show how our proposal may be elegantly integrated into CADP, a\ngeneric framework for both the design and analysis of distributed systems and\nthe development of verification tools."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0606108v1", 
    "title": "A Product Oriented Modelling Concept: Holons for systems synchronisation   and interoperability", 
    "arxiv-id": "cs/0606108v1", 
    "author": "Khalid Benali", 
    "publish": "2006-06-26T19:24:14Z", 
    "summary": "Nowadays, enterprises are confronted to growing needs for traceability,\nproduct genealogy and product life cycle management. To meet those needs, the\nenterprise and applications in the enterprise environment have to manage flows\nof information that relate to flows of material and that are managed in shop\nfloor level. Nevertheless, throughout product lifecycle coordination needs to\nbe established between reality in the physical world (physical view) and the\nvirtual world handled by manufacturing information systems (informational\nview). This paper presents the \"Holon\" modelling concept as a means for the\nsynchronisation of both physical view and informational views. Afterwards, we\nshow how the concept of holon can play a major role in ensuring\ninteroperability in the enterprise context."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0606112v1", 
    "title": "Product Centric Holons for Synchronisation and Interoperability in   Manufacturing Environments", 
    "arxiv-id": "cs/0606112v1", 
    "author": "G\u00e9rard Morel", 
    "publish": "2006-06-27T11:21:23Z", 
    "summary": "In the last few years, lot of work has been done in order to ensure\nenterprise applications interoperability; however, proposed solutions focus\nmainly on enterprise processes. Indeed, throughout product lifecycle\ncoordination needs to be established between reality in the physical world\n(physical view) and the virtual world handled by manufacturing information\nsystems (informational view). This paper presents a holonic approach that\nenables synchronisation of both physical and informational views. A model\ndriven approach for interoperability is proposed to ensure interoperability of\nholon based models with other applications in the enterprise."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0607044v1", 
    "title": "Use of UML and Model Transformations for Workflow Process Definitions", 
    "arxiv-id": "cs/0607044v1", 
    "author": "Valdis Vitolins", 
    "publish": "2006-07-11T09:23:09Z", 
    "summary": "Currently many different modeling languages are used for workflow definitions\nin BPM systems. Authors of this paper analyze the two most popular graphical\nlanguages, with highest possibility of wide practical usage - UML Activity\ndiagrams (AD) and Business Process Modeling Notation (BPMN). The necessary in\npractice workflow aspects are briefly discussed, and on this basis a natural AD\nprofile is proposed, which covers all of them. A functionally equivalent BPMN\nsubset is also selected. The semantics of both languages in the context of\nprocess execution (namely, mapping to BPEL) is also analyzed in the paper. By\nanalyzing AD and BPMN metamodels, authors conclude that an exact transformation\nfrom AD to BPMN is not trivial even for the selected subset, though these\nlanguages are considered to be similar. Authors show how this transformation\ncould be defined in the MOLA transformation language."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0607063v1", 
    "title": "Prioritizing Software Inspection Results using Static Profiling", 
    "arxiv-id": "cs/0607063v1", 
    "author": "Leon Moonen", 
    "publish": "2006-07-12T20:35:10Z", 
    "summary": "Static software checking tools are useful as an additional automated software\ninspection step that can easily be integrated in the development cycle and\nassist in creating secure, reliable and high quality code. However, an often\nquoted disadvantage of these tools is that they generate an overly large number\nof warnings, including many false positives due to the approximate analysis\ntechniques. This information overload effectively limits their usefulness.\n  In this paper we present ELAN, a technique that helps the user prioritize the\ninformation generated by a software inspection tool, based on a demand-driven\ncomputation of the likelihood that execution reaches the locations for which\nwarnings are reported. This analysis is orthogonal to other prioritization\ntechniques known from literature, such as severity levels and statistical\nanalysis to reduce false positives. We evaluate feasibility of our technique\nusing a number of case studies and assess the quality of our predictions by\ncomparing them to actual values obtained by dynamic profiling."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0607116v1", 
    "title": "Program Spectra Analysis in Embedded Software: A Case Study", 
    "arxiv-id": "cs/0607116v1", 
    "author": "Arjan JC van Gemund", 
    "publish": "2006-07-26T09:26:14Z", 
    "summary": "Because of constraints imposed by the market, embedded software in consumer\nelectronics is almost inevitably shipped with faults and the goal is just to\nreduce the inherent unreliability to an acceptable level before a product has\nto be released. Automatic fault diagnosis is a valuable tool to capture\nsoftware faults without extra effort spent on testing. Apart from a debugging\naid at design and integration time, fault diagnosis can help analyzing problems\nduring operation, which allows for more accurate system recovery. In this paper\nwe discuss perspectives and limitations for applying a particular fault\ndiagnosis technique, namely the analysis of program spectra, in the area of\nembedded software in consumer electronics devices. We illustrate these by our\nfirst experience with a test case from industry."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0607121v1", 
    "title": "Object-Based Groupware: Theory, Design and Implementation Issues", 
    "arxiv-id": "cs/0607121v1", 
    "author": "Gleb G. Pogodayev", 
    "publish": "2006-07-27T10:16:16Z", 
    "summary": "Document management software systems are having a wide audience at present.\nHowever, groupware as a term has a wide variety of possible definitions.\nGroupware classification attempt is made in this paper. Possible approaches to\ngroupware are considered including document management, document control and\nmailing systems. Lattice theory and concept modelling are presented as a\ntheoretical background for the systems in question. Current technologies in\nstate-of-the-art document managenent software are discussed. Design and\nimplementation aspects for user-friendly integrate enterprise systems are\ndescribed. Results for a real system to be implemented are given. Perspectives\nof the field in question are discussed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEW.2005.18", 
    "link": "http://arxiv.org/pdf/cs/0608038v1", 
    "title": "Morphisms of Coloured Petri Nets", 
    "arxiv-id": "cs/0608038v1", 
    "author": "Joachim Wehler", 
    "publish": "2006-08-07T19:03:07Z", 
    "summary": "We introduce the concept of a morphism between coloured nets. Our definition\ngeneralizes Petris definition for ordinary nets. A morphism of coloured nets\nmaps the topological space of the underlying undirected net as well as the\nkernel and cokernel of the incidence map. The kernel are flows along the\ntransition-bordered fibres of the morphism, the cokernel are classes of\nmarkings of the place-bordered fibres. The attachment of bindings, colours,\nflows and marking classes to a subnet is formalized by using concepts from\nsheaf theory. A coloured net is a sheaf-cosheaf pair over a Petri space and a\nmorphism between coloured nets is a morphism between such pairs. Coloured nets\nand their morphisms form a category. We prove the existence of a product in the\nsubcategory of sort-respecting morphisms. After introducing markings our\nconcepts generalize to coloured Petri nets."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0608111v2", 
    "title": "An Architectural Style for Ajax", 
    "arxiv-id": "cs/0608111v2", 
    "author": "Arie van Deursen", 
    "publish": "2006-08-29T10:10:32Z", 
    "summary": "A new breed of web application, dubbed AJAX, is emerging in response to a\nlimited degree of interactivity in large-grain stateless Web interactions. At\nthe heart of this new approach lies a single page interaction model that\nfacilitates rich interactivity. We have studied and experimented with several\nAJAX frameworks trying to understand their architectural properties. In this\npaper, we summarize three of these frameworks and examine their properties and\nintroduce the SPIAR architectural style. We describe the guiding software\nengineering principles and the constraints chosen to induce the desired\nproperties. The style emphasizes user interface component development, and\nintermediary delta-communication between client/server components, to improve\nuser interactivity and ease of development. In addition, we use the concepts\nand principles to discuss various open issues in AJAX frameworks and\napplication development."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0609024v2", 
    "title": "Linux, Open Source and Unicode", 
    "arxiv-id": "cs/0609024v2", 
    "author": "Prashant", 
    "publish": "2006-09-06T18:12:27Z", 
    "summary": "The paper is taken out."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0609025v2", 
    "title": "A XML Schema Definition based Universal User Interface", 
    "arxiv-id": "cs/0609025v2", 
    "author": "Prashant", 
    "publish": "2006-09-06T18:21:23Z", 
    "summary": "The article is taken out for change of contents."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0609147v2", 
    "title": "Identifying Crosscutting Concerns Using Fan-in Analysis", 
    "arxiv-id": "cs/0609147v2", 
    "author": "Leon Moonen", 
    "publish": "2006-09-26T17:50:19Z", 
    "summary": "Aspect mining is a reverse engineering process that aims at finding\ncrosscutting concerns in existing systems. This paper proposes an aspect mining\napproach based on determining methods that are called from many different\nplaces, and hence have a high fan-in, which can be seen as a symptom of\ncrosscutting functionality. The approach is semi-automatic, and consists of\nthree steps: metric calculation, method filtering, and call site analysis.\nCarrying out these steps is an interactive process supported by an Eclipse\nplug-in called FINT. Fan-in analysis has been applied to three open source Java\nsystems, totaling around 200,000 lines of code. The most interesting concerns\nidentified are discussed in detail, which includes several concerns not\npreviously discussed in the aspect-oriented literature. The results show that a\nsignificant number of crosscutting concerns can be recognized using fan-in\nanalysis, and each of the three steps can be supported by tools."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0610094v2", 
    "title": "Migrating Multi-page Web Applications to Single-page AJAX Interfaces", 
    "arxiv-id": "cs/0610094v2", 
    "author": "Arie van Deursen", 
    "publish": "2006-10-15T07:36:19Z", 
    "summary": "Recently, a new web development technique for creating interactive web\napplications, dubbed AJAX, has emerged. In this new model, the single-page web\ninterface is composed of individual components which can be updated/replaced\nindependently. With the rise of AJAX web applications classical multi-page web\napplications are becoming legacy systems. If until a year ago, the concern\nrevolved around migrating legacy systems to web-based settings, today we have a\nnew challenge of migrating web applications to single-page AJAX applications.\nGaining an understanding of the navigational model and user interface structure\nof the source application is the first step in the migration process. In this\npaper, we explore how reverse engineering techniques can help analyze classic\nweb applications for this purpose. Our approach, using a schema-based\nclustering technique, extracts a navigational model of web applications, and\nidentifies candidate user interface components to be migrated to a single-page\nAJAX interface. Additionally, results of a case study, conducted to evaluate\nour tool, are presented."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0610096v1", 
    "title": "Partial Evaluation for Program Comprehension", 
    "arxiv-id": "cs/0610096v1", 
    "author": "Sandrine Blazy", 
    "publish": "2006-10-16T09:05:05Z", 
    "summary": "Program comprehension is the most tedious and time consuming task of software\nmaintenance, an important phase of the software life cycle. This is\nparticularly true while maintaining scientific application programs that have\nbeen written in Fortran for decades and that are still vital in various domains\neven though more modern languages are used to implement their user interfaces.\nVery often, programs have evolved as their application domains increase\ncontinually and have become very complex due to extensive modifications. This\ngenerality in programs is implemented by input variables whose value does not\nvary in the context of a given application. Thus, it is very interesting for\nthe maintainer to propagate such information, that is to obtain a simplified\nprogram, which behaves like the initial one when used according to the\nrestriction. We have adapted partial evaluation for program comprehension. Our\npartial evaluator performs mainly two tasks: constant propagation and\nstatements simplification. It includes an interprocedural alias analysis. As\nour aim is program comprehension rather than optimization, there are two main\ndifferences with classical partial evaluation. We do not change the original"
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0610097v1", 
    "title": "Reuse of Specification Patterns with the B Method", 
    "arxiv-id": "cs/0610097v1", 
    "author": "R\u00e9gine Laleau", 
    "publish": "2006-10-16T09:07:14Z", 
    "summary": "This paper describes an approach for reusing specification patterns.\nSpecification patterns are design patterns that are expressed in a formal\nspecification language. Reusing a specification pattern means instantiating it\nor composing it with other specification patterns. Three levels of composition\nare defined: juxtaposition, composition with inter-patterns links and\nunification. This paper shows through examples how to define specification\npatterns in B, how to reuse them directly in B, and also how to reuse the\nproofs associated with specification patterns."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2007.7", 
    "link": "http://arxiv.org/pdf/cs/0611071v1", 
    "title": "Capabilities Engineering: Constructing Change-Tolerant Systems", 
    "arxiv-id": "cs/0611071v1", 
    "author": "Shawn A. Bohner", 
    "publish": "2006-11-15T17:12:54Z", 
    "summary": "We propose a Capabilities-based approach for building long-lived, complex\nsystems that have lengthy development cycles. User needs and technology evolve\nduring these extended development periods, and thereby, inhibit a fixed\nrequirements-oriented solution specification. In effect, for complex emergent\nsystems, the traditional approach of baselining requirements results in an\nunsatisfactory system. Therefore, we present an alternative approach,\nCapabilities Engineering, which mathematically exploits the structural\nsemantics of the Function Decomposition graph - a representation of user needs\n- to formulate Capabilities. For any given software system, the set of derived\nCapabilities embodies change-tolerant characteristics. More specifically, each\nindividual Capability is a functional abstraction constructed to be highly\ncohesive and to be minimally coupled with its neighbors. Moreover, the\nCapability set is chosen to accommodate an incremental development approach,\nand to reflect the constraints of technology feasibility and implementation\nschedules. We discuss our validation activities to empirically prove that the\nCapabilities-based approach results in change-tolerant systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0611072v2", 
    "title": "Reconciling Synthesis and Decomposition: A Composite Approach to   Capability Identification", 
    "arxiv-id": "cs/0611072v2", 
    "author": "Robert P. Broadwater", 
    "publish": "2006-11-15T22:32:43Z", 
    "summary": "Stakeholders' expectations and technology constantly evolve during the\nlengthy development cycles of a large-scale computer based system.\nConsequently, the traditional approach of baselining requirements results in an\nunsatisfactory system because it is ill-equipped to accommodate such change. In\ncontrast, systems constructed on the basis of Capabilities are more\nchange-tolerant; Capabilities are functional abstractions that are neither as\namorphous as user needs nor as rigid as system requirements. Alternatively,\nCapabilities are aggregates that capture desired functionality from the users'\nneeds, and are designed to exhibit desirable software engineering\ncharacteristics of high cohesion, low coupling and optimum abstraction levels.\nTo formulate these functional abstractions we develop and investigate two\nalgorithms for Capability identification: Synthesis and Decomposition. The\nsynthesis algorithm aggregates detailed rudimentary elements of the system to\nform Capabilities. In contrast, the decomposition algorithm determines\nCapabilities by recursively partitioning the overall mission of the system into\nmore detailed entities. Empirical analysis on a small computer based library\nsystem reveals that neither approach is sufficient by itself. However, a\ncomposite algorithm based on a complementary approach reconciling the two polar\nperspectives results in a more feasible set of Capabilities. In particular, the\ncomposite algorithm formulates Capabilities using the cohesion and coupling\nmeasures as defined by the decomposition algorithm and the abstraction level as\ndetermined by the synthesis algorithm."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0611110v3", 
    "title": "The Implications of Network-Centric Software Systems on Software   Architecture: A Critical Evaluation", 
    "arxiv-id": "cs/0611110v3", 
    "author": "James D. Arthur", 
    "publish": "2006-11-21T19:23:32Z", 
    "summary": "The purpose of this paper is to evaluate the impact of emerging\nnetwork-centric software systems on the field of software architecture. We\nfirst develop an insight concerning the term \"network-centric\" by presenting\nits origin and its implications within the context of software architecture. On\nthe basis of this insight, we present our definition of a network-centric\nframework and its distinguishing characteristics. We then enumerate the\nchallenges that face the field of software architecture as software development\nshifts from a platform-centric to a network-centric model. In order to face\nthese challenges, we propose a formal approach embodied in a new architectural\nstyle that supports overcoming these challenges at the architectural level.\nFinally, we conclude by presenting an illustrative example to demonstrate the\nusefulness of the concepts of network centricity, summarizing our\ncontributions, and linking our approach to future work that needs to be done in\nthis area."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0612082v1", 
    "title": "Developing efficient parsers in Prolog: the CLF manual (v1.0)", 
    "arxiv-id": "cs/0612082v1", 
    "author": "Thierry Despeyroux", 
    "publish": "2006-12-18T08:40:53Z", 
    "summary": "This document describes a couple of tools that help to quickly design and\ndevelop computer (formalized) languages. The first one use Flex to perform\nlexical analysis and the second is an extention of Prolog DCGs to perfom\nsyntactical analysis. Initially designed as a new component for the Centaur\nsystem, these tools are now available independently and can be used to\nconstruct efficient Prolog parsers that can be integrated in Prolog or\nheterogeneous systems. This is the initial version of the CLF documentation.\nUpdated version will be available online when necessary."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0612092v1", 
    "title": "Agile Adoption Process Framework", 
    "arxiv-id": "cs/0612092v1", 
    "author": "James Arthur", 
    "publish": "2006-12-19T16:35:46Z", 
    "summary": "Today many organizations aspire to adopt agile processes in hope of\novercoming some of the difficulties they are facing with their current software\ndevelopment process. There is no structured framework for the agile adoption\nprocess. This paper presents a 3-Stage process framework that assists\norganization and guides organizations through their agile adoption efforts. The\nProcess Framework has been received significantly positive feedback from\nexperts and leaders in agile adoption industry."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0612131v2", 
    "title": "Architecting Network-Centric Software Systems: A Style-Based Beginning", 
    "arxiv-id": "cs/0612131v2", 
    "author": "Amine Chigani James D. Arthur Shawn Bohner", 
    "publish": "2006-12-23T01:00:32Z", 
    "summary": "With the advent of potent network technology, software development has\nevolved from traditional platform-centric construction to network-centric\nevolution. This change involves largely the way we reason about systems as\nevidenced in the introduction of Network- Centric Operations (NCO).\nUnfortunately, it has resulted in conflicting interpretations of how to map NCO\nconcepts to the field of software architecture. In this paper, we capture the\ncore concepts and goals of NCO, investigate the implications of these concepts\nand goals on software architecture, and identify the operational\ncharacteristics that distinguish network-centric software systems from other\nsystems. More importantly, we use architectural design principles to propose an\noutline for a network-centric architectural style that helps in characterizing\nnetwork-centric software systems and that provides a means by which their\ndistinguishing operational characteristics can be realized."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0701010v1", 
    "title": "Determining the Applicability of Agile Practices to Mission and   Life-critical Systems", 
    "arxiv-id": "cs/0701010v1", 
    "author": "James Arthur", 
    "publish": "2007-01-01T05:08:06Z", 
    "summary": "Adopting agile practices brings about many benefits and improvements to the\nsystem being developed. However, in mission and life-critical systems, adopting\nan inappropriate agile practice has detrimental impacts on the system in\nvarious phases of its lifecycle as well as precludes desired qualities from\nbeing actualized. This paper presents a three-stage process that provides\nguidance to organizations on how to identify the agile practices they can\nbenefit from without causing any impact to the mission and life critical system\nbeing developed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0701132v2", 
    "title": "Certifying controls and systems software", 
    "arxiv-id": "cs/0701132v2", 
    "author": "Mardavij Roozbehani", 
    "publish": "2007-01-21T19:34:45Z", 
    "summary": "Software system certification presents itself with many challenges, including\nthe necessity to certify the system at the level of functional requirements,\ncode and binary levels, the need to chase down run-time errors, and the need\nfor proving timing properties of the eventual, compiled system. This paper\nillustrates possible approaches for certifying code that arises from control\nsystems requirements as far as stability properties are concerned. The relative\nsimplicity of the certification process should encourage the development of\nsystematic procedures for certifying control system codes for more complex\nenvironments."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0701200v1", 
    "title": "Reasoning from a schema and from an analog in software code reuse", 
    "arxiv-id": "cs/0701200v1", 
    "author": "Fran\u00e7oise Detienne", 
    "publish": "2007-01-31T16:41:02Z", 
    "summary": "The activity of design involves the decomposition of problems into\nsubproblems and the development and evaluation of solutions. In many cases,\nsolution development is not done from scratch. Designers often evoke and adapt\nsolutions developed in the past. These solutions may come from an internal\nsource, i.e. the memory of the designers, and/or from an external source. The\ngoal of this paper is to analyse the characteristics of the cognitive\nmechanisms, the knowledge and the representations involved in the code reuse\nactivity performed by experienced programmers. More generally, the focus is the\ncontrol structure of the reuse activity. Data collected in an experiment in\nwhich programmers had to design programs are analyzed. Two code reuse\nsituations are distinguished depending on whether or not the processes involved\nin reuse start before the elaboration of what acts as a source-solution. Our\nanalysis highlights the use of reasoning from a schema and from an analog in\nthe code reuse activity."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0702083v1", 
    "title": "Improving Prolog programs: Refactoring for Prolog", 
    "arxiv-id": "cs/0702083v1", 
    "author": "Bart Demoen", 
    "publish": "2007-02-14T09:53:37Z", 
    "summary": "Refactoring is an established technique from the object-oriented (OO)\nprogramming community to restructure code: it aims at improving software\nreadability, maintainability and extensibility. Although refactoring is not\ntied to the OO-paradigm in particular, its ideas have not been applied to Logic\nProgramming until now.\n  This paper applies the ideas of refactoring to Prolog programs. A catalogue\nis presented listing refactorings classified according to scope. Some of the\nrefactorings have been adapted from the OO-paradigm, while others have been\nspecifically designed for Prolog. The discrepancy between intended and\noperational semantics in Prolog is also addressed by some of the refactorings.\n  In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and\nthe experience with applying ViPReSS to a large Prolog legacy system is\nreported. The main conclusion is that refactoring is both a viable technique in\nProlog and a rather desirable one."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0703012v1", 
    "title": "Pre-Requirement Specification Traceability: Bridging the Complexity Gap   through Capabilities", 
    "arxiv-id": "cs/0703012v1", 
    "author": "Manuel P\u00e9rez-Qui\u00f1ones", 
    "publish": "2007-03-02T20:46:26Z", 
    "summary": "Pre-Requirement Specification traceability is the activity of capturing\nrelations between requirements and their sources, in particular user needs.\nRequirements are formal technical specifications in the solution space; needs\nare natural language expressions codifying user expectations in the problem\nspace. Current traceability techniques are challenged by the complexity gap\nthat results from the disparity between the spaces, and thereby, often neglect\ntraceability to and from requirements. We identify the existence of an\nintermediary region -- the transition space -- which structures the progression\nfrom needs to requirements. More specifically, our approach to developing\nchange-tolerant systems, termed Capabilities Engineering, identifies highly\ncohesive, minimally coupled, optimized functional abstractions called\nCapabilities in the transition space. These Capabilities link the problem and\nsolution spaces through directives (entities derived from user needs).\nDirectives connect the problem and transition spaces; Capabilities link the\ntransition and solution spaces. Furthermore, the process of Capabilities\nEngineering addresses specific traceability challenges. It supports the\nevolution of traces, provides semantic and structural information about\ndependencies, incorporates human factors, generates traceability relations with\nnegligible overhead, and thereby, fosters pre-Requirement Specification\ntraceability."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0703021v1", 
    "title": "Addressing Components' Evolvement and Execution Behavior to Measure   Component-Based Software Reliability", 
    "arxiv-id": "cs/0703021v1", 
    "author": "Mei-Huei Tang", 
    "publish": "2007-03-05T20:58:44Z", 
    "summary": "Software reliability is an important quality attrib-ute, often evaluated as\neither a function of time or of system structures. The goal of this study is to\nhave this metric cover both for component-based software, be-cause its\nreliability strongly depends on the quality of constituent components and their\ninteractions. To achieve this, we apply a convolution modeling ap-proach, based\non components' execution behavior, to integrate their individual reliability\nevolvement and simultaneously address failure fixes in the time do-main.\nModeling at the component level can be more economical to accommodate software\nevolution, be-cause the reliability metric can be evaluated by reus-ing the\nquality measures of unaffected components and adapting only to the affected\nones to save cost. The adaptation capability also supports the incremental\nsoftware development processes that constantly add in new components over time.\nExperiments were con-ducted to discuss the usefulness of this approach."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0703069v3", 
    "title": "Portlet Wrappers using JavaScript", 
    "arxiv-id": "cs/0703069v3", 
    "author": "Paul Fodor", 
    "publish": "2007-03-14T23:32:43Z", 
    "summary": "In this paper we extend the classical portal (with static portlets) design\nwith HTML DOM Web clipping on the client browser using dynamic JavaScript\nportlets: the portal server supplies the user/passwords for all services\nthrough https and the client browser retrieves web pages and\ncuts/selects/changes the desired parts using paths (XPath) in the Web page\nstructure. This operation brings along a set of advantages: dynamic wrapping of\nexisting legacy websites in the client browser, the reloading of only changed\nportlets instead of whole portal, low bandwidth on the server, the elimination\nof re-writing the URL links in the portal, and last but not least, a support\nfor Java applets in portlets by putting the login cookies on the client\nbrowser. Our solution is compliant with JSR168 Portlet Specification allowing\nportability across all vendor platforms."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/cs/0703080v1", 
    "title": "A Systematic Approach to Web-Application Development", 
    "arxiv-id": "cs/0703080v1", 
    "author": "Paul Fodor", 
    "publish": "2007-03-15T12:55:46Z", 
    "summary": "Designing a web-application from a specification involves a series of\nwell-planned and well executed steps leading to the final product. This often\ninvolves critical changes in design while testing the application, which itself\nis slow and cumbersome. Traditional approaches either fully automate the\nweb-application development process, or let developers write everything from\nscratch. Our approach is based on a middle-ground, with precise control on the\nworkflow and usage of a set of custom-made software tools to automate a\nsignificant part of code generation."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0704.1294v1", 
    "title": "A Disciplined Approach to Adopting Agile Practices: The Agile Adoption   Framework", 
    "arxiv-id": "0704.1294v1", 
    "author": "Shawn Bohner", 
    "publish": "2007-04-10T19:11:51Z", 
    "summary": "Many organizations aspire to adopt agile processes to take advantage of the\nnumerous benefits that it offers to an organization. Those benefits include,\nbut are not limited to, quicker return on investment, better software quality,\nand higher customer satisfaction. To date however, there is no structured\nprocess (at least in the public domain) that guides organizations in adopting\nagile practices. To address this problem we present the Agile Adoption\nFramework. The framework consists of two components: an agile measurement\nindex, and a 4-Stage process, that together guide and assist the agile adoption\nefforts of organizations. More specifically, the agile measurement index is\nused to identify the agile potential of projects and organizations. The 4-Stage\nprocess, on the other hand, helps determine (a) whether or not organizations\nare ready for agile adoption, and (b) guided by their potential, what set of\nagile practices can and should be introduced."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0705.3616v1", 
    "title": "On How Developers Test Open Source Software Systems", 
    "arxiv-id": "0705.3616v1", 
    "author": "Arie van Deursen", 
    "publish": "2007-05-24T16:21:35Z", 
    "summary": "Engineering software systems is a multidisciplinary activity, whereby a\nnumber of artifacts must be created - and maintained - synchronously. In this\npaper we investigate whether production code and the accompanying tests\nco-evolve by exploring a project's versioning system, code coverage reports and\nsize-metrics. Three open source case studies teach us that testing activities\nusually start later on during the lifetime and are more \"phased\", although we\ndid not observe increasing testing activity before releases. Furthermore, we\nnote large differences in the levels of test coverage given the proportion of\ntest code."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0705.4415v1", 
    "title": "PERCEVAL: a Computer-Driven System for Experimentation on Auditory and   Visual Perception", 
    "arxiv-id": "0705.4415v1", 
    "author": "Bernard Teston", 
    "publish": "2007-05-30T15:31:07Z", 
    "summary": "Since perception tests are highly time-consuming, there is a need to automate\nas many operations as possible, such as stimulus generation, procedure control,\nperception testing, and data analysis. The computer-driven system we are\npresenting here meets these objectives. To achieve large flexibility, the tests\nare controlled by scripts. The system's core software resembles that of a\nlexical-syntactic analyzer, which reads and interprets script files sent to it.\nThe execution sequence (trial) is modified in accordance with the commands and\ndata received. This type of operation provides a great deal of flexibility and\nsupports a wide variety of tests such as auditory-lexical decision making,\nphoneme monitoring, gating, phonetic categorization, word identification, voice\nquality, etc. To achieve good performance, we were careful about timing\naccuracy, which is the greatest problem in computerized perception tests."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0706.1456v2", 
    "title": "A Generic Model of Contracts for Embedded Systems", 
    "arxiv-id": "0706.1456v2", 
    "author": "Roberto Passerone", 
    "publish": "2007-06-11T12:22:15Z", 
    "summary": "We present the mathematical foundations of the contract-based model developed\nin the framework of the SPEEDS project. SPEEDS aims at developing methods and\ntools to support \"speculative design\", a design methodology in which\ndistributed designers develop different aspects of the overall system, in a\nconcurrent but controlled way. Our generic mathematical model of contract\nsupports this style of development. This is achieved by focusing on behaviors,\nby supporting the notion of \"rich component\" where diverse (functional and\nnon-functional) aspects of the system can be considered and combined, by\nrepresenting rich components via their set of associated contracts, and by\nformalizing the whole process of component composition."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0707.1639v1", 
    "title": "Interface groups and financial transfer architectures", 
    "arxiv-id": "0707.1639v1", 
    "author": "Alban Ponse", 
    "publish": "2007-07-11T14:55:26Z", 
    "summary": "Analytic execution architectures have been proposed by the same authors as a\nmeans to conceptualize the cooperation between heterogeneous collectives of\ncomponents such as programs, threads, states and services. Interface groups\nhave been proposed as a means to formalize interface information concerning\nanalytic execution architectures. These concepts are adapted to organization\narchitectures with a focus on financial transfers. Interface groups (and\nmonoids) now provide a technique to combine interface elements into interfaces\nwith the flexibility to distinguish between directions of flow dependent on\nentity naming.\n  The main principle exploiting interface groups is that when composing a\nclosed system of a collection of interacting components, the sum of their\ninterfaces must vanish in the interface group modulo reflection. This certainly\nmatters for financial transfer interfaces.\n  As an example of this, we specify an interface group and within it some\nspecific interfaces concerning the financial transfer architecture for a part\nof our local academic organization.\n  Financial transfer interface groups arise as a special case of more general\nservice architecture interfaces."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0707.2291v2", 
    "title": "An Integrated Crosscutting Concern Migration Strategy and its   Application to JHotDraw", 
    "arxiv-id": "0707.2291v2", 
    "author": "Arie van Deursen", 
    "publish": "2007-07-16T09:38:23Z", 
    "summary": "In this paper we propose a systematic strategy for migrating crosscutting\nconcerns in existing object-oriented systems to aspect-based solutions. The\nproposed strategy consists of four steps: mining, exploration, documentation\nand refactoring of crosscutting concerns. We discuss in detail a new approach\nto aspect refactoring that is fully integrated with our strategy, and apply the\nwhole strategy to an object-oriented system, namely the JHotDraw framework. The\nresult of this migration is made available as an open-source project, which is\nthe largest aspect refactoring available to date. We report on our experiences\nwith conducting this case study and reflect on the success and challenges of\nthe migration process, as well as on the feasibility of automatic aspect\nrefactoring."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0707.4166v1", 
    "title": "Parsimony Principles for Software Components and Metalanguages", 
    "arxiv-id": "0707.4166v1", 
    "author": "Todd L. Veldhuizen", 
    "publish": "2007-07-27T19:29:29Z", 
    "summary": "Software is a communication system. The usual topic of communication is\nprogram behavior, as encoded by programs. Domain-specific libraries are\ncodebooks, domain-specific languages are coding schemes, and so forth. To turn\nmetaphor into method, we adapt toolsfrom information theory--the study of\nefficient communication--to probe the efficiency with which languages and\nlibraries let us communicate programs. In previous work we developed an\ninformation-theoretic analysis of software reuse in problem domains. This new\npaper uses information theory to analyze tradeoffs in the design of components,\ngenerators, and metalanguages. We seek answers to two questions: (1) How can we\njudge whether a component is over- or under-generalized? Drawing on minimum\ndescription length principles, we propose that the best component yields the\nmost succinct representation of the use cases. (2) If we view a programming\nlanguage as an assemblage of metalanguages, each providing a complementary\nstyle of abstraction, how can these metalanguages aid or hinder us in\nefficiently describing software? We describe a complex triangle of interactions\nbetween the power of an abstraction mechanism, the amount of reuse it enables,\nand the cognitive difficulty of its use."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4641v1", 
    "title": "UML 2.0 - Overview and Perspectives in SoC Design", 
    "arxiv-id": "0710.4641v1", 
    "author": "Tim Schattkowsky", 
    "publish": "2007-10-25T08:11:39Z", 
    "summary": "The design productivity gap requires more efficient design methods. Software\nsystems have faced the same challenge and seem to have mastered it with the\nintroduction of more abstract design methods. The UML has become the standard\nfor software systems modeling and thus the foundation of new design methods.\nAlthough the UML is defined as a general purpose modeling language, its\napplication to hardware and hardware/software codesign is very limited. In\norder to successfully apply the UML at these fields, it is essential to\nunderstand its capabilities and to map it to a new domain."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4682v1", 
    "title": "Applying UML and MDA to Real Systems Design", 
    "arxiv-id": "0710.4682v1", 
    "author": "Ian Oliver", 
    "publish": "2007-10-25T09:07:10Z", 
    "summary": "Traditionally system design has been made from a black box/functionality only\nperspective which forces the developer to concentrate on how the functionality\ncan be decomposed and recomposed into so called components. While this\ntechnique is well established and well known it does suffer fromsome drawbacks;\nnamely that the systems produced can often be forced into certain, incompatible\narchitectures, difficult to maintain or reuse and the code itself difficult to\ndebug. Now that ideas such as the OMG's Model Based Architecture (MDA) or Model\nBased Engineering (MBE) and the ubiquitous modelling language UML are being\nused (allegedly) and desired we face a number of challenges to existing\ntechniques."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4700v1", 
    "title": "A Decompilation Approach to Partitioning Software for   Microprocessor/FPGA Platforms", 
    "arxiv-id": "0710.4700v1", 
    "author": "Frank Vahid", 
    "publish": "2007-10-25T09:22:50Z", 
    "summary": "In this paper, we present a software compilation approach for\nmicroprocessor/FPGA platforms that partitions a software binary onto custom\nhardware implemented in the FPGA. Our approach imposes less restrictions on\nsoftware tool flow than previous compiler approaches, allowing software\ndesigners to use any software language and compiler. Our approach uses a\nback-end partitioning tool that utilizes decompilation techniques to recover\nimportant high-level information, resulting in performance comparable to\nhigh-level compiler-based approaches."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4755v1", 
    "title": "Model Reuse through Hardware Design Patterns", 
    "arxiv-id": "0710.4755v1", 
    "author": "Juan Carlos Lopez", 
    "publish": "2007-10-25T09:53:16Z", 
    "summary": "Increasing reuse opportunities is a well-known problem for software designers\nas well as for hardware designers. Nonetheless, current software and hardware\nengineering practices have embraced different approaches to this problem.\nSoftware designs are usually modelled after a set of proven solutions to\nrecurrent problems called design patterns. This approach differs from the\ncomponent-based reuse usually found in hardware designs: design patterns do not\nspecify unnecessary implementation details. Several authors have already\nproposed translating structural design patterns concepts to hardware design. In\nthis paper we extend the discussion to behavioural design patterns.\nSpecifically, we describe how the hardware version of the Iterator can be used\nto enhance model reuse."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4793v1", 
    "title": "Unified Modeling of Complex Real-Time Control Systems", 
    "arxiv-id": "0710.4793v1", 
    "author": "Cai Chi-Lan", 
    "publish": "2007-10-25T11:50:54Z", 
    "summary": "Complex real-time control system is a software dense and algorithms dense\nsystem, which needs modern software engineering techniques to design. UML is an\nobject-oriented industrial standard modeling language, used more and more in\nreal-time domain. This paper first analyses the advantages and problems of\nusing UML for real-time control systems design. Then, it proposes an extension\nof UML-RT to support time-continuous subsystems modeling. So we can unify\nmodeling of complex real-time control systems on UML-RT platform, from\nrequirement analysis, model design, simulation, until generation code."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0710.4829v1", 
    "title": "AutoMoDe - Model-Based Development of Automotive Software", 
    "arxiv-id": "0710.4829v1", 
    "author": "Bernhard Schatz", 
    "publish": "2007-10-25T12:08:52Z", 
    "summary": "This paper describes first results from the AutoMoDe (Automotive Model-Based\nDevelopment) project. The overall goal of the project is to develop an\nintegrated methodology for model-based development of automotive control\nsoftware, based on problem-specific design notations with an explicit formal\nfoundation. Based on the existing AutoFOCUS framework, a tool prototype is\nbeing developed in order to illustrate and validate the key elements of our\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0711.0538v1", 
    "title": "Spreadsheet Engineering: A Research Framework", 
    "arxiv-id": "0711.0538v1", 
    "author": "Thomas A. Grossman", 
    "publish": "2007-11-04T19:24:57Z", 
    "summary": "Spreadsheet engineering adapts the lessons of software engineering to\nspreadsheets, providing eight principles as a framework for organizing\nspreadsheet programming recommendations. Spreadsheets raise issues inadequately\naddressed by software engineering. Spreadsheets are a powerful modeling\nlanguage, allowing strategic rapid model change, and enabling exploratory\nmodeling. Spreadsheets users learn slowly with experience because they focus on\nthe problem domain not programming. The heterogeneity of spreadsheet users\nrequires a taxonomy to guide recommendations. Deployment of best practices is\ndifficult and merits research."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ECBS.2007.61", 
    "link": "http://arxiv.org/pdf/0711.0607v1", 
    "title": "Exploring the Composition of Unit Test Suites", 
    "arxiv-id": "0711.0607v1", 
    "author": "Serge Demeyer", 
    "publish": "2007-11-05T11:05:42Z", 
    "summary": "In agile software development, test code can considerably contribute to the\noverall source code size. Being a valuable asset both in terms of verification\nand documentation, the composition of a test suite needs to be well understood\nin order to identify opportunities as well as weaknesses for further evolution.\nIn this paper, we argue that the visualization of structural characteristics is\na viable means to support the exploration of test suites. Thanks to general\nagreement on a limited set of key test design principles, such visualizations\nare relatively easy to interpret. In particular, we present visualizations that\nsupport testers in (i) locating test cases; (ii) examining the relation between\ntest code and production code; and (iii) studying the composition of and\ndependencies within test cases. By means of two case studies, we demonstrate\nhow visual patterns help to identify key test suite characteristics. This\napproach forms the first step in assisting a developer to build up\nunderstanding about test suites beyond code reading."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0711.0836v3", 
    "title": "Machine structure oriented control code logic", 
    "arxiv-id": "0711.0836v3", 
    "author": "C. A. Middelburg", 
    "publish": "2007-11-06T11:01:21Z", 
    "summary": "Control code is a concept that is closely related to a frequently occurring\npractitioner's view on what is a program: code that is capable of controlling\nthe behaviour of some machine. We present a logical approach to explain issues\nconcerning control codes that are independent of the details of the behaviours\nthat are controlled. Using this approach, such issues can be explained at a\nvery abstract level. We illustrate this among other things by means of an\nexample about the production of a new compiler from an existing one. The\napproach is based on abstract machine models, called machine structures. We\nintroduce a model of systems that provide execution environments for the\nexecutable codes of machine structures and use it to go into portability of\ncontrol codes."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0712.0109v1", 
    "title": "Recommended Practices for Spreadsheet Testing", 
    "arxiv-id": "0712.0109v1", 
    "author": "Raymond R. Panko", 
    "publish": "2007-12-01T21:27:59Z", 
    "summary": "This paper presents the authors recommended practices for spreadsheet\ntesting. Documented spreadsheet error rates are unacceptable in corporations\ntoday. Although improvements are needed throughout the systems development life\ncycle, credible improvement programs must include comprehensive testing.\nSeveral forms of testing are possible, but logic inspection is recommended for\nmodule testing. Logic inspection appears to be feasible for spreadsheet\ndevelopers to do, and logic inspection appears to be safe and effective."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0712.2943v1", 
    "title": "Software (Re-)Engineering with PSF", 
    "arxiv-id": "0712.2943v1", 
    "author": "Bob Diertens", 
    "publish": "2007-12-18T12:25:02Z", 
    "summary": "This paper investigates the usefulness of PSF in software engineering and\nreengineering. PSF is based on ACP (Algebra of Communicating Processes) and as\nsome architectural description languages are based on process algebra, we\ninvestigate whether PSF can be used at the software architecture level, but we\nalso use PSF at lower abstract levels. As a case study we reengineer the\ncompiler from the Toolkit of PSF."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0712.3115v1", 
    "title": "Software (Re-)Engineering with PSF II: from architecture to   implementation", 
    "arxiv-id": "0712.3115v1", 
    "author": "Bob Diertens", 
    "publish": "2007-12-19T08:11:37Z", 
    "summary": "This paper presents ongoing research on the application of PSF in the field\nof software engineering and reengineering. We build a new implementation for\nthe simulator of the PSF Toolkit starting from the specification in PSF of the\narchitecture of a simple simulator and extend it with features to obtain the\narchitecture of a full simulator. We apply refining and constraining techniques\non the specification of the architecture to obtain a specification low enough\nto build an implementation from."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0712.3128v1", 
    "title": "Software (Re-)Engineering with PSF III: an IDE for PSF", 
    "arxiv-id": "0712.3128v1", 
    "author": "Bob Diertens", 
    "publish": "2007-12-19T09:46:23Z", 
    "summary": "We describe the design of an integrated development environment (IDE) for\nPSF. In the software engineering process we used process algebra in the form of\nPSF for the specification of the architecture of the IDE. This specification is\nrefined to a PSF specification of the IDE system as a ToolBus application, by\napplying vertical and horizontal implementation techniques. We implemented the\nvarious tools as specified and connected them with a ToolBus script extracted\nfrom the system specification."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0801.4774v1", 
    "title": "Source Code Protection for Applications Written in Microsoft Excel and   Google Spreadsheet", 
    "arxiv-id": "0801.4774v1", 
    "author": "Thomas A. Grossman", 
    "publish": "2008-01-30T21:35:17Z", 
    "summary": "Spreadsheets are used to develop application software that is distributed to\nusers. Unfortunately, the users often have the ability to change the\nprogramming statements (\"source code\") of the spreadsheet application. This\ncauses a host of problems. By critically examining the suitability of\nspreadsheet computer programming languages for application development, six\n\"application development features\" are identified, with source code protection\nbeing the most important. We investigate the status of these features and\ndiscuss how they might be implemented in the dominant Microsoft Excel\nspreadsheet and in the new Google Spreadsheet. Although Google Spreadsheet\ncurrently provides no source code control, its web-centric delivery model\noffers technical advantages for future provision of a rich set of features.\nExcel has a number of tools that can be combined to provide \"pretty good\nprotection\" of source code, but weak passwords reduce its robustness. User\naccess to Excel source code must be considered a programmer choice rather than\nan attribute of the spreadsheet."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0801.4775v1", 
    "title": "Spreadsheet Assurance by \"Control Around\" is a Viable Alternative to the   Traditional Approach", 
    "arxiv-id": "0801.4775v1", 
    "author": "Jacques de Swart", 
    "publish": "2008-01-30T21:53:43Z", 
    "summary": "The traditional approach to spreadsheet auditing generally consists of\nauditing every distinct formula within a spreadsheet. Although tools are\ndeveloped to support auditors during this process, the approach is still very\ntime consuming and therefore relatively expensive. As an alternative to the\ntraditional \"control through\" approach, this paper discusses a \"control around\"\napproach. Within the proposed approach not all distinct formulas are audited\nseparately, but the relationship between input data and output data of a\nspreadsheet is audited through comparison with a shadow model developed in a\nmodelling language. Differences between the two models then imply possible\nerrors in the spreadsheet. This paper describes relevant issues regarding the\n\"control around\" approach and the circumstances in which this approach is\npreferred above a traditional spreadsheet audit approach."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0801.4802v1", 
    "title": "Investigating the Potential of Test-Driven Development for Spreadsheet   Engineering", 
    "arxiv-id": "0801.4802v1", 
    "author": "Kevin McDaid", 
    "publish": "2008-01-31T00:39:38Z", 
    "summary": "It is widely documented that the absence of a structured approach to\nspreadsheet engineering is a key factor in the high level of spreadsheet\nerrors. In this paper we propose and investigate the application of Test-Driven\nDevelopment to the creation of spreadsheets. Test-Driven Development is an\nemerging development technique in software engineering that has been shown to\nresult in better quality software code. It has also been shown that this code\nrequires less testing and is easier to maintain. Through a pair of case studies\nwe demonstrate that Test-Driven Development can be applied to the development\nof spreadsheets. We present the detail of these studies preceded by a clear\nexplanation of the technique and its application to spreadsheet engineering. A\nsupporting tool under development by the authors is also documented along with\nproposed research to determine the effectiveness of the methodology and the\nassociated tool."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0802.1586v1", 
    "title": "Program Promises", 
    "arxiv-id": "0802.1586v1", 
    "author": "Simen Hagen", 
    "publish": "2008-02-12T08:40:51Z", 
    "summary": "The framework of promise theory offers an alternative way of understanding\nprogramming models, especially in distributed systems. We show that promise\ntheory can express some familiar constructs and resolve some problems in\nprogram interface design, using fewer and simpler concepts than the Unified\nModelling Language (UML)."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0802.3628v1", 
    "title": "Dynamic data models: an application of MOP-based persistence in Common   Lisp", 
    "arxiv-id": "0802.3628v1", 
    "author": "Simon E. B. Thierry", 
    "publish": "2008-02-25T14:20:18Z", 
    "summary": "The data model of an application, the nature and format of data stored across\nexecutions, is typically a very rigid part of its early specification, even\nwhen prototyping, and changing it after code that relies on it was written can\nprove quite expensive and error-prone.\n  Code and data in a running Lisp image can be dynamically modified. A\nMOP-based persistence library can bring this dynamicity to the data model. This\nenables to extend the easy prototyping way of development to the storage of\ndata and helps avoiding interruptions of service. This article presents the\nconditions to do this portably and transparently."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0802.3895v1", 
    "title": "Complexity Metrics for Spreadsheet Models", 
    "arxiv-id": "0802.3895v1", 
    "author": "Andrej Bregar", 
    "publish": "2008-02-27T00:37:56Z", 
    "summary": "Several complexity metrics are described which are related to logic\nstructure, data structure and size of spreadsheet models. They primarily\nconcentrate on the dispersion of cell references and cell paths. Most metrics\nare newly defined, while some are adapted from traditional software\nengineering. Their purpose is the identification of cells which are liable to\nerrors. In addition, they can be used to estimate the values of dependent\nprocess metrics, such as the development duration and effort, and especially to\nadjust the cell error rate in accordance with the contents of each individual\ncell, in order to accurately asses the reliability of a model. Finally, two\nconceptual constructs - the reference branching condition cell and the\ncondition block - are discussed, aiming at improving the reliability,\nmodifiability, auditability and comprehensibility of logical tests."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0802.3940v1", 
    "title": "Spreadsheet Structure Discovery with Logic Programming", 
    "arxiv-id": "0802.3940v1", 
    "author": "Jocelyn Paine", 
    "publish": "2008-02-27T00:25:47Z", 
    "summary": "Our term \"structure discovery\" denotes the recovery of structure, such as the\ngrouping of cells, that was intended by a spreadsheet's author but is not\nexplicit in the spreadsheet. We are implementing structure discovery tools in\nthe logic-programming language Prolog for our spreadsheet analysis program\nModel Master, by writing grammars for spreadsheet structures. The objective is\nan \"intelligent structure monitor\" to run beside Excel, allowing users to\nreconfigure spreadsheets to the representational needs of the task at hand.\nThis could revolutionise spreadsheet \"best practice\". We also describe a\nformulation of spreadsheet reverse-engineering based on \"arrows\"."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.0015v1", 
    "title": "EuSpRIG 2006 Commercial Spreadsheet Review", 
    "arxiv-id": "0803.0015v1", 
    "author": "Simon Murphy", 
    "publish": "2008-02-29T21:49:18Z", 
    "summary": "This management summary provides an outline of a commercial spreadsheet\nreview process. The aim of this process is to ensure remedial or enhancement\nwork can safely be undertaken on a spreadsheet with a commercially acceptable\nlevel of risk of introducing new errors."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.0162v1", 
    "title": "A Software Development Methodology for Research and Prototyping in   Financial Markets", 
    "arxiv-id": "0803.0162v1", 
    "author": "Ben Van Vliet", 
    "publish": "2008-03-03T01:05:40Z", 
    "summary": "The objective of this paper is to develop a standardized methodology for\nsoftware development in the very unique industry and culture of financial\nmarkets. The prototyping process we present allows the development team to\ndeliver for review and comment intermediate-level models based upon clearly\ndefined customer requirements. This spreadsheet development methodology is\npresented within a larger business context, that of trading system development,\nthe subject of an upcoming book by the authors of this paper."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.0163v1", 
    "title": "Rapid Spreadsheet Reshaping with Excelsior: multiple drastic changes to   content and layout are easy when you represent enough structure", 
    "arxiv-id": "0803.0163v1", 
    "author": "Duncan Williamson", 
    "publish": "2008-03-03T01:11:39Z", 
    "summary": "Spreadsheets often need changing in ways made tedious and risky by Excel. For\nexample: simultaneously altering many tables' size, orientation, and position;\ninserting cross-tabulations; moving data between sheets; splitting and merging\nsheets. A safer, faster restructuring tool is, we claim, Excelsior. The result\nof a research project into reducing spreadsheet risk, Excelsior is the first\never tool for modularising spreadsheets; i.e. for building them from components\nwhich can be independently created, tested, debugged, and updated. It\nrepresents spreadsheets in a way that makes these components explicit,\nseparates them from layout, and allows both components and layout to be changed\nwithout breaking dependent formulae. Here, we report experiments to test that\nthis does indeed make such changes easier. In one, we automatically generated a\ncross-tabulation and added it to a spreadsheet. In the other, we generated new\nversions of a 10,000-cell housing-finance spreadsheet containing many\ninterconnected 20*40 tables. We varied table sizes from 5*10 to 200*2,000;\nmoved tables between sheets; and flipped table orientations. Each change\ngenerated a spreadsheet with different structure but identical outputs; each\nchange took just a few minutes."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.0666v1", 
    "title": "An approach to control collaborative processes in PLM systems", 
    "arxiv-id": "0803.0666v1", 
    "author": "Abdelaziz Bouras", 
    "publish": "2008-03-05T14:04:19Z", 
    "summary": "Companies that collaborate within the product development processes need to\nimplement an effective management of their collaborative activities. Despite\nthe implementation of a PLM system, the collaborative activities are not\nefficient as it might be expected. This paper presents an analysis of the\nproblems related to the collaborative work using a PLM system. From this\nanalysis, we propose an approach for improving collaborative processes within a\nPLM system, based on monitoring indicators. This approach leads to identify and\ntherefore to mitigate the brakes of the collaborative work."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.1751v1", 
    "title": "TellTable Spreadsheet Audit: from Technical Possibility to Operating   Prototype", 
    "arxiv-id": "0803.1751v1", 
    "author": "Neil Smith", 
    "publish": "2008-03-12T11:36:41Z", 
    "summary": "At the 2003 EuSpRIG meeting, we presented a framework and software\ninfrastructure to generate and analyse an audit trail for a spreadsheet file.\nThis report describes the results of a pilot implementation of this software\n(now called TellTable; see www.telltable.com), along with developments in the\nserver infrastructure and availability, extensions to other \"Office Suite\"\nfiles, integration of the audit tool into the server interface, and related\ndevelopments, licensing and reports. We continue to seek collaborators and\npartners in what is primarily an open-source project with some shared-source\ncomponents."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0803.2027v1", 
    "title": "Excelsior: Bringing the Benefits of Modularisation to Excel", 
    "arxiv-id": "0803.2027v1", 
    "author": "Jocelyn Paine", 
    "publish": "2008-03-13T18:41:21Z", 
    "summary": "Excel lacks features for modular design. Had it such features, as do most\nprogramming languages, they would save time, avoid unneeded programming, make\nmistakes less likely, make code-control easier, help organisations adopt a\nuniform house style, and open business opportunities in buying and selling\nspreadsheet modules. I present Excelsior, a system for bringing these benefits\nto Excel."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0804.0366v1", 
    "title": "Merging Object and Process Diagrams for Business Information Modeling", 
    "arxiv-id": "0804.0366v1", 
    "author": "Patrick Ch\u00e9nais", 
    "publish": "2008-04-02T14:50:29Z", 
    "summary": "While developing an information system for the University of Bern, we were\nfaced with two major issues: managing software changes and adapting Business\nInformation Models. Software techniques well-suited to software development\nteams exist, yet the models obtained are often too complex for the business\nuser. We will first highlight the conceptual problems encountered while\ndesigning the Business Information Model. We will then propose merging class\ndiagrams and business process modeling to achieve a necessary transparency. We\nwill finally present a modeling tool we developed which, using pilot case\nstudies, helps to show some of the advantages of a dual model approach."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0805.0650v1", 
    "title": "Plat_Forms -- a contest: The web development platform comparison", 
    "arxiv-id": "0805.0650v1", 
    "author": "Lutz Prechelt", 
    "publish": "2008-05-06T06:53:01Z", 
    "summary": "\"Plat_Forms\" is a competition in which top-class teams of three programmers\ncompete to implement the same requirements for a web-based system within 30\nhours, each team using a different technology platform (Java EE, .NET, PHP,\nPerl, Python, or Ruby on Rails). The results will provide new insights into the\nreal (rather than purported) pros, cons, and emergent properties of each\nplatform. The evaluation will analyze many aspects of each solution, both\nexternal (usability, functionality, reliability, performance, etc.) and\ninternal (structure, understandability, flexibility, etc.)."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0805.1740v1", 
    "title": "Detecting Errors in Spreadsheets", 
    "arxiv-id": "0805.1740v1", 
    "author": "Roland T. Mittermeir", 
    "publish": "2008-05-12T20:31:41Z", 
    "summary": "The paper presents two complementary strategies for identifying errors in\nspreadsheet programs. The strategies presented are grounded on the assumption\nthat spreadsheets are software, albeit of a different nature than conventional\nprocedural software. Correspondingly, strategies for identifying errors have to\ntake into account the inherent properties of spreadsheets as much as they have\nto recognize that the conceptual models of 'spreadsheet programmers' differ\nfrom the conceptual models of conventional programmers. Nevertheless, nobody\ncan and will write a spreadsheet, without having such a conceptual model in\nmind, be it of numeric nature or be it of geometrical nature focused on some\nlayout."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0805.4218v1", 
    "title": "A Structured Methodology for Spreadsheet Modelling", 
    "arxiv-id": "0805.4218v1", 
    "author": "Kamalesen Rajalingham", 
    "publish": "2008-05-27T20:56:35Z", 
    "summary": "In this paper, we discuss the problem of the software engineering of a class\nof business spreadsheet models. A methodology for structured software\ndevelopment is proposed, which is based on structured analysis of data,\nrepresented as Jackson diagrams. It is shown that this analysis allows a\nstraightforward modularisation, and that individual modules may be represented\nwith indentation in the block-structured form of structured programs. The\nbenefits of structured format are discussed, in terms of comprehensibility,\nease of maintenance, and reduction in errors. The capability of the methodology\nto provide a modular overview in the model is described, and examples are\ngiven. The potential for a reverse-engineering tool, to transform existing\nspreadsheet models is discussed."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0805.4219v1", 
    "title": "Building Financial Accuracy into Spreadsheets", 
    "arxiv-id": "0805.4219v1", 
    "author": "Andrew Hawker", 
    "publish": "2008-05-27T21:11:48Z", 
    "summary": "Students learning how to apply spreadsheets to accounting problems are not\nalways well served by the built-in financial functions. Problems can arise\nbecause of differences between UK and US practice, through anomalies in the\nfunctions themselves, and because the promptings of Wizards' engender an\nattitude of filling in the blanks on the screen, and hoping for the best. Some\nexamples of these problems are described, and suggestions are presented for\nways of improving the situation. Principally, it is suggested that spreadsheet\nprompts and 'Help' screens should offer integrated guidance, covering some\naspects of financial practice, as well as matters of spreadsheet technique."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0805.4224v1", 
    "title": "Classification of Spreadsheet Errors", 
    "arxiv-id": "0805.4224v1", 
    "author": "Brian Knight", 
    "publish": "2008-05-27T21:27:42Z", 
    "summary": "This paper describes a framework for a systematic classification of\nspreadsheet errors. This classification or taxonomy of errors is aimed at\nfacilitating analysis and comprehension of the different types of spreadsheet\nerrors. The taxonomy is an outcome of an investigation of the widespread\nproblem of spreadsheet errors and an analysis of specific types of these\nerrors. This paper contains a description of the various elements and\ncategories of the classification and is supported by appropriate examples."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0806.1100v1", 
    "title": "Design Patterns for Complex Event Processing", 
    "arxiv-id": "0806.1100v1", 
    "author": "Adrian Paschke", 
    "publish": "2008-06-06T07:39:49Z", 
    "summary": "Currently engineering efficient and successful event-driven applications\nbased on the emerging Complex Event Processing (CEP) technology, is a laborious\ntrial and error process. The proposed CEP design pattern approach should\nsupport CEP engineers in their design decisions to build robust and efficient\nCEP solutions with well understood tradeoffs and should enable an\ninterdisciplinary and efficient communication process about successful CEP\nsolutions in different application domains."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0806.2730v1", 
    "title": "A Process Algebra Software Engineering Environment", 
    "arxiv-id": "0806.2730v1", 
    "author": "B. Diertens", 
    "publish": "2008-06-17T09:20:54Z", 
    "summary": "In previous work we described how the process algebra based language PSF can\nbe used in software engineering, using the ToolBus, a coordination architecture\nalso based on process algebra, as implementation model. In this article we\nsummarize that work and describe the software development process more formally\nby presenting the tools we use in this process in a CASE setting, leading to\nthe PSF-ToolBus software engineering environment. We generalize the refine step\nin this environment towards a process algebra based software engineering\nworkbench of which several instances can be combined to form an environment."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0807.0161v1", 
    "title": "Increase of Software Safety", 
    "arxiv-id": "0807.0161v1", 
    "author": "Arkadiy Khandjian", 
    "publish": "2008-07-01T13:53:44Z", 
    "summary": "New model of software safety is offered. Distribution of mistakes in program\non stages of life cycle is researched. Study of ways of increase of reliability\nof software at help simulation program is leaded."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0807.3187v1", 
    "title": "When, why and how to test spreadsheets", 
    "arxiv-id": "0807.3187v1", 
    "author": "Louise Pryor", 
    "publish": "2008-07-20T21:10:34Z", 
    "summary": "Testing is a vital part of software development, and spreadsheets are like\nany other software in this respect. This paper discusses the testing of\nspreadsheets in the light of one practitioner's experience. It considers the\nconcept of software testing and how it differs from reviewing, and describes\nwhen it might take place. Different types of testing are described, and some\ntechniques for performing them presented. Some of the commonly encountered\nproblems are discussed."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0807.4224v1", 
    "title": "Encapsulation theory fundamentals", 
    "arxiv-id": "0807.4224v1", 
    "author": "Edmund Kirwan", 
    "publish": "2008-07-26T08:55:36Z", 
    "summary": "This paper proposes a theory of encapsulation, establishing a relationship\nbetween encapsulation and information hiding through the concept of potential\nstructural complexity (P.S.C.), the maximum possible number of source code\ndependencies that can exist between program units in a software system. The\nP.S.C. of various, simple systems is examined in an attempt to demonstrate how\nP.S.C. changes as program units are encapsulated among different configurations\nof subsystems."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0808.0055v1", 
    "title": "Integrating OPC Data into GSN Infrastructures", 
    "arxiv-id": "0808.0055v1", 
    "author": "Patrice Moreaux", 
    "publish": "2008-08-01T04:38:22Z", 
    "summary": "This paper presents the design and the implementation of an interface\nsoftware component between OLE for Process Control (OPC) formatted data and the\nGlobal Sensor Network (GSN) framework for management of data from sensors. This\ninterface, named wrapper in the GSN context, communicates in Data Access mode\nwith an OPC server and converts the received data to the internal GSN format,\naccording to several temporal modes. This work is realized in the context of a\nPh.D. Thesis about the control of distributed information fusion systems. The\ndeveloped component allows the injection of OPC data, like measurements or\nindustrial processes states information, into a distributed information fusion\nsystem deployed in a GSN framework. The component behaves as a client of the\nOPC server. Developed in Java and based on the Opensaca Utgard, it can be\ndeployed on any computation node supporting a Java virtual machine. The\nexperiments show the component conformity according to the Data Access 2.05a\nspecification of the OPC standard and to the temporal modes."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0808.0347v1", 
    "title": "Towards a Process for Developing Maintenance Tools in Academia", 
    "arxiv-id": "0808.0347v1", 
    "author": "Hausi A. M\u00fcller", 
    "publish": "2008-08-03T20:14:30Z", 
    "summary": "Building of tools--from simple prototypes to industrial-strength\napplications--is a pervasive activity in academic research. When proposing a\nnew technique for software maintenance, effective tool support is typically\nrequired to demonstrate the feasibility and effectiveness of the approach.\nHowever, even though tool building is both pervasive and requiring significant\ntime and effort, it is still pursued in an ad hoc manner. In this paper, we\naddress these issues by proposing a dedicated development process for tool\nbuilding that takes the unique characteristics of an academic research\nenvironment into account. We first identify process requirements based on a\nreview of the literature and our extensive tool building experience in the\ndomain of maintenance tools. We then outline a process framework based on work\nproducts that accommodates the requirements while providing needed flexibility\nfor tailoring the process to account for specific tool building approaches and\nproject constraints. The work products are concrete milestones of the process,\ntracking progress, rationalizing (design) decisions, and documenting the\ncurrent state of the tool building project. Thus, the work products provide\nimportant input for strategic project decisions and rapid initiation of new\nteam members. Leveraging a dedicated tool building process promises tools that\nare designed, build, and maintained in a more disciplined, predictable and\nefficient manner."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0808.3292v1", 
    "title": "Network Motifs in Object-Oriented Software Systems", 
    "arxiv-id": "0808.3292v1", 
    "author": "Jing Liu", 
    "publish": "2008-08-25T02:53:47Z", 
    "summary": "Nowadays, software has become a complex piece of work that may be beyond our\ncontrol. Understanding how software evolves over time plays an important role\nin controlling software development processes. Recently, a few researchers\nfound the quantitative evidence of structural duplication in software systems\nor web applications, which is similar to the evolutionary trend found in\nbiological systems. To investigate the principles or rules of software\nevolution, we introduce the relevant theories and methods of complex networks\ninto structural evolution and change of software systems. According to the\nresults of our experiment on network motifs, we find that the stability of a\nmotif shows positive correlation with its abundance and a motif with high Z\nscore tends to have stable structure. These findings imply that the evolution\nof software systems is based on functional cloning as well as structural\nduplication and tends to be structurally stable. So, the work presented in this\npaper will be useful for the analysis of structural changes of software systems\nin reverse engineering."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0809.1409v1", 
    "title": "Domain Specific Software Architecture for Design Center Automation", 
    "arxiv-id": "0809.1409v1", 
    "author": "Vijaya Balakrishna", 
    "publish": "2008-09-08T19:07:17Z", 
    "summary": "Domain specific software architecture aims at software reuse through\nconstruction of domain architecture reference model. The constructed reference\nmodel presents a set of individual components and their interaction points.\nWhen starting on a new large software project, the design engineer starts with\npre-constructed model, which can be easily browsed and picks up opportunities\nof use in the new solution design. This report discusses application of domain\nreference design methods by deriving domain specific reference architecture for\na product ordering system in a design center. The product in this case is\ninstock and special order blinds from different manufacturers in a large supply\nstore. The development of mature domain specific reference software\narchitecture for this domain is not the objective of this report. However, this\nreport would like to capture the method used in one such process and that is\nthe primary concern of this report. This report lists subjective details of\nsuch a process applied to the domain of ordering custom and instock blinds from\na large home construction and goods supply store. This report also describes\nthe detailed process of derivation of knowledge models, unified knowledge\nmodels and the reference architecture for this domain. However, this domain\nmodel is only partially complete which may not be used for any real\napplications. This report is a result of a course project undertaken while\nstudying this methodology."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0809.4108v1", 
    "title": "The ADAPT Tool: From AADL Architectural Models to Stochastic Petri Nets   through Model Transformation", 
    "arxiv-id": "0809.4108v1", 
    "author": "Mohamed Kaaniche", 
    "publish": "2008-09-24T07:26:30Z", 
    "summary": "ADAPT is a tool that aims at easing the task of evaluating dependability\nmeasures in the context of modern model driven engineering processes based on\nAADL (Architecture Analysis and Design Language). Hence, its input is an AADL\narchitectural model annotated with dependability-related information. Its\noutput is a dependability evaluation model in the form of a Generalized\nStochastic Petri Net (GSPN). The latter can be processed by existing\ndependability evaluation tools, to compute quantitative measures such as\nreliability, availability, etc.. ADAPT interfaces OSATE (the Open Source AADL\nTool Environment) on the AADL side and SURF-2, on the dependability evaluation\nside. In addition, ADAPT provides the GSPN in XML/XMI format, which represents\na gateway to other dependability evaluation tools, as the processing techniques\nfor XML files allow it to be easily converted to a tool-specific GSPN."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0809.4109v1", 
    "title": "Software dependability modeling using an industry-standard architecture   description language", 
    "arxiv-id": "0809.4109v1", 
    "author": "Mohamed Kaaniche", 
    "publish": "2008-09-24T07:26:34Z", 
    "summary": "Performing dependability evaluation along with other analyses at\narchitectural level allows both making architectural tradeoffs and predicting\nthe effects of architectural decisions on the dependability of an application.\nThis paper gives guidelines for building architectural dependability models for\nsoftware systems using the AADL (Architecture Analysis and Design Language). It\npresents reusable modeling patterns for fault-tolerant applications and shows\nhow the presented patterns can be used in the context of a subsystem of a\nreal-life application."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0809.4812v1", 
    "title": "Control software analysis, Part I Open-loop properties", 
    "arxiv-id": "0809.4812v1", 
    "author": "Fernando Alegre", 
    "publish": "2008-09-28T01:44:28Z", 
    "summary": "As the digital world enters further into everyday life, questions are raised\nabout the increasing challenges brought by the interaction of real-time\nsoftware with physical devices. Many accidents and incidents encountered in\nareas as diverse as medical systems, transportation systems or weapon systems\nare ultimately attributed to \"software failures\". Since real-time software that\ninteracts with physical systems might as well be called control software, the\nlong litany of accidents due to real-time software failures might be taken as\nan equally long list of opportunities for control systems engineering. In this\npaper, we are interested only in run-time errors in those pieces of software\nthat are a direct implementation of control system specifications: For\nwell-defined and well-understood control architectures such as those present in\nstandard textbooks on digital control systems, the current state of theoretical\ncomputer science is well-equipped enough to address and analyze control\nalgorithms. It appears that a central element to these analyses is Lyapunov\nstability theory, which translate into invariant theory in computer\nimplementations."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0810.0874v2", 
    "title": "Software Engineering & Systems Design Nature", 
    "arxiv-id": "0810.0874v2", 
    "author": "Kirill A Sorudeykin", 
    "publish": "2008-10-06T04:50:01Z", 
    "summary": "The main problems of Software Engineering appear as a result of\nincompatibilities. For example, the quality of organization of the production\nprocess depends on correspondence with existent resources and on a common\nunderstanding of project goals by all team members. Software design is another\nexample. Its successfulness rides on the architecture's conformity with a\nproject's concepts. This is a point of great nicety. All elements should create\na single space of interaction. And if the laws of such a space are imperfect,\nmissequencing comes and the concept of a software system fails. We must do our\nbest for this not to happen. To that end, having a subtle perception of systems\nstructures is essential. Such knowledge can be based only on a fresh approach\nto the logical law."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.1947v1", 
    "title": "Pilotage des processus collaboratifs dans les syst\u00e8mes PLM. Quels   indicateurs pour quelle \u00e9valuation des performances ?", 
    "arxiv-id": "0811.1947v1", 
    "author": "Abdelaziz Bouras", 
    "publish": "2008-11-12T20:08:51Z", 
    "summary": "Les entreprises qui collaborent dans un processus de d\\'eveloppement de\nproduit ont besoin de mettre en oeuvre une gestion efficace des activit\\'es\ncollaborative. Malgr\\'e la mise en place d'un PLM, les activit\\'es\ncollaborative sont loin d'\\^etre aussi efficace que l'on pourrait s'y attendre.\nCet article propose une analyse des probl\\'ematiques de la collaboration avec\nun syst\\`eme PLM. A partir de ces analyses, nous proposons la mise en place\nd'indicateurs et d'actions sur les processus visant \\`a identifier puis\natt\\'enuer les freins dans le travail collaboratif. ----- Companies that\ncollaborate within the product development processes need to implement an\neffective management of their collaborative activities. Despite the\nimplementation of a PLM system, the collaborative activities are not efficient\nas it might be expected. This paper presents an analysis of the problems\nrelated to the collaborative work using a PLM system, identified through a\nsurvey. From this analysis, we propose an approach for improving collaborative\nprocesses within a PLM system, based on monitoring indicators. This approach\nleads to identify and therefore to mitigate the brakes of the collaborative\nwork."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.1950v1", 
    "title": "Collaborative process control: Observation of tracks generated by PLM   system", 
    "arxiv-id": "0811.1950v1", 
    "author": "Abdelaziz Bouras", 
    "publish": "2008-11-12T20:09:41Z", 
    "summary": "This paper aims at analyzing the problems related to collaborative work using\na PLM system. This research is mainly focused on the organisational aspects of\nSMEs involved in networks composed of large companies, subcontractors and other\nindustrial partners. From this analysis, we propose the deployment of an\napproach based on an observation process of tracks generated by PLM system. The\nspecific contributions are of two fold. First is to identify the brake points\nof collaborative work. The second, thanks to the exploitation of generated\ntracks, it allows reducing risks by reacting in real time to the incidents or\ndysfunctions that may occur. The overall system architecture based on services\ntechnology and supporting the proposed approach is described, as well as\nassociated prototype developed using an industrial PLM system."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.2578v1", 
    "title": "Encapsulation theory: the configuration efficiency limit", 
    "arxiv-id": "0811.2578v1", 
    "author": "Edmund Kirwan", 
    "publish": "2008-11-16T14:47:16Z", 
    "summary": "This paper shows how maximum possible configuration efficiency of an\nindefinitely large software system is constrained by chosing a fixed upper\nlimit to the number of program units per subsystem. It is then shown how the\nconfiguration efficiency of an indefinitely large software system depends on\nthe ratio of the total number of informaiton hiding violational software units\ndivided by the total number of program units."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.3492v1", 
    "title": "Dynamic System Adaptation by Constraint Orchestration", 
    "arxiv-id": "0811.3492v1", 
    "author": "E. P. de Vink", 
    "publish": "2008-11-21T09:47:26Z", 
    "summary": "For Paradigm models, evolution is just-in-time specified coordination\nconducted by a special reusable component McPal. Evolution can be treated\nconsistently and on-the-fly through Paradigm's constraint orchestration, also\nfor originally unforeseen evolution. UML-like diagrams visually supplement such\nmigration, as is illustrated for the case of a critical section solution\nevolving into a pipeline architecture."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.3620v1", 
    "title": "Solving package dependencies: from EDOS to Mancoosi", 
    "arxiv-id": "0811.3620v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2008-11-21T19:45:50Z", 
    "summary": "Mancoosi (Managing the Complexity of the Open Source Infrastructure) is an\nongoing research project funded by the European Union for addressing some of\nthe challenges related to the \"upgrade problem\" of interdependent software\ncomponents of which Debian packages are prototypical examples. Mancoosi is the\nnatural continuation of the EDOS project which has already contributed tools\nfor distribution-wide quality assurance in Debian and other GNU/Linux\ndistributions. The consortium behind the project consists of several European\npublic and private research institutions as well as some commercial GNU/Linux\ndistributions from Europe and South America. Debian is represented by a small\ngroup of Debian Developers who are working in the ranks of the involved\nuniversities to drive and integrate back achievements into Debian. This paper\npresents relevant results from EDOS in dependency management and gives an\noverview of the Mancoosi project and its objectives, with a particular focus on\nthe prospective benefits for Debian."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s00236-009-0099-2", 
    "link": "http://arxiv.org/pdf/0811.3621v1", 
    "title": "Description of the CUDF Format", 
    "arxiv-id": "0811.3621v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2008-11-21T19:46:46Z", 
    "summary": "This document contains several related specifications, together they describe\nthe document formats related to the solver competition which will be organized\nby Mancoosi. In particular, this document describes: - DUDF (Distribution\nUpgradeability Description Format), the document format to be used to submit\nupgrade problem instances from user machines to a (distribution-specific)\ndatabase of upgrade problems; - CUDF (Common Upgradeability Description\nFormat), the document format used to encode upgrade problems, abstracting over\ndistribution-specific details. Solvers taking part in the competition will be\nfed with input in CUDF format."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0811.4364v1", 
    "title": "Revisiting the Core Ontology and Problem in Requirements Engineering", 
    "arxiv-id": "0811.4364v1", 
    "author": "Stephane Faulkner", 
    "publish": "2008-11-26T16:36:29Z", 
    "summary": "In their seminal paper in the ACM Transactions on Software Engineering and\nMethodology, Zave and Jackson established a core ontology for Requirements\nEngineering (RE) and used it to formulate the \"requirements problem\", thereby\ndefining what it means to successfully complete RE. Given that stakeholders of\nthe system-to-be communicate the information needed to perform RE, we show that\nZave and Jackson's ontology is incomplete. It does not cover all types of basic\nconcerns that the stakeholders communicate. These include beliefs, desires,\nintentions, and attitudes. In response, we propose a core ontology that covers\nthese concerns and is grounded in sound conceptual foundations resting on a\nfoundational ontology. The new core ontology for RE leads to a new formulation\nof the requirements problem that extends Zave and Jackson's formulation. We\nthereby establish new standards for what minimum information should be\nrepresented in RE languages and new criteria for determining whether RE has\nbeen successfully completed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0812.3716v1", 
    "title": "Context-aware adaptation for group communication support applications   with dynamic architecture", 
    "arxiv-id": "0812.3716v1", 
    "author": "Mohamed Jmaiel", 
    "publish": "2008-12-19T07:53:12Z", 
    "summary": "In this paper, we propose a refinement-based adaptation approach for the\narchitecture of distributed group communication support applications. Unlike\nmost of previous works, our approach reaches implementable, context-aware and\ndynamically adaptable architectures. To model the context, we manage\nsimultaneously four parameters that influence Qos provided by the application.\nThese parameters are: the available bandwidth, the exchanged data communication\npriority, the energy level and the available memory for processing. These\nparameters make it possible to refine the choice between the various\narchitectural configurations when passing from a given abstraction level to the\nlower level which implements it. Our approach allows the importance degree\nassociated with each parameter to be adapted dynamically. To implement\nadaptation, we switch between the various configurations of the same level, and\nwe modify the state of the entities of a given configuration when necessary. We\nadopt the direct and mediated Producer- Consumer architectural styles and\ngraphs for architecture modelling. In order to validate our approach we\nelaborate a simulation model."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0812.3719v1", 
    "title": "Architecture Logicielles pour des Applications h\u00e9t\u00e9rog\u00e8nes,   distribu\u00e9es et reconfigurables", 
    "arxiv-id": "0812.3719v1", 
    "author": "Philippe Roose", 
    "publish": "2008-12-19T08:03:21Z", 
    "summary": "The recent apparition of mobile wireless sensor aware to their physical\nenvironment and able to process information must allow proposing applications\nable to take into account their physical context and to react according to the\nchanges of the environment. It suppose to design applications integrating both\nsoftware and hardware components able to communicate. Applications must use\ncontext information from components to measure the quality of the proposed\nservices in order to adapt them in real time. This work is interested in the\nintegration of sensors in distributed applications. It present a service\noriented software architecture allowing to manage and to reconfigure\napplications in heterogeneous environment where entities of different nature\ncollaborate: software components and wireless sensors."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0901.2069v1", 
    "title": "Encapsulation theory: the transformation equations of absolute   information hiding", 
    "arxiv-id": "0901.2069v1", 
    "author": "Edmund Kirwan", 
    "publish": "2009-01-14T17:28:18Z", 
    "summary": "This paper describes how the maximum potential number of edges of an\nencapsulated graph varies as the graph is transformed, that is, as nodes are\ncreated and modified. The equations governing these changes of maximum\npotential number of edges caused by the transformations are derived and briefly\nanalysed."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0901.3620v1", 
    "title": "Enterprise model verification and validation: an approach", 
    "arxiv-id": "0901.3620v1", 
    "author": "Fran\u00e7ois Prunet", 
    "publish": "2009-01-23T08:42:21Z", 
    "summary": "This article presents a Verification and Validation approach which is used\nhere in order to complete the classical tool box the industrial user may\nutilize in Enterprise Modeling and Integration domain. This approach, which has\nbeen defined independently from any application domain is based on several\nformal concepts and tools presented in this paper. These concepts are property\nconcepts, property reference matrix, properties graphs, enterprise modeling\ndomain ontology, conceptual graphs and formal reasoning mechanisms."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0902.0924v1", 
    "title": "Towards a Theory of Requirements Elicitation: Acceptability Condition   for the Relative Validity of Requirements", 
    "arxiv-id": "0902.0924v1", 
    "author": "Stephane Faulkner", 
    "publish": "2009-02-05T15:26:41Z", 
    "summary": "A requirements engineering artifact is valid relative to the stakeholders of\nthe system-to-be if they agree on the content of that artifact. Checking\nrelative validity involves a discussion between the stakeholders and the\nrequirements engineer. This paper proposes (i) a language for the\nrepresentation of information exchanged in a discussion about the relative\nvalidity of an artifact; (ii) the acceptability condition, which, when it\nverifies in a discussion captured in the proposed language, signals that the\nrelative validity holds for the discussed artifact and for the participants in\nthe discussion; and (iii) reasoning procedures to automatically check the\nacceptability condition in a discussions captured by the proposed language."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0902.3136v1", 
    "title": "A distributed editing environment for XML documents", 
    "arxiv-id": "0902.3136v1", 
    "author": "Laurent Th\u00e9ry", 
    "publish": "2009-02-18T13:23:52Z", 
    "summary": "XML is based on two essential aspects: the modelization of data in a tree\nlike structure and the separation between the information itself and the way it\nis displayed. XML structures are easily serializable. The separation between an\nabstract representation and one or several views on it allows the elaboration\nof specialized interfaces to visualize or modify data. A lot of developments\nwere made to interact with XML data but the use of these applications over the\nInternet is just starting. This paper presents a prototype of a distributed\nediting environment over the Internet. The key point of our system is the way\nuser interactions are handled. Selections and modifications made by a user are\nnot directly reflected on the concrete view, they are serialized in XML and\ntransmitted to a server which applies them to the document and broadcasts\nupdates to the views. This organization has several advantages. XML documents\ncoding selection and modification operations are usually smaller than the\nedited document and can be directly processed with a transformation engine\nwhich can adapt them to different representations. In addition, several\nselections or modifications can be combined into an unique XML document. This\nallows one to update multiple views with different frequencies and fits the\nrequirement of an asynchronous communication mode like HTTP."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.0053v1", 
    "title": "Workflow Patterns in Process Modeling", 
    "arxiv-id": "0903.0053v1", 
    "author": "Florin Fortis", 
    "publish": "2009-02-28T08:30:56Z", 
    "summary": "This paper proposes an introduction to one of the newest modelling methods,\nan executable model based on workflows. We present the terminology for some\nbasic workflow patterns, as described in the Workflow Management Coalition\nTerminology and Glossary."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.0054v1", 
    "title": "Considerations on Resource Usage in Exceptions and Failures in Workflows", 
    "arxiv-id": "0903.0054v1", 
    "author": "Victoria Iordan", 
    "publish": "2009-02-28T08:40:04Z", 
    "summary": "The paper presents a description of some point of view of different authors\nrelated to the failures and exceptions that appear in workflows, as a direct\nconsequence of unavailability of resources involved in the workflow. Each of\nthese interpretations is typical for a certain situation, depending on the\nauthors' interpretation of failures and exceptions in workflows modeling real\ndynamical systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.0571v1", 
    "title": "Adaptation of Black-Box Software Components", 
    "arxiv-id": "0903.0571v1", 
    "author": "Rolf Andreas Rasenack", 
    "publish": "2009-03-03T17:01:54Z", 
    "summary": "The globalization of the software market leads to crucial problems for\nsoftware companies. More competition between software companies arises and\nleads to the force on companies to develop ever newer software products in ever\nshortened time interval. Therefor the time to market for software systems is\nshortened and obviously the product life cycle is shortened too[...]The\napproach introduced here presents the novel technique together with a\nsupportive environment that enables developers to cope with the adaptability of\nblack-box software components. A supported environment will be designed that\nchecks the compatibility of black-box software components with the assistance\nof their specifications."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.0914v1", 
    "title": "Artificial table testing dynamically adaptive systems", 
    "arxiv-id": "0903.0914v1", 
    "author": "Benoit Baudry", 
    "publish": "2009-03-05T05:36:22Z", 
    "summary": "Dynamically Adaptive Systems (DAS) are systems that modify their behavior and\nstructure in response to changes in their surrounding environment. Critical\nmission systems increasingly incorporate adaptation and response to the\nenvironment; examples include disaster relief and space exploration systems.\nThese systems can be decomposed in two parts: the adaptation policy that\nspecifies how the system must react according to the environmental changes and\nthe set of possible variants to reconfigure the system. A major challenge for\ntesting these systems is the combinatorial explosions of variants and\nenvi-ronment conditions to which the system must react. In this paper we focus\non testing the adaption policy and propose a strategy for the selection of\nenvi-ronmental variations that can reveal faults in the policy. Artificial\nShaking Table Testing (ASTT) is a strategy inspired by shaking table testing\n(STT), a technique widely used in civil engineering to evaluate building's\nstructural re-sistance to seismic events. ASTT makes use of artificial\nearthquakes that simu-late violent changes in the environmental conditions and\nstresses the system adaptation capability. We model the generation of\nartificial earthquakes as a search problem in which the goal is to optimize\ndifferent types of envi-ronmental variations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.3797v1", 
    "title": "Personal report of the 3rd ECMDA-FA'07 conference", 
    "arxiv-id": "0903.3797v1", 
    "author": "Beno\u00eet Combemale", 
    "publish": "2009-03-23T06:54:51Z", 
    "summary": "Manuscripts notes taken during the conference ECMDA 2008. I give the full\nconference program (title of the article and name of the person who introduced)\ndetailing some of the presentations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/RE.2008.13", 
    "link": "http://arxiv.org/pdf/0903.4267v1", 
    "title": "Delving into Transition to the Semantic Web", 
    "arxiv-id": "0903.4267v1", 
    "author": "Lucian Luca", 
    "publish": "2009-03-25T09:14:39Z", 
    "summary": "The semantic technologies pose new challenge for the way in which we built\nand operate systems. They are tools used to represent significances,\nassociations, theories, separated from data and code. Their goal is to create,\nto discover, to represent, to organize, to process, to manage, to ratiocinate,\nto represent, to share and use the significances and knowledge to fulfill the\nbusiness, personal or social goals."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0903.4875v2", 
    "title": "Extensible Component Based Architecture for FLASH, A Massively Parallel,   Multiphysics Simulation Code", 
    "arxiv-id": "0903.4875v2", 
    "author": "A. Siegal", 
    "publish": "2009-03-27T18:52:05Z", 
    "summary": "FLASH is a publicly available high performance application code which has\nevolved into a modular, extensible software system from a collection of\nunconnected legacy codes. FLASH has been successful because its capabilities\nhave been driven by the needs of scientific applications, without compromising\nmaintainability, performance, and usability. In its newest incarnation, FLASH3\nconsists of inter-operable modules that can be combined to generate different\napplications. The FLASH architecture allows arbitrarily many alternative\nimplementations of its components to co-exist and interchange with each other,\nresulting in greater flexibility. Further, a simple and elegant mechanism\nexists for customization of code functionality without the need to modify the\ncore implementation of the source. A built-in unit test framework providing\nverifiability, combined with a rigorous software maintenance process, allow the\ncode to operate simultaneously in the dual mode of production and development.\nIn this paper we describe the FLASH3 architecture, with emphasis on solutions\nto the more challenging conflicts arising from solver complexity, portable\nperformance requirements, and legacy codes. We also include results from user\nsurveys conducted in 2005 and 2007, which highlight the success of the code."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0903.5024v1", 
    "title": "Analysis Paralysis: when to stop?", 
    "arxiv-id": "0903.5024v1", 
    "author": "Er. Akshay Bhardwaj", 
    "publish": "2009-03-29T06:04:28Z", 
    "summary": "Analysis of a system constitutes the most important aspect of the systems\ndevelopment life cycle.But it is also the most confusing and time consuming of\nall the stages.The critical question always remains: How much and till when to\nanalyse? Ed Yourdon has called this phenomenon as Analysis Paralysis. In this\npaper, I suggest a model which can actually help in arriving at a satisfactory\nanswer to this problem."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.0293v2", 
    "title": "INFRAWEBS axiom editor - a graphical ontology-driven tool for creating   complex logical expressions", 
    "arxiv-id": "0904.0293v2", 
    "author": "Ivan Dilov", 
    "publish": "2009-04-02T02:29:47Z", 
    "summary": "The current INFRAWEBS European research project aims at developing ICT\nframework enabling software and service providers to generate and establish\nopen and extensible development platforms for Web Service applications. One of\nthe concrete project objectives is developing a full-life-cycle software\ntoolset for creating and maintaining Semantic Web Services (SWSs) supporting\nspecific applications based on Web Service Modelling Ontology (WSMO) framework.\nAccording to WSMO, functional and behavioural descriptions of a SWS may be\nrepresented by means of complex logical expressions (axioms). The paper\ndescribes a specialized user-friendly tool for constructing and editing such\naxioms - INFRAWEBS Axiom Editor. After discussing the main design principles of\nthe Editor, its functional architecture is briefly presented. The tool is\nimplemented in Eclipse Graphical Environment Framework and Eclipse Rich Client\nPlatform."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.2769v2", 
    "title": "Non Homogeneous Poisson Process Model based Optimal Modular Software   Testing using Fault Tolerance", 
    "arxiv-id": "0904.2769v2", 
    "author": "Sanjay Chaudhary", 
    "publish": "2009-04-17T19:40:05Z", 
    "summary": "In software development process we come across various modules. Which raise\nthe idea of priority of the different modules of a software so that important\nmodules are tested on preference. This approach is desirable because it is not\npossible to test each module regressively due to time and cost constraints.\nThis paper discuss on some parameters, required to prioritize several modules\nof a software and provides measure of optimal time and cost for testing based\non non homogeneous Poisson process."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3633v1", 
    "title": "Business Process Modeling Notation - An Overview", 
    "arxiv-id": "0904.3633v1", 
    "author": "Alexandra Fortis", 
    "publish": "2009-04-23T09:11:50Z", 
    "summary": "BPMN represents an industrial standard created to offer a common and user\nfriendly notation to all the participants to a business process. The present\npaper aims to briefly present the main features of this notation as well as an\ninterpretation of some of the main patterns characterizing a business process\nmodeled by the working fluxes."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3634v1", 
    "title": "Tailored business solutions by workflow technologies", 
    "arxiv-id": "0904.3634v1", 
    "author": "Alexandra Fortis", 
    "publish": "2009-04-23T09:22:22Z", 
    "summary": "VISP (Virtual Internet Service Provider) is an IST-STREP project, which is\nconducting research in the field of these new technologies, targeted to\ntelecom/ISP companies. One of the first tasks of the VISP project is to\nidentify the most appropriate technologies in order to construct the VISP\nplatform. This paper presents the most significant results in the field of\nchoreography and orchestration, two key domains that must accompany process\nmodeling in the construction of a workflow environment."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3648v1", 
    "title": "Computer Aided Optimization of the Unconventional Processing", 
    "arxiv-id": "0904.3648v1", 
    "author": "Mihai Titu", 
    "publish": "2009-04-23T10:38:24Z", 
    "summary": "The unconventional technologies, currently applied at a certain category of\nmaterials, difficult to be processed through usual techniques, have undergone\nduring the last 60 years all the stages, since their discovery to their use on\na large scale. They are based on elementary mechanisms which run the processing\nthrough classic methods, yet, they use in addition the interconnections of\nthese methods. This leads to a plus in performance by increasing the outcomes\nprecision, reducing the processing time, increasing the quality of the finite\nproduct, etc. This performance can be much increased by using the computer and\na software product in assisting the human operator in the processing by an\nunconventional method such as; the electric or electro-chemical erosion, the\ncomplex electric-electro-chemical erosion, the processing by a laser fascicle\nand so on. The present work presents such an application based on a data base\ncombining the previous experimental results, which proposes a method of\noptimization of the outcomes."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3698v1", 
    "title": "Semantic Linkage of Control Systems", 
    "arxiv-id": "0904.3698v1", 
    "author": "Karl Hayo Siemsen", 
    "publish": "2009-04-23T14:02:18Z", 
    "summary": "Control systems are sets of interconnected hardware and software components\nwhich regulate the behaviour of processes. The software of modern control\nsystems rises for some years by requirements regarding the flexibility and\nfunctionality. Thus the force of innovation grows on enterprises, since ever\nnewer products in ever shorter time intervals must be made available.\nAssociated hereby is the crucial shortening of the product life cycle, whose\neffects show up in reduced care of the software and the spares inventory. The\naim, the concept presented here and developed in a modeling environment, is\nproved and ensures a minimum functionality of software components. Replacing\nsoftware components of a control system verified for functionality by a\nframework at run-time and if necessary the software conditions will become\nadapted. Quintessential point of this implementation is the usage of an\nabstract syntax tree. Within its hierarchical structure meta information is\nattached to nodes and processed by the framework. With the development of the\nconcept for semantic proving of software components the lifetime of\nsoftware-based products is increased."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3716v1", 
    "title": "Failover of Software Services with State Replication", 
    "arxiv-id": "0904.3716v1", 
    "author": "Rolf Andreas Rasenack", 
    "publish": "2009-04-23T15:01:45Z", 
    "summary": "Computing systems are becoming more and more complex and assuming more and\nmore responsibilities in all sectors of human activity. Applications do not run\nlocally on a single computer any more. A lot of today's applications are built\nas distributed system; with services on different computers communicating with\neach other. Distributed systems arise everywhere. The Internet is one of the\nbest-known distributed systems and used by nearly everyone today. It is obvious\nthat we are more and more dependent on computer services. Many people expect to\nbe able to buy things like clothing or electronic equipment even at night on\nthe Internet. Computers are expected to be operational and available 7 days a\nweek, 24 hours a day. Downtime, even for maintenance, is no longer acceptable."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.3718v1", 
    "title": "Architecture of the Neurath Basic Model View Controller", 
    "arxiv-id": "0904.3718v1", 
    "author": "R. A. Rasenack", 
    "publish": "2009-04-23T15:05:01Z", 
    "summary": "The idea of the Neurath Basic Model View Controller (NBMVC) appeared during\nthe discussion of the design of domain-specific modeling tools based on the\nNeurath Modeling Language [Yer06]. The NBMVC is the core of the modeling\nprocess within the modeling environment. It reduces complexity out of the\ndesign process by providing domain-specific interfaces between the developer\nand the model. These interfaces help to organize and manipulate the model. The\norganization includes, for example, a layer with visual components to drop them\nin and filter them out. The control routines includes, for example, model\ntransformations."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0904.4411v1", 
    "title": "Ing\u00e9nierie syst\u00e8me d'un syst\u00e8me d'information d'entreprise   centr\u00e9 sur le produit bas\u00e9e sur un cadre de mod\u00e9lisation   multi-\u00e9chelles : application \u00e0 un cas d'\u00e9tude de l'AIP lorrain", 
    "arxiv-id": "0904.4411v1", 
    "author": "Jean-Yves Bron", 
    "publish": "2009-04-28T14:24:23Z", 
    "summary": "Through its projects, the ?Atelier Inter-\\'etablissements de Productique\nLorrain? (AIPL), as the owner and contractor of rank 1, is committed to provide\nhis customers (teachers, training courses, students etc...) credible teaching\nmaterials at the scale of a real industrial flexible production of goods and\nservices. In this changing context, its managerial team has chosen to suppress\nthe CIM concept, which proposes an integrated enterprise, to steering\ndistributed system information (SI), heterogeneous, autonomous and scalable\ndepending on the ephemeral cooperation between industrial partners who now\nexchanges information and material flows. These aspects are studied in research\non CRAN (Centre de Recherche en Automatique de Nancy ? Research Centre for\nAutomatic Control) as part of a thesis based on the recursive aspect of systems\nand their models and their multi-scale aspects and multi-views, in a\nModel-Based-System-Engineering (MBSE) methodology proposal of an\nSystem-Engineering (SE) focused on the product. To validate this research, a\nMBSE has been implemented on a case study to AIPL: the \"eLearning in\neProduction? project."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0905.4226v1", 
    "title": "Strong Dependencies between Software Components", 
    "arxiv-id": "0905.4226v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2009-05-26T14:20:59Z", 
    "summary": "Component-based systems often describe context requirements in terms of\nexplicit inter-component dependencies. Studying large instances of such\nsystems?such as free and open source software (FOSS) distributions?in terms of\ndeclared dependencies between packages is appealing. It is however also\nmisleading when the language to express dependencies is as expressive as\nboolean formulae, which is often the case. In such settings, a more appropriate\nnotion of component dependency exists: strong dependency. This paper introduces\nsuch notion as a first step towards modeling semantic, rather then syntactic,\ninter-component relationships. Furthermore, a notion of component sensitivity\nis derived from strong dependencies, with ap- plications to quality assurance\nand to the evaluation of upgrade risks. An empirical study of strong\ndependencies and sensitivity is presented, in the context of one of the\nlargest, freely available, component-based system."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0905.4592v1", 
    "title": "Mod\u00e9lisation des facteurs influen\u00e7ant la performance de la   cha\u00eene logistique", 
    "arxiv-id": "0905.4592v1", 
    "author": "Lorraine Trilling", 
    "publish": "2009-05-28T09:49:02Z", 
    "summary": "Improvement of industrial performance such as cost, lead-time, adaptability,\nvariety and traceability is the major finality of companies. At this need\ncorresponds the necessity to collaborate and to strengthen their coordination\nmechanisms. Information exchange becomes then a strategic question: what is the\nnature of the information that can be shared with customers and suppliers?\nWhich impact on the performance of a company is expectable? What about the\nperformance of the whole supply chain? It is essential for a company to\nidentify the information whose exchange contributes to its performance and to\ncontrol its information flows. This study aims to release from the literature\nthe main tendencies of collaboration practices and information exchanges\nleading to the performance and to propose a model of hypothesis gathering these\npractices."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0905.4613v1", 
    "title": "Athos - The C# GUI Generator", 
    "arxiv-id": "0905.4613v1", 
    "author": "Daniela Ilea", 
    "publish": "2009-05-28T11:17:25Z", 
    "summary": "This application comes to help software architects and developers during the\nlong process between user's stories, designing the application's structure and\nactually coding it."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0905.4863v1", 
    "title": "Derivation of UML Based Performance Models for Design Assessment in a   Reuse Based Software Development Approach", 
    "arxiv-id": "0905.4863v1", 
    "author": "R. Vasantha", 
    "publish": "2009-05-29T13:42:19Z", 
    "summary": "Reuse-based software development provides an opportunity for better quality\nand increased productivity in the software products. One of the most critical\naspects of the quality of a software system is its performance. The systematic\napplication of software performance engineering techniques throughout the\ndevelopment process can help to identify design alternatives that preserve\ndesirable qualities such as extensibility and reusability while meeting\nperformance objectives. In the present scenario, most of the performance\nfailures are due to a lack of consideration of performance issues early in the\ndevelopment process, especially in the design phase. These performance failures\nresults in damaged customer relations, lost productivity for users, cost\noverruns due to tuning or redesign, and missed market windows. In this paper,\nwe propose UML based Performance Models for design assessment in a reuse based\nsoftware development scenario."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0906.1429v1", 
    "title": "Am\u00e9liorer les performances de l'industrie logicielle par une meilleure   compr\u00e9hension des besoins", 
    "arxiv-id": "0906.1429v1", 
    "author": "Pierre M\u00e9vellec", 
    "publish": "2009-06-08T08:03:23Z", 
    "summary": "Actual organization are structured and act with the help of their information\nsystems. In spite of considerable progresses made by computer technology, we\nnote that actors are very often critical on their information systems.\nDifficulties to product specifications enough detailed for functional profile\nand interpretable by information system expert is one of reason of this gap\nbetween hopes and reality. Our proposition wants to get over this obstacle by\norganizing user requirements in a common language of operational profile and\ntechnical expert.-- Les organisations actuelles se structurent et agissent en\ns'appuyant sur leurs syst\\`emes d'information. Malgr\\'e les progr\\`es\nconsid\\'erables r\\'ealis\\'es par la technologie informatique, on constate que\nles acteurs restent tr\\`es souvent critiques par rapport \\`a leur syst\\`emes\nd'information. Une des causes de cet \\'ecart entre les espoirs et la\nr\\'ealit\\'e trouve sa source dans la difficult\\'e \\`a produire un cahier des\ncharges suffisamment d\\'etaill\\'e pour les op\\'erationnels et interpr\\'etable\npar les sp\\'ecialistes des syst\\`emes d'information. Notre proposition vise \\`a\nsurmonter cet obstacle en organisant l'expression des besoins dans un langage\ncommun aux op\\'erationnels et aux experts techniques. Pour cela, le langage\npropos\\'e pour exprimer les besoins est bas\\'e sur la notion de but.\nL'ing\\'enierie dirig\\'ee par les mod\\`eles est pr\\'esente \\`a toute les\n\\'etapes, c'est-\\`a-dire au moment de la capture et de l'interpr\\'etation."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0906.1667v1", 
    "title": "Analyser Framework to verify Software Component", 
    "arxiv-id": "0906.1667v1", 
    "author": "Rolf Andreas Rasenack", 
    "publish": "2009-06-09T09:14:25Z", 
    "summary": "Today, it is important for software companies to build software systems in a\nshort time-interval, to reduce costs and to have a good market position.\nTherefore well organized and systematic development approaches are required.\nReusing software components, which are well tested, can be a good solution to\ndevelop software applications in effective manner. The reuse of software\ncomponents is less expensive and less time consuming than a development from\nscratch. But it is dangerous to think that software components can be match\ntogether without any problems. Software components itself are well tested, of\ncourse, but even if they composed together problems occur. Most problems are\nbased on interaction respectively communication. Avoiding such errors a\nframework has to be developed for analysing software components. That framework\ndetermines the compatibility of corresponding software components.The promising\napproach discussed here, presents a novel technique for analysing software\ncomponents by applying an Abstract Syntax Language Tree (ASLT). A supportive\nenvironment will be designed that checks the compatibility of black-box\nsoftware components. This article is concerned to the question how can be\ncoupled software components verified by using an analyzer framework and\ndetermines the usage of the ASLT. Black-box Software Components and Abstract\nSyntax Language Tree are the basis for developing the proposed framework and\nare discussed here to provide the background knowledge. The practical\nimplementation of this framework is discussed and shows the result by using a\ntest environment."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.parco.2009.08.001", 
    "link": "http://arxiv.org/pdf/0906.3245v1", 
    "title": "Goal-oriented Data Warehouse Quality Measurement", 
    "arxiv-id": "0906.3245v1", 
    "author": "Jes\u00fas Pardillo", 
    "publish": "2009-06-17T16:49:03Z", 
    "summary": "Requirements engineering is known to be a key factor for the success of\nsoftware projects. Inside this discipline, goal-oriented requirements\nengineering approaches have shown specially suitable to deal with projects\nwhere it is necessary to capture the alignment between system requirements and\nstakeholders' needs, as is the case of data-warehousing projects. However, the\nmere alignment of data-warehouse system requirements with business goals is not\nenough to assure better data-warehousing products; measures and techniques are\nalso needed to assure the data-warehouse quality. In this paper, we provide a\nmodelling framework for data-warehouse quality measurement (i*DWQM). This\nframework, conceived as an i* extension, provides support for the definition of\ndata-warehouse requirements analysis models that include quantifiable quality\nscenarios, defined in terms of well-formed measures. This extension has been\ndefined by means of a UML profiling architecture. The resulting framework has\nbeen implemented in the Eclipse development platform."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2.4", 
    "link": "http://arxiv.org/pdf/0906.3916v1", 
    "title": "An Introduction to Simulation-Based Techniques for Automated Service   Composition", 
    "arxiv-id": "0906.3916v1", 
    "author": "Fabio Patrizi", 
    "publish": "2009-06-22T03:54:04Z", 
    "summary": "This work is an introduction to the author's contributions to the SOC area,\nresulting from his PhD research activity. It focuses on the problem of\nautomatically composing a desired service, given a set of available ones and a\ntarget specification. As for description, services are represented as\nfinite-state transition systems, so to provide an abstract account of their\nbehavior, seen as the set of possible conversations with external clients. In\naddition, the presence of a finite shared memory is considered, that services\ncan interact with and which provides a basic form of communication. Rather than\ndescribing technical details, we offer an informal overview of the whole work,\nand refer the reader to the original papers, referenced throughout this work,\nfor all details."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2.9", 
    "link": "http://arxiv.org/pdf/0906.3921v1", 
    "title": "Fairness as a QoS Measure for Web Services", 
    "arxiv-id": "0906.3921v1", 
    "author": "Paola Campli", 
    "publish": "2009-06-22T06:08:14Z", 
    "summary": "Service Oriented Architectures (SOAs) are component-based architectures,\ncharacterized by reusability, modularization and composition, usually offered\nby HTTP (web services) and often equipped with a Quality of Services (QoS)\nmeasure. In order to guarantee the fairness property to each client requesting\na service, we propose a fair version of the (Soft) Concurrent Constraint\nlanguage to deal with the negotiation phases of the Service Level Agreement\n(SLA) protocol."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2.2", 
    "link": "http://arxiv.org/pdf/0906.3924v1", 
    "title": "Service-oriented Context-aware Framework", 
    "arxiv-id": "0906.3924v1", 
    "author": "Bal\u00e1zs Pataki", 
    "publish": "2009-06-22T06:26:17Z", 
    "summary": "Location- and context-aware services are emerging technologies in mobile and\ndesktop environments, however, most of them are difficult to use and do not\nseem to be beneficial enough. Our research focuses on designing and creating a\nservice-oriented framework that helps location- and context-aware,\nclient-service type application development and use. Location information is\ncombined with other contexts such as the users' history, preferences and\ndisabilities. The framework also handles the spatial model of the environment\n(e.g. map of a room or a building) as a context. The framework is built on a\nsemantic backend where the ontologies are represented using the OWL description\nlanguage. The use of ontologies enables the framework to run inference tasks\nand to easily adapt to new context types. The framework contains a\ncompatibility layer for positioning devices, which hides the technical\ndifferences of positioning technologies and enables the combination of location\ndata of various sources."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2.8", 
    "link": "http://arxiv.org/pdf/0906.3930v1", 
    "title": "Towards a Unifying View of QoS-Enhanced Web Service Description and   Discovery Approaches", 
    "arxiv-id": "0906.3930v1", 
    "author": "Sylvia Ilieva", 
    "publish": "2009-06-22T06:56:40Z", 
    "summary": "The number of web services increased vastly in the last years. Various\nproviders offer web services with the same functionality, so for web service\nconsumers it is getting more complicated to select the web service, which best\nfits their requirements. That is why a lot of the research efforts point to\ndiscover semantic means for describing web services taking into account not\nonly functional characteristics of services, but also the quality of service\n(QoS) properties such as availability, reliability, response time, trust, etc.\nThis motivated us to research current approaches presenting complete solutions\nfor QoS enabled web service description, publication and discovery. In this\npaper we present comparative analysis of these approaches according to their\ncommon principals. Based on such analysis we extract the essential aspects from\nthem and propose a pattern for the development of QoS-aware service-oriented\narchitectures."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2.7", 
    "link": "http://arxiv.org/pdf/0906.4149v1", 
    "title": "Adaptive Process Management in Highly Dynamic and Pervasive Scenarios", 
    "arxiv-id": "0906.4149v1", 
    "author": "Massimiliano de Leoni", 
    "publish": "2009-06-23T01:34:37Z", 
    "summary": "Process Management Systems (PMSs) are currently more and more used as a\nsupporting tool for cooperative processes in pervasive and highly dynamic\nsituations, such as emergency situations, pervasive healthcare or domotics/home\nautomation. But in all such situations, designed processes can be easily\ninvalidated since the execution environment may change continuously due to\nfrequent unforeseeable events. This paper aims at illustrating the theoretical\nframework and the concrete implementation of SmartPM, a PMS that features a set\nof sound and complete techniques to automatically cope with unplanned\nexceptions. PMS SmartPM is based on a general framework which adopts the\nSituation Calculus and Indigolog."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0906.4900v1", 
    "title": "Proceedings Fourth European Young Researchers Workshop on Service   Oriented Computing", 
    "arxiv-id": "0906.4900v1", 
    "author": "Maurice H. ter Beek", 
    "publish": "2009-06-26T10:52:52Z", 
    "summary": "Service-Oriented Computing (SOC) is an emerging new paradigm for distributed\nand object-oriented computing by allowing autonomous, platform-independent\ncomputational entities (called services) to be built (described, discovered,\ncomposed, orchestrated) within and across organizational boundaries. Like no\nother computing paradigm before, SOC is destined to exert a lasting influence\non the business domain, among others (e-commerce, e-government, e-business,\ne-learning, e-health, etc.).\n  The Young Researchers workshop series on Service-Oriented Computing is meant\nto be a platform for junior researchers from industry and academics alike. Its\ncore objectives are to exchange information regarding advancements in the state\nof the art and practice of SOC, as well as to identify emerging research topics\nand the future trends in this domain.\n  Following the success of the previous three workshops, the 4th European Young\nResearchers Workshop on Service-Oriented Computing (YR-SOC 2009) introduced two\nnovelties: it was organised outside of the UK and it saw the introduction of a\nnumber of tutorials, thus making the workshop a 3-day event. YR-SOC 2009 took\nplace at the CNR Institute of Information Science and Technologies in Pisa,\nItaly, and was organised by Maurice ter Beek, Barry Norton, Stephan\nReiff-Marganiec and Monika Solanki.\n  The contributions in this volume cover aspects such as automated service\ncomposition, context-aware SOC, service-oriented programming, QoS-aware SOC,\nservice-oriented architectures, SOC modelling and analysis, process management,\nweb services, ontologies and the semantic web."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0906.5393v1", 
    "title": "Measurable & Scalable NFRs using Fuzzy Logic and Likert Scale", 
    "arxiv-id": "0906.5393v1", 
    "author": "Faisal Munir Malik", 
    "publish": "2009-06-30T01:22:27Z", 
    "summary": "Most of the research related to Non Functional Requirements (NFRs) have\npresented NFRs frameworks by integrating non functional requirements with\nfunctional requirements while we proposed that measurement of NFRs is possible\ne.g. cost and performance and NFR like usability can be scaled. Our novel\nhybrid approach integrates three things rather than two i.e. Functional\nRequirements (FRs), Measurable NFRs (M-NFRs) and Scalable NFRs (S-NFRs). We\nhave also found the use of Fuzzy Logic and Likert Scale effective for handling\nof discretely measurable as well as scalable NFRs as these techniques can\nprovide a simple way to arrive at a discrete or scalable NFR in contrast to\nvague, ambiguous, imprecise, noisy or missing NFR. Our approach can act as\nbaseline for new NFR and aspect oriented frameworks by using all types of UML\ndiagrams."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0907.0404v1", 
    "title": "Agent based Model for providing optimized, synchronized and failure free   execution of workflow process", 
    "arxiv-id": "0907.0404v1", 
    "author": "Saqib Saeed", 
    "publish": "2009-07-02T15:19:39Z", 
    "summary": "The main objective of this paper is to provide an optimized solution and\nalgorithm for the execution of a workflow process by ensuring the data\nconsistency, correctness, completeness among various tasks involved. The\nsolution proposed provides a synchronized and failure free flow of execution\namong various tasks involved in a workflow process. A synchronizing agent is\nbound at a very low level, i.e. with the workflow activity or task to get the\ndesired goals to be done and an algorithm is provided to show the execution of\nworkflow process completely."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0907.1238v1", 
    "title": "ChOrDa: a methodology for the modeling of business processes with BPMN", 
    "arxiv-id": "0907.1238v1", 
    "author": "Danilo Montesi", 
    "publish": "2009-07-07T15:29:58Z", 
    "summary": "In this paper we present a modeling methodology for BPMN, the standard\nnotation for the representation of business processes. Our methodology\nsimplifies the development of collaborative BPMN diagrams, enabling the\nautomated creation of skeleton process diagrams representing complex\nchoreographies. To evaluate and tune the methodology, we have developed a tool\nsupporting it, that we apply to the modeling of an international patenting\nprocess as a working example."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0907.2039v1", 
    "title": "Refining interfaces: the case of the B method", 
    "arxiv-id": "0907.2039v1", 
    "author": "Anamaria M. Moreira", 
    "publish": "2009-07-12T12:31:58Z", 
    "summary": "Model-driven design of software for safety-critical applications often relies\non mathematically grounded techniques such as the B method. Such techniques\nconsist in the successive applications of refinements to derive a concrete\nimplementation from an abstract specification. Refinement theory defines\nverification conditions to guarantee that such operations preserve the intended\nbehaviour of the abstract specifications. One of these conditions requires\nhowever that concrete operations have exactly the same signatures as their\nabstract counterpart, which is not always a practical requirement. This paper\nshows how changes of signatures can be achieved while still staying within the\nbounds of refinement theory. This makes it possible to take advantage of the\nmathematical guarantees and tool support provided for the current\nrefinement-based techniques, such as the B method."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0907.2072v2", 
    "title": "SMT-Based Bounded Model Checking for Embedded ANSI-C Software", 
    "arxiv-id": "0907.2072v2", 
    "author": "Joao Marques-Silva", 
    "publish": "2009-07-12T21:49:21Z", 
    "summary": "Propositional bounded model checking has been applied successfully to verify\nembedded software but is limited by the increasing propositional formula size\nand the loss of structure during the translation. These limitations can be\nreduced by encoding word-level information in theories richer than\npropositional logic and using SMT solvers for the generated verification\nconditions. Here, we investigate the application of different SMT solvers to\nthe verification of embedded software written in ANSI-C. We have extended the\nencodings from previous SMT-based bounded model checkers to provide more\naccurate support for finite variables, bit-vector operations, arrays,\nstructures, unions and pointers. We have integrated the CVC3, Boolector, and Z3\nsolvers with the CBMC front-end and evaluated them using both standard software\nmodel checking benchmarks and typical embedded applications from\ntelecommunications, control systems and medical devices. The experiments show\nthat our approach can analyze larger problems and substantially reduce the\nverification time."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0907.3983v2", 
    "title": "Service-oriented high level architecture", 
    "arxiv-id": "0907.3983v2", 
    "author": "Xichun Liu", 
    "publish": "2009-07-23T04:39:37Z", 
    "summary": "Service-oriented High Level Architecture (SOHLA) refers to the high level\narchitecture (HLA) enabled by Service-Oriented Architecture (SOA) and Web\nServices etc. techniques which supports distributed interoperating services.\nThe detailed comparisons between HLA and SOA are made to illustrate the\nimportance of their combination. Then several key enhancements and changes of\nHLA Evolved Web Service API are introduced in comparison with native APIs, such\nas Federation Development and Execution Process, communication mechanisms, data\nencoding, session handling, testing environment and performance analysis. Some\napproaches are summarized including Web-Enabling HLA at the communication\nlayer, HLA interface specification layer, federate interface layer and\napplication layer. Finally the problems of current research are discussed, and\nthe future directions are pointed out."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0908.0191v1", 
    "title": "The Levels of Conceptual Interoperability Model: Applying Systems   Engineering Principles to M&S", 
    "arxiv-id": "0908.0191v1", 
    "author": "Weiping WANG", 
    "publish": "2009-08-03T06:14:40Z", 
    "summary": "This paper describes the use of the Levels of Conceptual Interoperability\nModel (LCIM) as a framework for conceptual modeling and its descriptive and\nprescriptive uses. LCIM is applied to show its potential and shortcomings in\nthe current simulation interoperability approaches, in particular the High\nLevel Architecture (HLA) and Base Object Models (BOM). It emphasizes the need\nto apply rigorous engineering methods and principles and replace ad-hoc\napproaches."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0908.2506v1", 
    "title": "Software Engineering with Process Algebra: Modelling Client / Server   Architectures", 
    "arxiv-id": "0908.2506v1", 
    "author": "B. Diertens", 
    "publish": "2009-08-18T07:29:27Z", 
    "summary": "In previous work we described how the process algebra based language PSF can\nbe used in software engineering, using the ToolBus, a coordination architecture\nalso based on process algebra, as implementation model. We also described this\nsoftware development process more formally by presenting the tools we use in\nthis process in a CASE setting, leading to the PSF-ToolBus software engineering\nenvironment. In this article we summarize that work and describe a similar\nsoftware development process for implementation of software systems using a\nclient / server model and present this in a CASE setting as well."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.0732v1", 
    "title": "A Step towards Software Corrective Maintenance Using RCM model", 
    "arxiv-id": "0909.0732v1", 
    "author": "Shakeel Ahmad", 
    "publish": "2009-09-03T19:06:50Z", 
    "summary": "From the preliminary stage of software engineering, selection of appropriate\nenforcement of standards remained a challenge for stakeholders during entire\ncycle of software development, but it can lead to reduce the efforts desired\nfor software maintenance phase. Corrective maintenance is the reactive\nmodification of a software product performed after delivery to correct\ndiscovered faults. Studies conducted by different researchers reveal that\napproximately 50 to 75 percent of the effort is spent on maintenance, out of\nwhich about 17 to 21 percent is exercised on corrective maintenance. In this\npaper, authors proposed a RCM (Reduce Corrective Maintenance) model which\nrepresents the implementation process of number of checklists to guide the\nstakeholders of all phases of software development. These check lists will be\nfilled by corresponding stake holder of all phases before its start. More\nprecise usage of the check list in relevant phase ensures successful\nenforcement of analysis, design, coding and testing standards for reducing\nerrors in operation stage. Moreover authors represent the step by step\nintegration of checklists in software development life cycle through RCM model."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.1361v1", 
    "title": "Statechart Verification with iState", 
    "arxiv-id": "0909.1361v1", 
    "author": "Dai Tri Man Le", 
    "publish": "2009-09-08T19:44:49Z", 
    "summary": "This paper is the longer version of the extended abstract with the same name\npublished in FM 06. We describe in detail the algorithm to generate\nverification conditions from statechart structures implemented in the iState\ntool. This approach also suggests us a novel method to define a version of\npredicate semantics for statecharts analogous to how we assign predicate\nsemantics to programming languages."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.1364v1", 
    "title": "High level architecture evolved modular federation object model", 
    "arxiv-id": "0909.1364v1", 
    "author": "Weiping Wang", 
    "publish": "2009-09-08T00:50:30Z", 
    "summary": "To improve the agility, dynamics, composability, reusability, and development\nefficiency restricted by monolithic Federation Object Model (FOM), a modular\nFOM was proposed by High Level Architecture (HLA) Evolved product development\ngroup. This paper reviews the state-of-the-art of HLA Evolved modular FOM. In\nparticular, related concepts, the overall impact on HLA standards, extension\nprinciples, and merging processes are discussed. Also permitted and restricted\ncombinations, and merging rules are provided, and the influence on HLA\ninterface specification is given. The comparison between modular FOM and Base\nObject Model (BOM) is performed to illustrate the importance of their\ncombination. The applications of modular FOM are summarized. Finally, the\nsignificance to facilitate composable simulation both in academia and practice\nis presented and future directions are pointed out."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.2090v1", 
    "title": "Context Aware Adaptable Applications - A global approach", 
    "arxiv-id": "0909.2090v1", 
    "author": "Sophie Laplace", 
    "publish": "2009-09-11T06:19:01Z", 
    "summary": "Actual applications (mostly component based) requirements cannot be expressed\nwithout a ubiquitous and mobile part for end-users as well as for M2M\napplications (Machine to Machine). Such an evolution implies context management\nin order to evaluate the consequences of the mobility and corresponding\nmechanisms to adapt or to be adapted to the new environment. Applications are\nthen qualified as context aware applications. This first part of this paper\npresents an overview of context and its management by application adaptation.\nThis part starts by a definition and proposes a model for the context. It also\npresents various techniques to adapt applications to the context: from\nself-adaptation to supervised approached. The second part is an overview of\narchitectures for adaptable applications. It focuses on platforms based\nsolutions and shows information flows between application, platform and\ncontext. Finally it makes a synthesis proposition with a platform for adaptable\ncontext-aware applications called Kalimucho. Then we present implementations\ntools for software components and a dataflow models in order to implement the\nKalimucho platform."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.2103v1", 
    "title": "MESURE Tool to benchmark Java Card platforms", 
    "arxiv-id": "0909.2103v1", 
    "author": "Pierre Paradinas", 
    "publish": "2009-09-11T07:37:04Z", 
    "summary": "The advent of the Java Card standard has been a major turning point in smart\ncard technology. With the growing acceptance of this standard, understanding\nthe performance behavior of these platforms is becoming crucial. To meet this\nneed, we present in this paper a novel benchmarking framework to test and\nevaluate the performance of Java Card platforms. MESURE tool is the first\nframework which accuracy and effectiveness are independent from the particular\nJava Card platform tested and CAD used."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.2145v1", 
    "title": "A general XML-based distributed software architecture for accessing and   sharing ressources", 
    "arxiv-id": "0909.2145v1", 
    "author": "Laurent Romary", 
    "publish": "2009-09-11T11:08:07Z", 
    "summary": "This paper presents a general xml-based distributed software architecture in\nthe aim of accessing and sharing resources in an opened client/server\nenvironment. The paper is organized as follows : First, we introduce the idea\nof a \"General Distributed Software Architecture\". Second, we describe the\ngeneral framework in which this architecture is used. Third, we describe the\nprocess of information exchange and we introduce some technical issues involved\nin the implementation of the proposed architecture. Finally, we present some\nprojects which are currently using, or which should use, the proposed\narchitecture."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.2", 
    "link": "http://arxiv.org/pdf/0909.2183v1", 
    "title": "A Survey on Service Composition Middleware in Pervasive Environments", 
    "arxiv-id": "0909.2183v1", 
    "author": "Frederic Le Mouel", 
    "publish": "2009-09-11T14:43:54Z", 
    "summary": "The development of pervasive computing has put the light on a challenging\nproblem: how to dynamically compose services in heterogeneous and highly\nchanging environments? We propose a survey that defines the service composition\nas a sequence of four steps: the translation, the generation, the evaluation,\nand finally the execution. With this powerful and simple model we describe the\nmajor service composition middleware. Then, a classification of these service\ncomposition middleware according to pervasive requirements - interoperability,\ndiscoverability, adaptability, context awareness, QoS management, security,\nspontaneous management, and autonomous management - is given. The\nclassification highlights what has been done and what remains to do to develop\nthe service composition in pervasive environments."
},{
    "category": "cs.SE", 
    "doi": "10.1631/jzus.A0920258", 
    "link": "http://arxiv.org/pdf/0909.3414v1", 
    "title": "Three-dimensional conceptual model for service-oriented simulation", 
    "arxiv-id": "0909.3414v1", 
    "author": "Yifan Zhu", 
    "publish": "2009-09-18T11:56:33Z", 
    "summary": "In this letter, we propose a novel three-dimensional conceptual model for an\nemerging service-oriented simulation paradigm. The model can be used as a\nguideline or an analytic means to find the potential and possible future\ndirections of the current simulation frameworks. In particular, the model\ninspects the crossover between the disciplines of modeling and simulation,\nservice-orientation, and software/systems engineering. Finally, two specific\nsimulation frameworks are studied as examples."
},{
    "category": "cs.SE", 
    "doi": "10.1631/jzus.A0920258", 
    "link": "http://arxiv.org/pdf/0909.5087v1", 
    "title": "Towards maintainer script modernization in FOSS distributions", 
    "arxiv-id": "0909.5087v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2009-09-28T13:12:57Z", 
    "summary": "Free and Open Source Software (FOSS) distributions are complex software\nsystems, made of thousands packages that evolve rapidly, independently, and\nwithout centralized coordination. During packages upgrades, corner case\nfailures can be encountered and are hard to deal with, especially when they are\ndue to misbehaving maintainer scripts: executable code snippets used to\nfinalize package configuration. In this paper we report a software\nmodernization experience, the process of representing existing legacy systems\nin terms of models, applied to FOSS distributions. We present a process to\ndefine meta-models that enable dealing with upgrade failures and help rolling\nback from them, taking into account maintainer scripts. The process has been\napplied to widely used FOSS distributions and we report about such experiences."
},{
    "category": "cs.SE", 
    "doi": "10.1145/1595800.1595806", 
    "link": "http://arxiv.org/pdf/0909.5091v1", 
    "title": "Expressing advanced user preferences in component installation", 
    "arxiv-id": "0909.5091v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2009-09-28T13:31:17Z", 
    "summary": "State of the art component-based software collections - such as FOSS\ndistributions - are made of up to dozens of thousands components, with complex\ninter-dependencies and conflicts. Given a particular installation of such a\nsystem, each request to alter the set of installed components has potentially\n(too) many satisfying answers. We present an architecture that allows to\nexpress advanced user preferences about package selection in FOSS\ndistributions. The architecture is composed by a distribution-independent\nformat for describing available and installed packages called CUDF (Common\nUpgradeability Description Format), and a foundational language called MooML to\nspecify optimization criteria. We present the syntax and semantics of CUDF and\nMooML, and discuss the partial evaluation mechanism of MooML which allows to\ngain efficiency in package dependency solvers."
},{
    "category": "cs.SE", 
    "doi": "10.1145/1595800.1595806", 
    "link": "http://arxiv.org/pdf/0910.0493v1", 
    "title": "From Requirements to code: an Architecture-centric Approach for   producing Quality Systems", 
    "arxiv-id": "0910.0493v1", 
    "author": "Patrizio Pelliccione", 
    "publish": "2009-10-02T21:57:54Z", 
    "summary": "When engineering complex and distributed software and hardware systems\n(increasingly used in many sectors, such as manufacturing, aerospace,\ntransportation, communication, energy, and health-care), quality has become a\nbig issue, since failures can have economics consequences and can also endanger\nhuman life. Model-based specifications of a component-based system permit to\nexplicitly model the structure and behaviour of components and their\nintegration. In particular Software Architectures (SA) has been advocated as an\neffective means to produce quality systems. In this chapter by combining\ndifferent technologies and tools for analysis and development, we propose an\narchitecture-centric model-driven approach to validate required properties and\nto generate the system code. Functional requirements are elicited and used for\nidentifying expected properties the architecture shall express. The\narchitectural compliance to the properties is formally demonstrated, and the\nproduced architectural model is used to automatically generate the Java code.\nSuitable transformations assure that the code is conforming to both structural\nand behavioural SA constraints. This chapter describes the process and\ndiscusses how some existing tools and languages can be exploited to support the\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.1145/1595800.1595806", 
    "link": "http://arxiv.org/pdf/0910.1690v1", 
    "title": "Tool-Assisted Multi-Facet Analysis of Formal Specifications (Using   Alelier-B and ProB)", 
    "arxiv-id": "0910.1690v1", 
    "author": "Christian Attiogbe", 
    "publish": "2009-10-09T09:19:51Z", 
    "summary": "Tool-assisted analysis of software systems and convenient guides to practise\nthe formal methods are still motivating challenges. This article addresses\nthese challenges. We ex periment on analysing a formal specification from\nmultiple aspects. The B method and the Atelier-B tool are used for formal\nspecifications, for safety property analysis and for refinements. The ProB tool\nis used to supplement the study with model checking; it helps to discover\nerrors and there fore to improve the former specifications."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0910.1901v1", 
    "title": "Can Component/Service-Based Systems Be Proved Correct?", 
    "arxiv-id": "0910.1901v1", 
    "author": "Christian Attiogbe", 
    "publish": "2009-10-10T06:33:39Z", 
    "summary": "Component-oriented and service-oriented approaches have gained a strong\nenthusiasm in industries and academia with a particular interest for\nservice-oriented approaches. A component is a software entity with given\nfunctionalities, made available by a provider, and used to build other\napplication within which it is integrated. The service concept and its use in\nweb-based application development have a huge impact on reuse practices.\nAccordingly a considerable part of software architectures is influenced; these\narchitectures are moving towards service-oriented architectures. Therefore\napplications (re)use services that are available elsewhere and many\napplications interact, without knowing each other, using services available via\nservice servers and their published interfaces and functionalities. Industries\npropose, through various consortium, languages, technologies and standards.\nMore academic works are also undertaken concerning semantics and formalisation\nof components and service-based systems. We consider here both streams of works\nin order to raise research concerns that will help in building quality\nsoftware. Are there new challenging problems with respect to service-based\nsoftware construction? Besides, what are the links and the advances compared to\ndistributed systems?"
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.0428v1", 
    "title": "From Method Fragments to Method Services", 
    "arxiv-id": "0911.0428v1", 
    "author": "Carine Souveyet", 
    "publish": "2009-11-02T21:08:05Z", 
    "summary": "In Method Engineering (ME) science, the key issue is the consideration of\ninformation system development methods as fragments. Numerous ME approaches\nhave produced several definitions of method parts. Different in nature, these\nfragments have nevertheless some common disadvantages: lack of implementation\ntools, insufficient standardization effort, and so on. On the whole, the\nobserved drawbacks are related to the shortage of usage orientation. We have\nproceeded to an in-depth analysis of existing method fragments within a\ncomparison framework in order to identify their drawbacks. We suggest\novercoming them by an improvement of the ?method service? concept. In this\npaper, the method service is defined through the service paradigm applied to a\nspecific method fragment ? chunk. A discussion on the possibility to develop a\nunique representation of method fragment completes our contribution."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.0430v1", 
    "title": "Enhancing the Guidance of the Intentional Model \"MAP\": Graph Theory   Application", 
    "arxiv-id": "0911.0430v1", 
    "author": "Colette Rolland", 
    "publish": "2009-11-02T21:08:33Z", 
    "summary": "The MAP model was introduced in information system engineering in order to\nmodel processes on a flexible way. The intentional level of this model helps an\nengineer to execute a process with a strong relationship to the situation of\nthe project at hand. In the literature, attempts for having a practical use of\nmaps are not numerous. Our aim is to enhance the guidance mechanisms of the\nprocess execution by reusing graph algorithms. After clarifying the existing\nrelationship between graphs and maps, we improve the MAP model by adding\nqualitative criteria. We then offer a way to express maps with graphs and\npropose to use Graph theory algorithms to offer an automatic guidance of the\nmap. We illustrate our proposal by an example and discuss its limitations."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.1494v1", 
    "title": "Situational Method Engineering: Fundamentals and Experiences", 
    "arxiv-id": "0911.1494v1", 
    "author": "Rebecca Deneckere", 
    "publish": "2009-11-08T08:37:36Z", 
    "summary": "The work presented in this paper is related to the area of Situational Method\nEngineering (SME) which focuses on project-specific method construction. We\npropose a faceted framework to understand and classify issues in system\ndevelopment SME. The framework identifies four different but complementary\nviewpoints. Each view allows us to capture a particular aspect of situational\nmethods. Inter-relationships between these views show how they influence each\nother. In order to study, understand and classify a particular view of SME in\nits diversity, we associate a set of facets with each view. As a facet allows\nan in-depth description of one specific aspect of SME, the views show the\nvariety and diversity of these aspects."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.1495v1", 
    "title": "Method Chunks Selection by Multicriteria Techniques: an Extension of the   Assembly-based Approach", 
    "arxiv-id": "0911.1495v1", 
    "author": "Camille Salinesi", 
    "publish": "2009-11-08T08:38:06Z", 
    "summary": "The work presented in this paper is related to the area of situational method\nengineering (SME). In this domain, approaches are developed accordingly to\nspecific project specifications. We propose to adapt an existing method\nconstruction process, namely the assembly-based one. One of the particular\nfeatures of assembly-based SME approach is the selection of method chunks. Our\nproposal is to offer a better guidance in the retrieval of chunks by the\nintroduction of multicriteria techniques. To use them efficiently, we defined a\ntypology of projects characteristics, in order to identify all their critical\naspects, which will offer a priorisation to help the method engineer in the\nchoice between similar chunks."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.1496v1", 
    "title": "Improving Software Development Processes with Multicriteria Methods", 
    "arxiv-id": "0911.1496v1", 
    "author": "Camille Salinesi", 
    "publish": "2009-11-08T08:38:34Z", 
    "summary": "All software development processes include steps where several alternatives\ninduce a choice, a decision-making. Sometimes, methodologies offer a way to\nmake decisions. However, in a lot of cases, the arguments to carry out the\ndecision are very poor and the choice is made in an intuitive and hazardous\nway. The aim of our work is to offer a scientifically founded way to guide the\nengineer through tactical choices with the application of multicriteria methods\nin software development processes. This approach is illustrated with three\ncases: risks, use cases and tools within Rational Unified Process."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.3306v1", 
    "title": "Software Engineering Education by Example", 
    "arxiv-id": "0911.3306v1", 
    "author": "Pascal Urso", 
    "publish": "2009-11-17T13:36:59Z", 
    "summary": "Based on the old but famous distinction between \"in the small\" and \"in the\nlarge\" software development, at Nancy Universit\\'e, UHP Nancy 1, we experience\nfor a while software engineering education thanks to actual project\nengineering. This education method has the merit to enable students to discover\nand to overcome actual problems when faced to a large project which may be\nconducted by a large development team. The mode of education is a simulation of\nan actual software engineering project as encountered in \"real life\\'e\"\nactivities."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0911.3784v1", 
    "title": "Continuous Verification of Large Embedded Software using SMT-Based   Bounded Model Checking", 
    "arxiv-id": "0911.3784v1", 
    "author": "Joao Marques-Silva", 
    "publish": "2009-11-19T12:48:54Z", 
    "summary": "The complexity of software in embedded systems has increased significantly\nover the last years so that software verification now plays an important role\nin ensuring the overall product quality. In this context, SAT-based bounded\nmodel checking has been successfully applied to discover subtle errors, but for\nlarger applications, it often suffers from the state space explosion problem.\nThis paper describes a new approach called continuous verification to detect\ndesign errors as quickly as possible by looking at the Software Configuration\nManagement (SCM) system and by combining dynamic and static verification to\nreduce the state space to be explored. We also give a set of encodings that\nprovide accurate support for program verification and use different background\ntheories in order to improve scalability and precision in a completely\nautomatic way. A case study from the telecommunications domain shows that the\nproposed approach improves the error-detection capability and reduces the\noverall verification time by up to 50%."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0912.0473v1", 
    "title": "Inferring Information from Feature Diagrams to Product Line Economic   Models", 
    "arxiv-id": "0912.0473v1", 
    "author": "Jose Antonio Cerrada", 
    "publish": "2009-12-02T17:40:48Z", 
    "summary": "Existing economic models support the estimation of the costs and benefits of\ndeveloping and evolving a Software Product Line (SPL) as compared to\nundertaking traditional software development approaches. In addition, Feature\nDiagrams (FDs) are a valuable tool to scope the domain of a SPL. This paper\nproposes an algorithm to calculate, from a FD, the following information for\neconomic models: the total number of products of a SPL, the SPL homogeneity and\nthe commonality of the SPL requirements. The algorithm running time belongs to\nthe complexity class $O(f^42^c)$. In contrast to related work, the algorithm is\nfree of dependencies on off-the-self tools and is generally specified for an\nabstract FD notation, that works as a pivot language for most of the available\nnotations for feature modeling."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0912.0982v1", 
    "title": "Ethics Understanding of Software Professional In Risk Reducing   Reusability Coding Using Inclusion Set Theory", 
    "arxiv-id": "0912.0982v1", 
    "author": "Dr. A. Krishnan", 
    "publish": "2009-12-05T05:49:57Z", 
    "summary": "The technical skill or ability of an individual is different to person in\nsoftware developments of projects. So, it is necessary to identify the talent\nand attitude of an individual contribution can be uniformly distributed to the\ndifferent phases of software development cycle. The line of code analysis\nmetrics to understanding the various skills of the programmers in code\ndevelopment. By using the inclusion set theory of n (AUB) refer to strength and\nrisk free code developed from union of software professionals and system must\ncomprise of achievement of the system goal, effective memory utilization and\nintime delivery of the product."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0912.0983v1", 
    "title": "Architectural Design Activities for JAS", 
    "arxiv-id": "0912.0983v1", 
    "author": "Lena Khaled", 
    "publish": "2009-12-05T05:52:20Z", 
    "summary": "The critical part for building any software system is its architecture.\nArchitectural design is a design at a higher level of abstraction. A good\narchitecture ensures that software will satisfy its requirement. This paper\ndefines the most important activities of architectural design that used through\nbuilding any software; also it applies these activities on one type of\nElectronic Commerce (EC) applications that is Job Agency System(JAS) to show\nhow these activities can work through these types of applications."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0912.2301v1", 
    "title": "Fault Predictions in Object Oriented Software", 
    "arxiv-id": "0912.2301v1", 
    "author": "R Thushara", 
    "publish": "2009-12-11T18:10:27Z", 
    "summary": "The dynamic software development organizations optimize the usage of\nresources to deliver the products in the specified time with the fulfilled\nrequirements. This requires prevention or repairing of the faults as quick as\npossible. In this paper an approach for predicting the run-time errors in java\nis introduced. The paper is concerned with faults due to inheritance and\nviolation of java constraints. The proposed fault prediction model is designed\nto separate the faulty classes in the field of software testing. Separated\nfaulty classes are classified according to the fault occurring in the specific\nclass. The results are papered by clustering the faults in the class. This\nmodel can be used for predicting software reliability."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/0912.2320v1", 
    "title": "Identifying the Importance of Software Reuse in COCOMO81, COCOMOII", 
    "arxiv-id": "0912.2320v1", 
    "author": "G. SriRamGanesh", 
    "publish": "2009-12-11T19:01:23Z", 
    "summary": "Software project management is an interpolation of project planning, project\nmonitoring and project termination. The substratal goals of planning are to\nscout for the future, to diagnose the attributes that are essentially done for\nthe consummation of the project successfully, animate the scheduling and\nallocate resources for the attributes. Software cost estimation is a vital role\nin preeminent software project decisions such as resource allocation and\nbidding. This paper articulates the conventional overview of software cost\nestimation modus operandi available. The cost, effort estimates of software\nprojects done by the various companies are congregated, the results are\nsegregated with the present cost models and the MRE (Mean Relative Error) is\nenumerated. We have administered the historical data to COCOMO 81, COCOMOII\nmodel and identified that the stellar predicament is that no cost model gives\nthe exact estimate of a software project."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.0683v1", 
    "title": "Introducing Automated Regression Testing in Open Source Projects", 
    "arxiv-id": "1001.0683v1", 
    "author": "Christopher Oezbek", 
    "publish": "2010-01-05T11:50:23Z", 
    "summary": "To learn how to introduce automated regression testing to existing medium\nscale Open Source projects, a long-term field experiment was performed with the\nOpen Source project FreeCol. Results indicate that (1) introducing testing is\nboth beneficial for the project and feasible for an outside innovator, (2)\ntesting can enhance communication between developers, (3) signaling is\nimportant for engaging the project participants to fill a newly vacant position\nleft by a withdrawal of the innovator. Five prescriptive strategies are\nextracted for the innovator and two conjectures offered about the ability of an\nOpen Source project to learn about innovations."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.1970v1", 
    "title": "A Framework for Validation of Object Oriented Design Metrics", 
    "arxiv-id": "1001.1970v1", 
    "author": "M. Kumar", 
    "publish": "2010-01-12T18:26:55Z", 
    "summary": "A large number of metrics have been proposed for the quality of object\noriented software. Many of these metrics have not been properly validated due\nto poor methods of validation and non acceptance of metrics on scientific\ngrounds. In the literature, two types of validations namely internal\n(theoretical) and external (empirical) are recommended. In this study, the\nauthors have used both theoretical as well as empirical validation for\nvalidating already proposed set of metrics for the five quality factors. These\nmetrics were proposed by Kumar and Soni."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.2386v1", 
    "title": "Towards Improving the Mental Model of Software Developers through   Cartographic Visualization", 
    "arxiv-id": "1001.2386v1", 
    "author": "Oscar Nierstrasz", 
    "publish": "2010-01-14T08:36:43Z", 
    "summary": "Software is intangible and knowledge about software systems is typically\ntacit. The mental model of software developers is thus an important factor in\nsoftware engineering. It is our vision that developers should be able to refer\nto code as being \"up in the north\", \"over in the west\", or \"down-under in the\nsouth\". We want to provide developers, and everyone else involved in software\ndevelopment, with a *shared*, spatial and stable mental model of their software\nproject. We aim to reinforce this by embedding a cartographic visualization in\nthe IDE (Integrated Development Environment). The visualization is always\nvisible in the bottom-left, similar to the GPS navigation device for car\ndrivers. For each development task, related information is displayed on the\nmap. In this paper we present CODEMAP, an eclipse plug-in, and report on\npreliminary results from an ongoing user study with professional developers and\nstudents."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.3473v1", 
    "title": "Software Metrics Evaluation Based on Entropy", 
    "arxiv-id": "1001.3473v1", 
    "author": "Kamakshi Prasad", 
    "publish": "2010-01-20T07:08:06Z", 
    "summary": "Software engineering activities in the Industry has come a long way with\nvarious improve- ments brought in various stages of the software development\nlife cycle. The complexity of modern software, the commercial constraints and\nthe expectation for high quality products demand the accurate fault prediction\nbased on OO design metrics in the class level in the early stages of software\ndevelopment. The object oriented class metrics are used as quality predictors\nin the entire OO software development life cycle even when a highly iterative,\nincremental model or agile software process is employed. Recent research has\nshown some of the OO design metrics are useful for predicting fault-proneness\nof classes. In this paper the empirical validation of a set of metrics proposed\nby Chidamber and Kemerer is performed to assess their ability in predicting the\nsoftware quality in terms of fault proneness and degradation. We have also\nproposed the design complexity of object-oriented software with Weighted\nMethods per Class metric (WMC-CK metric) expressed in terms of Shannon entropy,\nand error proneness."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.3489v1", 
    "title": "Applying MVC and PAC patterns in mobile applications", 
    "arxiv-id": "1001.3489v1", 
    "author": "D. Simic", 
    "publish": "2010-01-20T07:51:59Z", 
    "summary": "Additional requirements are set for mobile applications in relation to\napplications for desktop computers. These requirements primarily concern the\nsupport to different platforms on which such applications are performed, as\nwell as the requirement for providing more modalities of input/output\ninteraction. These requirements have influence on the user interface and\ntherefore it is needed to consider the usability of MVC (Model-View-Controller)\nand PAC (Presentation-Abstraction-Control) design patterns for the separation\nof the user interface tasks from the business logic, specifically in mobile\napplications. One of the questions is making certain choices of design patterns\nfor certain classes of mobile applications. When using these patterns the\npossibilities of user interface automatic transformation should be kept in\nmind. Although the MVC design pattern is widely used in mobile applications, it\nis not universal, especially in cases where there are requirements for\nheterogeneous multi-modal input-output interactions."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.3497v2", 
    "title": "Mapping of SOA and RUP: DOA as Case Study", 
    "arxiv-id": "1001.3497v2", 
    "author": "Shakeel Ahmad", 
    "publish": "2010-01-20T08:11:10Z", 
    "summary": "SOA (Service Oriented Architecture) is a new trend towards increasing the\nprofit margins in an organization due to incorporating business services to\nbusiness practices. Rational Unified Process (RUP) is a unified method planning\nform for large business applications that provides a language for describing\nmethod content and processes. The well defined mapping of SOA and RUP leads to\nsuccessful completion of RUP software projects to provide services to their\nusers. DOA (Digital Office Assistant) is a multi user SOA type application that\nprovides appropriate viewer for each user to assist him through services. In\nthis paper authors proposed the mapping strategy of SOA with RUP by considering\nDOA as case study."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.3539v1", 
    "title": "Multicore Applications in Real Time Systems", 
    "arxiv-id": "1001.3539v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-20T10:30:31Z", 
    "summary": "Microprocessor roadmaps clearly show a trend towards multiple core CPUs.\nModern operating systems already make use of these CPU architectures by\ndistributing tasks between processing cores thereby increasing system\nperformance. This review article highlights a brief introduction of what a\nmulticore system is, the various methods adopted to program these systems and\nalso the industrial application of these high speed systems."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-540-95891-8_2", 
    "link": "http://arxiv.org/pdf/1001.3552v1", 
    "title": "Effective Defect Prevention Approach in Software Process for Achieving   Better Quality Levels", 
    "arxiv-id": "1001.3552v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-20T11:14:59Z", 
    "summary": "Defect prevention is the most vital but habitually neglected facet of\nsoftware quality assurance in any project. If functional at all stages of\nsoftware development, it can condense the time, overheads and wherewithal\nentailed to engineer a high quality product. The key challenge of an IT\nindustry is to engineer a software product with minimum post deployment\ndefects. This effort is an analysis based on data obtained for five selected\nprojects from leading software companies of varying software production\ncompetence. The main aim of this paper is to provide information on various\nmethods and practices supporting defect detection and prevention leading to\nthriving software generation. The defect prevention technique unearths 99% of\ndefects. Inspection is found to be an essential technique in generating ideal\nsoftware generation in factories through enhanced methodologies of abetted and\nunaided inspection schedules. On an average 13 % to 15% of inspection and 25% -\n30% of testing out of whole project effort time is required for 99% - 99.75% of\ndefect elimination. A comparison of the end results for the five selected\nprojects between the companies is also brought about throwing light on the\npossibility of a particular company to position itself with an appropriate\ncomplementary ratio of inspection testing."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSPS.2009.163", 
    "link": "http://arxiv.org/pdf/1001.3555v1", 
    "title": "Estimation of Defect proneness Using Design complexity Measurements in   Object- Oriented Software", 
    "arxiv-id": "1001.3555v1", 
    "author": "V. Kamakshi Prasad", 
    "publish": "2010-01-20T11:28:26Z", 
    "summary": "Software engineering is continuously facing the challenges of growing\ncomplexity of software packages and increased level of data on defects and\ndrawbacks from software production process. This makes a clarion call for\ninventions and methods which can enable a more reusable, reliable, easily\nmaintainable and high quality software systems with deeper control on software\ngeneration process. Quality and productivity are indeed the two most important\nparameters for controlling any industrial process. Implementation of a\nsuccessful control system requires some means of measurement. Software metrics\nplay an important role in the management aspects of the software development\nprocess such as better planning, assessment of improvements, resource\nallocation and reduction of unpredictability. The process involving early\ndetection of potential problems, productivity evaluation and evaluating\nexternal quality factors such as reusability, maintainability, defect proneness\nand complexity are of utmost importance. Here we discuss the application of CK\nmetrics and estimation model to predict the external quality parameters for\noptimizing the design process and production process for desired levels of\nquality. Estimation of defect-proneness in object-oriented system at design\nlevel is developed using a novel methodology where models of relationship\nbetween CK metrics and defect-proneness index is achieved. A multifunctional\nestimation approach captures the correlation between CK metrics and defect\nproneness level of software modules."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSPS.2009.163", 
    "link": "http://arxiv.org/pdf/1001.3725v1", 
    "title": "Effectiveness Of Defect Prevention In I.T. For Product Development", 
    "arxiv-id": "1001.3725v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-21T05:38:46Z", 
    "summary": "Defect Prevention is the most critical but most neglected component of the\nsoftware quality assurance in any project. If applied at all stages of software\ndevelopment, it can reduce the time, cost and resources required to engineer a\nhigh quality product. Software inspection has proved to be the most effective\nand efficient technique enabling defect detection and prevention. Inspections\ncarried at all phases of software life cycle have proved to be most beneficial\nand value added to the attributes of the software. Work is an analysis based on\nthe data collected for three different projects from a leading product based\ncompany. The purpose of the paper is to show that 55% to 65% of total number of\ndefects occurs at design phase. Position of this paper also emphasizes the\nimportance of inspections at all phases of the product development life cycle\nin order to achieve the minimal post deployment defects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSPS.2009.163", 
    "link": "http://arxiv.org/pdf/1001.3734v1", 
    "title": "Software Components for Web Services", 
    "arxiv-id": "1001.3734v1", 
    "author": "R. Selvarani", 
    "publish": "2010-01-21T06:56:10Z", 
    "summary": "Service-oriented computing has emerged as the new area to address software as\na service. This paper proposes a model for component based development for\nservice-oriented systems and have created best practice guidelines on software\ncomponent design."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSPS.2009.163", 
    "link": "http://arxiv.org/pdf/1001.3918v1", 
    "title": "Defect Prevention Approaches in Medium Scale it Enterprises", 
    "arxiv-id": "1001.3918v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-22T05:45:55Z", 
    "summary": "The software industry is successful, if it can draw the complete attention of\nthe customers towards it. This is achievable if the organization can produce a\nhigh quality product. To identify a product to be of high quality, it should be\nfree of defects, should be capable of producing expected results. It should be\ndelivered in an estimated cost, time and be maintainable with minimum effort.\nDefect Prevention is the most critical but often neglected component of the\nsoftware quality assurance in any project. If applied at all stages of software\ndevelopment, it can reduce the time, cost and resources required to engineer a\nhigh quality product."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.3919v1", 
    "title": "Mapping General System Characteristics to Non- Functional Requirements", 
    "arxiv-id": "1001.3919v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-22T06:09:51Z", 
    "summary": "The Function point analysis (FPA) method is the preferred scheme of\nestimation for project managers to determine the size, effort, schedule,\nresource loading and other such parameters. The FPA method by International\nFunction Point Users Group (IFPUG) has captured the critical implementation\nfeatures of an application through fourteen general system characteristics.\nHowever, Non- functional requirements (NFRs) such as functionality,\nreliability, efficiency, usability, maintainability, portability, etc. have not\nbeen included in the FPA estimation method. This paper discusses some of the\nNFRs and tries to determine a degree of influence for each of them. An attempt\nto factor the NFRs into estimation has been made. This approach needs to be\nvalidated with data collection and analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.4129v1", 
    "title": "Defect Prevention Approaches In Medium Scale It Enterprises", 
    "arxiv-id": "1001.4129v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-23T04:50:24Z", 
    "summary": "The software industry is successful, if it can draw the complete attention of\nthe customers towards it. This is achievable if the organization can produce a\nhigh quality product. To identify a product to be of high quality, it should be\nfree of defects, should be capable of producing expected results. It should be\ndelivered in an estimated cost, time and be maintainable with minimum effort."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.4192v1", 
    "title": "Review and Analysis of The Issues of Unified Modeling Language for   Visualizing, Specifying, Constructing and Documenting the Artifacts of a   Software-Intensive System", 
    "arxiv-id": "1001.4192v1", 
    "author": "S. S. Riaz Ahamed", 
    "publish": "2010-01-23T19:15:52Z", 
    "summary": "The UML allows us to specify models in a precise, complete and unambiguous\nmanner. In particular, the UML addresses the specification of all important\ndecisions regarding analysis, design and implementation. Although UML is not a\nvisual programming language, its models can be directly connected to a vast\nvariety of programming languages. This enables a dual approach to software\ndevelopment: the developer has a choice as to the means of input. UML can be\nused directly, from which code can be generated; or on the other hand, that\nwhich is best expressed as text can be entered into the program as code. In an\nideal world, the UML tool will be able to reverse-engineer any direct changes\nto code and the UML representations will be kept in sync with the code.\nHowever, without human intervention this is not always possible. There are\ncertain elements of information that are lost when moving from models to code.\nEven then, there are certain aspects of programming language code do seem to\npreserve more of their semantics and therefore permits automatic\nreverse-engineering of code back to a subset of the UML models."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.4193v1", 
    "title": "Studying the Feasibility and Importance of Software Testing: An Analysis", 
    "arxiv-id": "1001.4193v1", 
    "author": "S. S. Riaz Ahamed", 
    "publish": "2010-01-23T19:23:09Z", 
    "summary": "Software testing is a critical element of software quality assurance and\nrepresents the ultimate review of specification, design and coding. Software\ntesting is the process of testing the functionality and correctness of software\nby running it. Software testing is usually performed for one of two reasons:\ndefect detection, and reliability estimation. The problem of applying software\ntesting to defect detection is that software can only suggest the presence of\nflaws, not their absence (unless the testing is exhaustive). The problem of\napplying software testing to reliability estimation is that the input\ndistribution used for selecting test cases may be flawed. The key to software\ntesting is trying to find the modes of failure - something that requires\nexhaustively testing the code on all possible inputs. Software Testing,\ndepending on the testing method employed, can be implemented at any time in the\ndevelopment process."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.4220v2", 
    "title": "Modelling Variability for System Families", 
    "arxiv-id": "1001.4220v2", 
    "author": "Shamim H. Ripon", 
    "publish": "2010-01-24T02:51:46Z", 
    "summary": "In this paper, an approach to facilitate the treatment with variabilities in\nsystem families is presented by explicitly modelling variants. The proposed\nmethod of managing variability consists of a variant part, which models\nvariants and a decision table to depict the customisation decision regarding\neach variant. We have found that it is easy to implement and has advantage over\nother methods. We present this model as an integral part of modelling system\nfamilies."
},{
    "category": "cs.SE", 
    "doi": "10.1109/IADCC.2009.4809262", 
    "link": "http://arxiv.org/pdf/1001.4299v1", 
    "title": "Identification of Logical Errors through Monte-Carlo Simulation", 
    "arxiv-id": "1001.4299v1", 
    "author": "Lawrence I. Goldman", 
    "publish": "2010-01-25T02:02:33Z", 
    "summary": "The primary focus of Monte Carlo simulation is to identify and quantify risk\nrelated to uncertainty and variability in spreadsheet model inputs. The stress\nof Monte Carlo simulation often reveals logical errors in the underlying\nspreadsheet model that might be overlooked during day-to-day use or traditional\n\"what-if\" testing. This secondary benefit of simulation requires a trained eye\nto recognize warning signs of poor model construction."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1001.5310v1", 
    "title": "Proceedings Second Workshop on Formal Aspects of Virtual Organisations", 
    "arxiv-id": "1001.5310v1", 
    "author": "John Fitzgerald", 
    "publish": "2010-01-29T01:37:30Z", 
    "summary": "FAVO2009 was the second workshop on Formal Aspects of Virtual Organisations.\nThe purpose of the FAVO workshops is to encourage an active community of\nresearchers and practitioners using formal methods in the research and\ndevelopment of Virtual Organisations."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1001.5358v1", 
    "title": "Design of Run time Architectures for Real time UML Models an Actor   Centric Approach", 
    "arxiv-id": "1001.5358v1", 
    "author": "V. Kamakshi Prasad", 
    "publish": "2010-01-29T08:52:54Z", 
    "summary": "Although a lot of research has taken place in Object Oriented Design of\nsoftware for Real Time systems and mapping of design models to implementation\nmodels, these methodologies are applicable to systems which are less complex\nand small in source code size. However, in practice, the size of the software\nfor real time applications is growing. The run time architecture of real time\napplications is becoming increasingly complex. In this paper, we present a\ngeneric approach for mapping the design models to run time architectures\nresulting in combination of processes and threads. This method is applied in\ndevelopment of a communication subsystem of C4I complex and shall be presented\nas a case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.0154v1", 
    "title": "Usages et conception des TIC : Proposition d'un mod\u00e8le d'aide \u00e0 la   repr\u00e9sentation de probl\u00e8me de conception", 
    "arxiv-id": "1002.0154v1", 
    "author": "Pierre Humbert", 
    "publish": "2010-01-31T20:23:01Z", 
    "summary": "This paper considers economic intelligence contribution to exploit individual\nand collective images of change, in ICT design decision-making. Technical\ndevices meeting with real use situations often gives the opportunity to emerge\nmental images, that a innovation process, through its unprecedented nature, can\nnot anticipate. Although methodologies exists for quality and design project\nmanagement, the survey we conduct among small ICT publishers, show how they are\nnot very suitable for small firms. This elements taken into account, we try to\nbuild a proposition of exploration ? analyze ? sum up process, adapted to this\ntype of actors decisional process."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.0678v1", 
    "title": "FORMT: Form-based Mutation Testing of Logical Specifications", 
    "arxiv-id": "1002.0678v1", 
    "author": "Andreas Zinnen", 
    "publish": "2010-02-03T09:35:21Z", 
    "summary": "The draft paper defines a system, which is capable of maintaining bases of\ntest cases for logical specifications. The specifications, which are subject to\nthis system are transformed from their original shape in first-order logic to\nform-based expressions as originally introduced in logics of George\nSpencer-Brown. The innovation comes from the operations the system provides\nwhen injecting faults - so-called mutations - to the specifications. The system\npresented here applies to logical specifications from areas as different as\nprogramming, ontologies or hardware specifications."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.1005v1", 
    "title": "A Framework for Agile Development of Component-Based Applications", 
    "arxiv-id": "1002.1005v1", 
    "author": "Laurence Duchien", 
    "publish": "2010-02-04T14:52:44Z", 
    "summary": "Agile development processes and component-based software architectures are\ntwo software engineering approaches that contribute to enable the rapid\nbuilding and evolution of applications. Nevertheless, few approaches have\nproposed a framework to combine agile and component-based development, allowing\nan application to be tested throughout the entire development cycle. To address\nthis problematic, we have built CALICO, a model-based framework that allows\napplications to be safely developed in an iterative and incremental manner. The\nCALICO approach relies on the synchronization of a model view, which specifies\nthe application properties, and a runtime view, which contains the application\nin its execution context. Tests on the application specifications that require\nvalues only known at runtime, are automatically integrated by CALICO into the\nrunning application, and the captured needed values are reified at execution\ntime to resume the tests and inform the architect of potential problems. Any\nmodification at the model level that does not introduce new errors is\nautomatically propagated to the running system, allowing the safe evolution of\nthe application. In this paper, we illustrate the CALICO development process\nwith a concrete example and provide information on the current implementation\nof our framework."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.1188v1", 
    "title": "Framework for Visualizing Model-Driven Software Evolution and its   Application", 
    "arxiv-id": "1002.1188v1", 
    "author": "Karanam Madhavi", 
    "publish": "2010-02-05T10:35:59Z", 
    "summary": "Software Visualization encompasses the development and evaluation of methods\nfor graphically representing different aspects of methods of software,\nincluding its structure, execution and evolution. Creating visualizations helps\nthe user to better understand complex phenomena. It is also found by the\nsoftware engineering community that visualization is essential and important.\nIn order to visualize the evolution of the models in Model-Driven Software\nEvolution, authors have proposed a framework which consists of 7 key areas\n(views) and 22 key features for the assessment of Model Driven Software\nEvolution process and addresses a number of stakeholder concerns. The framework\nis derived by the application of the Goal Question Metric Paradigm. This paper\naims to describe an application of the framework by considering different\nvisualization tools/CASE tools which are used to visualize the models in\ndifferent views and to capture the information of models during their\nevolution. Comparison of such tools is also possible by using the framework."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.1199v1", 
    "title": "Reliable Mining of Automatically Generated Test Cases from Software   Requirements Specification (SRS)", 
    "arxiv-id": "1002.1199v1", 
    "author": "G. V. Uma", 
    "publish": "2010-02-05T11:07:46Z", 
    "summary": "Writing requirements is a two-way process. In this paper we use to classify\nFunctional Requirements (FR) and Non Functional Requirements (NFR) statements\nfrom Software Requirements Specification (SRS) documents. This is\nsystematically transformed into state charts considering all relevant\ninformation. The current paper outlines how test cases can be automatically\ngenerated from these state charts. The application of the states yields the\ndifferent test cases as solutions to a planning problem. The test cases can be\nused for automated or manual software testing on system level. And also the\npaper presents a method for reduction of test suite by using mining methods\nthereby facilitating the mining and knowledge extraction from test cases."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.1692v1", 
    "title": "The Importance Analysis of Use Case Map with Markov Chains", 
    "arxiv-id": "1002.1692v1", 
    "author": "Lee Sub Lee", 
    "publish": "2010-02-08T19:25:52Z", 
    "summary": "UCMs (Use Case Maps) model describes functional requirements and high-level\ndesigns with causal paths superimposed on a structure of components. It could\nprovide useful resources for software acceptance testing. However until now\nstatistical testing technologies for large scale software is not considered yet\nin UCMs model. Thus if one applies UCMs model to a large scale software using\ntraditional coverage based exhaustive tasting, then it requires too much costs\nfor the quality assurance. Therefore this paper proposes an importance analysis\nof UCMs model with Markov chains. With this approach not only highly frequently\nused usage scenarios but also important objects such as components,\nresponsibilities, stubs and plugins can also be identified from UCMs\nspecifications. Therefore careful analysis, design, implementation and\nefficient testing could be possible with the importance of scenarios and\nobjects during the full software life cycle. Consequently product reliability\ncan be obtained with low costs. This paper includes an importance analysis\nmethod that identifies important scenarios and objects and a case study to\nillustrate the applicability of the proposed approach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.2197v1", 
    "title": "Test Case Generation using Mutation Operators and Fault Classification", 
    "arxiv-id": "1002.2197v1", 
    "author": "Dr. Antony Selvadoss Thanamani", 
    "publish": "2010-02-10T20:08:36Z", 
    "summary": "Software testing is the important phase of software development process. But,\nthis phase can be easily missed by software developers because of their limited\ntime to complete the project. Since, software developers finish their software\nnearer to the delivery time; they dont get enough time to test their program by\ncreating effective test cases. . One of the major difficulties in software\ntesting is the generation of test cases that satisfy the given adequacy\ncriterion Moreover, creating manual test cases is a tedious work for software\ndevelopers in the final rush hours. A new approach which generates test cases\ncan help the software developers to create test cases from software\nspecifications in early stage of software development (before coding) and as\nwell as from program execution traces from after software development (after\ncoding). Heuristic techniques can be applied for creating quality test cases.\nMutation testing is a technique for testing software units that has great\npotential for improving the quality of testing, and to assure the high\nreliability of software. In this paper, a mutation testing based test cases\ngeneration technique has been proposed to generate test cases from program\nexecution trace, so that the test cases can be generated after coding. The\npaper details about the mutation testing implementation to generate test cases.\nThe proposed algorithm has been demonstrated for an example."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.2686v1", 
    "title": "Performance Analysis of View Maintenance Techniques for DW", 
    "arxiv-id": "1002.2686v1", 
    "author": "R. Selvarani", 
    "publish": "2010-02-13T06:40:19Z", 
    "summary": "A Data Warehouse stores integrated information as materialized views over\ndata from one or more remote sources. These materialized views must be\nmaintained in response to actual relation updates in the remote sources. The\ndata warehouse view maintenance techniques are classified into four major\ncategories self maintainable recomputation, not self maintainable\nrecomputation, self maintainable incremental maintenance, and not self\nmaintainable incremental maintenance. This paper provides a comprehensive\ncomparison of the techniques in these four categories in terms of the data\nwarehouse space usage and number of rows accessed in order to propagate an\nupdate from a remote data source to a target materialized view in the data\nwarehouse."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.2829v1", 
    "title": "Dynamic Cognitive Process Application of Blooms Taxonomy for Complex   Software Design in the Cognitive Domain", 
    "arxiv-id": "1002.2829v1", 
    "author": "R Selvarani", 
    "publish": "2010-02-15T08:30:48Z", 
    "summary": "Software design in Software Engineering is a critical and dynamic cognitive\nprocess. Accurate and flawless system design will lead to fast coding and early\ncompletion of a software project. Blooms taxonomy classifies cognitive domain\ninto six dynamic levels such as Knowledge at base level to Comprehension,\nApplication, Analysis, Synthesis and Evaluation at the highest level in the\norder of increasing complexity. A case study indicated in this paper is a gira\nsystem, which is a gprs based Intranet Remote Administration which monitors and\ncontrols the intranet from a mobile device. This paper investigates from this\ncase study that the System Design stage in Software Engineering uses all the\nsix levels of Blooms Taxonomy. The application of the highest levels of Blooms\nTaxonomy such as Synthesis and Evaluation in the design of gira indicates that\nSoftware Design in Software Development Life Cycle is a complex and critical\ncognitive process."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.3008v1", 
    "title": "Cognitive Process of Comprehension in Requirement Analysis in IT   Applications", 
    "arxiv-id": "1002.3008v1", 
    "author": "R. Selvarani", 
    "publish": "2010-02-16T06:19:35Z", 
    "summary": "Requirement Analysis is an important phase in software development which\ndeals with understanding the customers requirements. It includes the collection\nof information from the customer, which is regarding the customers requirements\nand what he expects from the software which is to be developed. By doing so,\nyou can have a better understanding of what the customer actually needs and\nhence can deliver the output as per the customers requirements. Studies are\nbeing carried out to bring about improvements in the process of requirement\nanalysis so that errors in software development could be minimized and hence\nimproved and reliable products could be delivered."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.3015v1", 
    "title": "GPRS Based Intranet Remote Administration GIRA", 
    "arxiv-id": "1002.3015v1", 
    "author": "Pushpavathi T. P", 
    "publish": "2010-02-16T06:53:21Z", 
    "summary": "In a world of increasing mobility, there is a growing need for people to\ncommunicate with each other and have timely access to information regardless of\nthe location of the individuals or the information. With the advent of moblle\ntechnology, the way of communication has changed. The gira system is basically\na mobile phone technology service. In this paper we discuss about a novel local\narea network control system called gprs based Intranet Remote Administration\ngira. This system finds application in a mobile handset. With this system, a\nnetwork administrator will have an effective remote control over the network.\ngira system is developed using gprs, gcf Generic Connection Framework of j2me,\nsockets and rmi technologies"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.3031v1", 
    "title": "Quantifying the Deign Quality of Object Oriented System The metric based   rules and heuristic", 
    "arxiv-id": "1002.3031v1", 
    "author": "Kamakshi Prasad", 
    "publish": "2010-02-16T08:37:49Z", 
    "summary": "The design structure of OO software has decisive impact on its quality. The\ndesign must be strongly correlated with quality characteristics like\nanalyzability, changeability, stability and testability, which are important\nfor maintaining the system. But due to the diversity and complexity of the\ndesign properties of OO system e.g. Polymorphism, encapsulation, coupling it\nbecomes cumbersome."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.3711v1", 
    "title": "Theory of Regulatory Compliance for Requirements Engineering", 
    "arxiv-id": "1002.3711v1", 
    "author": "Angelo Susi", 
    "publish": "2010-02-19T10:55:39Z", 
    "summary": "Regulatory compliance is increasingly being addressed in the practice of\nrequirements engineering as a main stream concern. This paper points out a gap\nin the theoretical foundations of regulatory compliance, and presents a theory\nthat states (i) what it means for requirements to be compliant, (ii) the\ncompliance problem, i.e., the problem that the engineer should resolve in order\nto verify whether requirements are compliant, and (iii) testable hypotheses\n(predictions) about how compliance of requirements is verified. The theory is\ninstantiated by presenting a requirements engineering framework that implements\nits principles, and is exemplified on a real-world case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1002.3996v2", 
    "title": "Improvement in RUP Project Management via Service Monitoring: Best   Practice of SOA", 
    "arxiv-id": "1002.3996v2", 
    "author": "Arjamand Bano", 
    "publish": "2010-02-21T19:23:17Z", 
    "summary": "Management of project planning, monitoring, scheduling, estimation and risk\nmanagement are critical issues faced by a project manager during development\nlife cycle of software. In RUP, project management is considered as core\ndiscipline whose activities are carried in all phases during development of\nsoftware products. On other side service monitoring is considered as best\npractice of SOA which leads to availability, auditing, debugging and tracing\nprocess. In this paper, authors define a strategy to incorporate the service\nmonitoring of SOA into RUP to improve the artifacts of project management\nactivities. Moreover, the authors define the rules to implement the features of\nservice monitoring, which help the project manager to carry on activities in\nwell define manner. Proposed frame work is implemented on RB (Resuming Bank)\napplication and obtained improved results on PM (Project Management) work."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1003.1456v1", 
    "title": "A Methodology for Empirical Quality Assessment of Object-Oriented Design", 
    "arxiv-id": "1003.1456v1", 
    "author": "M. Kumar", 
    "publish": "2010-03-07T11:49:16Z", 
    "summary": "The direct measurement of quality is difficult because there is no way we can\nmeasure quality factors. For measuring these factors, we have to express them\nin terms of metrics or models. Researchers have developed quality models that\nattempt to measure quality in terms of attributes, characteristics and metrics.\nIn this work we have proposed the methodology of controlled experimentation\ncoupled with power of Logical Scoring of Preferences to evaluate global quality\nof four object-oriented designs."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.16", 
    "link": "http://arxiv.org/pdf/1003.1476v1", 
    "title": "Concurrent Approach to Flynn's SPMD Classification through Java", 
    "arxiv-id": "1003.1476v1", 
    "author": "Bala Dhandayuthapani Veerasamy", 
    "publish": "2010-03-07T14:14:30Z", 
    "summary": "Parallel programming models exist as an abstraction of hardware and memory\narchitectures. There are several parallel programming models in commonly use;\nthey are shared memory model, thread model, message passing model, data\nparallel model, hybrid model, Flynn's models, embarrassingly parallel\ncomputations model, pipelined computations model. These models are not specific\nto a particular type of machine or memory architecture. This paper focuses the\nconcurrent approach to Flynn's SPMD classification in single processing\nenvironment through java program."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.2", 
    "link": "http://arxiv.org/pdf/1003.1682v1", 
    "title": "An Entry Point for Formal Methods: Specification and Analysis of Event   Logs", 
    "arxiv-id": "1003.1682v1", 
    "author": "Margaret Smith", 
    "publish": "2010-03-08T17:22:31Z", 
    "summary": "Formal specification languages have long languished, due to the grave\nscalability problems faced by complete verification methods. Runtime\nverification promises to use formal specifications to automate part of the more\nscalable art of testing, but has not been widely applied to real systems, and\noften falters due to the cost and complexity of instrumentation for online\nmonitoring. In this paper we discuss work in progress to apply an event-based\nspecification system to the logging mechanism of the Mars Science Laboratory\nmission at JPL. By focusing on log analysis, we exploit the \"instrumentation\"\nalready implemented and required for communicating with the spacecraft. We\nargue that this work both shows a practical method for using formal\nspecifications in testing and opens interesting research avenues, including a\nchallenging specification learning problem."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.7", 
    "link": "http://arxiv.org/pdf/1003.1741v2", 
    "title": "Formalization and Validation of Safety-Critical Requirements", 
    "arxiv-id": "1003.1741v2", 
    "author": "Stefano Tonetta", 
    "publish": "2010-03-08T22:05:18Z", 
    "summary": "The validation of requirements is a fundamental step in the development\nprocess of safety-critical systems. In safety critical applications such as\naerospace, avionics and railways, the use of formal methods is of paramount\nimportance both for requirements and for design validation. Nevertheless, while\nfor the verification of the design, many formal techniques have been conceived\nand applied, the research on formal methods for requirements validation is not\nyet mature. The main obstacles are that, on the one hand, the correctness of\nrequirements is not formally defined; on the other hand that the formalization\nand the validation of the requirements usually demands a strong involvement of\ndomain experts. We report on a methodology and a series of techniques that we\ndeveloped for the formalization and validation of high-level requirements for\nsafety-critical applications. The main ingredients are a very expressive formal\nlanguage and automatic satisfiability procedures. The language combines\nfirst-order, temporal, and hybrid logic. The satisfiability procedures are\nbased on model checking and satisfiability modulo theory. We applied this\ntechnology within an industrial project to the validation of railways\nrequirements."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.7", 
    "link": "http://arxiv.org/pdf/1003.3330v1", 
    "title": "Cloud Process Execution Engine - Evaluation of the Core Concepts", 
    "arxiv-id": "1003.3330v1", 
    "author": "Erich Schikuta", 
    "publish": "2010-03-17T09:25:21Z", 
    "summary": "In this technical report we describe describe the Domain Specific Language\n(DSL) of the Workflow Execution Execution (WEE). Instead of interpreting an XML\nbased workflow description language like BPEL, the WEE uses a minimized but\nexpressive set of statements that runs directly on to of a virtual machine that\nsupports the Ruby language.Frameworks/Virtual Machines supporting supporting\nthis language include Java, .NET and there exists also a standalone Virtual\nMachine. Using a DSL gives us the advantage of maintaining a very compact code\nbase of under 400 lines of code, as the host programming language implements\nall the concepts like parallelism, threads, checking for syntactic correctness.\nThe implementation just hooks into existing statements to keep track of the\nworkflow and deliver information about current existing context variables and\nstate to the environment that embeds WEE."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.7", 
    "link": "http://arxiv.org/pdf/1003.3338v1", 
    "title": "An Algebraic Formalization of the GoF Design Patterns", 
    "arxiv-id": "1003.3338v1", 
    "author": "Juan de Lara", 
    "publish": "2010-03-17T09:58:33Z", 
    "summary": "This document reports on the use of an algebraic, visual, formal approach to\nthe specification of patterns for the formalization of the GoF design patterns.\nThe approach is based on graphs, morphisms and operations from category theory\nand exploits triple graphs to annotate model elements with pattern roles. Being\nbased on category theory, the approach can be applied to formalize patterns in\ndifferent domains. Novel in our proposal is the possibility of describing\n(nested) variable submodels, inter-pattern synchronization across several\ndiagrams (e.g. class and sequence diagrams for UML design patterns), pattern\ncomposition, and conflict analysis."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.7", 
    "link": "http://arxiv.org/pdf/1003.3880v2", 
    "title": "Lessons from the Failure and Subsequent Success of a Complex Healthcare   Sector IT Project", 
    "arxiv-id": "1003.3880v2", 
    "author": "Ian Sommerville", 
    "publish": "2010-03-19T20:09:53Z", 
    "summary": "This paper argues that IT failures diagnosed as errors at the technical or\nproject management level are often mistakenly pointing to symptoms of failure\nrather than a project's underlying socio-complexity (complexity resulting from\nthe interactions of people and groups) which is usually the actual source of\nfailure. We propose a novel method, Stakeholder Impact Analysis, that can be\nused to identify risks associated with socio-complexity as it is grounded in\ninsights from the social sciences, psychology and management science. This\npaper demonstrates the effectiveness of Stakeholder Impact Analysis by using\nthe 1992 London Ambulance Service Computer Aided Dispatch project as a case\nstudy, and shows that had our method been used to identify the risks and had\nthey been mitigated, it would have reduced the risk of project failure. This\npaper's original contribution comprises expanding upon existing accounts of\nfailure by examining failures at a level of granularity not seen elsewhere that\nenables the underlying socio-complexity sources of risk to be identified."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.20.7", 
    "link": "http://arxiv.org/pdf/1003.4077v2", 
    "title": "Mapping The Best Practices of XP and Project Management: Well defined   approach for Project Manager", 
    "arxiv-id": "1003.4077v2", 
    "author": "Shakeel Ahmad", 
    "publish": "2010-03-22T06:27:17Z", 
    "summary": "Software engineering is one of the most recent additions in various\ndisciplines of system engineering. It has emerged as a key obedience of system\nengineering in a quick succession of time. Various Software Engineering\napproaches are followed in order to produce comprehensive software solutions of\naffordable cost with reasonable delivery timeframe with less uncertainty. All\nthese objectives are only satisfied when project's status is properly monitored\nand controlled; eXtreme Programming (XP) uses the best practices of AGILE\nmethodology and helps in development of small size software very sharply. In\nthis paper, authors proposed that via XP, high quality software with less\nuncertainty and under estimated cost can be developed due to proper monitoring\nand controlling of project. Moreover, authors give guidelines that how\nactivities of project management can be embedded into development life cycle of\nXP to enhance the quality of software products and reduce the uncertainty."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1003.5777v1", 
    "title": "Specifying Reusable Components", 
    "arxiv-id": "1003.5777v1", 
    "author": "Bertrand Meyer", 
    "publish": "2010-03-30T09:57:26Z", 
    "summary": "Reusable software components need expressive specifications. This paper\noutlines a rigorous foundation to model-based contracts, a method to equip\nclasses with strong contracts that support accurate design, implementation, and\nformal verification of reusable components. Model-based contracts\nconservatively extend the classic Design by Contract with a notion of model,\nwhich underpins the precise definitions of such concepts as abstract\nequivalence and specification completeness. Experiments applying model-based\ncontracts to libraries of data structures suggest that the method enables\naccurate specification of practical software."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.0604v1", 
    "title": "On Generation of Firewall Log Status Reporter (SRr) Using Perl", 
    "arxiv-id": "1004.0604v1", 
    "author": "Tzusheng Pei", 
    "publish": "2010-04-05T09:52:05Z", 
    "summary": "Computer System Administration and Network Administration are few such areas\nwhere Practical Extraction Reporting Language (Perl) has robust utilization\nthese days apart from Bioinformatics. The key role of a System/Network\nAdministrator is to monitor log files. Log file are updated every day. To scan\nthe summary of large log files and to quickly determine if there is anything\nwrong with the server or network we develop a Firewall Log Status Reporter\n(SRr). SRr helps to generate the reports based on the parameters of interest.\nSRr provides the facility to admin to generate the individual firewall report\nor all reports in one go. By scrutinizing the results of the reports admin can\ntrace how many times a particular request has been made from which source to\nwhich destination and can track the errors easily. Perl scripts can be seen as\nthe UNIX script replacement in future arena and SRr is one development with the\nsame hope that we can believe in. SRr is a generalized and customizable utility\ncompletely written in Perl and may be used for text mining and data mining\napplication in Bioinformatics research and development too."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.1239v1", 
    "title": "Analysis of Empirical Software Effort Estimation Models", 
    "arxiv-id": "1004.1239v1", 
    "author": "Dhavachelvan Ponnurangam", 
    "publish": "2010-04-08T03:58:30Z", 
    "summary": "Reliable effort estimation remains an ongoing challenge to software\nengineers. Accurate effort estimation is the state of art of software\nengineering, effort estimation of software is the preliminary phase between the\nclient and the business enterprise. The relationship between the client and the\nbusiness enterprise begins with the estimation of the software. The credibility\nof the client to the business enterprise increases with the accurate\nestimation. Effort estimation often requires generalizing from a small number\nof historical projects. Generalization from such limited experience is an\ninherently under constrained problem. Accurate estimation is a complex process\nbecause it can be visualized as software effort prediction, as the term\nindicates prediction never becomes an actual. This work follows the basics of\nthe empirical software effort estimation models. The goal of this paper is to\nstudy the empirical software effort estimation. The primary conclusion is that\nno single technique is best for all situations, and that a careful comparison\nof the results of several approaches is most likely to produce realistic\nestimates."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.1708v1", 
    "title": "Mathematical Principles in Software Quality Engineering", 
    "arxiv-id": "1004.1708v1", 
    "author": "Rakesh. L", 
    "publish": "2010-04-10T11:17:35Z", 
    "summary": "Mathematics has many useful properties for developing of complex software\nsystems. One is that it can exactly describe a physical situation of the object\nor outcome of an action. Mathematics support abstraction and this is an\nexcellent medium for modeling, since it is an exact medium there is a little\npossibility of ambiguity. This paper demonstrates that mathematics provides a\nhigh level of validation when it is used as a software medium. It also outlines\ndistinguishing characteristics of structural testing which is based on the\nsource code of the program tested. Structural testing methods are very amenable\nto rigorous definition, mathematical analysis and precise measurement. Finally,\nit also discusses functional and structural testing debate to have a sense of\ncomplete testing. Any program can be considered to be a function in the sense\nthat program input forms its domain and program outputs form its range. In\ngeneral discrete mathematics is more applicable to functional testing, while\ngraph theory pertains more to structural testing."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.1773v1", 
    "title": "A Model of Cloud Based Application Environment for Software Testing", 
    "arxiv-id": "1004.1773v1", 
    "author": "R. Baskaran", 
    "publish": "2010-04-11T08:14:56Z", 
    "summary": "Cloud computing is an emerging platform of service computing designed for\nswift and dynamic delivery of assured computing resources. Cloud computing\nprovide Service-Level Agreements (SLAs) for guaranteed uptime availability for\nenabling convenient and on-demand network access to the distributed and shared\ncomputing resources. Though the cloud computing paradigm holds its potential\nstatus in the field of distributed computing, cloud platforms are not yet to\nthe attention of majority of the researchers and practitioners. More\nspecifically, still the researchers and practitioners community has fragmented\nand imperfect knowledge on cloud computing principles and techniques. In this\ncontext, one of the primary motivations of the work presented in this paper is\nto reveal the versatile merits of cloud computing paradigm and hence the\nobjective of this work is defined to bring out the remarkable significances of\ncloud computing paradigm through an application environment. In this work, a\ncloud computing model for software testing is developed."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.2178v1", 
    "title": "G\u00e9n\u00e9Syst : G\u00e9n\u00e9ration d'un syst\u00e8me de transitions   \u00e9tiquet\u00e9es \u00e0 partir d'une sp\u00e9cification B \u00e9v\u00e9nementiel", 
    "arxiv-id": "1004.2178v1", 
    "author": "Nicolas Stouls", 
    "publish": "2010-04-13T13:25:12Z", 
    "summary": "The most expensive source of errors and the more difficult to detect in a\nformal development is the error during specification. Hence, the first step in\na formal development usually consists in exhibiting the set of all behaviors of\nthe specification, for instance with an automaton. Starting from this\nobservation, many researches are about the generation of a B machine from a\nbehavioral specification, such as UML. However, no backward verification are\ndone. This is why, we propose the GeneSyst tool, which aims at generating an\nautomaton describing at least all behaviors of the specification. The\nrefinement step is considered and appears as sub-automatons in the produced\nSLTS."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.2889v1", 
    "title": "Code forking in open-source software: a requirements perspective", 
    "arxiv-id": "1004.2889v1", 
    "author": "John Mylopoulos", 
    "publish": "2010-04-16T17:24:33Z", 
    "summary": "To fork a project is to copy the existing code base and move in a direction\ndifferent than that of the erstwhile project leadership. Forking provides a\nrapid way to address new requirements by adapting an existing solution.\nHowever, it can also create a plethora of similar tools, and fragment the\ndeveloper community. Hence, it is not always clear whether forking is the right\nstrategy. In this paper, we describe a mixed-methods exploratory case study\nthat investigated the process of forking a project. The study concerned the\nforking of an open-source tool for managing software projects, Trac. Trac was\nforked to address differing requirements in an academic setting. The paper\nmakes two contributions to our understanding of code forking. First, our\nexploratory study generated several theories about code forking in open source\nprojects, for further research. Second, we investigated one of these theories\nin depth, via a quantitative study. We conjectured that the features of the OSS\nforking process would allow new requirements to be addressed. We show that the\nforking process in this case was successful at fulfilling the new projects\nrequirements."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.3258v1", 
    "title": "Multiple Criteria Decision-Making Preprocessing Using Data Mining Tools", 
    "arxiv-id": "1004.3258v1", 
    "author": "A. Mosavi", 
    "publish": "2010-04-19T17:53:38Z", 
    "summary": "Real-life engineering optimization problems need Multiobjective Optimization\n(MOO) tools. These problems are highly nonlinear. As the process of Multiple\nCriteria Decision-Making (MCDM) is much expanded most MOO problems in different\ndisciplines can be classified on the basis of it. Thus MCDM methods have gained\nwide popularity in different sciences and applications. Meanwhile the\nincreasing number of involved components, variables, parameters, constraints\nand objectives in the process, has made the process very complicated. However\nthe new generation of MOO tools has made the optimization process more\nautomated, but still initializing the process and setting the initial value of\nsimulation tools and also identifying the effective input variables and\nobjectives in order to reach the smaller design space are still complicated. In\nthis situation adding a preprocessing step into the MCDM procedure could make a\nhuge difference in terms of organizing the input variables according to their\neffects on the optimization objectives of the system. The aim of this paper is\nto introduce the classification task of data mining as an effective option for\nidentifying the most effective variables of the MCDM systems. To evaluate the\neffectiveness of the proposed method an example has been given for 3D wing\ndesign."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.3263v1", 
    "title": "DRMS Co-design by F4MS", 
    "arxiv-id": "1004.3263v1", 
    "author": "Mohsine Eleuldj", 
    "publish": "2010-04-19T18:01:45Z", 
    "summary": "In this paper, we present Digital Rights Management systems (DRMS) which are\nbecoming more and more complex due to technology revolution in relation with\ntelecommunication networks, multimedia applications and the reading equipments\n(Mobile Phone, IPhone, PDA, DVD Player,..). The complexity of the DRMS,\ninvolves the use of new tools and methodologies that support software\ncomponents and hardware components coupled design. The traditional systems\ndesign approach has been somewhat hardware first in that the software\ncomponents are designed after the hardware has been designed and prototyped.\nThis leaves little flexibility in evaluating different design options and\nhardware-software mappings. The key of codesign is to avoid isolation between\nhardware and software designs to proceed in parallel, with feedback and\ninteraction between the two as the design progresses, in order to achieve high\nquality designs with a reduced design time. In this paper, we present the F4MS\n(Framework for Mixed Systems) which is a unified framework for software and\nhardware design environment, simulation and aided execution of mixed systems.\nTo illustrate this work we propose an implementation of DRMS business model\nbased on F4MS framework."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.3270v1", 
    "title": "Optimized Fuzzy Logic Based Framework for Effort Estimation in Software   Development", 
    "arxiv-id": "1004.3270v1", 
    "author": "Harsh Kumar Verma", 
    "publish": "2010-04-19T18:12:32Z", 
    "summary": "Software effort estimation at early stages of project development holds great\nsignificance for the industry to meet the competitive demands of today's world.\nAccuracy, reliability and precision in the estimates of effort are quite\ndesirable. The inherent imprecision present in the inputs of the algorithmic\nmodels like Constructive Cost Model (COCOMO) yields imprecision in the output,\nresulting in erroneous effort estimation. Fuzzy logic based cost estimation\nmodels are inherently suitable to address the vagueness and imprecision in the\ninputs, to make reliable and accurate estimates of effort. In this paper, we\npresent an optimized fuzzy logic based framework for software development\neffort prediction. The said framework tolerates imprecision, incorporates\nexperts knowledge, explains prediction rationale through rules, offers\ntransparency in the prediction system, and could adapt to changing environments\nwith the availability of new data. The traditional cost estimation model COCOMO\nis extended in the proposed study by incorporating the concept of fuzziness\ninto the measurements of size, mode of development for projects and the cost\ndrivers contributing to the overall development effort."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.3277v1", 
    "title": "A Pedagogical Evaluation and Discussion about the Lack of Cohesion in   Method (LCOM) Metric Using Field Experiment", 
    "arxiv-id": "1004.3277v1", 
    "author": "Ezekiel Okike", 
    "publish": "2010-04-19T18:30:52Z", 
    "summary": "Chidamber and Kemerer first defined a cohesion measure for object-oriented\nsoftware - the Lack of Cohesion in Methods (LCOM) metric. This paper presents a\npedagogic evaluation and discussion about the LCOM metric using field data from\nthree industrial systems. System 1 has 34 classes, System 2 has 383 classes and\nSystem 3 has 1055 classes. The main objectives of the study were to determine\nif the LCOM metric was appropriate in the measurement of class cohesion and the\ndetermination of properly and improperly designed classes in the studied\nsystems. Chidamber and Kemerer's suite of metric was used as metric tool.\nDescriptive statistics was used to analyze results. The result of the study\nshowed that in System 1, 78.8% (26 classes) were cohesive; System 2 54% (207\nclasses) were cohesive; System 3 30% (317 classes) were cohesive. We suggest\nthat the LCOM metric measures class cohesiveness and was appropriate in the\ndetermination of properly and improperly designed classes in the studied\nsystem."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.3640v1", 
    "title": "An Object-Oriented Metamodel for Bunge-Wand-Weber Ontology", 
    "arxiv-id": "1004.3640v1", 
    "author": "Rushikesh K. Joshi", 
    "publish": "2010-04-21T07:34:19Z", 
    "summary": "A UML based metamodel for Bunge-Wand-Weber (BWW) ontology is presented. BWW\nontology is a generic framework for analysis and conceptualization of real\nworld objects. It includes categories that can be applied to analyze and\nclassify objects found in an information system. In the context of BWW\nontology, the metamodel is a representation of the ontological categories and\nrelationships among them. An objective behind developing an object-oriented\nmetamodel has been to model BWW ontology in terms of widely used notions in\nsoftware development. The main contributions of this paper are a classification\nfor ontological categories, a description template, and representations through\nUML and typed based models."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.4447v1", 
    "title": "Maintainability Estimation Model for Object-Oriented Software in Design   Phase (MEMOOD)", 
    "arxiv-id": "1004.4447v1", 
    "author": "R. A. Khan", 
    "publish": "2010-04-26T09:30:32Z", 
    "summary": "Measuring software maintainability early in the development life cycle,\nespecially at the design phase, may help designers to incorporate required\nenhancement and corrections for improving maintainability of the final\nsoftware. This paper developed a multivariate linear model 'Maintainability\nEstimation Model for Object-Oriented software in Design phase' (MEMOOD), which\nestimates the maintainability of class diagrams in terms of their\nunderstandability and modifiability. While, in order to quantify class\ndiagram's understandability and modifiability the paper further developed two\nmore multivariate models. These two models use design level object-oriented\nmetrics, to quantify understandability and modifiability of class diagram. Such\nearly quantification of maintainability provides an opportunity to improve the\nmaintainability of class diagram and consequently the maintainability of final\nsoftware. All the three models have been validated through appropriate\nstatistical measures and contextual interpretation has been drawn."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1004.4463v1", 
    "title": "A Metrics Based Model for Understandability Quantification", 
    "arxiv-id": "1004.4463v1", 
    "author": "Khurram Mustafa", 
    "publish": "2010-04-26T10:10:07Z", 
    "summary": "Software developers and maintainers need to read and understand source\nprograms and other software artifacts. The increase in size and complexity of\nsoftware drastically affects several quality attributes, especially\nunderstandability and maintainability. False interpretation often leads to\nambiguities, misunderstanding and hence to faulty development results. Despite\nthe fact that software understandability is vital and one of the most\nsignificant components of the software development process, it is poorly\nmanaged. This is mainly due to the lack of its proper management and control.\nThe paper highlights the importance of understandability in general and as a\nfactor of software testability. Two major contributions are made in the paper.\nA relation between testability factors and object oriented characteristics has\nbeen established as a first contribution. In second contribution, a model has\nbeen proposed for estimating understandability of object oriented software\nusing design metrics. In addition, the proposed model has been validated using\nexperimental try-out."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0162v2", 
    "title": "Software Requirements Specification of the IUfA's UUIS -- a Team 4   COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0162v2", 
    "author": "Yu Ming Zhang", 
    "publish": "2010-05-02T20:00:42Z", 
    "summary": "This document presents the business requirement of Unified University\nInventory System (UUIS) in Technology-independent manner. All attempts have\nbeen made in using mostly business terminology and business language while\ndescribing the requirements in this document. Very minimal and commonly\nunderstood Technical terminology is used. Use case approach is used in modeling\nthe business requirements in this document."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0169v1", 
    "title": "Software Design Document, Testing, Deployment and Configuration   Management, and User Manual of the UUIS -- a Team 4 COMP5541-W10 Project   Approach", 
    "arxiv-id": "1005.0169v1", 
    "author": "Abdulrahman Al-Sharawi", 
    "publish": "2010-05-02T22:15:33Z", 
    "summary": "This document provides a description of the technical design for Unified\nUniversity Inventory System - Web Portal. This document's primary purpose is to\ndescribe the technical vision for how business requirements will be realized.\nThis document provides an architectural overview of the system to depict\ndifferent aspects of the system. This document also functions as a foundational\nreference point for developers."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0330v1", 
    "title": "Software Requirements Specification of the IUfA's UUIS -- a Team 1   COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0330v1", 
    "author": "Rana Hassan", 
    "publish": "2010-05-03T15:57:09Z", 
    "summary": "Unified University Inventory System (UUIS), is an inventory system created\nfor the Imaginary University of Arctica (IUfA) to facilitate its inventory\nmanagement, of all the faculties in one system. Team 1 elucidates the functions\nof the system and the characteristics of the users who have access to these\nfunctions. It shows the access restrictions to different functionalities of the\nsystem provided to users, who are the staff and students of the University.\nTeam 1, also, emphasises on the necessary steps required to prevent the\nsecurity of the system and its data."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0595v1", 
    "title": "Software Design Document, Testing, and Deployment and Configuration   Management of the UUIS - a Team 1 COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0595v1", 
    "author": "Rana Hassan", 
    "publish": "2010-05-04T17:54:20Z", 
    "summary": "The document presents a detailed description of the designs for the\nimplementation of the Unified University Inventory System for the Imaginary\nUniversity of Arctica. The document, through numerous diagrams and UI samples,\ngives the structure of the system and the functions of its modules. It also\ngives test cases and reports that support the system's architecture and design."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0609v1", 
    "title": "Software Requirements Specification of the IUfA's UUIS -- a Team 3   COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0609v1", 
    "author": "Yassine Amaiche", 
    "publish": "2010-05-04T19:07:27Z", 
    "summary": "The purpose of this document is to specify the requirements of the University\nUnified Inventory System, of the UIfA. The Team of Analysts used a Feedback\nWaterfall approach to collect the requirements. UML diagrams, such as Use case\ndiagrams, Block Diagrams, Domain Models, and interface prototypes are some of\nthe tools employed to develop the present document."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0665v2", 
    "title": "Software Design Document, Testing, Deployment and Configuration   Management of the UUIS--a Team 2 COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0665v2", 
    "author": "Yongxin Zhu", 
    "publish": "2010-05-05T04:16:21Z", 
    "summary": "The Software Design Document of UUIS describes the prototype design details\nof the system architecture, database layer, deployment and configuration\ndetails as well as test cases produced while working the design and\nimplementation of the prototype. The requirements specification of UUIS are\ndetailed in arXiv:1005.0783."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0783v2", 
    "title": "Software Requirements Specification of the IUfA's UUIS -- a Team 2   COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0783v2", 
    "author": "Yongxin Zhu", 
    "publish": "2010-05-05T15:59:50Z", 
    "summary": "In the 52-page document, we describe our approach to the Software\nRequirements Specification of the IUfA's UUIS prototype. This includes the\noverall system description, functional requirements, non-functional\nrequirements, use cases, the corresponding data dictionary for all entities\ninvolved, mock user interface (UI) design, and the overall projected cost\nestimate. The design specification of UUIS can be found in arXiv:1005.0665."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.0854v1", 
    "title": "Software Design Document, Testing, Deployment and Configuration   Management of the IUfA's UUIS -- a Team 3 COMP5541-W10 Project Approach", 
    "arxiv-id": "1005.0854v1", 
    "author": "Ren\u00e9 Toutant", 
    "publish": "2010-05-05T21:01:35Z", 
    "summary": "The purpose of this document is to provide technical specifications concerned\nto the Design of the University Unified Inventory System - Web Portal, of the\nUIfA. The Team of Developers used a Feedback Waterfall approach to build up the\nsystem, under an Object Oriented paradigm. The architectural model followed was\nthe Model-View-Controller, mixed with a Mapper layer between the database and\nthe Model. Some of the patterns utilized in the developing of the System were\nthe Observer Pattern, the Command Pattern, and the Mapper Pattern."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.2299v1", 
    "title": "Incorporating prediction models in the SelfLet framework: a plugin   approach", 
    "arxiv-id": "1005.2299v1", 
    "author": "Elisabetta Di Nitto", 
    "publish": "2010-05-13T11:18:41Z", 
    "summary": "A complex pervasive system is typically composed of many cooperating\n\\emph{nodes}, running on machines with different capabilities, and pervasively\ndistributed across the environment. These systems pose several new challenges\nsuch as the need for the nodes to manage autonomously and dynamically in order\nto adapt to changes detected in the environment. To address the above issue, a\nnumber of autonomic frameworks has been proposed. These usually offer either\npredefined self-management policies or programmatic mechanisms for creating new\npolicies at design time. From a more theoretical perspective, some works\npropose the adoption of prediction models as a way to anticipate the evolution\nof the system and to make timely decisions. In this context, our aim is to\nexperiment with the integration of prediction models within a specific\nautonomic framework in order to assess the feasibility of such integration in a\nsetting where the characteristics of dynamicity, decentralization, and\ncooperation among nodes are important. We extend an existing infrastructure\ncalled \\emph{SelfLets} in order to make it ready to host various prediction\nmodels that can be dynamically plugged and unplugged in the various component\nnodes, thus enabling a wide range of predictions to be performed. Also, we show\nin a simple example how the system works when adopting a specific prediction\nmodel from the literature."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.2882v1", 
    "title": "On Testing Constraint Programs", 
    "arxiv-id": "1005.2882v1", 
    "author": "Lebbah Yahia", 
    "publish": "2010-05-17T11:15:17Z", 
    "summary": "The success of several constraint-based modeling languages such as OPL, ZINC,\nor COMET, appeals for better software engineering practices, particularly in\nthe testing phase. This paper introduces a testing framework enabling automated\ntest case generation for constraint programming. We propose a general framework\nof constraint program development which supposes that a first declarative and\nsimple constraint model is available from the problem specifications analysis.\nThen, this model is refined using classical techniques such as constraint\nreformulation, surrogate and global constraint addition, or symmetry-breaking\nto form an improved constraint model that must be thoroughly tested before\nbeing used to address real-sized problems. We think that most of the faults are\nintroduced in this refinement step and propose a process which takes the first\ndeclarative model as an oracle for detecting non-conformities. We derive\npractical test purposes from this process to generate automatically test data\nthat exhibit non-conformities. We implemented this approach in a new tool\ncalled CPTEST that was used to automatically detect non-conformities on two\nclassical benchmark programs, namely the Golomb rulers and the car-sequencing\nproblem."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.4021v3", 
    "title": "Software Effort Estimation using Radial Basis and Generalized Regression   Neural Networks", 
    "arxiv-id": "1005.4021v3", 
    "author": "S. N. S. V. S. C. Ramesh", 
    "publish": "2010-05-21T17:33:44Z", 
    "summary": "Software development effort estimation is one of the most major activities in\nsoftware project management. A number of models have been proposed to construct\na relationship between software size and effort; however we still have problems\nfor effort estimation. This is because project data, available in the initial\nstages of project is often incomplete, inconsistent, uncertain and unclear. The\nneed for accurate effort estimation in software industry is still a challenge.\nArtificial Neural Network models are more suitable in such situations. The\npresent paper is concerned with developing software effort estimation models\nbased on artificial neural networks. The models are designed to improve the\nperformance of the network that suits to the COCOMO Model. Artificial Neural\nNetwork models are created using Radial Basis and Generalized Regression. A\ncase study based on the COCOMO81 database compares the proposed neural network\nmodels with the Intermediate COCOMO. The results were analyzed using five\ndifferent criterions MMRE, MARE, VARE, Mean BRE and Prediction. It is observed\nthat the Radial Basis Neural Network provided better results"
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.4123v1", 
    "title": "A Structured Framework for Assessing the \"Goodness\" of Agile Methods", 
    "arxiv-id": "1005.4123v1", 
    "author": "James. D. Arthur", 
    "publish": "2010-05-22T12:39:27Z", 
    "summary": "Agile Methods are designed for customization; they offer an organization or a\nteam the flexibility to adopt a set of principles and practices based on their\nculture and values. While that flexibility is consistent with the agile\nphilosophy, it can lead to the adoption of principles and practices that can be\nsub-optimal relative to the desired objectives. We question then, how can one\ndetermine if adopted practices are \"in sync\" with the identified principles,\nand to what extent those principles support organizational objectives? In this\nresearch, we focus on assessing the \"goodness\" of an agile method adopted by an\norganization based on (1) its adequacy, (2) the capability of the organization\nto provide the supporting environment to competently implement the method, and\n(3) its effectiveness. To guide our assessment, we propose the Objectives,\nPrinciples and Practices (OPP) framework. The design of the OPP framework\nrevolves around the identification of the agile objectives, principles that\nsupport the achievement of those objectives, and practices that reflect the\n\"spirit\" of those principles. Well-defined linkages between the objectives and\nprinciples, and between the principles and practices are also established to\nsupport the assessment process. We traverse these linkages in a top-down\nfashion to assess adequacy and a bottom-up fashion to assess capability and\neffectiveness. This is a work-in-progress paper, outlining our proposed\nresearch, preliminary results and future directions."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15057-9_9", 
    "link": "http://arxiv.org/pdf/1005.4271v1", 
    "title": "Selection of Architecture Styles using Analytic Network Process for the   Optimization of Software Architecture", 
    "arxiv-id": "1005.4271v1", 
    "author": "A. N. Aruna Kumari", 
    "publish": "2010-05-24T07:45:33Z", 
    "summary": "The continuing process of software systems enlargement in size and complexity\nbecomes system design extremely important for software production. In this way,\nthe role of software architecture is significantly important in software\ndevelopment. It serves as an evaluation and implementation plan for software\ndevelopment and software evaluation. Consequently, choosing the correct\narchitecture is a critical issue in software engineering domain.\nMoreover,software architecture selection is a multicriteria decision-making\nproblem in which different goals and objectives must be taken into\nconsideration. In this paper, more precise and suitable decisions in selection\nof architecture styles have been presented by using ANP inference to support\ndecisions of software architects in order to exploit properties of styles in\nthe best way to optimize the design of software architecture."
},{
    "category": "cs.SE", 
    "doi": "10.1016/S0550-3213(01)00405-9", 
    "link": "http://arxiv.org/pdf/1005.4363v1", 
    "title": "A model for semantic integration of business components", 
    "arxiv-id": "1005.4363v1", 
    "author": "Abderrahim Sekkaki", 
    "publish": "2010-05-24T15:45:10Z", 
    "summary": "Today, reusable components are available in several repositories. These last\nare certainly conceived for the reusing However, this re-use is not immediate;\nit requires, in the fact, to pass through some essential conceptual operations,\namong them in particular, research, integration, adaptation, and composition.\nWe are interested in the present work to the problem of semantic integration of\nheterogeneous Business Components. This problem is often put in syntactical\nterms, while the real stake is of semantic order. Our contribution concerns a\nmodel proposal for Business components integration as well as resolution method\nof semantic naming conflicts, met during the integration of Business\nComponents."
},{
    "category": "cs.SE", 
    "doi": "10.1016/S0550-3213(01)00405-8", 
    "link": "http://arxiv.org/pdf/1005.4525v1", 
    "title": "Towards an architecture for semantic integration of business components", 
    "arxiv-id": "1005.4525v1", 
    "author": "Abderrahim Sekkaki", 
    "publish": "2010-05-25T10:23:38Z", 
    "summary": "Today, reusable components are available in several repositorys. These are\ncertainly conceived for re-use. However, this re-use is not immediate, it\nrequires, in effect, to pass by some essential conceptual operations, among\nwhich in particular, research, integration, adaptation, and composition. We are\ninterested in the present work to the problem of semantic integration of\nheterogeneous Business Components. This problem is often put in syntactical\nterms, while the real stake is of semantic order. Our contribution concerns an\narchitecture proposal for Business components integration and a resolution\nmethod of semantic naming conflicts, met during the integration of Business\nComponents"
},{
    "category": "cs.SE", 
    "doi": "10.1016/S0550-3213(01)00405-8", 
    "link": "http://arxiv.org/pdf/1005.4975v1", 
    "title": "Filling the Gap between Business Process Modeling and Behavior Driven   Development", 
    "arxiv-id": "1005.4975v1", 
    "author": "Fernando Luis de Carvalho e Silva", 
    "publish": "2010-05-27T01:08:58Z", 
    "summary": "Behavior Driven Development (NORTH, 2006) is a specification technique that\nis growing in acceptance in the Agile methods communities. BDD allows to\nsecurely verify that all functional requirements were treated properly by\nsource code, by connecting the textual description of these requirements to\ntests.\n  On the other side, the Enterprise Information Systems (EIS) researchers and\npractitioners defends the use of Business Process Modeling (BPM) to, before\ndefining any part of the system, perform the modeling of the system's\nunderlying business process. Therefore, it can be stated that, in the case of\nEIS, functional requirements are obtained by identifying Use Cases from the\nbusiness process models.\n  The aim of this paper is, in a narrower perspective, to propose the use of\nFinite State Machines (FSM) to model business process and then connect them to\nthe BDD machinery, thus driving better quality for EIS. In a broader\nperspective, this article aims to provoke a discussion on the mapping of the\nvarious BPM notations, since there isn't a real standard for business process\nmodeling (Moller et al., 2007), to BDD.\n  Firstly a historical perspective of the evolution of previous proposals from\nwhich this one emerged will be presented, and then the reasons to change from\nModel Driven Development (MDD) to BDD will be presented also in a historical\nperspective. Finally the proposal of using FSM, specifically by using UML\nStatechart diagrams, will be presented, followed by some conclusions."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2010.2304", 
    "link": "http://arxiv.org/pdf/1006.1182v1", 
    "title": "A Parsing Scheme for Finding the Design Pattern and Reducing the   Development Cost of Reusable Object Oriented Software", 
    "arxiv-id": "1006.1182v1", 
    "author": "Mohammad Sabbir Hasan", 
    "publish": "2010-06-07T06:36:53Z", 
    "summary": "Because of the importance of object oriented methodologies, the research in\ndeveloping new measure for object oriented system development is getting\nincreased focus. The most of the metrics need to find the interactions between\nthe objects and modules for developing necessary metric and an influential\nsoftware measure that is attracting the software developers, designers and\nresearchers. In this paper a new interactions are defined for object oriented\nsystem. Using these interactions, a parser is developed to analyze the existing\narchitecture of the software. Within the design model, it is necessary for\ndesign classes to collaborate with one another. However, collaboration should\nbe kept to an acceptable minimum i.e. better designing practice will introduce\nlow coupling. If a design model is highly coupled, the system is difficult to\nimplement, to test and to maintain overtime. In case of enhancing software, we\nneed to introduce or remove module and in that case coupling is the most\nimportant factor to be considered because unnecessary coupling may make the\nsystem unstable and may cause reduction in the system's performance. So\ncoupling is thought to be a desirable goal in software construction, leading to\nbetter values for external software qualities such as maintainability,\nreusability and so on. To test this hypothesis, a good measure of class\ncoupling is needed. In this paper, based on the developed tool called Design\nAnalyzer we propose a methodology to reuse an existing system with the\nobjective of enhancing an existing Object oriented system keeping the coupling\nas low as possible."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2010.2304", 
    "link": "http://arxiv.org/pdf/1006.1244v1", 
    "title": "Exploring the Impact of Socio-Technical Core-Periphery Structures in   Open Source Software Development", 
    "arxiv-id": "1006.1244v1", 
    "author": "Jos van Hillegersberg", 
    "publish": "2010-06-07T12:44:06Z", 
    "summary": "In this paper we apply the social network concept of core-periphery structure\nto the sociotechnical structure of a software development team. We propose a\nsocio-technical pattern that can be used to locate emerging coordination\nproblems in Open Source projects. With the help of our tool and method called\nTESNA, we demonstrate a method to monitor the socio-technical core-periphery\nmovement in Open Source projects. We then study the impact of different\ncore-periphery movements on Open Source projects. We conclude that a steady\ncore-periphery shift towards the core is beneficial to the project, whereas\nshifts away from the core are clearly not good. Furthermore, oscillatory shifts\ntowards and away from the core can be considered as an indication of the\ninstability of the project. Such an analysis can provide developers with a good\ninsight into the health of an Open Source project. Researchers can gain from\nthe pattern theory, and from the method we use to study the core-periphery\nmovements."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.1683v1", 
    "title": "Object-oriented modelling with unified modelling language 2.0 for simple   software application based on agile methodology", 
    "arxiv-id": "1006.1683v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-09T02:16:17Z", 
    "summary": "Unified modelling language (UML) 2.0 introduced in 2002 has been developing\nand influencing object-oriented software engineering and has become a standard\nand reference for information system analysis and design modelling. There are\nmany concepts and theories to model the information system or software\napplication with UML 2.0, which can make ambiguities and inconsistencies for a\nnovice to learn to how to model the system with UML especially with UML 2.0.\nThis article will discuss how to model the simple software application by using\nsome of the diagrams of UML 2.0 and not by using the whole diagrams as\nsuggested by agile methodology. Agile methodology is considered as convenient\nfor novices because it can deliver the information technology environment to\nthe end-user quickly and adaptively with minimal documentation. It also has the\nability to deliver best performance software application according to the\ncustomer's needs. Agile methodology will make simple model with simple\ndocumentation, simple team and simple tools."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.1747v2", 
    "title": "On the Detection of High-Impact Refactoring Opportunities in Programs", 
    "arxiv-id": "1006.1747v2", 
    "author": "Syed M. Ali Shah", 
    "publish": "2010-06-09T09:07:47Z", 
    "summary": "We present a novel approach to detect refactoring opportunities by measuring\nthe participation of references between types in instances of patterns\nrepresenting design flaws. This technique is validated using an experiment\nwhere we analyse a set of 95 open-source Java programs for instances of four\npatterns representing modularisation problems. It turns out that our algorithm\ncan detect high impact refactorings opportunities - a small number of\nreferences such that the removal of those references removes the majority of\npatterns from the program."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.1955v1", 
    "title": "Distributed Agile Software Development: A Review", 
    "arxiv-id": "1006.1955v1", 
    "author": "Hema Date", 
    "publish": "2010-06-10T05:59:36Z", 
    "summary": "Distribution of software development is becoming more and more common in\norder to save the production cost and reduce the time to market. Large\ngeographical distance, different time zones and cultural differences in\ndistributed software development (DSD) leads to weak communication which\nadversely affects the project. Using agile practices for distributed\ndevelopment is also gaining momentum in various organizations to increase the\nquality and performance of the project. This paper explores the intersection of\nthese two significant trends for software development i.e. DSD and agile. We\ndiscuss the challenges faced by geographically distributed agile teams and\nproven practices to address these issues, which will help in building a\nsuccessful distributed team."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.2155v1", 
    "title": "Software Must Move! A Description of the Software Assembly Line", 
    "arxiv-id": "1006.2155v1", 
    "author": "William L. Anderson", 
    "publish": "2010-06-10T21:18:44Z", 
    "summary": "This paper describes a set of tools for automating and controlling the\ndevelopment and maintenance of software systems. The mental model is a software\nassembly line. Program design and construction take place at individual\nprogrammer workstations. Integration of individual software components takes\nplace at subsequent stations on the assembly line. Software is moved\nautomatically along the assembly line toward final packaging. Software under\nconstruction or maintenance is divided into packages. Each package of software\nis composed of a recipe and ingredients. Some new terms are introduced to\ndescribe the ingredients. The recipe specifies how ingredients are transformed\ninto products. The benefits of the Software Assembly Line for development,\nmaintenance, and management of large-scale computer systems are explained."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.2534v8", 
    "title": "The Application and Extension of Retrograde Software Analysis", 
    "arxiv-id": "1006.2534v8", 
    "author": "Aleksandar Perisic", 
    "publish": "2010-06-13T13:32:37Z", 
    "summary": "The retrograde software analysis is a method that emanates from executing a\nprogram backwards - instead of taking input data and following the execution\npath, we start from output data and by executing the program backwards, command\nby command, analyze data that could lead to the current output. The changed\nperspective forces a developer to think in a new way about the program. It can\nbe applied as a thorough procedure or casual method. With this method, we have\nmany advantages in testing, algorithm and system analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.2812v1", 
    "title": "Component Interaction Graph: A new approach to test component   composition", 
    "arxiv-id": "1006.2812v1", 
    "author": "Sisir Kumar Jena", 
    "publish": "2010-06-14T19:33:59Z", 
    "summary": "The key factor of component based software development is component\ncomposition technology. A Component interaction graph is used to describe the\ninterrelation of components. Drawing a complete component interaction graph\n(CIG) provides an objective basis and technical means for making the testing\noutline. Although many researches have focused on this subject, the quality of\nsystem that is composed of components has not been guaranteed. In this paper, a\nCIG is constructed from a state chart diagram and new test cases are generated\nto test the component composition."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.2840v1", 
    "title": "A Complexity measure based on Requirement Engineering Document", 
    "arxiv-id": "1006.2840v1", 
    "author": "D. S. Kushwaha", 
    "publish": "2010-06-14T20:17:55Z", 
    "summary": "Research shows, that the major issue in development of quality software is\nprecise estimation. Further this estimation depends upon the degree of\nintricacy inherent in the software i.e. complexity. This paper attempts to\nempirically demonstrate the proposed complexity which is based on IEEE\nRequirement Engineering document. It is said that a high quality SRS is pre\nrequisite for high quality software. Requirement Engineering document (SRS) is\na specification for a particular software product, program or set of program\nthat performs some certain functions for a specific environment. The various\ncomplexity measure given so far are based on Code and Cognitive metrics value\nof software, which are code based. So these metrics provide no leverage to the\ndeveloper of the code. Considering the shortcoming of code based approaches,\nthe proposed approach identifies complexity of software immediately after\nfreezing the requirement in SDLC process. The proposed complexity measure\ncompares well with established complexity measures. Finally the trend can be\nvalidated with the result of proposed measure. Ultimately, Requirement based\ncomplexity measure can be used to understand the complexity of proposed\nsoftware much before the actual implementation of design thus saving on cost\nand manpower wastage."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.3259v1", 
    "title": "Contents of COMP5541 Winter 2010 Final UUIS SRS and SDD Reports", 
    "arxiv-id": "1006.3259v1", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-06-16T16:12:30Z", 
    "summary": "This index covers the final course project reports for COMP5541 Winter 2010\nat Concordia University, Montreal, Canada, Tools and Techniques for Software\nEngineering by 4 teams trying to capture the requirements, provide the design\nspecification, configuration management, testing and quality assurance of their\npartial implementation of the Unified University Inventory System (UUIS) of an\nImaginary University of Arctica (IUfA). Their results are posted here for\ncomparative studies and analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.4562v1", 
    "title": "Engineering Semantic Web Applications by Using Object-Oriented Paradigm", 
    "arxiv-id": "1006.4562v1", 
    "author": "Abad Shah", 
    "publish": "2010-06-23T15:17:47Z", 
    "summary": "The web information resources are growing explosively in number and volume.\nNow to retrieve relevant data from web has become very difficult and\ntime-consuming. Semantic Web envisions that these web resources should be\ndeveloped in machine-processable way in order to handle irrelevancy and manual\nprocessing problems. Whereas, the Semantic Web is an extension of current web,\nin which web resources are equipped with formal semantics about their\ninterpretation through machines. These web resources are usually contained in\nweb applications and systems, and their formal semantics are normally\nrepresented in the form of web-ontologies. In this research paper, an\nobject-oriented design methodology (OODM) is upgraded for developing semantic\nweb applications. OODM has been developed for designing of web applications for\nthe current web. This methodology is good enough to develop web applications.\nIt also provides a systematic approach for the web applications development but\nit is not helpful in generating machine-pocessable content of web applications\nin their development. Therefore, this methodology needs to be extended. In this\npaper, we propose that extension in OODM. This new extended version is referred\nto as the semantic web object-oriented design methodology (SW-OODM)."
},{
    "category": "cs.SE", 
    "doi": "10.1080/01449290903186231", 
    "link": "http://arxiv.org/pdf/1006.4565v1", 
    "title": "An Overview: Extensible Markup Language Technology", 
    "arxiv-id": "1006.4565v1", 
    "author": "Zubaidah M. Hazza", 
    "publish": "2010-06-23T15:19:19Z", 
    "summary": "XML stands for the Extensible Markup Language. It is a markup language for\ndocuments, Nowadays XML is a tool to develop and likely to become a much more\ncommon tool for sharing data and store. XML can communicate structured\ninformation to other users. In other words, if a group of users agree to\nimplement the same kinds of tags to describe a certain kind of information, XML\napplications can assist these users in communicating their information in an\nmore robust and efficient manner. XML can make it easier to exchange\ninformation between cooperating entities. In this paper we will present the XML\ntechnique by fourth factors Strength of XML, XML Parser, XML Goals and Types of\nXML Parsers."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1006.4740v1", 
    "title": "Support for Evolving Software Architectures in the ArchWare ADL", 
    "arxiv-id": "1006.4740v1", 
    "author": "Mark Greenwood", 
    "publish": "2010-06-24T10:39:47Z", 
    "summary": "Software that cannot evolve is condemned to atrophy: it cannot accommodate\nthe constant revision and re-negotiation of its business goals nor intercept\nthe potential of new technology. To accommodate change in software systems we\nhave defined an active software architecture to be: dynamic in that the\nstructure and cardinality of the components and interactions are changeable\nduring execution; updatable in that components can be replaced; decomposable in\nthat an executing system may be (partially) stopped and split up into its\ncomponents and interactions; and reflective in that the specification of\ncomponents and interactions may be evolved during execution. Here we describe\nthe facilities of the ArchWare architecture description language (ADL) for\nspecifying active architectures. The contribution of the work is the unique\ncombination of concepts including: a {\\pi}-calculus based communication and\nexpression language for specifying executable architectures; hyper-code as an\nunderlying representation of system execution that can be used for\nintrospection; a decomposition operator to incrementally break up executing\nsystems; and structural reflection for creating new components and binding them\ninto running systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1006.4829v1", 
    "title": "Constructing Active Architectures in the ArchWare ADL", 
    "arxiv-id": "1006.4829v1", 
    "author": "Mark Greenwood", 
    "publish": "2010-06-24T16:36:07Z", 
    "summary": "Software that cannot change is condemned to atrophy: it cannot accommodate\nthe constant revision and re-negotiation of its business goals nor intercept\nthe potential of new technology. To accommodate change in such systems we have\ndefined an active software architecture to be: dynamic in that the structure\nand cardinality of the components and interactions are not statically known;\nupdatable in that components can be replaced dynamically; and evolvable in that\nit permits its executing specification to be changed. Here we describe the\nfacilities of the ArchWare architecture description language (ADL) for\nspecifying active architectures. The contribution of the work is the unique\ncombination of concepts including: a {\\pi}-calculus based communication and\nexpression language for specifying executable architectures; hyper-code as an\nunderlying representation of system execution; a decomposition operator to\nbreak up and introspect on executing systems; and structural reflection for\ncreating new components and binding them into running systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1006.4892v1", 
    "title": "Mapping Business Process Modeling constructs to Behavior Driven   Development Ubiquitous Language", 
    "arxiv-id": "1006.4892v1", 
    "author": "Rodrigo Soares Manhaes", 
    "publish": "2010-06-25T01:00:30Z", 
    "summary": "Behavior-Driven Development (BDD) is a specification technique that\nautomatically certifies that all functional requirements are treated properly\nby source code, through the connection of the textual description of these\nrequirements to automated tests. Given that in some areas, in special\nEnterprise Information Systems, requirements are identified by Business Process\nModeling - which uses graphical notations of the underlying business processes,\nthis paper aims to provide a mapping from the basic constructs that form the\nmost common BPM languages to Behavior Driven Development constructs."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1006.5761v1", 
    "title": "Automated co-evolution of GMF editor models", 
    "arxiv-id": "1006.5761v1", 
    "author": "Alfonso Pierantonio", 
    "publish": "2010-06-30T03:22:59Z", 
    "summary": "The Eclipse Graphical Modeling (GMF) Framework provides the major approach\nfor implementing visual languages on top of the Eclipse platform. GMF relies on\na family of modeling languages to describe different aspects of the visual\nlanguage and its implementation in an editor. GMF uses a model-driven approach\nto map the different GMF models to Java code. The framework, as it stands,\nprovides very little support for evolution. In particular, there is no support\nfor propagating changes from say the domain model (i.e., the abstract syntax of\nthe visual language) to other models. We analyze the resulting co-evolution\nchallenge, and we provide a transformation-based solution, say GMF model\nadapters, that serve the propagation of abstract-syntax changes based on the\ninterpretation of difference models."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1006.5804v1", 
    "title": "Observation-Driven Configuration of Complex Software Systems", 
    "arxiv-id": "1006.5804v1", 
    "author": "Aled Sage", 
    "publish": "2010-06-30T08:39:25Z", 
    "summary": "The ever-increasing complexity of software systems makes them hard to\ncomprehend, predict and tune due to emergent properties and non-deterministic\nbehaviour. Complexity arises from the size of software systems and the wide\nvariety of possible operating environments: the increasing choice of platforms\nand communication policies leads to ever more complex performance\ncharacteristics. In addition, software systems exhibit different behaviour\nunder different workloads. Many software systems are designed to be\nconfigurable so that policies can be chosen to meet the needs of various\nstakeholders. For complex software systems it can be difficult to accurately\npredict the effects of a change and to know which configuration is most\nappropriate. This thesis demonstrates that it is useful to run automated\nexperiments that measure a selection of system configurations. Experiments can\nfind configurations that meet the stakeholders' needs, find interesting\nbehavioural characteristics, and help produce predictive models of the system's\nbehaviour. The design and use of ACT (Automated Configuration Tool) for running\nsuch experiments is described, in combination a number of search strategies for\ndeciding on the configurations to measure. Design Of Experiments (DOE) is\ndiscussed, with emphasis on Taguchi Methods. These statistical methods have\nbeen used extensively in manufacturing, but have not previously been used for\nconfiguring software systems. The novel contribution here is an industrial case\nstudy, applying the combination of ACT and Taguchi Methods to DC-Directory, a\nproduct from Data Connection Ltd (DCL). The case study investigated the\napplicability of Taguchi Methods for configuring complex software systems.\nTaguchi Methods were found to be useful for modelling and configuring DC-\nDirectory, making them a valuable addition to the techniques available to\nsystem administrators and developers."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1008.1671v1", 
    "title": "A Parsing Scheme for Finding the Design Pattern and Reducing the   Development Cost of Reusable Object Oriented Software", 
    "arxiv-id": "1008.1671v1", 
    "author": "Mohammad Sabbir Hasan", 
    "publish": "2010-08-10T09:23:09Z", 
    "summary": "Because of the importance of object oriented methodologies, the research in\ndeveloping new measure for object oriented system development is getting\nincreased focus. The most of the metrics need to find the interactions between\nthe objects and modules for developing necessary metric and an influential\nsoftware measure that is attracting the software developers, designers and\nresearchers. In this paper a new interactions are defined for object oriented\nsystem. Using these interactions, a parser is developed to analyze the existing\narchitecture of the software. Within the design model, it is necessary for\ndesign classes to collaborate with one another. However, collaboration should\nbe kept to an acceptable minimum i.e. better designing practice will introduce\nlow coupling. If a design model is highly coupled, the system is difficult to\nimplement, to test and to maintain overtime. In case of enhancing software, we\nneed to introduce or remove module and in that case coupling is the most\nimportant factor to be considered because unnecessary coupling may make the\nsystem unstable and may cause reduction in the system's performance. So\ncoupling is thought to be a desirable goal in software construction, leading to\nbetter values for external software qualities such as maintainability,\nreusability and so on. To test this hypothesis, a good measure of class\ncoupling is needed. In this paper, based on the developed tool called Design\nAnalyzer we propose a methodology to reuse an existing system with the\nobjective of enhancing an existing Object oriented system keeping the coupling\nas low as possible."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1008.2458v3", 
    "title": "A Case Study in Matching Service Descriptions to Implementations in an   Existing System", 
    "arxiv-id": "1008.2458v3", 
    "author": "Girish M. Rama", 
    "publish": "2010-08-14T16:26:14Z", 
    "summary": "A number of companies are trying to migrate large monolithic software systems\nto Service Oriented Architectures. A common approach to do this is to first\nidentify and describe desired services (i.e., create a model), and then to\nlocate portions of code within the existing system that implement the described\nservices. In this paper we describe a detailed case study we undertook to match\na model to an open-source business application. We describe the systematic\nmethodology we used, the results of the exercise, as well as several\nobservations that throw light on the nature of this problem. We also suggest\nand validate heuristics that are likely to be useful in partially automating\nthe process of matching service descriptions to implementations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1008.2542v1", 
    "title": "Integration of Design Patterns and Mobile Applications in a Management   System for Monitoring Maintenance Cathode Plates of Mining Company Quebrada   Blanca SA", 
    "arxiv-id": "1008.2542v1", 
    "author": "Oscar Sandoval Carlos", 
    "publish": "2010-08-15T18:41:05Z", 
    "summary": "This document presents the integration of design patterns and mobile\napplications, in the development of software management of plates (SIGEP) that\nallows to support in the solutions to problematics that they appear in the\nprocess of maintaining of plates copper cathodes of a Mining Company, in our\ncase for Quebrada Blanca S.A. (CMQB S.A.). These problematics mainly are\nrelated to the little control over the tasks carried out in the maintaining to\nthe cathodic plates, and the lack of information that leads to this practice,\noriginates a deficient management and it does not allow to make opportune\ndecisions referring to these elements, and therefore it does to project and to\nadminister the life utility of the plates of cathodes, generating lifted costs\nassociated to this process. As the process of maintaining a cathode plates\nconstantly changing process, with respect to maintenance strategies in the\nsystem design SIGEP recognizing the flexibility and reuse in the design of\nsystem components, this achieved through design patterns used. The SIGEP\nimplementation of the system and the incorporation of a mobile application,\nmeant for CMQB S.A. increase control of the tasks carried out plates cathodes,\nallowing the company to detailed information on the maintenance of these\nelements, allowing among other things, identify cathode plates which are more\nexpensive, and therefore knowing what must be replaced."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WICSA.2004.1310691", 
    "link": "http://arxiv.org/pdf/1008.2647v1", 
    "title": "Searching publications on software testing", 
    "arxiv-id": "1008.2647v1", 
    "author": "C. A. Middelburg", 
    "publish": "2010-08-16T13:16:03Z", 
    "summary": "This note concerns a search for publications in which the pragmatic concept\nof a test as conducted in the practice of software testing is formalized, a\ntheory about software testing based on such a formalization is presented or it\nis demonstrated on the basis of such a theory that there are solid grounds to\ntest software in cases where in principle other forms of analysis could be\nused. This note reports on the way in which the search has been carried out and\nthe main outcomes of the search. The message of the note is that the\nfundamentals of software testing are not yet complete in some respects."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1008.3321v1", 
    "title": "Software Development Standard and Software Engineering Practice: A Case   Study of Bangladesh", 
    "arxiv-id": "1008.3321v1", 
    "author": "Md. Shoyaib", 
    "publish": "2010-08-19T15:01:14Z", 
    "summary": "Improving software process to achieve high quality in a software development\norganization is the key factor to success. Bangladeshi software firms have not\nexperienced much in this particular area in comparison to other countries. The\nISO 9001 and CMM standard has become a basic part of software development. The\nmain objectives of our study are: 1) To understand the software development\nprocess uses by the software developer firms in Bangladesh 2) To identify the\ndevelopment practices based on established quality standard and 3) To establish\na standardized and coherent process for the development of software for a\nspecific project. It is revealed from this research that software industries of\nBangladesh are lacking in target set for software process and improvement,\ninvolvement of quality control activities, and standardize business expertise\npractice. This paper investigates the Bangladeshi software industry in the\nlight of the above challenges."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1008.4174v1", 
    "title": "Spreadsheets Grow Up: Three Spreadsheet Engineering Methodologies for   Large Financial Planning Models", 
    "arxiv-id": "1008.4174v1", 
    "author": "Ozgur Ozluk", 
    "publish": "2010-08-24T23:01:46Z", 
    "summary": "Many large financial planning models are written in a spreadsheet programming\nlanguage (usually Microsoft Excel) and deployed as a spreadsheet application.\nThree groups, FAST Alliance, Operis Group, and BPM Analytics (under the name\n\"Spreadsheet Standards Review Board\") have independently promulgated\nstandardized processes for efficiently building such models. These spreadsheet\nengineering methodologies provide detailed guidance on design, construction\nprocess, and quality control. We summarize and compare these methodologies.\nThey share many design practices, and standardized, mechanistic procedures to\nconstruct spreadsheets. We learned that a written book or standards document is\nby itself insufficient to understand a methodology. These methodologies\nrepresent a professionalization of spreadsheet programming, and can provide a\nmeans to debug a spreadsheet that contains errors. We find credible the\nassertion that these spreadsheet engineering methodologies provide enhanced\nproductivity, accuracy and maintainability for large financial planning models"
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.0152v1", 
    "title": "Three Controlled Experiments in Software Engineering with the Two-Tier   Programming Toolkit: Final Report", 
    "arxiv-id": "1009.0152v1", 
    "author": "Epameinondas Gasparis", 
    "publish": "2010-09-01T10:18:05Z", 
    "summary": "Three controlled experiments testing the benefits that Java programmers gain\nfrom using the Two-Tier Programming Toolkit have recently been concluded. The\nfirst experiment offers statistically significant evidence (p-value: 0.02) that\nprogrammers who undertook only minimal (1-hour) training in using the current\nprototype exhibit 76% productivity gains in key tasks in software development\nand maintenance. The second experiment shows that the use of the TTP Toolkit is\nlikely (p-value: 0.10) to almost triple the accuracy of programmers performing\ntasks associated with software quality. The third experiment shows that the TTP\nToolkit does not offer significant productivity gains in performing very short\n(under 10 min.) tasks."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.1412v1", 
    "title": "Spreadsheet Refactoring", 
    "arxiv-id": "1009.1412v1", 
    "author": "Patrick O'Beirne", 
    "publish": "2010-09-07T21:35:50Z", 
    "summary": "Refactoring is a change made to the internal structure of software to make it\neasier to understand and cheaper to modify without changing its observable\nbehaviour. A database refactoring is a small change to the database schema\nwhich improves its design without changing its semantics. This paper presents\nexample 'spreadsheet refactorings', derived from the above and taking into\naccount the unique characteristics of spreadsheet formulas and VBA code. The\ntechniques are constrained by the tightly coupled data and code in\nspreadsheets."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2765v1", 
    "title": "How do Range Names Hinder Novice Spreadsheet Debugging Performance?", 
    "arxiv-id": "1009.2765v1", 
    "author": "Kevin McDaid", 
    "publish": "2010-09-14T20:15:45Z", 
    "summary": "Although experts diverge on how best to improve spreadsheet quality, it is\ngenerally agreed that more time needs to be spent testing spreadsheets.\nIdeally, experienced and trained spreadsheet engineers would carry this out,\nbut quite often this is neither practical nor possible. Many spreadsheets are a\nlegacy, developed by staff that have since moved on, or indeed modified by many\nstaff no longer employed by the organisation. When such spreadsheets fall into\nthe hands of inexperienced, non-experts, any features that reduce error\nvisibility may become a risk. Range names are one such feature, and this paper,\nbuilding on previous research, investigates in a more structured and controlled\nmanner the effect they have on the debugging performance of novice spreadsheet\nusers."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2775v1", 
    "title": "Spreadsheet Risk Management in Organisations", 
    "arxiv-id": "1009.2775v1", 
    "author": "Eoin Langan", 
    "publish": "2010-09-14T20:32:29Z", 
    "summary": "The paper examines in the context of financial reporting, the controls that\norganisations have in place to manage spreadsheet risk and errors. There has\nbeen widespread research conducted in this area, both in Ireland and\ninternationally. This paper describes a study involving 19 participants (2 case\nstudies and 17 by survey) from Ireland. Three areas are examined; firstly, the\nextent of spreadsheet usage, secondly, the level of complexity employed in\nspreadsheets, and finally, the controls in place regarding spreadsheets. The\nfindings support previous findings of Panko (1998), that errors occur\nfrequently in spreadsheets and that there is little or unenforced controls\nemployed, however this research finds that attitudes are changing with regard\nto spreadsheet risk and that one organisation is implementing a comprehensive\nproject regarding policies on the development and control of spreadsheets.\nFurther research could be undertaken in the future to examine the development\nof a \"best practice model\" both for the reduction in errors and to minimise the\nrisk in spreadsheet usage."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2785v1", 
    "title": "The Detection of Human Spreadsheet Errors by Humans versus Inspection   (Auditing) Software", 
    "arxiv-id": "1009.2785v1", 
    "author": "Raymond R. Panko", 
    "publish": "2010-09-14T20:54:42Z", 
    "summary": "Previous spreadsheet inspection experiments have had human subjects look for\nseeded errors in spreadsheets. In this study, subjects attempted to find errors\nin human-developed spreadsheets to avoid the potential artifacts created by\nerror seeding. Human subject success rates were compared to the successful\nrates for error-flagging by spreadsheet static analysis tools (SSATs) applied\nto the same spreadsheets. The human error detection results were comparable to\nthose of studies using error seeding. However, Excel Error Check and\nSpreadsheet Professional were almost useless for correctly flagging natural\n(human) errors in this study."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2787v1", 
    "title": "Teaching Spreadsheets: Curriculum Design Principles", 
    "arxiv-id": "1009.2787v1", 
    "author": "Francoise Tort", 
    "publish": "2010-09-14T21:14:24Z", 
    "summary": "EuSpRIG concerns direct researchers to revisit spreadsheet education, taking\ninto account error auditing tools, checklists, and good practices. This paper\naims at elaborating principles to design a spreadsheet curriculum. It mainly\nfocuses on two important issues. Firstly, it is necessary to establish the\nspreadsheet invariants to be taught, especially those concerning errors and\ngood practices. Secondly, it is important to take into account the learners'\nICT experience, and to encourage them to attitudes that foster self-learning.\nWe suggest key principles for spreadsheet teaching, and we illustrate them with\nteaching guidelines."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2797v1", 
    "title": "What we understand is what we get: Assessment in Spreadsheets", 
    "arxiv-id": "1009.2797v1", 
    "author": "Michael Kohlhase", 
    "publish": "2010-09-14T21:39:37Z", 
    "summary": "In previous work we have studied how an explicit representation of background\nknowledge associated with a specific spreadsheet can be exploited to alleviate\nusability problems with spreadsheet-based applications. We have implemented\nthis approach in the SACHS system to provide a semantic help system for\nspreadsheets applications. In this paper, we evaluate the (comprehension)\ncoverage of SACHS on an Excel-based financial controlling system via a\n\"Wizard-of-Oz\" experiment. This shows that SACHS adds significant value, but\nsystematically misses important classes of explanations. For judgements about\nthe information contained in spreadsheets, we provide a first approach for an\n\"assessment module\" in SACHS."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.2902v1", 
    "title": "Informal Control code logic", 
    "arxiv-id": "1009.2902v1", 
    "author": "Jan A. Bergstra", 
    "publish": "2010-09-15T12:02:04Z", 
    "summary": "General definitions as well as rules of reasoning regarding control code\nproduction, distribution, deployment, and usage are described. The role of\ntesting, trust, confidence and risk analysis is considered. A rationale for\ncontrol code testing is sought and found for the case of safety critical\nembedded control code."
},{
    "category": "cs.SE", 
    "doi": "10.3329/jbas.v32i2.2432", 
    "link": "http://arxiv.org/pdf/1009.3462v1", 
    "title": "On Modelling and Analysis of Dynamic Reconfiguration of Dependable   Real-Time Systems", 
    "arxiv-id": "1009.3462v1", 
    "author": "Anirban Bhattacharyya", 
    "publish": "2010-09-17T16:09:29Z", 
    "summary": "This paper motivates the need for a formalism for the modelling and analysis\nof dynamic reconfiguration of dependable real-time systems. We present\nrequirements that the formalism must meet, and use these to evaluate well\nestablished formalisms and two process algebras that we have been developing,\nnamely, Webpi and CCSdp. A simple case study is developed to illustrate the\nmodelling power of these two formalisms. The paper shows how Webpi and CCSdp\nrepresent a significant step forward in modelling adaptive and dependable\nreal-time systems."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.35.1", 
    "link": "http://arxiv.org/pdf/1009.3710v1", 
    "title": "Optimizing Computation of Recovery Plans for BPEL Applications", 
    "arxiv-id": "1009.3710v1", 
    "author": "Marsha Chechik", 
    "publish": "2010-09-20T07:19:26Z", 
    "summary": "Web service applications are distributed processes that are composed of\ndynamically bounded services. In our previous work [15], we have described a\nframework for performing runtime monitoring of web service against behavioural\ncorrectness properties (described using property patterns and converted into\nfinite state automata). These specify forbidden behavior (safety properties)\nand desired behavior (bounded liveness properties). Finite execution traces of\nweb services described in BPEL are checked for conformance at runtime. When\nviolations are discovered, our framework automatically proposes and ranks\nrecovery plans which users can then select for execution. Such plans for safety\nviolations essentially involve \"going back\" - compensating the executed actions\nuntil an alternative behaviour of the application is possible. For bounded\nliveness violations, recovery plans include both \"going back\" and \"re-planning\"\n- guiding the application towards a desired behaviour. Our experience, reported\nin [16], identified a drawback in this approach: we compute too many plans due\nto (a) overapproximating the number of program points where an alternative\nbehaviour is possible and (b) generating recovery plans for bounded liveness\nproperties which can potentially violate safety properties. In this paper, we\ndescribe improvements to our framework that remedy these problems and describe\ntheir effectiveness on a case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.35.5", 
    "link": "http://arxiv.org/pdf/1009.3714v1", 
    "title": "Browser-based Analysis of Web Framework Applications", 
    "arxiv-id": "1009.3714v1", 
    "author": "Michael Goedicke", 
    "publish": "2010-09-20T07:19:50Z", 
    "summary": "Although web applications evolved to mature solutions providing sophisticated\nuser experience, they also became complex for the same reason. Complexity\nprimarily affects the server-side generation of dynamic pages as they are\naggregated from multiple sources and as there are lots of possible processing\npaths depending on parameters. Browser-based tests are an adequate instrument\nto detect errors within generated web pages considering the server-side process\nand path complexity a black box. However, these tests do not detect the cause\nof an error which has to be located manually instead. This paper proposes to\ngenerate metadata on the paths and parts involved during server-side processing\nto facilitate backtracking origins of detected errors at development time.\nWhile there are several possible points of interest to observe for\nbacktracking, this paper focuses user interface components of web frameworks."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.35.6", 
    "link": "http://arxiv.org/pdf/1009.3715v1", 
    "title": "Contracting the Facebook API", 
    "arxiv-id": "1009.3715v1", 
    "author": "Tevfik Bultan", 
    "publish": "2010-09-20T07:19:53Z", 
    "summary": "In recent years, there has been an explosive growth in the popularity of\nonline social networks such as Facebook. In a new twist, third party developers\nare now able to create their own web applications which plug into Facebook and\nwork with Facebook's \"social\" data, enabling the entire Facebook user base of\nmore than 400 million active users to use such applications. These client\napplications can contain subtle errors that can be hard to debug if they misuse\nthe Facebook API. In this paper we present an experience report on applying\nMicrosoft's new code contract system for the .NET framework to the Facebook\nAPI.We wrote contracts for several classes in the Facebook API wrapper which\nallows Microsoft .NET developers to implement Facebook applications. We\nevaluated the usefulness of these contracts during implementation of a new\nFacebook application. Our experience indicates that having code contracts\nprovides a better and quicker software development experience."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.35.6", 
    "link": "http://arxiv.org/pdf/1009.3911v1", 
    "title": "Deriving Specifications of Dependable Systems: toward a Method", 
    "arxiv-id": "1009.3911v1", 
    "author": "Manuel Mazzara", 
    "publish": "2010-09-20T18:50:08Z", 
    "summary": "This paper proposes a method for deriving formal specifications of systems.\nTo accomplish this task we pass through a non trivial number of steps, concepts\nand tools where the first one, the most important, is the concept of method\nitself, since we realized that computer science has a proliferation of\nlanguages but very few methods. We also propose the idea of Layered Fault\nTolerant Specification (LFTS) to make the method extensible to dependable\nsystems. The principle is layering the specification, for the sake of clarity,\nin (at least) two different levels, the first one for the normal behavior and\nthe others (if more than one) for the abnormal. The abnormal behavior is\ndescribed in terms of an Error Injector (EI) which represents a model of the\nerroneous interference coming from the environment. This structure has been\ninspired by the notion of idealized fault tolerant component but the\ncombination of LFTS and EI using rely guarantee thinking to describe\ninterference can be considered one of the main contributions of this work. The\nprogress toward this method and the way to layer specifications has been made\nexperimenting on the Transportation and the Automotive Case Studies of the\nDEPLOY project."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.36.1", 
    "link": "http://arxiv.org/pdf/1009.4259v1", 
    "title": "Modeling and Analyzing Adaptive User-Centric Systems in Real-Time Maude", 
    "arxiv-id": "1009.4259v1", 
    "author": "Andreas Schroeder", 
    "publish": "2010-09-22T04:02:03Z", 
    "summary": "Pervasive user-centric applications are systems which are meant to sense the\npresence, mood, and intentions of users in order to optimize user comfort and\nperformance. Building such applications requires not only state-of-the art\ntechniques from artificial intelligence but also sound software engineering\nmethods for facilitating modular design, runtime adaptation and verification of\ncritical system requirements.\n  In this paper we focus on high-level design and analysis, and use the\nalgebraic rewriting language Real-Time Maude for specifying applications in a\nreal-time setting. We propose a generic component-based approach for modeling\npervasive user-centric systems and we show how to analyze and prove crucial\nproperties of the system architecture through model checking and simulation.\nFor proving time-dependent properties we use Metric Temporal Logic (MTL) and\npresent analysis algorithms for model checking two subclasses of MTL formulas:\ntime-bounded response and time-bounded safety MTL formulas. The underlying idea\nis to extend the Real-Time Maude model with suitable clocks, to transform the\nMTL formulas into LTL formulas over the extended specification, and then to use\nthe LTL model checker of Maude. It is shown that these analyses are sound and\ncomplete for maximal time sampling. The approach is illustrated by a simple\nadaptive advertising scenario in which an adaptive advertisement display can\nreact to actions of the users in front of the display."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.36.1", 
    "link": "http://arxiv.org/pdf/1009.5087v1", 
    "title": "Bus Protocols: MSC-Based Specifications and Translation into Program of   Verification Tool for Formal Verification", 
    "arxiv-id": "1009.5087v1", 
    "author": "Kamrul Hasan Talukder", 
    "publish": "2010-09-26T12:38:05Z", 
    "summary": "Message Sequence Charts (MSCs) are an appealing visual formalism mainly used\nin the early stages of system design to capture the system requirements.\nHowever, if we move towards an implementation, an executable specifications\nrelated in some fashion to the MSC-based requirements must be obtained. The\nMSCs can be used effectively to specify the bus protocol in the way where\nhigh-level transition systems is used to capture the control flow of the system\ncomponents of the protocol and MSCs to describe the non-atomic component\ninteractions. This system of specification is amenable to formal verification.\nIn this paper, we present the way how we can specify the bus protocols using\nMSCs and how these specifications can be translated into program of\nverification tool (we have used Symbolic Model Verifier (SMV)) for the use of\nformal verification. We have contributed to the following tasks in this\nrespect. Firstly, the way to specify the protocol using MSC has been presented.\nSecondly, a translator that translates the specifications (described in a\ntextual input file) into SMV programs has been constructed. Finally, we have\npresented the verification result of the AMBA bus protocol using the SMV\nprogram found through the translation process. The SMV program found through\nthe translation process can be used in order to automatically verify various\nproperties of any bus protocol specified."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.36.1", 
    "link": "http://arxiv.org/pdf/1009.5088v1", 
    "title": "Modelling Variability for System Families", 
    "arxiv-id": "1009.5088v1", 
    "author": "Khademul Islam Molla", 
    "publish": "2010-09-26T12:38:18Z", 
    "summary": "In this paper, an approach to facilitate the treatment with variabilities in\nsystem families is presented by explicitly modelling variants. The proposed\nmethod of managing variability consists of a variant part, which models\nvariants and a decision table to depict the customisation decision regarding\neach variant. We have found that it is easy to implement and has advantage over\nother methods. We present this model as an integral part of modelling system\nfamilies."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.36.1", 
    "link": "http://arxiv.org/pdf/1009.5698v1", 
    "title": "Defending the future: An MSc module in End User Computing Risk   Management", 
    "arxiv-id": "1009.5698v1", 
    "author": "Simon Thorne", 
    "publish": "2010-09-28T21:15:49Z", 
    "summary": "This paper describes the rationale, curriculum and subject matter of a new\nMSc module being taught on an MSc Finance and Information Management course at\nthe University of Wales Institute in Cardiff. Academic research on spreadsheet\nrisks now has some penetration in academic literature and there is a growing\nbody of knowledge on the subjects of spreadsheet error, human factors,\nspreadsheet engineering, \"best practice\", spreadsheet risk management and\nvarious techniques used to mitigate spreadsheet errors. This new MSc module in\nEnd User Computing Risk Management is an attempt to pull all of this research\nand practitioner experience together to arm the next generation of finance\nspreadsheet champions with the relevant knowledge, techniques and critical\nperspective on an emerging discipline."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.36.1", 
    "link": "http://arxiv.org/pdf/1009.5705v1", 
    "title": "Spreadsheets - the Good, the Bad and the Downright Ugly", 
    "arxiv-id": "1009.5705v1", 
    "author": "Angus Dunn", 
    "publish": "2010-09-28T21:46:50Z", 
    "summary": "Spreadsheets are ubiquitous, heavily relied on throughout vast swathes of\nfinance, commerce, industry, academia and Government. They are also\nacknowledged to be extraordinarily and unacceptably prone to error. If these\ntwo points are accepted, it has to follow that their uncontrolled use has the\npotential to inflict considerable damage. One approach to controlling such\nerror should be to define as \"good practice\" a set of characteristics that a\nspreadsheet must possess and as \"bad practice\" another set that it must avoid.\nDefining such characteristics should, in principle, perfectly do-able. However,\nbeing able to say with authority at a definite moment that any particular\nspreadsheet complies with these characteristics is very much more difficult.\nThe author asserts that the use of automated spreadsheet development could\nmarkedly help in ensuring and demonstrating such compliance."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37", 
    "link": "http://arxiv.org/pdf/1010.2337v1", 
    "title": "Proceedings International Workshop on Component and Service   Interoperability", 
    "arxiv-id": "1010.2337v1", 
    "author": "Gwen Sala\u00fcn", 
    "publish": "2010-10-12T09:51:51Z", 
    "summary": "This volume contains the proceedings of WCSI 2010, the International Workshop\non Component and Service Interoperability. WCSI 2010 was held in Malaga (Spain)\non June 29th, 2010 as a satellite event of the TOOLS 2010 Federated\nConferences. The papers published in this volume tackle different issues that\nare currently central to our community, namely definition of expressive\ninterface languages, formal models and approaches to software composition and\nadaptation, interface-based compatibility and substitutability, and\nverification techniques for distributed software."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37", 
    "link": "http://arxiv.org/pdf/1010.2354v1", 
    "title": "Predicting Coding Effort in Projects Containing XML Code", 
    "arxiv-id": "1010.2354v1", 
    "author": "Marlon Dumas", 
    "publish": "2010-10-12T11:30:47Z", 
    "summary": "This paper studies the problem of predicting the coding effort for a\nsubsequent year of development by analysing metrics extracted from project\nrepositories, with an emphasis on projects containing XML code. The study\nconsiders thirteen open source projects and applies machine learning algorithms\nto generate models to predict one-year coding effort, measured in terms of\nlines of code added, modified and deleted. Both organisational and code metrics\nassociated to revisions are taken into account. The results show that coding\neffort is highly determined by the expertise of developers while source code\nmetrics have little effect on improving the accuracy of estimations of coding\neffort. The study also shows that models trained on one project are unreliable\nat estimating effort in other projects."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37.1", 
    "link": "http://arxiv.org/pdf/1010.2822v1", 
    "title": "Contract Aware Components, 10 years after", 
    "arxiv-id": "1010.2822v1", 
    "author": "No\u00ebl Plouzeau", 
    "publish": "2010-10-14T05:15:54Z", 
    "summary": "The notion of contract aware components has been published roughly ten years\nago and is now becoming mainstream in several fields where the usage of\nsoftware components is seen as critical. The goal of this paper is to survey\ndomains such as Embedded Systems or Service Oriented Architecture where the\nnotion of contract aware components has been influential. For each of these\ndomains we briefly describe what has been done with this idea and we discuss\nthe remaining challenges."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37.3", 
    "link": "http://arxiv.org/pdf/1010.2825v1", 
    "title": "Components Interoperability through Mediating Connector Patterns", 
    "arxiv-id": "1010.2825v1", 
    "author": "Paola Inverardi", 
    "publish": "2010-10-14T05:16:22Z", 
    "summary": "A key objective for ubiquitous environments is to enable system\ninteroperability between system's components that are highly heterogeneous. In\nparticular, the challenge is to embed in the system architecture the necessary\nsupport to cope with behavioral diversity in order to allow components to\ncoordinate and communicate. The continuously evolving environment further asks\nfor an automated and on-the-fly approach. In this paper we present the design\nbuilding blocks for the dynamic and on-the-fly interoperability between\nheterogeneous components. Specifically, we describe an Architectural Pattern\ncalled Mediating Connector, that is the key enabler for communication. In\naddition, we present a set of Basic Mediator Patterns, that describe the basic\nmismatches which can occur when components try to interact, and their\ncorresponding solutions."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37.6", 
    "link": "http://arxiv.org/pdf/1010.2827v1", 
    "title": "Multilevel Contracts for Trusted Components", 
    "arxiv-id": "1010.2827v1", 
    "author": "Christian Attiogb\u00e9", 
    "publish": "2010-10-14T05:16:31Z", 
    "summary": "This article contributes to the design and the verification of trusted\ncomponents and services. The contracts are declined at several levels to cover\nthen different facets, such as component consistency, compatibility or\ncorrectness. The article introduces multilevel contracts and a\ndesign+verification process for handling and analysing these contracts in\ncomponent models. The approach is implemented with the COSTO platform that\nsupports the Kmelia component model. A case study illustrates the overall\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37.6", 
    "link": "http://arxiv.org/pdf/1010.3045v1", 
    "title": "The Emerging Web of Social Machines", 
    "arxiv-id": "1010.3045v1", 
    "author": "Vin\u00edcius Garcia", 
    "publish": "2010-10-14T23:53:07Z", 
    "summary": "We define a notion of social machine and envisage an algebra that can\ndescribe networks of such. To start with, social machines are defined as tuples\nof input, output, processes, constraints, state, requests and responses; apart\nfrom defining the machines themselves, the algebra defines a set of connectors\nand conditionals that can be used to describe the interactions between any\nnumber of machines in a multitude of ways, as a means to represent real\nmachines interacting in the real web, such as Twitter, Twitter running on top\nof Amazon AWS, mashups built using Twitter and, obviously, other social\nmachines. This work is not a theoretical paper as yet; but, in more than one\nsense, we think we have found a way to describe web based information systems\nand are starting to work on what could be a practical way of dealing with the\ncomplexity of this emerging web of social machines that is all around us. This\nversion should be read as work in progress and comments, observations, bugs...\nare most welcome and should be sent to the email of the first, corresponding\nauthor."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.37.6", 
    "link": "http://arxiv.org/pdf/1010.3100v1", 
    "title": "On Object-Orientation", 
    "arxiv-id": "1010.3100v1", 
    "author": "B. Diertens", 
    "publish": "2010-10-15T09:01:45Z", 
    "summary": "Although object-orientation has been around for several decades, its key\nconcept abstraction has not been exploited for proper application of\nobject-orientation in other phases of software development than the\nimplementation phase. We mention some issues that lead to a lot of confusion\nand obscurity with object-orientation and its application in software\ndevelopment. We describe object-orientation as abstract as possible such that\nit can be applied to all phases of software development."
},{
    "category": "cs.SE", 
    "doi": "10.4304/jsw.7.5.1149-1154", 
    "link": "http://arxiv.org/pdf/1010.4092v3", 
    "title": "Predicting Bugs' Components via Mining Bug Reports", 
    "arxiv-id": "1010.4092v3", 
    "author": "Hongping Hu", 
    "publish": "2010-10-20T03:08:09Z", 
    "summary": "The number of bug reports in complex software increases dramatically. Now\nbugs are triaged manually, bug triage or assignment is a labor-intensive and\ntime-consuming task. Without knowledge about the structure of the software,\ntesters often specify the component of a new bug wrongly. Meanwhile, it is\ndifficult for triagers to determine the component of the bug only by its\ndescription. We dig out the components of 28,829 bugs in Eclipse bug project\nhave been specified wrongly and modified at least once. It results in these\nbugs have to be reassigned and delays the process of bug fixing. The average\ntime of fixing wrongly-specified bugs is longer than that of\ncorrectly-specified ones. In order to solve the problem automatically, we use\nhistorical fixed bug reports as training corpus and build classifiers based on\nsupport vector machines and Na\\\"ive Bayes to predict the component of a new\nbug. The best prediction accuracy reaches up to 81.21% on our validation corpus\nof Eclipse project. Averagely our predictive model can save about 54.3 days for\ntriagers and developers to repair a bug. Keywords: bug reports; bug triage;\ntext classification; predictive model"
},{
    "category": "cs.SE", 
    "doi": "10.1109/MCSE.2011.35", 
    "link": "http://arxiv.org/pdf/1010.4891v1", 
    "title": "Mayavi: a package for 3D visualization of scientific data", 
    "arxiv-id": "1010.4891v1", 
    "author": "Ga\u00ebl Varoquaux", 
    "publish": "2010-10-23T16:34:51Z", 
    "summary": "Mayavi is an open-source, general-purpose, 3D scientific visualization\npackage. It seeks to provide easy and interactive tools for data visualization\nthat fit with the scientific user's workflow. For this purpose, Mayavi provides\nseveral entry points: a full-blown interactive application; a Python library\nwith both a MATLAB-like interface focused on easy scripting and a feature-rich\nobject hierarchy; widgets associated with these objects for assembling in a\ndomain-specific application, and plugins that work with a general purpose\napplication-building framework. In this article, we present an overview of the\nvarious features of Mayavi, we then provide insight on the design and\nengineering decisions made in implementing Mayavi, and finally discuss a few\nnovel applications."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MCSE.2011.35", 
    "link": "http://arxiv.org/pdf/1010.6155v1", 
    "title": "Well-formedness and typing rules for UML Composite Structures", 
    "arxiv-id": "1010.6155v1", 
    "author": "Iulian Ober", 
    "publish": "2010-10-29T09:05:27Z", 
    "summary": "Starting from version 2.0, UML introduced hierarchical composite structures,\nwhich are an expressive way of defining complex software architectures, but\nwhich have a very loosely defined semantics in the standard. In this paper we\npropose a set of consistency rules that disambiguate the meaning of UML\ncomposite structures. Our primary goal was to have an operational model of\ncomposite structures for the OMEGA UML profile, an executable profile dedicated\nto the formal specification and validation of real-time systems, developed in a\npast project to which we contributed. However, the rules and principles stated\nhere are applicable to other hierarchical component models based on the same\nconcepts, such as SysML. The presented ruleset is supported by an OCL\nformalization which is described in this report. This formalization was applied\non different complex models for the evaluation and validation of the proposed\nprinciples."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.0278v1", 
    "title": "Formalization of the data flow diagram rules for consistency check", 
    "arxiv-id": "1011.0278v1", 
    "author": "Siow Yen yen", 
    "publish": "2010-11-01T09:51:34Z", 
    "summary": "In system development life cycle (SDLC), a system model can be developed\nusing Data Flow Diagram (DFD). DFD is graphical diagrams for specifying,\nconstructing and visualizing the model of a system. DFD is used in defining the\nrequirements in a graphical view. In this paper, we focus on DFD and its rules\nfor drawing and defining the diagrams. We then formalize these rules and\ndevelop the tool based on the formalized rules. The formalized rules for\nconsistency check between the diagrams are used in developing the tool. This is\nto ensure the syntax for drawing the diagrams is correct and strictly followed.\nThe tool automates the process of manual consistency check between data flow\ndiagrams."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.0280v1", 
    "title": "From UML Specification into Implementation using Object Mapping", 
    "arxiv-id": "1011.0280v1", 
    "author": "Rosziati Ibrahim", 
    "publish": "2010-11-01T10:03:32Z", 
    "summary": "In information systems, a system is analyzed using a modeling tool. Analysis\nis an important phase prior to implementation in order to obtain the correct\nrequirements of the system. During the requirements phase, the software\nrequirements specification (SRS) is used to specify the system requirements.\nThen, this requirements specification is used to implement the system. The\nrequirements specification can be represented using either a structure approach\nor an object-oriented approach. A UML (Unified Modeling Language) specification\nis a well-known for representation of requirements specification in an\nobject-oriented approach. In this paper, we present one case study and discuss\nhow mapping from UML specification into implementation is done. The case study\ndoes not require advanced programming skills. However, it does require\nfamiliarity in creating and instantiating classes, object-oriented programming\nwith inheritance, data structure, file processing and control loop. For the\ncase study, UML specification is used in requirements phase and Borland C++ is\nused in implementation phase. Based on the case study, it shows that the\nproposed approach improved the understanding of mapping from UML specification\ninto implementation."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.0594v1", 
    "title": "Heuristic Approach of Automated Test Data Generation for Program having   Array of Different Dimensions and Loops with Variable Number of Iteration", 
    "arxiv-id": "1011.0594v1", 
    "author": "Bichitra Kalita", 
    "publish": "2010-11-02T12:19:46Z", 
    "summary": "Normally, program execution spends most of the time on loops. Automated test\ndata generation devotes special attention to loops for better coverage.\nAutomated test data generation for programs having loops with variable number\nof iteration and variable length array is a challenging problem. It is so\nbecause the number of paths may increase exponentially with the increase of\narray size for some programming constructs, like merge sort. We propose a\nmethod that finds heuristic for different types of programming constructs with\nloops and arrays. Linear search, Bubble sort, merge sort, and matrix\nmultiplication programs are included in an attempt to highlight the difference\nin execution between single loop, variable length array and nested loops with\none and two dimensional arrays. We have used two parameters/heuristics to\npredict the minimum number of iterations required for generating automated test\ndata. They are longest path level (kL) and saturation level (kS). The\nproceedings of our work includes the instrumentation of source code at the\nelementary level, followed by the application of the random inputs until all\nfeasible paths or all paths having longest paths are collected. However,\nduplicate paths are avoided by using a filter. Our test data is the random\nnumbers that cover each feasible path."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.0604v1", 
    "title": "Improving the Technical Aspects of Software Testing in Enterprises", 
    "arxiv-id": "1011.0604v1", 
    "author": "Tim A. Majchrzak", 
    "publish": "2010-11-02T12:54:18Z", 
    "summary": "Many software developments projects fail due to quality problems. Software\ntesting enables the creation of high quality software products. Since it is a\ncumbersome and expensive task, and often hard to manage, both its technical\nbackground and its organizational implementation have to be well founded. We\nworked with regional companies that develop software in order to learn about\ntheir distinct weaknesses and strengths with regard to testing. Analyzing and\ncomparing the strengths, we derived best practices. In this paper we explain\nthe project's background and sketch the design science research methodology\nused. We then introduce a graphical categorization framework that helps\ncompanies in judging the applicability of recommendations. Eventually, we\npresent details on five recommendations for tech-nical aspects of testing. For\neach recommendation we give im-plementation advice based on the categorization\nframework."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.1021v1", 
    "title": "What's the point of documentation?", 
    "arxiv-id": "1011.1021v1", 
    "author": "Louise Pryor", 
    "publish": "2010-11-03T22:08:02Z", 
    "summary": "We give a brief characterisation of the purposes and forms of documentation\nin and of spreadsheets."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.1551v1", 
    "title": "An Introduction to Software Engineering and Fault Tolerance", 
    "arxiv-id": "1011.1551v1", 
    "author": "Alexander Romanovsky", 
    "publish": "2010-11-06T10:27:01Z", 
    "summary": "This book consists of the chapters describing novel approaches to integrating\nfault tolerance into software development process. They cover a wide range of\ntopics focusing on fault tolerance during the different phases of the software\ndevelopment, software engineering techniques for verification and validation of\nfault tolerance means, and languages for supporting fault tolerance\nspecification and implementation. Accordingly, the book is structured into the\nfollowing three parts: Part A: Fault tolerance engineering: from requirements\nto code; Part B: Verification and validation of fault tolerant systems; Part C:\nLanguages and Tools for engineering fault tolerant systems."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.2163v1", 
    "title": "Component Based Development", 
    "arxiv-id": "1011.2163v1", 
    "author": "Debayan Bose", 
    "publish": "2010-11-09T18:00:35Z", 
    "summary": "Component Based Approach has been introduced in core engineering discipline\nlong back but the introduction to component based concept in software\nperspective is recently developed by Object Management Group. Its benefits from\nthe re-usability point of view is enormous. The intertwining relationship of\ndomain engineering with component based software engineering is analyzed. The\nobject oriented approach and its basic difference with component approach is of\ngreat concern. The present study highlights the life-cycle, cost effectiveness\nand the basic study of component based software from application perspective."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.2238v1", 
    "title": "Introducing Business Language Driven Development", 
    "arxiv-id": "1011.2238v1", 
    "author": "Fernando Luiz de Carvalho e Silva", 
    "publish": "2010-11-09T23:55:05Z", 
    "summary": "A classical problem in Software Engineering is how to certify that every\nsystem requirement is correctly implemented by source code. This problem,\nalbeit well studied, can still be considered an open one, given the problems\nfaced by software development organizations. Trying to solve this problem,\nBehavior-Driven Development (BDD) is a specification technique that\nautomatically certifies that all functional requirements are treated properly\nby source code, through the connection of the textual description of these\nrequirements to automated tests. However, in some areas, such as Enterprise\nInformation Systems, requirements are identified by Business Process Modeling -\nwhich uses graphical notations of the underlying business processes. Therefore,\nthe aim of this paper is to present Business Language Driven Development\n(BLDD), a method that aims to extend BDD, by connecting business process models\ndirectly to source code, while keeping the expressiveness of text descriptions\nwhen they are better fitted than graphical artifacts."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.2652v1", 
    "title": "A Formal Model for Dynamically Adaptable Services", 
    "arxiv-id": "1011.2652v1", 
    "author": "Jorge Fox", 
    "publish": "2010-11-11T13:42:45Z", 
    "summary": "The growing complexity of software systems as well as changing conditions in\ntheir operating environment demand systems that are more flexible, adaptive and\ndependable. The service-oriented computing paradigm is in widespread use to\nsupport such adaptive systems, and, in many domains, adaptations may occur\ndynamically and in real time. In addition, services from heterogeneous,\npossibly unknown sources may be used. This motivates a need to ensure the\ncorrect behaviour of the adapted systems, and its continuing compliance to time\nbounds and other QoS properties. The complexity of dynamic adaptation (DA) is\nsignificant, but currently not well understood or formally specified. This\npaper elaborates a well-founded model and theory of DA, introducing formalisms\nwritten using COWS. The model is evaluated for reliability and responsiveness\nproperties with the model checker CMC."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2010.1406", 
    "link": "http://arxiv.org/pdf/1011.5389v1", 
    "title": "A Model for Configuration Management of Open Software Systems", 
    "arxiv-id": "1011.5389v1", 
    "author": "Paolo Milazzo", 
    "publish": "2010-11-24T15:03:12Z", 
    "summary": "The article proposes a model for the configuration management of open\nsystems. The model aims at validation of configurations against given\nspecifications. An extension of decision graphs is proposed to express\nspecifications. The proposed model can be used by software developers to\nvalidate their own configurations across different versions of the components,\nor to validate configurations that include components by third parties. The\nmodel can also be used by end-users to validate compatibility among different\nconfigurations of the same application. The proposed model is first discussed\nin some application scenarios and then formally defined. Moreover, a type\ndiscipline is given to formally define validation of a configuration against a\nsystem specification"
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1011.6461v1", 
    "title": "Precisely Analyzing Loss in Interface Adapter Chains", 
    "arxiv-id": "1011.6461v1", 
    "author": "Yoo Chung", 
    "publish": "2010-11-30T05:24:01Z", 
    "summary": "Interface adaptation allows code written for one interface to be used with a\nsoftware component with another interface. When multiple adapters are chained\ntogether to make certain adaptations possible, we need a way to analyze how\nwell the adaptation is done in case there are more than one chains that can be\nused. We introduce an approach to precisely analyzing the loss in an interface\nadapter chain using a simple form of abstract interpretation."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1012.0038v1", 
    "title": "Testing by C++ template metaprograms", 
    "arxiv-id": "1012.0038v1", 
    "author": "Norbert Pataki", 
    "publish": "2010-11-30T21:56:11Z", 
    "summary": "Testing is one of the most indispensable tasks in software engineering. The\nrole of testing in software development has grown significantly because testing\nis able to reveal defects in the code in an early stage of development. Many\nunit test frameworks compatible with C/C++ code exist, but a standard one is\nmissing. Unfortunately, many unsolved problems can be mentioned with the\nexisting methods, for example usually external tools are necessary for testing\nC++ programs.\n  In this paper we present a new approach for testing C++ programs. Our\nsolution is based on C++ template metaprogramming facilities, so it can work\nwith the standard-compliant compilers. The metaprogramming approach ensures\nthat the overhead of testing is minimal at runtime. This approach also supports\nthat the specification language can be customized among other advantages.\nNevertheless, the only necessary tool is the compiler itself."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1012.1153v1", 
    "title": "Mobiles ortsbezogenes Projektmanagement", 
    "arxiv-id": "1012.1153v1", 
    "author": "Jan Zibuschka", 
    "publish": "2010-12-06T12:37:37Z", 
    "summary": "Classic project management and its tools usually deal with the management of\nthree variables, and their relationships with each other. These are the factors\nof time, resources (cost) and quality. If one of the variables is to be\nimproved, it always has negative effects on the other two. However, these\nfactors only partially describe the reality of project management. What current\nproject management tools often only consider implicitly is the location of an\nactivity. In this paper, the implications of using location data for project\nmanagement are clarified and a system that offers mobile support in planning\nand implementing projects.\n  -----\n  Klassisches Projektmanagement und seine Werkzeuge befassen sich meist mit der\nVerwaltung dreier Gr\\\"o{\\ss}en und ihrer Zusammenh\\\"ange untereinander. Dabei\nhandelt es sich um die Faktoren Zeit, Ressourcen (Kosten) und Qualit\\\"at. Falls\neine der Gr\\\"o{\\ss}en verbessert werden soll, hat dies immer negative\nAuswirkungen auf die anderen beiden Gr\\\"o{\\ss}en. Diese Gr\\\"o{\\ss}en\nbeschreiben die Ph\\\"anomene des Projektmanagement jedoch nur unvollst\\\"andig.\nWas bei Projektmanagementwerkzeugen bis dato oft nur implizit durch den\nProjektleiter einbezogen wird ist ein Ortsbezug. In diesem Beitrag werden die\nImplikationen durch diesen Ortsbezug konkretisiert und ein System dargestellt,\nwelches Projektleiter bei der Planung und Umsetzung von Projekten mobil wie\nauch station\\\"ar unterst\\\"utzt."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1012.1640v1", 
    "title": "Constraint-Guided Workflow Composition Based on the EDAM Ontology", 
    "arxiv-id": "1012.1640v1", 
    "author": "Tiziana Margaria", 
    "publish": "2010-12-07T23:38:12Z", 
    "summary": "Methods for the automatic composition of services into executable workflows\nneed detailed knowledge about the application domain,in particular about the\navailable services and their behavior in terms of input/output data\ndescriptions. In this paper we discuss how the EMBRACE data and methods\nontology (EDAM) can be used as background knowledge for the composition of\nbioinformatics workflows. We show by means of a small example domain that the\nEDAM knowledge facilitates finding possible workflows, but that additional\nknowledge is required to guide the search towards actually adequate solutions.\nWe illustrate how the ability to flexibly formulate domain-specific and\nproblem-specific constraints supports the work ow development process."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1012.2469v1", 
    "title": "UCMExporter: Supporting Scenario Transformations from Use Case Maps", 
    "arxiv-id": "1012.2469v1", 
    "author": "Yong He", 
    "publish": "2010-12-11T15:56:40Z", 
    "summary": "The Use Case Maps (UCM) scenario notation is applicable to many requirements\nengineering activities. However, other scenario notations, such as Message\nSequence Charts (MSC) and UML Sequence Diagrams (SD), have shown to be better\nsuited for detailed design. In order to use the notation that is best\nappropriate for each phase in an efficient manner, a mechanism has to be\ndevised to automatically transfer the knowledge acquired during the\nrequirements analysis phase (using UCM) to the design phase (using MSC or SD).\nThis paper introduces UCMEXPORTER, a new tool that implements such a mechanism\nand reduces the gap between high-level requirements and detailed design.\nUCMEXPORTER automatically transforms individual UCM scenarios to UML Sequence\nDiagrams, MSC scenarios, and even TTCN-3 test skeletons. We highlight the\ncurrent capabilities of the tool as well as architectural solutions addressing\nthe main challenges faced during such transformation, including the handling of\nconcurrent scenario paths, the generation of customized messages, and tool\ninteroperability."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1012.5739v1", 
    "title": "Business Mereology: Imaginative Definitions of Insourcing and   Outsourcing Transformations", 
    "arxiv-id": "1012.5739v1", 
    "author": "S. F. M. van Vlijmen", 
    "publish": "2010-12-28T11:28:17Z", 
    "summary": "Outsourcing, the passing on of tasks by organizations to other organizations,\noften including the personnel and means to perform these tasks, has become an\nimportant IT-business strategy over the past decades.\n  We investigate imaginative definitions for outsourcing relations and\noutsourcing transformations. Abstract models of an extreme and unrealistic\nsimplicity are considered in order to investigate possible definitions of\noutsourcing. Rather than covering all relevant practical cases an imaginative\ndefinition of a concept provides obvious cases of its instantiation from which\nmore refined or liberal definitions may be derived.\n  A definition of outsourcing induces to a complementary definition of\ninsourcing. Outsourcing and insourcing have more complex variations in which\nmultiple parties are involved. All of these terms both refer to state\ntransformations and to state descriptions pertaining to the state obtained\nafter such transformations. We make an attempt to disambiguate the terminology\nin that respect and we make an attempt to characterize the general concept of\nsourcing which captures some representative cases.\n  Because mereology is the most general theory of parthood relations we coin\nbusiness mereology as the general theory in business studies which concerns the\nfull variety of sourcing relations and transformations."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1101.0105v1", 
    "title": "Integration of Communication Analysis and the OO Method: Manual   derivation of the Conceptual Model. The SuperStationery Co. lab demo", 
    "arxiv-id": "1101.0105v1", 
    "author": "Marcela Ruiz", 
    "publish": "2010-12-30T16:25:55Z", 
    "summary": "This document presents a lab demo that exemplifies the manual derivation of\nan OO Method conceptual model, taking as input a Communication Analysis\nrequirements model. In addition, it is described how the conceptual model is\ncreated in the OLIVANOVA Modeler tool. The lab demo corresponds to part of the\nbusiness processes of a fictional small and medium enterprise named\nSuperStationery Co. This company provides stationery and office material to its\nclients. The company acts as a as intermediary: the company has a catalogue of\nproducts that are bought from suppliers and sold to clients. This lab demo,\nbesides illustrating the derivation technique, demonstrates that the technique\nis feasible in practice. Also, the results of this lab demo provide a valuable\nfeedback in order to improve the derivation technique."
},{
    "category": "cs.SE", 
    "doi": "10.2316/P.2011.720-059", 
    "link": "http://arxiv.org/pdf/1101.1718v1", 
    "title": "Precise Schedulability Analysis for unfeasible to notify separately for   comprehensive - EDF Scheduling of interrupted Hard Real-Time Tasks on the   similar Multiprocessors", 
    "arxiv-id": "1101.1718v1", 
    "author": "Jagbeer Singh", 
    "publish": "2011-01-10T08:31:45Z", 
    "summary": "In Real-time system, utilization based schedulability test is a common\napproach to determine whether or not tasks can be admitted without violating\ndeadline requirements. The exact problem has previously been proven intractable\neven upon single processors; sufficient conditions are presented here for\ndetermining whether a given periodic task system will meet all deadlines if\nscheduled non-preemptively upon a multiprocessor platform using the\nearliest-deadline first scheduling algorithm. Many real-time scheduling\nalgorithms have been developed recently to reduce affinity in the portable\ndevices that use processors. Extensive power aware scheduling techniques have\nbeen published for energy reduction, but most of them have been focused solely\non reducing the processor affinity. The non-preemptive scheduling of periodic\ntask systems upon processing platforms comprised of several same processors is\nconsidered."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.46.1", 
    "link": "http://arxiv.org/pdf/1101.4731v1", 
    "title": "Interface Theories for (A)synchronously Communicating Modal   I/O-Transition Systems", 
    "arxiv-id": "1101.4731v1", 
    "author": "Stephan Janisch", 
    "publish": "2011-01-25T06:57:34Z", 
    "summary": "Interface specifications play an important role in component-based software\ndevelopment. An interface theory is a formal framework supporting composition,\nrefinement and compatibility of interface specifications. We present different\ninterface theories which use modal I/O-transition systems as their underlying\ndomain for interface specifications: synchronous interface theories, which\nemploy a synchronous communication schema, as well as a novel interface theory\nfor asynchronous communication where components communicate via FIFO-buffers."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.46.1", 
    "link": "http://arxiv.org/pdf/1101.5341v2", 
    "title": "A practical guide to Message Structures: a modelling technique for   information systems analysis and design", 
    "arxiv-id": "1101.5341v2", 
    "author": "Marcela Ruiz", 
    "publish": "2011-01-27T17:11:24Z", 
    "summary": "Despite the increasing maturity of model-driven software development (MDD),\nsome research challenges remain open in the field of information systems (IS).\nFor instance, there is a need to improve modelling techniques so that they\ncover several development stages in an integrated way, and they facilitate the\ntransition from analysis to design. This paper presents Message Structures, a\ntechnique for the specification of communicative interactions between the IS\nand organisational actors. This technique can be used both in the analysis\nstage and in the design stage. During analysis, it allows abstracting from the\ntechnology that will support the IS, and to complement business process\ndiagramming techniques with the specification of the communicational needs of\nthe organisation. During design, Message Structures serves two purposes: (i) it\nallows to systematically derive a specification of the IS memory (e.g. a UML\nclass diagram), (ii) and it allows to reason the user interface design using\nabstract patterns. This technique is part of Communication Analysis, a\ncommunication-oriented requirements engineering method, but it can be adopted\nin order to extend widely-used business process and functional requirements\nmodelling techniques (e.g. BPMN, Use Cases). Moreover, the paper presents two\ntools that support Message Structures, one uses the Xtext technology, and the\nother uses the Eclipse Modelling Framework. Industrial experience has shown us\nthat the technique can be adopted and applied in complex projects."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.46.1", 
    "link": "http://arxiv.org/pdf/1101.5957v1", 
    "title": "A Kind of Representation of Common Knowledge and its Application in   Requirements Analysis", 
    "arxiv-id": "1101.5957v1", 
    "author": "Jianmin Wang", 
    "publish": "2011-01-31T13:31:20Z", 
    "summary": "Since the birth of software engineering, it always are recognized as one pure\nengineering subject, therefore, the foundational scientific problems are not\npaid much attention. This paper proposes that Requirements Analysis, the kernel\nprocess of software engineering, can be modeled based on the concept of \"common\nknowledge\". Such a model would make us understand the nature of this process.\nThis paper utilizes the formal language as the tool to characterize the \"common\nknowledge\"-based Requirements Analysis model, and theoretically proves that :\n1) the precondition of success of software projects regardless of cost would be\nthat the participants in a software project have fully known the requirement\nspecification, if the participants do not understand the meaning of the other\nparticipants; 2) the precondition of success of software projects regardless of\ncost would be that the union set of knowledge of basic facts of the\nparticipants in a software project can fully cover the requirement\nspecification, if the participants can always understand the meaning of the\nother participants. These two theorems may have potential meanings to propose\nnew software engineering methodology."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2102", 
    "link": "http://arxiv.org/pdf/1102.0583v1", 
    "title": "Thin Client Web-Based Campus Information Systems for Fiji National   University", 
    "arxiv-id": "1102.0583v1", 
    "author": "Bimal Aklesh Kumar", 
    "publish": "2011-02-02T23:04:49Z", 
    "summary": "Fiji National University is encountering many difficulties with its current\nadministrative systems. These difficulties include accessibility, scalability,\nperformance, flexibility and integration. We propose a new campus information\nsystem, FNU-CIS to addresses these difficulties. FNU-CIS has the potential to\nprovide wide range of the services for students and staffs at the university.\nIn order to assist in the design and implementation of proposed FNU-CIS, we\npresent an overview, software architecture and prototype implementation of our\nproposed system. We discuss the key properties of our system, compare it with\nother similar systems available and outline our future plans for research in\nFNU-CIS implementation."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.1059v2", 
    "title": "Code-based Automated Program Fixing", 
    "arxiv-id": "1102.1059v2", 
    "author": "Bertrand Meyer", 
    "publish": "2011-02-05T08:31:07Z", 
    "summary": "Many programmers, when they encounter an error, would like to have the\nbenefit of automatic fix suggestions---as long as they are, most of the time,\nadequate. Initial research in this direction has generally limited itself to\nspecific areas, such as data structure classes with carefully designed\ninterfaces, and relied on simple approaches. To provide high-quality fix\nsuggestions in a broad area of applicability, the present work relies on the\npresence of contracts in the code, and on the availability of dynamic analysis\nto gather evidence on the values taken by expressions derived from the program\ntext. The ideas have been built into the AutoFix-E2 automatic fix generator.\nApplications of AutoFix-E2 to general-purpose software, such as a library to\nmanipulate documents, show that the approach provides an improvement over\nprevious techniques, in particular purely model-based approaches."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.3529v1", 
    "title": "A Tool for the Certification of PLCs based on a Coq Semantics for   Sequential Function Charts", 
    "arxiv-id": "1102.3529v1", 
    "author": "Jan Olaf Blech", 
    "publish": "2011-02-17T09:04:29Z", 
    "summary": "In this report we describe a tool framework for certifying properties of\nPLCs: CERTPLC. CERTPLC can handle PLC descriptions provided in the Sequential\nFunction Chart (SFC) language of the IEC 61131-3 standard. It provides routines\nto certify properties of systems by delivering an independently checkable\nformal system description and proof (called certificate) for the desired\nproperties. We focus on properties that can be described as inductive\ninvariants. System descriptions and certificates are generated and handled\nusing the COQ proof assistant. Our tool framework is used to provide supporting\nevidence for the safety of embedded systems in the industrial automation domain\nto third-party authorities. In this document we describe the tool framework:\nusage scenarios, the archi-tecture, semantics of PLCs and their realization in\nCOQ, proof generation and the construction of certificates."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.4162v1", 
    "title": "Comparative Study on DFD to UML Diagrams Transformations", 
    "arxiv-id": "1102.4162v1", 
    "author": "Aamer Nadeem", 
    "publish": "2011-02-21T08:38:49Z", 
    "summary": "Most of legacy systems use nowadays were modeled and documented using\nstructured approach. Expansion of these systems in terms of functionality and\nmaintainability requires shift towards object-oriented documentation and\ndesign, which has been widely accepted by the industry. In this paper, we\npresent a survey of the existing Data Flow Diagram (DFD) to Unified Modeling\nlanguage (UML) transformation techniques. We analyze transformation techniques\nusing a set of parameters, identified in the survey. Based on identified\nparameters, we present an analysis matrix, which describes the strengths and\nweaknesses of transformation techniques. It is observed that most of the\ntransformation approaches are rule based, which are incomplete and defined at\nabstract level that does not cover in depth transformation and automation\nissues. Transformation approaches are data centric, which focuses on data-store\nfor class diagram generation. Very few of the transformation techniques have\nbeen applied on case study as a proof of concept, which are not comprehensive\nand majority of them are partially automated."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.4178v1", 
    "title": "Mixed-Variable Requirements Roadmaps and their Role in the Requirements   Engineering of Adaptive Systems", 
    "arxiv-id": "1102.4178v1", 
    "author": "Neil A. Ernst", 
    "publish": "2011-02-21T10:14:56Z", 
    "summary": "The requirements roadmap concept is introduced as a solution to the problem\nof the requirements engineering of adaptive systems. The concept requires a new\ngeneral definition of the requirements problem which allows for quantitative\n(numeric) variables, together with qualitative (binary boolean) propositional\nvariables, and distinguishes monitored from controlled variables for use in\ncontrol loops. We study the consequences of these changes, and argue that the\nrequirements roadmap concept bridges the gap between current general\ndefinitions of the requirements problem and its notion of solution, and the\nresearch into the relaxation of requirements, the evaluation of their partial\nsatisfaction, and the monitoring and control of requirements, all topics of\nparticular interest in the engineering of requirements for adaptive systems\n[Cheng et al. 2009]. From the theoretical perspective, we show clearly and\nformally the fundamental differences between more traditional conception of\nrequirements engineering (e.g., Zave & Jackson [1997]) and the requirements\nengineering of adaptive systems (from Fickas & Feather [1995], over Letier &\nvan Lamsweerde [2004], and up to Whittle et al. [2010] and the most recent\nresearch). From the engineering perspective, we define a proto-framework for\nearly requirements engineering of adaptive systems, which illustrates the\nfeatures needed in future requirements frameworks for this class of systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.4179v1", 
    "title": "Runtime Adaptability driven by Negotiable Quality Requirements", 
    "arxiv-id": "1102.4179v1", 
    "author": "Adina Mosincat", 
    "publish": "2011-02-21T10:17:29Z", 
    "summary": "Two of the common features of business and the web are diversity and\ndynamism. Diversity results in users having different preferences for the\nquality requirements of a system. Diversity also makes possible alternative\nimplementations for functional requirements, called variants, each of them\nproviding different quality. The quality provided by the system may vary due to\ndifferent variant components and changes in the environment. The challenge is\nto dynamically adapt to quality variations and to find the variant that best\nfulfills the multi-criteria quality requirements driven by user preferences and\ncurrent runtime conditions. For service-oriented systems this challenge is\naugmented by their distributed nature and lack of control over the constituent\nservices and their provided quality of service (QoS). We propose a novel\napproach to runtime adaptability that detects QoS changes, updates the system\nmodel with runtime information, and uses the model to select the variant to\nexecute at runtime. We introduce negotiable maintenance goals to express user\nquality preferences in the requirements model and automatically interpret them\nquantitatively for system execution. Our lightweight selection strategy selects\nthe variant that best fulfills the user required multi-criteria QoS based on\nupdated QoS values."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.4518v3", 
    "title": "BP Variability Case Studies Development using different Modeling   Approaches", 
    "arxiv-id": "1102.4518v3", 
    "author": "Vicente Pelechano", 
    "publish": "2011-02-22T14:21:31Z", 
    "summary": "Variability in Business Process modeling has already been faced by different\nauthors from the literature. Depending on the context in which each author\nfaces the modeling problem, we find different approaches (C-EPC, C-YAWL,\nFEATURE-EPC, PESOA, PROVOP, or WORKLETS). In this report we present four of the\nmost representative approaches (C-EPC, PESOA, PROVOP and WORKLETS) which are\npresented by means of the different case studies found in the literature."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.4519v3", 
    "title": "Theoretical Count of Function Points for Non-Measurable Items", 
    "arxiv-id": "1102.4519v3", 
    "author": "Nilo Serpa", 
    "publish": "2011-02-22T14:21:31Z", 
    "summary": "This paper studies and proposes a technique of function point counting for\nitems classified as non-measurable. The main objective is to expand the\nconventional technique of counting to ensure that this comprises consistently\nthe tasks involved in building portals and sites in general. In addition, it\nalso applies to measure the cost of continued activities related to these web\napplications. The extended technique is potentially useful to measure several\nproducts associated with information systems, including periodicals publishable\nin intranets."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1102.5193v1", 
    "title": "Lightweight Service Oriented Architecture for Pervasive Computing", 
    "arxiv-id": "1102.5193v1", 
    "author": "Michel Riveill", 
    "publish": "2011-02-25T08:54:48Z", 
    "summary": "Pervasive computing appears like a new computing era based on networks of\nobjects and devices evolving in a real world, radically different from\ndistributed computing, based on networks of computers and data storages.\nContrary to most context-aware approaches, we work on the assumption that\npervasive software must be able to deal with a dynamic software environment\nbefore processing contextual data. After demonstrating that SOA (Service\noriented Architecture) and its numerous principles are well adapted for\npervasive computing, we present our extended SOA model for pervasive computing,\ncalled Service Lightweight Component Architecture (SLCA). SLCA presents various\nadditional principles to meet completely pervasive software constraints:\nsoftware infrastructure based on services for devices, local orchestrations\nbased on lightweight component architecture and finally encapsulation of those\norchestrations into composite services to address distributed composition of\nservices. We present a sample application of the overall approach as well as\nsome relevant measures about SLCA performances."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1103.0125v1", 
    "title": "Search-based software test data generation using evolutionary   computation", 
    "arxiv-id": "1103.0125v1", 
    "author": "P. Maragathavalli", 
    "publish": "2011-03-01T10:19:11Z", 
    "summary": "Search-based Software Engineering has been utilized for a number of software\nengineering activities. One area where Search-Based Software Engineering has\nseen much application is test data generation. Evolutionary testing designates\nthe use of metaheuristic search methods for test case generation. The search\nspace is the input domain of the test object, with each individual or potential\nsolution, being an encoded set of inputs to that test object. The fitness\nfunction is tailored to find test data for the type of test that is being\nundertaken. Evolutionary Testing (ET) uses optimizing search techniques such as\nevolutionary algorithms to generate test data. The effectiveness of GA-based\ntesting system is compared with a Random testing system. For simple programs\nboth testing systems work fine, but as the complexity of the program or the\ncomplexity of input domain grows, GA-based testing system significantly\noutperforms Random testing."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1103.1497v1", 
    "title": "Drag and Drop: Influences on the Design of Reusable Software Components", 
    "arxiv-id": "1103.1497v1", 
    "author": "G. Suresh Reddy", 
    "publish": "2011-03-08T11:44:02Z", 
    "summary": "The fundamental unit of large scale software construction is the component. A\ncomponent is the fundamental user interface object in Java. Everything you see\non the display in a java application is a component. The ability to let users\ndrag a component from the Interface and drop into your application is almost a\nrequirement of a modern, commercial user interface. The CBD approach brings\nhigh component reusability and easy maintainability, and reduces\ntime-to-market. This paper describes the component repository which provides\nfunctionality for component reuse process through the drag and drop mechanism\nand it's influences on the reusable components"
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100080", 
    "link": "http://arxiv.org/pdf/1103.3021v1", 
    "title": "A study of the existing libraries to read from configuration files (from   C++)", 
    "arxiv-id": "1103.3021v1", 
    "author": "Claire Mouton", 
    "publish": "2011-03-15T20:28:40Z", 
    "summary": "A study of the existing libraries to read from configuration files (from C++)"
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3569v1", 
    "title": "Detect Related Bugs from Source Code Using Bug Information", 
    "arxiv-id": "1103.3569v1", 
    "author": "Hongping Hu", 
    "publish": "2011-03-18T07:50:51Z", 
    "summary": "Open source projects often maintain open bug repositories during development\nand maintenance, and the reporters often point out straightly or implicitly the\nreasons why bugs occur when they submit them. The comments about a bug are very\nvaluable for developers to locate and fix the bug. Meanwhile, it is very common\nin large software for programmers to override or overload some methods\naccording to the same logic. If one method causes a bug, it is obvious that\nother overridden or overloaded methods maybe cause related or similar bugs. In\nthis paper, we propose and implement a tool Rebug- Detector, which detects\nrelated bugs using bug information and code features. Firstly, it extracts bug\nfeatures from bug information in bug repositories; secondly, it locates bug\nmethods from source code, and then extracts code features of bug methods;\nthirdly, it calculates similarities between each overridden or overloaded\nmethod and bug methods; lastly, it determines which method maybe causes\npotential related or similar bugs. We evaluate Rebug-Detector on an open source\nproject: Apache Lucene-Java. Our tool totally detects 61 related bugs,\nincluding 21 real bugs and 10 suspected bugs, and it costs us about 15.5\nminutes. The results show that bug features and code features extracted by our\ntool are useful to find real bugs in existing projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3686v2", 
    "title": "Integration of Communication Analysis and the OO-Method: Rules for the   manual derivation of the Conceptual Model", 
    "arxiv-id": "1103.3686v2", 
    "author": "Marcela Ruiz", 
    "publish": "2011-03-18T18:48:21Z", 
    "summary": "Enterprise information systems can be developed following a model-driven\nparadigm. This way, models that represent the organisational work practice are\nused to produce models that represent the information system. Current software\ndevelopment methods are starting to provide guidelines for the construction of\nconceptual models, taking as input requirements models. This paper proposes the\nintegration of two methods: Communication Analysis (a communication-oriented\nrequirements engineering method [Espa\\~na, Gonz\\'alez et al. 2009]) and the\nOO-Method (a model-driven object-oriented software development method [Pastor\nand Molina 2007]). For this purpose, a systematic technique for deriving\nOO-Method Conceptual Models from business process and requirements models is\nproposed. The business process specifications (which include message\nstructures) are processed in order to obtain static and dynamic views of the\ncomputerised information system. Then, using the OLIVANOVA framework, software\nsource code can be generated automatically [CARE Technologies]."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3779v1", 
    "title": "Validation Measures in CMMI", 
    "arxiv-id": "1103.3779v1", 
    "author": "Mahmoud Khraiwesh", 
    "publish": "2011-03-19T13:49:42Z", 
    "summary": "Validation is one of the software engineering disciplines that help build\nquality into software. The major objective of software validation process is to\ndetermine that the software performs its intended functions correctly and\nprovide information about its quality and reliability. This paper identifies\ngeneral measures for the specific goals and its specific practices of\nValidation Process Area (PA) in Capability Maturity Model Integration (CMMI).\nCMMI is developed by Software Engineering Institute (SEI). CMMI is a framework\nfor improvement and assessment of a software development process. CMMI needs a\nmeasurement program that is practical. The method we used to define the\nmeasures is to apply the Goal Question Metrics (GQM) paradigm to the specific\ngoals and its specific practices of Validation Process Area in CMMI."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3807v1", 
    "title": "Framework for Clique-based Fusion of Graph Streams in Multi-function   System Testing", 
    "arxiv-id": "1103.3807v1", 
    "author": "Mark Sh. Levin", 
    "publish": "2011-03-19T20:10:21Z", 
    "summary": "The paper describes a framework for multi-function system testing.\nMulti-function system testing is considered as fusion (or revelation) of\nclique-like structures. The following sets are considered: (i) subsystems\n(system parts or units / components / modules), (ii) system functions and a\nsubset of system components for each system function, and (iii) function\nclusters (some groups of system functions which are used jointly). Test\nprocedures (as units testing) are used for each subsystem. The procedures lead\nto an ordinal result (states, colors) for each component, e.g., [1,2,3,4]\n(where 1 corresponds to 'out of service', 2 corresponds to 'major faults', 3\ncorresponds to 'minor faults', 4 corresponds to 'trouble free service'). Thus,\nfor each system function a graph over corresponding system components is\nexamined while taking into account ordinal estimates/colors of the components.\nFurther, an integrated graph (i.e., colored graph) for each function cluster is\nconsidered (this graph integrates the graphs for corresponding system\nfunctions). For the integrated graph (for each function cluster) structure\nrevelation problems are under examination (revelation of some subgraphs which\ncan lead to system faults): (1) revelation of clique and quasi-clique (by\nvertices at level 1, 2, etc.; by edges/interconnection existence) and (2)\ndynamical problems (when vertex colors are functions of time) are studied as\nwell: existence of a time interval when clique or quasi-clique can exist.\nNumerical examples illustrate the approach and problems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3844v1", 
    "title": "Composition of Management System for Smart Homes", 
    "arxiv-id": "1103.3844v1", 
    "author": "Alexander Klapproth", 
    "publish": "2011-03-20T11:38:19Z", 
    "summary": "The paper addresses modular hierarchical design (composition) of a management\nsystem for smart homes. The management system consists of security subsystem\n(access control, alarm control), comfort subsystem (temperature, etc.),\nintelligence subsystem (multimedia, houseware). The design solving process is\nbased on Hierarchical Morphological Multicriteria Design (HMMD) approach: (1)\ndesign of a tree-like system model, (2) generation of design alternatives for\nleaf nodes of the system model, (3) Bottom-Up process: (i) multicriteria\nselection of design alternatives for system parts/components and (ii) composing\nthe selected alternatives into a resultant combination (while taking into\naccount ordinal quality of the alternatives above and their compatibility). A\nrealistic numerical example illustrates the design process of a management\nsystem for smart homes."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.3845v1", 
    "title": "Course on System Design (structural approach)", 
    "arxiv-id": "1103.3845v1", 
    "author": "Mark Sh. Levin", 
    "publish": "2011-03-20T11:41:12Z", 
    "summary": "The article describes a course on system design (structural approach) which\ninvolves the following: issues of systems engineering; structural models; basic\ntechnological problems (structural system modeling, modular design,\nevaluation/comparison, revelation of bottlenecks, improvement/upgrade,\nmultistage design, modeling of system evolution); solving methods\n(optimization, combinatorial optimization, multicriteria decision making);\ndesign frameworks; and applications. The course contains lectures and a set of\nspecial laboratory works. The laboratory works consist in designing and\nimplementing a set of programs to solve multicriteria problems\n(ranking/selection, multiple choice problem, clustering, assignment). The\nprograms above are used to solve some standard problems (e.g., hierarchical\ndesign of a student plan, design of a marketing strategy). Concurrently, each\nstudent can examine a unique applied problem from his/her applied domain(s)\n(e.g., telemetric system, GSM network, integrated security system, testing of\nmicroprocessor systems, wireless sensor, corporative communication network,\nnetwork topology). Mainly, the course is targeted to developing the student\nskills in modular analysis and design of various multidisciplinary composite\nsystems (e.g., software, electronic devices, information, computers,\ncommunications). The course was implemented in Moscow Institute of Physics and\nTechnology (State University)."
},{
    "category": "cs.SE", 
    "doi": "10.1109/COMPSAC.2010.27", 
    "link": "http://arxiv.org/pdf/1103.4056v1", 
    "title": "Software is a directed multigraph (and so is software process)", 
    "arxiv-id": "1103.4056v1", 
    "author": "Grzegorz Timoszuk", 
    "publish": "2011-03-21T15:37:38Z", 
    "summary": "For a software system, its architecture is typically defined as the\nfundamental organization of the system incorporated by its components, their\nrelationships to one another and their environment, and the principles\ngoverning their design. If contributed to by the artifacts coresponding to\nengineering processes that govern the system's evolution, the definition gets\nnatually extended into the architecture of software and software process.\nObviously, as long as there were no software systems, managing their\narchitecture was no problem at all; when there were only small systems,\nmanaging their architecture became a mild problem; and now we have gigantic\nsoftware systems, and managing their architecture has become an equally\ngigantic problem (to paraphrase Edsger Dijkstra). In this paper we propose a\nsimple, yet we believe effective, model for organizing architecture of software\nsystems. First of all we postulate that only a hollistic approach that supports\ncontinuous integration and verification for all software and software process\narchitectural artifacts is the one worth taking. Next we indicate a graph-based\nmodel that not only allows collecting and maintaining the architectural\nknowledge in respect to both software and software process, but allows to\nconveniently create various quantitive metric to asses their respective quality\nor maturity. Such model is actually independent of the development\nmethodologies that are currently in-use, that is it could well be applied for\nprojects managed in an adaptive, as well as in a formal approach. Eventually we\nargue that the model could actually be implemented by already existing tools,\nin particular graph databases are a convenient implementation of architectural\nrepository."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-28798-5_13", 
    "link": "http://arxiv.org/pdf/1103.4749v1", 
    "title": "Lost in translation: data integration tools meet the Semantic Web   (experiences from the Ondex project)", 
    "arxiv-id": "1103.4749v1", 
    "author": "Phillip Lord", 
    "publish": "2011-03-24T12:39:08Z", 
    "summary": "More information is now being published in machine processable form on the\nweb and, as de-facto distributed knowledge bases are materializing, partly\nencouraged by the vision of the Semantic Web, the focus is shifting from the\npublication of this information to its consumption. Platforms for data\nintegration, visualization and analysis that are based on a graph\nrepresentation of information appear first candidates to be consumers of\nweb-based information that is readily expressible as graphs. The question is\nwhether the adoption of these platforms to information available on the\nSemantic Web requires some adaptation of their data structures and semantics.\nOndex is a network-based data integration, analysis and visualization platform\nwhich has been developed in a Life Sciences context. A number of features,\nincluding semantic annotation via ontologies and an attention to provenance and\nevidence, make this an ideal candidate to consume Semantic Web information, as\nwell as a prototype for the application of network analysis tools in this\ncontext. By analyzing the Ondex data structure and its usage, we have found a\nset of discrepancies and errors arising from the semantic mismatch between a\nprocedural approach to network analysis and the implications of a web-based\nrepresentation of information. We report in the paper on the simple methodology\nthat we have adopted to conduct such analysis, and on issues that we have found\nwhich may be relevant for a range of similar platforms"
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-28798-5_13", 
    "link": "http://arxiv.org/pdf/1103.5616v1", 
    "title": "To Parallelize or Not to Parallelize, Speed Up Issue", 
    "arxiv-id": "1103.5616v1", 
    "author": "Alaa Ismail Elnashar", 
    "publish": "2011-03-29T12:09:44Z", 
    "summary": "Running parallel applications requires special and expensive processing\nresources to obtain the required results within a reasonable time. Before\nparallelizing serial applications, some analysis is recommended to be carried\nout to decide whether it will benefit from parallelization or not. In this\npaper we discuss the issue of speed up gained from parallelization using\nMessage Passing Interface (MPI) to compromise between the overhead of\nparallelization cost and the gained parallel speed up. We also propose an\nexperimental method to predict the speed up of MPI applications."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-28798-5_13", 
    "link": "http://arxiv.org/pdf/1105.0153v1", 
    "title": "The Development of Electronic Payment System for Universities in   Indonesia: On Resolving Key Success Factors", 
    "arxiv-id": "1105.0153v1", 
    "author": "Nico Saputro", 
    "publish": "2011-05-01T08:35:14Z", 
    "summary": "It is known that IT projects are high-risk. To achieve project success, the\nstrategies to avoid and reduce risks must be designed meticulously and\nimplemented accordingly. This paper presents methods for avoiding and reducing\nrisks throughout the development of an information system, specifically\nelectronic payment system to handle tuition in the universities in Indonesia.\nThe university policies, regulations and system models are design in such a way\nto resolve the project key success factors. By implementing the proposed\nmethods, the system has been successfully developed and currently operated. The\nresearch is conducted in Parahyangan Catholic University, Bandung, Indonesia."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2202", 
    "link": "http://arxiv.org/pdf/1105.0390v1", 
    "title": "MCA Based Performance Evaluation of Project Selection", 
    "arxiv-id": "1105.0390v1", 
    "author": "Bijan Sarkar", 
    "publish": "2011-05-02T18:06:48Z", 
    "summary": "Multi-criteria decision support systems are used in various fields of human\nactivities. In every alternative multi-criteria decision making problem can be\nrepresented by a set of properties or constraints. The properties can be\nqualitative & quantitative. For measurement of these properties, there are\ndifferent unit, as well as there are different optimization techniques.\nDepending upon the desired goal, the normalization aims for obtaining reference\nscales of values of these properties. This paper deals with a new additive\nratio assessment method. In order to make the appropriate decision and to make\na proper comparison among the available alternatives Analytic Hierarchy Process\n(AHP) and ARAS have been used. The uses of AHP is for analysis the structure of\nthe project selection problem and to assign the weights of the properties and\nthe ARAS method is used to obtain the final ranking and select the best one\namong the projects. To illustrate the above mention methods survey data on the\nexpansion of optical fibre for a telecommunication sector is used. The decision\nmaker can also used different weight combination in the decision making process\naccording to the demand of the system."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2202", 
    "link": "http://arxiv.org/pdf/1105.0768v4", 
    "title": "Collaborative Software Development on the Web", 
    "arxiv-id": "1105.0768v4", 
    "author": "Bertrand Meyer", 
    "publish": "2011-05-04T09:07:54Z", 
    "summary": "Software development environments (IDEs) have not followed the IT industry's\ninexorable trend towards distribution. They do too little to address the\nproblems raised by today's increasingly distributed projects; neither do they\nfacilitate collaborative and interactive development practices. A consequence\nis the continued reliance of today's IDEs on paradigms such as traditional\nconfiguration management, which were developed for earlier modes of operation\nand hamper collaborative projects. This contribution describes a new paradigm:\ncloud-based development, which caters to the specific needs of distributed and\ncollaborative projects. The CloudStudio IDE embodies this paradigm by enabling\ndevelopers to work on a shared project repository. Configuration management\nbecomes unobtrusive; it replaces the explicit update-modify-commit cycle by\ninteractive editing and real-time conflict tracking and management. A case\nstudy involving three teams of pairs demonstrates the usability of CloudStudio\nand its advantages for collaborative software development over traditional\nconfiguration management practices."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2202", 
    "link": "http://arxiv.org/pdf/1105.1191v1", 
    "title": "Software Architecture for Fiji National University Campus Information   Systems", 
    "arxiv-id": "1105.1191v1", 
    "author": "Bimal Aklesh Kumar", 
    "publish": "2011-05-05T23:17:47Z", 
    "summary": "Software Architecture defines the overview of the system which consists of\nvarious components and their relationships among the software. Architectural\ndesign is very important in the development of large scale software solution\nand plays a very active role in achieving business goals, quality and reusable\nsolution. It is often difficult to choose the best software architecture for\nyour system from the several candidate types available. In this paper we look\nat the several architectural types and compare them based on the key\nrequirements of our system, and select the most appropriate architecture for\nthe implementation of campus information systems at Fiji National University.\nFinally we provide details of proposed architecture and outline future plans\nfor implementation of our system."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2202", 
    "link": "http://arxiv.org/pdf/1105.1633v3", 
    "title": "Computation of WCET using Program Slicing and Real-Time Model-Checking", 
    "arxiv-id": "1105.1633v3", 
    "author": "Franck Cassez", 
    "publish": "2011-05-09T11:24:16Z", 
    "summary": "Computing accurate WCET on modern complex architectures is a challenging\ntask. This problem has been devoted a lot of attention in the last decade but\nthere are still some open issues. First, the control flow graph (CFG) of a\nbinary program is needed to compute the WCET and this CFG is built using some\ninternal knowledge of the compiler that generated the binary code; moreover\nonce constructed the CFG has to be manually annotated with loop bounds. Second,\nthe algorithms to compute the WCET (combining Abstract Interpretation and\nInteger Linear Programming) are tailored for specific architectures: changing\nthe architecture (e.g. replacing an ARM7 by an ARM9) requires the design of a\nnew ad hoc algorithm. Third, the tightness of the computed results (obtained\nusing the available tools) are not compared to actual execution times measured\non the real hardware. In this paper we address the above mentioned problems. We\nfirst describe a fully automatic method to compute a CFG based solely on the\nbinary program to analyse. Second, we describe the model of the hardware as a\nproduct of timed automata, and this model is independent from the program\ndescription. The model of a program running on a hardware is obtained by\nsynchronizing (the automaton of) the program with the (timed automata) model of\nthe hardware. Computing the WCET is reduced to a reachability problem on the\nsynchronised model and solved using the model-checker UPPAAL. Finally, we\npresent a rigorous methodology that enables us to compare our computed results\nto actual execution times measured on a real platform, the ARM920T."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2202", 
    "link": "http://arxiv.org/pdf/1105.2335v1", 
    "title": "Hierarchical Complexity: Measures of High Level Modularity", 
    "arxiv-id": "1105.2335v1", 
    "author": "Alejandro Fern\u00e1ndez", 
    "publish": "2011-05-11T23:03:38Z", 
    "summary": "Software is among the most complex endeavors of the human mind; large scale\nsystems can have tens of millions of lines of source code. However, seldom is\ncomplexity measured above the lowest level of code, and sometimes source code\nfiles or low level modules. In this paper a hierarchical approach is explored\nin order to find a set of metrics that can measure higher levels of\norganization. These metrics are then used on a few popular free software\npackages (totaling more than 25 million lines of code) to check their\nefficiency and coherency."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2201", 
    "link": "http://arxiv.org/pdf/1105.4468v1", 
    "title": "Standardization of information systems development processes and banking   industry adaptations", 
    "arxiv-id": "1105.4468v1", 
    "author": "Tuna Ozcer", 
    "publish": "2011-05-23T11:26:10Z", 
    "summary": "This paper examines the current system development processes of three major\nTurkish banks in terms of compliance to internationally accepted system\ndevelopment and software engineering standards to determine the common process\nproblems of banks. After an in-depth investigation into system development and\nsoftware engineering standards, related process-based standards were selected.\nQuestions were then prepared covering the whole system development process by\napplying the classical Waterfall life cycle model. Each question is made up of\nguidance and suggestions from the international system development standards.\nTo collect data, people from the information technology departments of three\nmajor banks in Turkey were interviewed. Results have been aggregated by\nexamining the current process status of the three banks together. Problematic\nissues were identified using the international system development standards."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2201", 
    "link": "http://arxiv.org/pdf/1105.6128v1", 
    "title": "MDA based-approach for UML Models Complete Comparison", 
    "arxiv-id": "1105.6128v1", 
    "author": "Salma Mouline", 
    "publish": "2011-05-30T22:34:51Z", 
    "summary": "If a modeling task is distributed, it will frequently be necessary to\nintegrate models developed by different team members. Problems occur in the\nmodels integration step and particularly, in the comparison phase of the\nintegration. This issue had been discussed in several domains and various\nmodels. However, previous approaches have not correctly handled the semantic\ncomparison. In the current paper, we provide a MDA-based approach for models\ncomparison which aims at comparing UML models. We develop an hybrid approach\nwhich takes into account syntactic, semantic and structural comparison aspects.\nFor this purpose, we use the domain ontology as well as other resources such as\ndictionaries. We propose a decision support system which permits the user to\nvalidate (or not) correspondences extracted in the comparison phase. For\nimplementation, we propose an extension of the generic correspondence metamodel\nAMW in order to transform UML models to the correspondence model."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISED.2010.52", 
    "link": "http://arxiv.org/pdf/1106.2686v1", 
    "title": "Automated Software Testing Using Metahurestic Technique Based on An Ant   Colony Optimization", 
    "arxiv-id": "1106.2686v1", 
    "author": "Km Baby", 
    "publish": "2011-06-14T12:17:44Z", 
    "summary": "Software testing is an important and valuable part of the software\ndevelopment life cycle. Due to time, cost and other circumstances, exhaustive\ntesting is not feasible that's why there is a need to automate the software\ntesting process. Testing effectiveness can be achieved by the State Transition\nTesting (STT) which is commonly used in real time, embedded and web-based type\nof software systems. Aim of the current paper is to present an algorithm by\napplying an ant colony optimization technique, for generation of optimal and\nminimal test sequences for behavior specification of software. Present paper\napproach generates test sequence in order to obtain the complete software\ncoverage. This paper also discusses the comparison between two metaheuristic\ntechniques (Genetic Algorithm and Ant Colony optimization) for transition based\ntesting"
},{
    "category": "cs.SE", 
    "doi": "10.1109/ISED.2010.52", 
    "link": "http://arxiv.org/pdf/1106.3961v2", 
    "title": "Stochastic Semantics and Statistical Model Checking for Networks of   Priced Timed Automata", 
    "arxiv-id": "1106.3961v2", 
    "author": "Zheng Wang", 
    "publish": "2011-06-20T16:26:27Z", 
    "summary": "This paper offers a natural stochastic semantics of Networks of Priced Timed\nAutomata (NPTA) based on races between components. The semantics provides the\nbasis for satisfaction of probabilistic Weighted CTL properties (PWCTL),\nconservatively extending the classical satisfaction of timed automata with\nrespect to TCTL. In particular the extension allows for hard real-time\nproperties of timed automata expressible in TCTL to be refined by performance\nproperties, e.g. in terms of probabilistic guarantees of time- and cost-bounded\nproperties. A second contribution of the paper is the application of\nStatistical Model Checking (SMC) to efficiently estimate the correctness of\nnon-nested PWCTL model checking problems with a desired level of confidence,\nbased on a number of independent runs of the NPTA. In addition to applying\nclassical SMC algorithms, we also offer an extension that allows to efficiently\ncompare performance properties of NPTAs in a parametric setting. The third\ncontribution is an efficient tool implementation of our result and applications\nto several case studies."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.55.2", 
    "link": "http://arxiv.org/pdf/1106.4091v1", 
    "title": "Bigraphical Refinement", 
    "arxiv-id": "1106.4091v1", 
    "author": "Thomas Hildebrandt", 
    "publish": "2011-06-21T05:24:16Z", 
    "summary": "We propose a mechanism for the vertical refinement of bigraphical reactive\nsystems, based upon a mechanism for limiting observations and utilising the\nunderlying categorical structure of bigraphs. We present a motivating example\nto demonstrate that the proposed notion of refinement is sensible with respect\nto the theory of bigraphical reactive systems; and we propose a sufficient\ncondition for guaranteeing the existence of a safety-preserving vertical\nrefinement. We postulate the existence of a complimentary notion of horizontal\nrefinement for bigraphical agents, and finally we discuss the connection of\nthis work to the general refinement of Reeves and Streader."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.55.6", 
    "link": "http://arxiv.org/pdf/1106.4095v1", 
    "title": "Refinement for Probabilistic Systems with Nondeterminism", 
    "arxiv-id": "1106.4095v1", 
    "author": "David Streader", 
    "publish": "2011-06-21T05:24:55Z", 
    "summary": "Before we combine actions and probabilities two very obvious questions should\nbe asked. Firstly, what does \"the probability of an action\" mean? Secondly, how\ndoes probability interact with nondeterminism? Neither question has a single\nuniversally agreed upon answer but by considering these questions at the outset\nwe build a novel and hopefully intuitive probabilistic event-based formalism.\n  In previous work we have characterised refinement via the notion of testing.\nBasically, if one system passes all the tests that another system passes (and\nmaybe more) we say the first system is a refinement of the second. This is, in\nour view, an important way of characterising refinement, via the question \"what\nsort of refinement should I be using?\"\n  We use testing in this paper as the basis for our refinement. We develop\ntests for probabilistic systems by analogy with the tests developed for\nnon-probabilistic systems. We make sure that our probabilistic tests, when\nperformed on non-probabilistic automata, give us refinement relations which\nagree with for those non-probabilistic automata. We formalise this property as\na vertical refinement."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.55.8", 
    "link": "http://arxiv.org/pdf/1106.4097v1", 
    "title": "Formalising the Continuous/Discrete Modeling Step", 
    "arxiv-id": "1106.4097v1", 
    "author": "Runlei Huang", 
    "publish": "2011-06-21T05:25:11Z", 
    "summary": "Formally capturing the transition from a continuous model to a discrete model\nis investigated using model based refinement techniques. A very simple model\nfor stopping (eg. of a train) is developed in both the continuous and discrete\ndomains. The difference between the two is quantified using generic results\nfrom ODE theory, and these estimates can be compared with the exact solutions.\nSuch results do not fit well into a conventional model based refinement\nframework; however they can be accommodated into a model based retrenchment.\nThe retrenchment is described, and the way it can interface to refinement\ndevelopment on both the continuous and discrete sides is outlined. The approach\nis compared to what can be achieved using hybrid systems techniques."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.55.10", 
    "link": "http://arxiv.org/pdf/1106.4099v1", 
    "title": "Perspicuity and Granularity in Refinement", 
    "arxiv-id": "1106.4099v1", 
    "author": "Eerke Boiten", 
    "publish": "2011-06-21T05:25:23Z", 
    "summary": "This paper reconsiders refinements which introduce actions on the concrete\nlevel which were not present at the abstract level. It draws a distinction\nbetween concrete actions which are \"perspicuous\" at the abstract level, and\nchanges of granularity of actions between different levels of abstraction.\n  The main contribution of this paper is in exploring the relation between\nthese different methods of \"action refinement\", and the basic refinement\nrelation that is used. In particular, it shows how the \"refining skip\" method\nis incompatible with failures-based refinement relations, and consequently some\ndecisions in designing Event-B refinement are entangled."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.55.10", 
    "link": "http://arxiv.org/pdf/1106.4700v1", 
    "title": "Verifying Eiffel Programs with Boogie", 
    "arxiv-id": "1106.4700v1", 
    "author": "Bertrand Meyer", 
    "publish": "2011-06-23T12:45:37Z", 
    "summary": "Static program verifiers such as Spec#, Dafny, jStar, and VeriFast define the\nstate of the art in automated functional verification techniques. The next open\nchallenges are to make verification tools usable even by programmers not fluent\nin formal techniques. This paper presents AutoProof, a verification tool that\ntranslates Eiffel programs to Boogie and uses the Boogie verifier to prove\nthem. In an effort to be usable with real programs, AutoProof fully supports\nseveral advanced object-oriented features including polymorphism, inheritance,\nand function objects. AutoProof also adopts simple strategies to reduce the\namount of annotations needed when verifying programs (e.g., frame conditions).\nThe paper illustrates the main features of AutoProof's translation, including\nsome whose implementation is underway, and demonstrates them with examples and\na case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.56", 
    "link": "http://arxiv.org/pdf/1106.5962v1", 
    "title": "Proceedings Second International Workshop on Algebraic Methods in   Model-based Software Engineering", 
    "arxiv-id": "1106.5962v1", 
    "author": "Vlad Rusu", 
    "publish": "2011-06-29T15:05:15Z", 
    "summary": "Over the past years there has been quite a lot of activity in the algebraic\ncommunity about using algebraic methods for providing support to model-driven\nsoftware engineering. The aim of this workshop is to gather researchers working\non the development and application of algebraic methods to provide rigorous\nsupport to model-based software engineering. The topics relevant to the\nworkshop are all those related to the use of algebraic methods in software\nengineering, including but not limited to: formally specifying and verifying\nmodel-based software engineering concepts and related ones (MDE, UML, OCL, MOF,\nDSLs, ...); tool support for the above; integration of formal and informal\nmethods; and theoretical frameworks (algebraic, rewriting-based, category\ntheory-based, ...). The workshop's main goal is to examine, discuss, and relate\nthe existing projects within the algebraic community that address common\nopen-issues in model-driven software engineering."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.56.5", 
    "link": "http://arxiv.org/pdf/1107.0067v1", 
    "title": "Prototyping the Semantics of a DSL using ASF+SDF: Link to Formal   Verification of DSL Models", 
    "arxiv-id": "1107.0067v1", 
    "author": "Luc Engelen", 
    "publish": "2011-06-30T21:45:00Z", 
    "summary": "A formal definition of the semantics of a domain-specific language (DSL) is a\nkey prerequisite for the verification of the correctness of models specified\nusing such a DSL and of transformations applied to these models. For this\nreason, we implemented a prototype of the semantics of a DSL for the\nspecification of systems consisting of concurrent, communicating objects. Using\nthis prototype, models specified in the DSL can be transformed to labeled\ntransition systems (LTS). This approach of transforming models to LTSs allows\nus to apply existing tools for visualization and verification to models with\nlittle or no further effort. The prototype is implemented using the ASF+SDF\nMeta-Environment, an IDE for the algebraic specification language ASF+SDF,\nwhich offers efficient execution of the transformation as well as the ability\nto read models and produce LTSs without any additional pre or post processing."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.56.5", 
    "link": "http://arxiv.org/pdf/1107.1166v1", 
    "title": "Reachability Analysis of Time Basic Petri Nets: a Time Coverage Approach", 
    "arxiv-id": "1107.1166v1", 
    "author": "Lorenzo Capra", 
    "publish": "2011-07-06T15:52:36Z", 
    "summary": "We introduce a technique for reachability analysis of Time-Basic (TB) Petri\nnets, a powerful formalism for real- time systems where time constraints are\nexpressed as intervals, representing possible transition firing times, whose\nbounds are functions of marking's time description. The technique consists of\nbuilding a symbolic reachability graph relying on a sort of time coverage, and\novercomes the limitations of the only available analyzer for TB nets, based in\nturn on a time-bounded inspection of a (possibly infinite) reachability-tree.\nThe graph construction algorithm has been automated by a tool-set, briefly\ndescribed in the paper together with its main functionality and analysis\ncapability. A running example is used throughout the paper to sketch the\nsymbolic graph construction. A use case describing a small real system - that\nthe running example is an excerpt from - has been employed to benchmark the\ntechnique and the tool-set. The main outcome of this test are also presented in\nthe paper. Ongoing work, in the perspective of integrating with a\nmodel-checking engine, is shortly discussed."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.57.2", 
    "link": "http://arxiv.org/pdf/1107.1198v1", 
    "title": "QuantUM: Quantitative Safety Analysis of UML Models", 
    "arxiv-id": "1107.1198v1", 
    "author": "Stefan Leue", 
    "publish": "2011-07-06T17:54:32Z", 
    "summary": "When developing a safety-critical system it is essential to obtain an\nassessment of different design alternatives. In particular, an early safety\nassessment of the architectural design of a system is desirable. In spite of\nthe plethora of available formal quantitative analysis methods it is still\ndifficult for software and system architects to integrate these techniques into\ntheir every day work. This is mainly due to the lack of methods that can be\ndirectly applied to architecture level models, for instance given as UML\ndiagrams. Also, it is necessary that the description methods used do not\nrequire a profound knowledge of formal methods. Our approach bridges this gap\nand improves the integration of quantitative safety analysis methods into the\ndevelopment process. All inputs of the analysis are specified at the level of a\nUML model. This model is then automatically translated into the analysis model,\nand the results of the analysis are consequently represented on the level of\nthe UML model. Thus the analysis model and the formal methods used during the\nanalysis are hidden from the user. We illustrate the usefulness of our approach\nusing an industrial strength case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.57.2", 
    "link": "http://arxiv.org/pdf/1107.2683v1", 
    "title": "An IDE to Build and Check Task Flow Models", 
    "arxiv-id": "1107.2683v1", 
    "author": "Hermenegildo Fernandez Santos", 
    "publish": "2011-07-13T21:58:38Z", 
    "summary": "This paper presents the Eclipse plug-ins for the Task Flow model in the\nDiscovery Method. These plug-ins provide an IDE for the Task Algebra compiler\nand the model-checking tools. The Task Algebra is the formal representation for\nthe Task Model and it is based on simple and compound tasks. The model-checking\ntechniques were developed to validate Task Models represented in the algebra."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.57.2", 
    "link": "http://arxiv.org/pdf/1107.3201v1", 
    "title": "Estimation of Characteristics of a Software Team for Implementing   Effective Inspection Process through Inspection Performance Metric", 
    "arxiv-id": "1107.3201v1", 
    "author": "Suma. V", 
    "publish": "2011-07-16T07:07:44Z", 
    "summary": "The continued existence of any software industry depends on its capability to\ndevelop nearly zero-defect product, which is achievable through effective\ndefect management. Inspection has proven to be one of the promising techniques\nof defect management. Introductions of metrics like, Depth of Inspection (DI, a\nprocess metric) and Inspection Performance Metric (IPM, a people metric) enable\none to have an appropriate measurement of inspection technique. This article\nelucidates a mathematical approach to estimate the IPM value without depending\non shop floor defect count at every time. By applying multiple linear\nregression models, a set of characteristic coefficients of the team is\nevaluated. These coefficients are calculated from the empirical projects that\nare sampled from the teams of product-based and service-based IT industries. A\nsample of three verification projects indicates a close match between the IPM\nvalues obtained from the defect count (IPMdc) and IPM values obtained using the\nteam coefficients using the mathematical model (IPMtc). The IPM values observed\nonsite and IPM values produced by our model which are strongly matching,\nsupport the predictive capability of IPM through team coefficients. Having\nfinalized the value of IPM that a company should achieve for a project, it can\ntune the inspection influencing parameters to realize the desired quality level\nof IPM. Evaluation of team coefficients resolves several defect-associated\nissues, which are related to the management, stakeholders, outsourcing agents\nand customers. In addition, the coefficient vector will further aid the\nstrategy of PSP and TSP"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.57.2", 
    "link": "http://arxiv.org/pdf/1107.3297v1", 
    "title": "Semantic annotation of requirements for automatic UML class diagram   generation", 
    "arxiv-id": "1107.3297v1", 
    "author": "Sondes Bouabid", 
    "publish": "2011-07-17T12:51:30Z", 
    "summary": "The increasing complexity of software engineering requires effective methods\nand tools to support requirements analysts' activities. While much of a\ncompany's knowledge can be found in text repositories, current content\nmanagement systems have limited capabilities for structuring and interpreting\ndocuments. In this context, we propose a tool for transforming text documents\ndescribing users' requirements to an UML model. The presented tool uses Natural\nLanguage Processing (NLP) and semantic rules to generate an UML class diagram.\nThe main contribution of our tool is to provide assistance to designers\nfacilitating the transition from a textual description of user requirements to\ntheir UML diagrams based on GATE (General Architecture of Text) by formulating\nnecessary rules that generate new semantic annotations."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.58.1", 
    "link": "http://arxiv.org/pdf/1108.0228v1", 
    "title": "Modelling and Simulation of Asynchronous Real-Time Systems using Timed   Rebeca", 
    "arxiv-id": "1108.0228v1", 
    "author": "Marjan Sirjani", 
    "publish": "2011-08-01T03:57:54Z", 
    "summary": "In this paper we propose an extension of the Rebeca language that can be used\nto model distributed and asynchronous systems with timing constraints. We\nprovide the formal semantics of the language using Structural Operational\nSemantics, and show its expressiveness by means of examples. We developed a\ntool for automated translation from timed Rebeca to the Erlang language, which\nprovides a first implementation of timed Rebeca. We can use the tool to set the\nparameters of timed Rebeca models, which represent the environment and\ncomponent variables, and use McErlang to run multiple simulations for different\nsettings. Timed Rebeca restricts the modeller to a pure asynchronous\nactor-based paradigm, where the structure of the model represents the service\noriented architecture, while the computational model matches the network\ninfrastructure. Simulation is shown to be an effective analysis support,\nspecially where model checking faces almost immediate state explosion in an\nasynchronous setting."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.58.1", 
    "link": "http://arxiv.org/pdf/1108.0427v1", 
    "title": "A Methodology for assessing Agile Software Development Approaches", 
    "arxiv-id": "1108.0427v1", 
    "author": "Shvetha Soundararajan", 
    "publish": "2011-08-01T20:22:21Z", 
    "summary": "Agile methods provide an organization or a team the flexibility to adopt a\nselected subset of principles and practices based on their culture, their\nvalues, and the types of systems that they develop. More specifically, every\norganization or team implements a customized agile method, tailored to better\naccommodate its needs. However, the extent to which a customized method\nsupports the organizational objectives, or rather the 'goodness' of that method\nis questionable. Existing agile assessment approaches focus on a comparative\nanalysis, or are limited in scope and application. In this research, we propose\na structured, systematic and comprehensive approach to assess the 'goodness' of\nagile methods. We examine an agile method based on (1) its adequacy, (2) the\ncapability of the organization to support the adopted principles and practices\nspecified by the method, and (3) the method's effectiveness. We propose the\nObjectives, Principles and Practices (OPP) Framework to guide our assessment.\nThe Framework identifies (1) objectives of the agile philosophy, (2) principles\nthat support the objectives, (3) practices that are reflective of the\nprinciples, (4) the linkages between the objectives, principles and practices,\nand (5) indicators for each practice to assess the effectiveness of the\npractice and the extent to which the organization supports its implementation.\nIn this document, we discuss our solution approach, preliminary results, and\nfuture work."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.59.7", 
    "link": "http://arxiv.org/pdf/1108.0467v1", 
    "title": "On the reaction time of some synchronous systems", 
    "arxiv-id": "1108.0467v1", 
    "author": "Guy Vidal-Naquet", 
    "publish": "2011-08-02T02:27:25Z", 
    "summary": "This paper presents an investigation of the notion of reaction time in some\nsynchronous systems. A state-based description of such systems is given, and\nthe reaction time of such systems under some classic composition primitives is\nstudied. Reaction time is shown to be non-compositional in general. Possible\nsolutions are proposed, and applications to verification are discussed. This\nframework is illustrated by some examples issued from studies on real-time\nembedded systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100094", 
    "link": "http://arxiv.org/pdf/1108.1068v2", 
    "title": "Stateful Testing: Finding More Errors in Code and Contracts", 
    "arxiv-id": "1108.1068v2", 
    "author": "Bertrand Meyer", 
    "publish": "2011-08-04T12:38:29Z", 
    "summary": "Automated random testing has shown to be an effective approach to finding\nfaults but still faces a major unsolved issue: how to generate test inputs\ndiverse enough to find many faults and find them quickly. Stateful testing, the\nautomated testing technique introduced in this article, generates new test\ncases that improve an existing test suite. The generated test cases are\ndesigned to violate the dynamically inferred contracts (invariants)\ncharacterizing the existing test suite. As a consequence, they are in a good\nposition to detect new errors, and also to improve the accuracy of the inferred\ncontracts by discovering those that are unsound. Experiments on 13 data\nstructure classes totalling over 28,000 lines of code demonstrate the\neffectiveness of stateful testing in improving over the results of long\nsessions of random testing: stateful testing found 68.4% new errors and\nimproved the accuracy of automatically inferred contracts to over 99%, with\njust a 7% time overhead."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100094", 
    "link": "http://arxiv.org/pdf/1108.1314v1", 
    "title": "Security Model For Service-Oriented Architecture", 
    "arxiv-id": "1108.1314v1", 
    "author": "Oldooz Karimi", 
    "publish": "2011-08-05T12:12:31Z", 
    "summary": "In this article, we examine how security applies to Service Oriented\nArchitecture (SOA). Before we discuss security for SOA, lets take a step back\nand examine what SOA is. SOA is an architectural approach which involves\napplications being exposed as \"services\". Originally, services in SOA were\nassociated with a stack of technologies which included SOAP, WSDL, and UDDI.\nThis article addresses the defects of traditional enterprise application\nintegration by combining service oriented-architecture and web service\ntechnology. Application integration is then simplified to development and\nintegration of services to tackle connectivity of isomerous enterprise\napplication integration, security, loose coupling between systems and process\nrefactoring and optimization."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100094", 
    "link": "http://arxiv.org/pdf/1108.1352v1", 
    "title": "Program slicing techniques and its applications", 
    "arxiv-id": "1108.1352v1", 
    "author": "Dr. M. Hemalatha", 
    "publish": "2011-08-05T15:40:43Z", 
    "summary": "Program understanding is an important aspect in Software Maintenance and\nReengineering. Understanding the program is related to execution behaviour and\nrelationship of variable involved in the program. The task of finding all\nstatements in a program that directly or indirectly influence the value for an\noccurrence of a variable gives the set of statements that can affect the value\nof a variable at some point in a program is called a program slice. Program\nslicing is a technique for extracting parts of computer programs by tracing the\nprograms' control and data flow related to some data item. This technique is\napplicable in various areas such as debugging, program comprehension and\nunderstanding, program integration, cohesion measurement, re-engineering,\nmaintenance, testing where it is useful to be able to focus on relevant parts\nof large programs. This paper focuses on the various slicing techniques (not\nlimited to) like static slicing, quasi static slicing, dynamic slicing and\nconditional slicing. This paper also includes various methods in performing the\nslicing like forward slicing, backward slicing, syntactic slicing and semantic\nslicing. The slicing of a program is carried out using Java which is a object\noriented programming language."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100094", 
    "link": "http://arxiv.org/pdf/1108.1447v1", 
    "title": "Impact of Software Requirement Volatility Pattern on Project Dynamics:   Evidences from a Case Study", 
    "arxiv-id": "1108.1447v1", 
    "author": "Subhajit Dasgupta", 
    "publish": "2011-08-06T04:06:55Z", 
    "summary": "Requirements are found to change in various ways during the course of a\nproject. This can affect the process in widely different manner and extent.\nHere we present a case study where-in we investigate the impact of requirement\nvolatility pattern on project performance. The project setting described in the\ncase is emulated on a validated system dynamics model representing the\nwaterfall model. The findings indicate deviations in project outcome from the\nestimated thereby corroborating to previous findings. The results reinforce the\napplicability of system dynamics approach to analyze project performance under\nrequirement volatility, which is expected to speed up adoption of the same in\norganizations and in the process contribute to more project successes."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100094", 
    "link": "http://arxiv.org/pdf/1108.1461v4", 
    "title": "Understanding need of \"Uncertainty Analysis\" in the system Design   process", 
    "arxiv-id": "1108.1461v4", 
    "author": "Kardile Vilas Vasantrao", 
    "publish": "2011-08-06T08:00:31Z", 
    "summary": "Software project development process is requiring accurate software cost and\nschedule estimation for achieve goal or success. A lot it referred to as the\n\"Intricate brainteaser\" because of its conscience attribute which is impact by\ncomplexity and uncertainty, Generally estimation is not as difficult or\npuzzling as people think. In fact, generating accurate estimates is\nstraightforward-once you understand the intensity of uncertainty and module\nwhich contribute itself process. In our everyday life, we enhance our\nestimation based on past experience in which problem solve by which method and\nin which condition and which opportune provide that method to produce better\nresult . So, Instead of unexplained treatises and inflexible modeling\ntechniques, this will guide highlights a proven set of procedures,\nunderstandable formulas, and heuristics that individuals and complete team can\napply to their projects to help achieve estimation ability with choose\nappropriate development approaches In the early stage of software life cycle\nproject manager are inefficient to estimate the effort, schedule, cost\nestimation and its development approach .This in turn, confuses the manager to\nbid effectively on software project and choose incorrect development approach.\nThat will directly effect on productivity cycle and increase level of\nuncertainty. This becomes a strong cause of project failure. So to avoid such\nproblem if we know level and sources of uncertainty in model design, It will\ndirective the developer to design accurate software cost and schedule\nestimation. which are require l for software project success. This paper\ndemonstrates need of uncertainty analysis module at the modeling process for\nassist to recognize modular uncertainty system development process and the role\nof uncertainty at different stages in the modeling"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.60.2", 
    "link": "http://arxiv.org/pdf/1108.1862v1", 
    "title": "Input-output Conformance Testing for Channel-based Service Connectors", 
    "arxiv-id": "1108.1862v1", 
    "author": "Leonid Makhnist", 
    "publish": "2011-08-09T06:38:27Z", 
    "summary": "Service-based systems are software systems composed of autonomous components\nor services provided by different vendors, deployed on remote machines and\naccessible through the web. One of the challenges of modern software\nengineering is to ensure that such a system behaves as intended by its\ndesigner. The Reo coordination language is an extensible notation for formal\nmodeling and execution of service compositions. Services that have no prior\nknowledge about each other communicate through advanced channel connectors\nwhich guarantee that each participant, service or client, receives the right\ndata at the right time. Each channel is a binary relation that imposes\nsynchronization and data constraints on input and output messages. Furthermore,\nchannels are composed together to realize arbitrarily complex behavioral\nprotocols. During this process, a designer may introduce errors into the\nconnector model or the code for their execution, and thus affect the behavior\nof a composed service. In this paper, we present an approach for model-based\ntesting of coordination protocols designed in Reo. Our approach is based on the\ninput-output conformance (ioco) testing theory and exploits the mapping of\nautomata-based semantic models for Reo to equivalent process algebra\nspecifications."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.2", 
    "link": "http://arxiv.org/pdf/1108.2349v1", 
    "title": "Specification and Verification of Context-dependent Services", 
    "arxiv-id": "1108.2349v1", 
    "author": "Mubarak Mohammad", 
    "publish": "2011-08-11T08:51:32Z", 
    "summary": "Current approaches for the discovery, specification, and provision of\nservices ignore the relationship between the service contract and the\nconditions in which the service can guarantee its contract. Moreover, they do\nnot use formal methods for specifying services, contracts, and compositions.\nWithout a formal basis it is not possible to justify through formal\nverification the correctness conditions for service compositions and the\nsatisfaction of contractual obligations in service provisions. We remedy this\nsituation in this paper. We present a formal definition of services with\ncontext-dependent contracts. We define a composition theory of services with\ncontext-dependent contracts taking into consideration functional,\nnonfunctional, legal and contextual information. Finally, we present a formal\nverification approach that transforms the formal specification of service\ncomposition into extended timed automata that can be verified using the model\nchecking tool UPPAAL."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.3", 
    "link": "http://arxiv.org/pdf/1108.2350v1", 
    "title": "Product Lines for Service Oriented Applications - PL for SOA", 
    "arxiv-id": "1108.2350v1", 
    "author": "Mercy N. Njima", 
    "publish": "2011-08-11T08:51:38Z", 
    "summary": "PL for SOA proposes, formally, a software engineering methodology,\ndevelopment techniques and support tools for the provision of service product\nlines. We propose rigorous modeling techniques for the specification and\nverification of formal notations and languages for service computing with\ninclinations of variability. Through these cutting-edge technologies, increased\nlevels of flexibility and adaptivity can be achieved. This will involve\ndeveloping semantics of variability over behavioural models of services. Such\ntools will assist organizations to plan, optimize and control the quality of\nsoftware service provision, both at design and at run time by making it\npossible to develop flexible and cost-effective software systems that support\nhigh levels of reuse. We tackle this challenge from two levels. We use feature\nmodeling from product line engineering and, from a services point of view, the\norchestration language Orc. We introduce the Smart Grid as the service product\nline to apply the techniques to."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.2357v1", 
    "title": "Automated Functional Testing based on the Navigation of Web Applications", 
    "arxiv-id": "1108.2357v1", 
    "author": "Juan Carlos Due\u00f1as", 
    "publish": "2011-08-11T09:27:41Z", 
    "summary": "Web applications are becoming more and more complex. Testing such\napplications is an intricate hard and time-consuming activity. Therefore,\ntesting is often poorly performed or skipped by practitioners. Test automation\ncan help to avoid this situation. Hence, this paper presents a novel approach\nto perform automated software testing for web applications based on its\nnavigation. On the one hand, web navigation is the process of traversing a web\napplication using a browser. On the other hand, functional requirements are\nactions that an application must do. Therefore, the evaluation of the correct\nnavigation of web applications results in the assessment of the specified\nfunctional requirements. The proposed method to perform the automation is done\nin four levels: test case generation, test data derivation, test case\nexecution, and test case reporting. This method is driven by three kinds of\ninputs: i) UML models; ii) Selenium scripts; iii) XML files. We have\nimplemented our approach in an open-source testing framework named Automatic\nTesting Platform. The validation of this work has been carried out by means of\na case study, in which the target is a real invoice management system developed\nusing a model-driven approach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.2384v1", 
    "title": "Maximal Structuring of Acyclic Process Models", 
    "arxiv-id": "1108.2384v1", 
    "author": "Mathias Weske", 
    "publish": "2011-08-11T12:09:47Z", 
    "summary": "This paper contributes to the solution of the problem of transforming a\nprocess model with an arbitrary topology into an equivalent structured process\nmodel. In particular, this paper addresses the subclass of process models that\nhave no equivalent well-structured representation but which, nevertheless, can\nbe partially structured into their maximally-structured representation. The\nstructuring is performed under a behavioral equivalence notion that preserves\nobserved concurrency of tasks in equivalent process models. The paper gives a\nfull characterization of the subclass of acyclic process models that have no\nequivalent well-structured representation but do have an equivalent\nmaximally-structured one, as well as proposes a complete structuring method."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.3342v1", 
    "title": "Patterns for Business-to-consumer E-Commerce Applications", 
    "arxiv-id": "1108.3342v1", 
    "author": "Eduardo B. Fernandez", 
    "publish": "2011-08-16T20:03:01Z", 
    "summary": "E-commerce is one of the most important web applications. We present here a\nset of patterns that describe shopping carts, products, catalogue, customer\naccounts, shipping, and invoices. We combine them in the form of composite\npatterns, which in turn make up a domain model for business-to-consumer\ne-commerce. We also indicate how to add security constraints to this model.\nThis domain model can be used as a computation-independent model from which\nspecific applications can be produced using a model-driven architecture\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.4094v1", 
    "title": "OSD: A Source Level Bug Localization Technique Incorporating Control   Flow and State Information in Object Oriented Program", 
    "arxiv-id": "1108.4094v1", 
    "author": "Partha Pratim Ray", 
    "publish": "2011-08-20T07:21:51Z", 
    "summary": "Bug localization in object oriented program ha s always been an important\nissue in softeware engineering. In this paper, I propose a source level bug\nlocalization technique for object oriented embedded programs. My proposed\ntechnique, presents the idea of debugging an object oriented program in class\nlevel, incorporating the object state information into the Class Dependence\nGraph (ClDG). Given a program (having buggy statement) and an input that fails\nand others pass, my approach uses concrete as well as symbolic execution to\nsynthesize the passing inputs that marginally from the failing input in their\ncontrol flow behavior. A comparison of the execution traces of the failing\ninput and the passing input provides necessary clues to the root-cause of the\nfailure. A state trace difference, regarding the respective nodes of the ClDG\nis obtained, which leads to detect the bug in the program."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.4770v1", 
    "title": "A Classification Framework for Web Browser Cross-Context Communication", 
    "arxiv-id": "1108.4770v1", 
    "author": "Ivan Budiselic", 
    "publish": "2011-08-24T07:35:21Z", 
    "summary": "Demand for more advanced Web applications is the driving force behind Web\nbrowser evolution. Recent requirements for Rich Internet Applications, such as\nmashing-up data and background processing, are emphasizing the need for\nbuilding and executing Web applications as a coordination of browser execution\ncontexts. Since development of such Web applications depends on cross-context\ncommunication, many browser primitives and client-side frameworks have been\ndeveloped to support this communication. In this paper we present a\nsystematization of cross-context communication systems for Web browsers. Based\non an analysis of previous research, requirements for modern Web applications\nand existing systems, we extract a framework for classifying cross-context\ncommunica-tion systems. Using the framework, we evaluate the current ecosystem\nof cross-context communication and outline directions for future Web research\nand engineering."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1108.5295v1", 
    "title": "Checking Finite State Machine Conformance when there are Distributed   Observations", 
    "arxiv-id": "1108.5295v1", 
    "author": "Robert M Hierons", 
    "publish": "2011-08-26T13:34:54Z", 
    "summary": "This paper concerns state-based systems that interact with their environment\nat physically distributed interfaces, called ports. When such a system is used\na projection of the global trace, called a local trace, is observed at each\nport. This leads to the environment having reduced observational power: the set\nof local traces observed need not uniquely define the global trace that\noccurred. We consider the previously defined implementation relation\n$\\sqsubseteq_s$ and start by investigating the problem of defining a language\n${\\mathcal {\\tilde L}} (M)$ for a multi-port finite state machine (FSM) $M$\nsuch that $N \\sqsubseteq_s M$ if and only if every global trace of $N$ is in\n${\\mathcal {\\tilde L}} (M)$. The motivation is that if we can produce such a\nlanguage ${\\mathcal {\\tilde L}} (M)$ then this can potentially be used to\ninform development and testing. We show that ${\\mathcal {\\tilde L}} (M)$ can be\nuniquely defined but need not be regular. We then prove that it is generally\nundecidable whether $N \\sqsubseteq_s M$, a consequence of this result being\nthat it is undecidable whether there is a test case that is capable of\ndistinguishing two states or two multi-port FSM in distributed testing. This\nresult complements a previous result that it is undecidable whether there is a\ntest case that is guaranteed to distinguish two states or multi-port FSMs. We\nalso give some conditions under which $N \\sqsubseteq_s M$ is decidable. We then\nconsider the implementation relation $\\sqsubseteq_s^k$ that only concerns input\nsequences of length $k$ or less. Naturally, given FSMs $N$ and $M$ it is\ndecidable whether $N \\sqsubseteq_s^k M$ since only a finite set of traces is\nrelevant. We prove that if we place bounds on $k$ and the number of ports then\nwe can decide $N \\sqsubseteq_s^k M$ in polynomial time but otherwise this\nproblem is NP-hard."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.0070v2", 
    "title": "Enhance accuracy in Software cost and schedule estimation by using   \"Uncertainty Analysis and Assessment\" in the system modeling process", 
    "arxiv-id": "1110.0070v2", 
    "author": "Kardile Vilas Vasantrao", 
    "publish": "2011-10-01T04:28:36Z", 
    "summary": "Accurate software cost and schedule estimation are essential for software\nproject success. Often it referred to as the \"black art\" because of its\ncomplexity and uncertainty, software estimation is not as difficult or puzzling\nas people think. In fact, generating accurate estimates is straightforward-once\nyou understand the intensity of uncertainty and framework for the modeling\nprocess. The mystery to successful software estimation-distilling academic\ninformation and real-world experience into a practical guide for working\nsoftware professionals. Instead of arcane treatises and rigid modeling\ntechniques, this will guide highlights a proven set of procedures,\nunderstandable formulas, and heuristics that individuals and development teams\ncan apply to their projects to help achieve estimation proficiency with choose\nappropriate development approaches In the early stage of software life cycle\nproject manager are inefficient to estimate the effort, schedule, cost\nestimation and its development approach .This in turn, confuses the manager to\nbid effectively on software project and choose incorrect development approach.\nThat will directly effect on productivity cycle and increase level of\nuncertainty. This becomes a strong cause of project failure. So to avoid such\nproblem if we know level and sources of uncertainty in model design, It will\ndirective the developer to design accurate software cost and schedule\nestimation, which are essential for software project success. However once the\nrequired efforts have estimated, little is done to recalibrate and reduce the\nuncertainty of the initial estimates."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.1354v1", 
    "title": "Sources of Inter-package Conflicts in Debian", 
    "arxiv-id": "1110.1354v1", 
    "author": "Stefano Zacchiroli", 
    "publish": "2011-10-06T19:21:55Z", 
    "summary": "Inter-package conflicts require the presence of two or more packages in a\nparticular configuration, and thus tend to be harder to detect and localize\nthan conventional (intra-package) defects. Hundreds of such inter-package\nconflicts go undetected by the normal testing and distribution process until\nthey are later reported by a user. The reason for this is that current\nmeta-data is not fine-grained and accurate enough to cover all common types of\nconflicts. A case study of inter-package conflicts in Debian has shown that\nwith more detailed package meta-data, at least one third of all package\nconflicts could be prevented relatively easily, while another one third could\nbe found by targeted testing of packages that share common resources or\ncharacteristics. This paper reports the case study and proposes ideas to detect\ninter-package conflicts in the future."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.1957v1", 
    "title": "Stratified Outsourcing Theory", 
    "arxiv-id": "1110.1957v1", 
    "author": "S. F. M. van Vlijmen", 
    "publish": "2011-10-10T08:29:37Z", 
    "summary": "The terminology of sourcing, outsourcing and insourcing is developed in\ndetail on the basis of the preliminary definitions of outsourcing and\ninsourcing and related activities and competences as given in our three\nprevious papers on business mereology, on the concept of a sourcement, and on\noutsourcing competence respectively.\n  Besides providing more a detailed semantic analysis we will introduce,\nexplain, and illustrate a number of additional concepts including: principal\nunit of a sourcement, theme of a sourcement, current sourcement, (un)stable\nsourcement, and sourcement transformation.\n  A three level terminology is designed: (i) factual level: operational facts\nthat hold for sourcements including histories thereof, (ii) business level:\nroles and objectives of various parts of the factual level description, thus\nexplaining each partner's business process and business objectives, (iii)\ncontract level: specification of intended facts and intended business models as\nfound at the business level. Orthogonal to these three conceptual levels, are\nfour temporal aspects: history, now (actuality), transformation, and\ntransition.\n  A detailed description of the well-known range of sourcement transformations\nis given."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.2258v1", 
    "title": "A comparative study of process mediator components that support   behavioral incompatibility", 
    "arxiv-id": "1110.2258v1", 
    "author": "Mohd Sapiyan Baba", 
    "publish": "2011-10-11T03:15:39Z", 
    "summary": "Most businesses these days use the web services technology as a medium to\nallow interaction between a service provider and a service requestor. However,\nboth the service provider and the requestor would be unable to achieve their\nbusiness goals when there are miscommunications between their processes. This\nresearch focuses on the process incompatibility between the web services and\nthe way to automatically resolve them by using a process mediator. This paper\npresents an overview of the behavioral incompatibility between web services and\nthe overview of process mediation in order to resolve the complications faced\ndue to the incompatibility. Several state-of the-art approaches have been\nselected and analyzed to understand the existing process mediation components.\nThis paper aims to provide a valuable gap analysis that identifies the\nimportant research areas in process mediation that have yet to be fully\nexplored."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.3379v1", 
    "title": "Identifying Reference Objects by Hierarchical Clustering in Java   Environment", 
    "arxiv-id": "1110.3379v1", 
    "author": "Dr. G. Geetha", 
    "publish": "2011-10-15T04:25:47Z", 
    "summary": "Recently Java programming environment has become so popular. Java programming\nlanguage is a language that is designed to be portable enough to be executed in\nwide range of computers ranging from cell phones to supercomputers. Computer\nprograms written in Java are compiled into Java Byte code instructions that are\nsuitable for execution by a Java Virtual Machine implementation. Java virtual\nMachine is commonly implemented in software by means of an interpreter for the\nJava Virtual Machine instruction set. As an object oriented language, Java\nutilizes the concept of objects. Our idea is to identify the candidate objects'\nreferences in a Java environment through hierarchical cluster analysis using\nreference stack and execution stack."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.3384v1", 
    "title": "X-ray view on a Class using Conceptual Analysis in Java Environment", 
    "arxiv-id": "1110.3384v1", 
    "author": "Prof. Mritunjay Kumar Rai", 
    "publish": "2011-10-15T05:26:48Z", 
    "summary": "Modularity is one of the most important principles in software engineering\nand a necessity for every practical software. Since the design space of\nsoftware is generally quite large, it is valuable to provide automatic means to\nhelp modularizing it. An automatic technique for software modularization is\nobject- oriented concept analysis (OOCA). X-ray view of the class is one of the\naspect of this Object oriented concept analysis. We shall use this concept in a\njava environment."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.61.4", 
    "link": "http://arxiv.org/pdf/1110.4500v2", 
    "title": "Semantic conflict resolution for integration of business components", 
    "arxiv-id": "1110.4500v2", 
    "author": "Abderrahim Sekkaki", 
    "publish": "2011-10-20T11:24:20Z", 
    "summary": "Reusing and integrating Business Components in a new Information System\nrequires detection and resolution of semantic conflicts. Moreover, most of\nintegration and semantic conflict resolution systems rely on ontology alignment\nmethods based on domain ontology. This work is positioned at the intersection\nof two research areas: Integration of reusable B C and alignment of ontologies\nfor semantic conflict resolution. Our contribution concerns both the proposal\nof a BC integration solution based on ontologies alignment and a method for\nenriching the domain ontology used as a support for alignment"
},{
    "category": "cs.SE", 
    "doi": "10.1109/NOTERE.2011.5957993", 
    "link": "http://arxiv.org/pdf/1110.4501v1", 
    "title": "An Ontology-Based Method for Semantic Integration of Business Components", 
    "arxiv-id": "1110.4501v1", 
    "author": "Larbi Kzaz", 
    "publish": "2011-10-20T11:25:40Z", 
    "summary": "Building new business information systems from reusable components is today\nan approach widely adopted and used. Using this approach in analysis and design\nphases presents a great interest and requires the use of a particular class of\ncomponents called Business Components (BC). Business Components are today\ndeveloped by several manufacturers and are available in many repositories.\nHowever, reusing and integrating them in a new Information System requires\ndetection and resolution of semantic conflicts. Moreover, most of integration\nand semantic conflict resolution systems rely on ontology alignment methods\nbased on domain ontology. This work is positioned at the intersection of two\nresearch areas: Integration of reusable Business Components and alignment of\nontologies for semantic conflict resolution. Our contribution concerns both the\nproposal of a BC integration solution based on ontologies alignment and a\nmethod for enriching the domain ontology used as a support for alignment."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1110.6879v1", 
    "title": "Incorporating Agile with MDA Case Study: Online Polling System", 
    "arxiv-id": "1110.6879v1", 
    "author": "Shweta Singh", 
    "publish": "2011-10-31T18:10:57Z", 
    "summary": "Nowadays agile software development is used in greater extend but for small\norganizations only, whereas MDA is suitable for large organizations but yet not\nstandardized. In this paper the pros and cons of Model Driven Architecture\n(MDA) and Extreme programming have been discussed. As both of them have some\nlimitations and cannot be used in both large scale and small scale\norganizations a new architecture has been proposed. In this model it is tried\nto opt the advantages and important values to overcome the limitations of both\nthe software development procedures. In support to the proposed architecture\nthe implementation of it on Online Polling System has been discussed and all\nthe phases of software development have been explained."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.0562v1", 
    "title": "Model-driven system development: Experimental design and report of the   pilot experiment", 
    "arxiv-id": "1111.0562v1", 
    "author": "\u00d3scar Pastor", 
    "publish": "2011-11-02T16:56:31Z", 
    "summary": "This report describes de design of an experiment that intends to compare two\nvariants of a modeldriven system development method, so as to assess the impact\nof requirements engineering practice in the quality of the conceptual models.\nThe conceptual modelling method being assessed is the OO-Method [Pastor and\nMolina 2007]. One of its variants includes Communication Analysis, a\ncommunication-oriented requirements engineering method [Espa\\~na, Gonz\\'alez et\nal. 2009] and a set of guidelines to derive conceptual models from requirements\nmodels [Espa\\~na, Ruiz et al. 2011; Gonz\\'alez, Espa\\~na et al. 2011]. The\nother variant is an ad-hoc, text-based requirements practice similar to the one\nthat is applied in industrial projects by OO-Method practitioners. The goal of\nthe research, summarised according to the Goal/Question/Metric template [Basili\nand Rombach 1988], is to:\n  *) analyse the resulting models of two model-based information systems\nanalysis method variants; namely, the OO-Method (OOM) and the integration of\nCommunication Analysis and the OO-Method (CA+OOM),\n  *) for the purpose of carrying out a comparative evaluation\n  *) with respect to performance of the subject and acceptance of the method;\n  *) from the viewpoint of the information systems researcher\n  *) in the context of bachelor students."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.0870v1", 
    "title": "A Formal Approach for Agent Based Large Concurrent Intelligent Systems", 
    "arxiv-id": "1111.0870v1", 
    "author": "Jagdish L. Raheja", 
    "publish": "2011-09-29T08:54:13Z", 
    "summary": "Large Intelligent Systems are so complex these days that an urgent need for\ndesigning such systems in best available way is evolving. Modeling is the\nuseful technique to show a complex real world system into the form of\nabstraction, so that analysis and implementation of the intelligent system\nbecome easy and is useful in gathering the prior knowledge of system that is\nnot possible to experiment with the real world complex systems. This paper\ndiscusses a formal approach of agent-based large systems modeling for\nintelligent systems, which describes design level precautions, challenges and\ntechniques using autonomous agents, as its fundamental modeling abstraction. We\nare discussing Ad-Hoc Network System as a case study in which we are using\nmobile agents where nodes are free to relocate, as they form an Intelligent\nSystems. The designing is very critical in this scenario and it can reduce the\nwhole cost, time duration and risk involved in the project."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.1022v1", 
    "title": "Towards the integration of formal specification in the \u00c1ncora   methodology", 
    "arxiv-id": "1111.1022v1", 
    "author": "Mart\u00edn Jos\u00e9 Jos\u00e9", 
    "publish": "2011-11-04T00:52:06Z", 
    "summary": "There are some non-formal methodologies such as RUP, OpenUP, agile\nmethodologies such as SCRUP, XP and techniques like those proposed by UML,\nwhich allow the development of software. The software industry has struggled to\ngenerate quality software, as importance has not been given to the engineering\nrequirements, resulting in a poor specification of requirements and software of\npoor quality. In order to generate a contribution to the specification of\nrequirements, this article describes a methodological proposal, implementing\nformal methods to the results of the process of requirements analysis of the\nmethodology \\'Ancora."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.1586v1", 
    "title": "Evaluation of Computability Criterions for Runtime Web Service   Integration", 
    "arxiv-id": "1111.1586v1", 
    "author": "S. Abarna", 
    "publish": "2011-11-07T14:12:44Z", 
    "summary": "Today's competitive environment drives the enterprises to extend their focus\nand collaborate with their business partners to carry out the necessities.\nTight coordination among business partners assists to share and integrate the\nservice logic globally. But integrating service logics across diverse\nenterprises leads to exponential problem which stipulates developers to\ncomprehend the whole service and must resolve suitable method to integrate the\nservices. It is complex and time-consuming task. So the present focus is to\nhave a mechanized system to analyze the Business logics and convey the proper\nmode to integrate them. There is no standard model to undertake these issues\nand one such a framework proposed in this paper examines the Business logics\nindividually and suggests proper structure to integrate them. One of the\ninnovative concepts of proposed model is Property Evaluation System which\nscrutinizes the service logics and generates Business Logic Property Schema\n(BLPS) for the required services. BLPS holds necessary information to recognize\nthe correct structure for integrating the service logics. At the time of\nintegration, System consumes this BLPS schema and suggests the feasible ways to\nintegrate the service logics. Also if the service logics are attempted to\nintegrate in invalid structure or attempted to violate accessibility levels,\nsystem will throw exception with necessary information. This helps developers\nto ascertain the efficient structure to integrate the services with least\neffort."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.1598v1", 
    "title": "Design and Validation of Safety Cruise Control System for Automobiles", 
    "arxiv-id": "1111.1598v1", 
    "author": "Ashwin Tumma", 
    "publish": "2011-11-07T14:45:06Z", 
    "summary": "In light of the recent humongous growth of the human population worldwide,\nthere has also been a voluminous and uncontrolled growth of vehicles, which has\nconsequently increased the number of road accidents to a large extent. In lieu\nof a solution to the above mentioned issue, our system is an attempt to\nmitigate the same using synchronous programming language. The aim is to develop\na safety crash warning system that will address the rear end crashes and also\ntake over the controlling of the vehicle when the threat is at a very high\nlevel. Adapting according to the environmental conditions is also a prominent\nfeature of the system. Safety System provides warnings to drivers to assist in\navoiding rear-end crashes with other vehicles. Initially the system provides a\nlow level alarm and as the severity of the threat increases the level of\nwarnings or alerts also rises. At the highest level of threat, the system\nenters in a Cruise Control Mode, wherein the system controls the speed of the\nvehicle by controlling the engine throttle and if permitted, the brake system\nof the vehicle. We focus on this crash area as it has a very high percentage of\nthe crash-related fatalities. To prove the feasibility, robustness and\nreliability of the system, we have also proved some of the properties of the\nsystem using temporal logic along with a reference implementation in ESTEREL.\nTo bolster the same, we have formally verified various properties of the system\nalong with their proofs."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.1826v1", 
    "title": "Monitoring Software Reliability using Statistical Process control: An   MMLE approach", 
    "arxiv-id": "1111.1826v1", 
    "author": "R. R. L. Kantam", 
    "publish": "2011-11-08T08:35:03Z", 
    "summary": "This paper consider an MMLE (Modified Maximum Likelihood Estimation) based\nscheme to estimate software reliability using exponential distribution. The\nMMLE is one of the generalized frameworks of software reliability models of Non\nHomogeneous Poisson Processes (NHPPs). The MMLE gives analytical estimators\nrather than an iterative approximation to estimate the parameters. In this\npaper we proposed SPC (Statistical Process Control) Charts mechanism to\ndetermine the software quality using inter failure times data. The Control\ncharts can be used to measure whether the software process is statistically\nunder control or not."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.1904v1", 
    "title": "Aspects of Assembly and Cascaded Aspects of Assembly: Logical and   Temporal Properties", 
    "arxiv-id": "1111.1904v1", 
    "author": "Michel Riveill", 
    "publish": "2011-11-08T13:34:36Z", 
    "summary": "Highly dynamic computing environments, like ubiquitous and pervasive\ncomputing environments, require frequent adaptation of applications. This has\nto be done in a timely fashion, and the adaptation process must be as fast as\npossible and mastered. Moreover the adaptation process has to ensure a\nconsistent result when finished whereas adaptations to be implemented cannot be\nanticipated at design time. In this paper we present our mechanism for\nself-adaptation based on the aspect oriented programming paradigm called Aspect\nof Assembly (AAs). Using AAs: (1) the adaptations process is fast and its\nduration is mastered; (2) adaptations' entities are independent of each other\nthanks to the weaver logical merging mechanism; and (3) the high variability of\nthe software infrastructure can be managed using a mono or multi-cycle weaving\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.2750v1", 
    "title": "Finite State Machine Based Evaluation Model for Web Service Reliability   Analysis", 
    "arxiv-id": "1111.2750v1", 
    "author": "Lakshmi. P", 
    "publish": "2011-11-07T14:01:25Z", 
    "summary": "Now-a-days they are very much considering about the changes to be done at\nshorter time since the reaction time needs are decreasing every moment.\nBusiness Logic Evaluation Model (BLEM) are the proposed solution targeting\nbusiness logic automation and facilitating business experts to write\nsophisticated business rules and complex calculations without costly custom\nprogramming. BLEM is powerful enough to handle service manageability issues by\nanalyzing and evaluating the computability and traceability and other criteria\nof modified business logic at run time. The web service and QOS grows\nexpensively based on the reliability of the service. Hence the service provider\nof today things that reliability is the major factor and any problem in the\nreliability of the service should overcome then and there in order to achieve\nthe expected level of reliability. In our paper we propose business logic\nevaluation model for web service reliability analysis using Finite State\nMachine (FSM) where FSM will be extended to analyze the reliability of composed\nset of service i.e., services under composition, by analyzing reliability of\neach participating service of composition with its functional work flow\nprocess. FSM is exploited to measure the quality parameters. If any change\noccurs in the business logic the FSM will automatically measure the\nreliability."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.2826v1", 
    "title": "Concurrent Development of Model and Implementation", 
    "arxiv-id": "1111.2826v1", 
    "author": "S. Gruner", 
    "publish": "2011-11-11T19:15:09Z", 
    "summary": "This paper considers how a formal mathematically-based model can be used in\nsupport of evolutionary software development, and in particular how such a\nmodel can be kept consistent with the implementation as it changes to meet new\nrequirements. A number of techniques are listed can make use of such a model to\nenhance the development process, and also ways to keep model and implementation\nconsistent. The effectiveness of these techniques is investigated through two\ncase studies concerning the development of small e-business applications, a\ntravel agent and a mortgage broker. Some successes are reported, notably in the\nuse of rapid throwaway modelling to investigate design alternatives, and also\nin the use of close team working and modelbased trace-checking to maintain\nsynchronisation between model and implementation throughout the development.\nThe main areas of weakness were seen to derive from deficiencies in tool\nsupport. Recommendations are therefore made for future improvements to tools\nsupporting formal models which would, in principle, make this co-evolutionary\napproach attractive to industrial software developers. It is claimed that in\nfact tools already exist that provide the desired facilities, but these are not\nnecessarily production-quality, and do not all support the same notations, and\nhence cannot be used together."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2011.2408", 
    "link": "http://arxiv.org/pdf/1111.3001v1", 
    "title": "A Comprehensive Study of Commonly Practiced Heavy and Light Weight   Software Methodologies", 
    "arxiv-id": "1111.3001v1", 
    "author": "Usman Ali Khan", 
    "publish": "2011-11-13T11:01:52Z", 
    "summary": "Software has been playing a key role in the development of modern society.\nSoftware industry has an option to choose suitable methodology/process model\nfor its current needs to provide solutions to give problems. Though some\ncompanies have their own customized methodology for developing their software\nbut majority agrees that software methodologies fall under two categories that\nare heavyweight and lightweight. Heavyweight methodologies (Waterfall Model,\nSpiral Model) are also known as the traditional methodologies, and their\nfocuses are detailed documentation, inclusive planning, and extroverted design.\nLightweight methodologies (XP, SCRUM) are, referred as agile methodologies.\nLight weight methodologies focused mainly on short iterative cycles, and rely\non the knowledge within a team. The aim of this paper is to describe the\ncharacteristics of popular heavyweight and lightweight methodologies that are\nwidely practiced in software industries. We have discussed the strengths and\nweakness of the selected models. Further we have discussed the strengths and\nweakness between the two opponent methodologies and some criteria is also\nillustrated that help project managers for the selection of suitable model for\ntheir projects."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.1", 
    "link": "http://arxiv.org/pdf/1111.4736v1", 
    "title": "GMF: A Model Migration Case for the Transformation Tool Contest", 
    "arxiv-id": "1111.4736v1", 
    "author": "Markus Herrmannsdoerfer", 
    "publish": "2011-11-21T05:24:14Z", 
    "summary": "Using a real-life evolution taken from the Graphical Modeling Framework, we\ninvite submissions to explore ways in which model transformation and migration\ntools can be used to migrate models in response to metamodel adaptation."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.4", 
    "link": "http://arxiv.org/pdf/1111.4739v1", 
    "title": "HelloWorld! An Instructive Case for the Transformation Tool Contest", 
    "arxiv-id": "1111.4739v1", 
    "author": "Steffen Mazanek", 
    "publish": "2011-11-21T05:24:36Z", 
    "summary": "This case comprises several primitive tasks that can be solved straight away\nwith most transformation tools. The aim is to cover the most important kinds of\nprimitive operations on models, i.e. create, read, update and delete (CRUD). To\nthis end, tasks such as a constant transformation, a model-to-text\ntransformation, a very basic migration transformation or diverse simple queries\nor in-place operations on graphs have to be solved.\n  The motivation for this case is that the results expectedly will be very\ninstructive for beginners. Also, it is really hard to compare transformation\nlanguages along complex cases, because the complexity of the respective case\nmight hide the basic language concepts and constructs."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.5", 
    "link": "http://arxiv.org/pdf/1111.4740v1", 
    "title": "Solving the TTC 2011 Model Migration Case with Edapt", 
    "arxiv-id": "1111.4740v1", 
    "author": "Markus Herrmannsdoerfer", 
    "publish": "2011-11-21T05:24:44Z", 
    "summary": "This paper gives an overview of the Edapt solution to the GMF model migration\ncase of the Transformation Tool Contest 2011."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.6", 
    "link": "http://arxiv.org/pdf/1111.4741v1", 
    "title": "Solving the TTC 2011 Model Migration Case with UML-RSDS", 
    "arxiv-id": "1111.4741v1", 
    "author": "S. Kolahdouz-Rahimi", 
    "publish": "2011-11-21T05:24:51Z", 
    "summary": "In this paper we apply the UML-RSDS notation and tools to the GMF model\nmigration case study and explain how to use the UML-RSDS tools."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.8", 
    "link": "http://arxiv.org/pdf/1111.4743v1", 
    "title": "Solving the TTC 2011 Compiler Optimization Case with QVTR-XSLT", 
    "arxiv-id": "1111.4743v1", 
    "author": "Volker Stolz", 
    "publish": "2011-11-21T05:25:11Z", 
    "summary": "In this short paper we present our solution for the Compiler Optimization\ncase study of the Transformation Tool Contest (TTC) 2011 using the QVTR-XSLT\ntool. The tool supports editing and execution of the graphical notation of QVT\nRelations language"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.11", 
    "link": "http://arxiv.org/pdf/1111.4746v1", 
    "title": "Solving the TTC 2011 Compiler Optimization Case with GROOVE", 
    "arxiv-id": "1111.4746v1", 
    "author": "Eduardo Zambon", 
    "publish": "2011-11-21T05:25:30Z", 
    "summary": "This report presents a partial solution to the Compiler Optimization case\nstudy using GROOVE. We explain how the input graphs provided with the case\nstudy were adapted into a GROOVE representation and we describe an initial\nsolution for Task 1. This solution allows us to automatically reproduce the\nsteps of the constant folding example given in the case description. We did not\nsolve Task 2."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.13", 
    "link": "http://arxiv.org/pdf/1111.4748v1", 
    "title": "Solving the TTC 2011 Reengineering Case with VIATRA2", 
    "arxiv-id": "1111.4748v1", 
    "author": "G\u00e1bor Bergmann", 
    "publish": "2011-11-21T05:25:42Z", 
    "summary": "The current paper presents a solution of the Program Understanding: A\nReengineering Case for the Transformation Tool Contest using the VIATRA2 model\ntransformation tool."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.14", 
    "link": "http://arxiv.org/pdf/1111.4749v1", 
    "title": "Solving the TTC 2011 Reengineering Case with Edapt", 
    "arxiv-id": "1111.4749v1", 
    "author": "Markus Herrmannsdoerfer", 
    "publish": "2011-11-21T05:25:49Z", 
    "summary": "This paper gives an overview of the Edapt solution to the reengineering case\nof the Transformation Tool Contest 2011."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.17", 
    "link": "http://arxiv.org/pdf/1111.4752v1", 
    "title": "Solving the TTC 2011 Reengineering Case with Henshin", 
    "arxiv-id": "1111.4752v1", 
    "author": "Johannes Tietje", 
    "publish": "2011-11-21T05:26:26Z", 
    "summary": "This paper presents the Henshin solution to the Model Transformations for\nProgram Understanding case study as part of the Transformation Tool Contest\n2011."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.18", 
    "link": "http://arxiv.org/pdf/1111.4753v1", 
    "title": "Saying Hello World with Edapt - A Solution to the TTC 2011 Instructive   Case", 
    "arxiv-id": "1111.4753v1", 
    "author": "Markus Herrmannsdoerfer", 
    "publish": "2011-11-21T05:26:33Z", 
    "summary": "This paper gives an overview of the Edapt solution to the hello world case of\nthe Transformation Tool Contest 2011."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.19", 
    "link": "http://arxiv.org/pdf/1111.4754v1", 
    "title": "Saying Hello World with GROOVE - A Solution to the TTC 2011 Instructive   Case", 
    "arxiv-id": "1111.4754v1", 
    "author": "Eduardo Zambon", 
    "publish": "2011-11-21T05:26:42Z", 
    "summary": "This report presents a solution to the Hello World case study of TTC 2011\nusing GROOVE. We provide and explain the grammar that we used to solve the case\nstudy. Every requested question of the case study was solved by a single rule\napplication."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.22", 
    "link": "http://arxiv.org/pdf/1111.4756v1", 
    "title": "Saying Hello World with Henshin - A Solution to the TTC 2011 Instructive   Case", 
    "arxiv-id": "1111.4756v1", 
    "author": "Johannes Tietje", 
    "publish": "2011-11-21T05:27:11Z", 
    "summary": "This paper gives an overview of the Henshin solution to the Hello World case\nstudy of the Transformation Tool Contest 2011, intended to show basic language\nconcepts and constructs."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.25", 
    "link": "http://arxiv.org/pdf/1111.4758v1", 
    "title": "Saying Hello World with VIATRA2 - A Solution to the TTC 2011 Instructive   Case", 
    "arxiv-id": "1111.4758v1", 
    "author": "G\u00e1bor Bergmann", 
    "publish": "2011-11-21T05:27:33Z", 
    "summary": "The paper presents a solution of the Hello World! An Instructive Case for the\nTransformation Tool Contest using the VIATRA2 model transformation tool."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.20", 
    "link": "http://arxiv.org/pdf/1111.4761v1", 
    "title": "Saying HelloWorld with QVTR-XSLT - A Solution to the TTC 2011   Instructive Case", 
    "arxiv-id": "1111.4761v1", 
    "author": "Volker Stolz", 
    "publish": "2011-11-21T05:51:52Z", 
    "summary": "In this short paper we present our solution for the Hello World case study of\nthe Transformation Tool Contest (TTC) 2011 using the QVTR-XSLT tool. The tool\nsupports editing and execution of the graphical notation of QVT Relations\nlanguage. The case study consists of a set of simple transformation tasks which\ncovers the basic functions required for a transformation language, such as\ncreating, reading/querying, updating and deleting of model elements. We design\na transformation for each of the tasks."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.26", 
    "link": "http://arxiv.org/pdf/1111.4763v1", 
    "title": "Saying Hello World with UML-RSDS - A Solution to the 2011 Instructive   Case", 
    "arxiv-id": "1111.4763v1", 
    "author": "S. Kolahdouz-Rahimi", 
    "publish": "2011-11-21T05:55:55Z", 
    "summary": "In this paper we apply the UML-RSDS notation and tools to the \"Hello World\"\ncase studies and explain the underlying development process for this model\ntransformation approach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.4764v1", 
    "title": "Saying Hello World with Epsilon - A Solution to the 2011 Instructive   Case", 
    "arxiv-id": "1111.4764v1", 
    "author": "Fiona A. C. Polack", 
    "publish": "2011-11-21T05:56:02Z", 
    "summary": "Epsilon is an extensible platform of integrated and task-specific languages\nfor model management. With solutions to the 2011 TTC Hello World case, this\npaper demonstrates some of the key features of the Epsilon Object Language (an\nextension and reworking of OCL), which is at the core of Epsilon. In addition,\nthe paper introduces several of the task-specific languages provided by Epsilon\nincluding the Epsilon Generation Language (for model-to-text transformation),\nthe Epsilon Validation Language (for model validation) and Epsilon Flock (for\nmodel migration)."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5002v1", 
    "title": "Drivers of the Cost of Spreadsheet Audit", 
    "arxiv-id": "1111.5002v1", 
    "author": "David Colver", 
    "publish": "2011-11-21T20:50:55Z", 
    "summary": "A review of 75 formal audit assignments shows that the effort taken to\nidentify defects in financial models taken from the domain of limited recourse\n(project) finance is uncorrelated with common measures of the physical\ncharacteristics of the spreadsheets concerned."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5007v1", 
    "title": "Leveraging User Profile and Behaviour to Design Practical Spreadsheet   Controls for the Finance Function", 
    "arxiv-id": "1111.5007v1", 
    "author": "Nancy Wu", 
    "publish": "2011-11-21T20:59:57Z", 
    "summary": "Recognizing that the use of spreadsheets within finance will likely not\nsubside in the near future, this paper discusses a major barrier that is\npreventing more organizations from adopting enterprise spreadsheet management\nprograms. But even without a corporate mandated effort to improve spreadsheet\ncontrols, finance functions can still take simple yet effective steps to start\nmanaging the risk of errors in key spreadsheets by strategically selecting\ncontrols that complement existing user practice"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5133v3", 
    "title": "Decentralised LTL Monitoring", 
    "arxiv-id": "1111.5133v3", 
    "author": "Yli\u00e8s Falcone", 
    "publish": "2011-11-22T09:35:20Z", 
    "summary": "Users wanting to monitor distributed or component-based systems often\nperceive them as monolithic systems which, seen from the outside, exhibit a\nuniform behaviour as opposed to many components displaying many local\nbehaviours that together constitute the system's global behaviour. This level\nof abstraction is often reasonable, hiding implementation details from users\nwho may want to specify the system's global behaviour in terms of an LTL\nformula. However, the problem that arises then is how such a specification can\nactually be monitored in a distributed system that has no central data\ncollection point, where all the components' local behaviours are observable. In\nthis case, the LTL specification needs to be decomposed into sub-formulae\nwhich, in turn, need to be distributed amongst the components' locally attached\nmonitors, each of which sees only a distinct part of the global behaviour. The\nmain contribution of this paper is an algorithm for distributing and monitoring\nLTL formulae, such that satisfac- tion or violation of specifications can be\ndetected by local monitors alone. We present an implementation and show that\nour algorithm introduces only a minimum delay in detecting\nsatisfaction/violation of a specification. Moreover, our practical results show\nthat the communication overhead introduced by the local monitors is\nconsiderably lower than the number of messages that would need to be sent to a\ncentral data collection point."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5170v1", 
    "title": "Concurrent Models for Function Execution", 
    "arxiv-id": "1111.5170v1", 
    "author": "Bob Diertens", 
    "publish": "2011-11-22T12:07:40Z", 
    "summary": "We derive an abstract computational model from a sequential computational\nmodel that is generally used for function execution. This abstract\ncomputational model allows for the concurrent execution of functions. We\ndiscuss concurrent models for function execution as implementations from the\nabstract computational model. We give an example of a particular concurrent\nfunction construct that can be implemented on a concurrent machine model using\nmulti-threading. The result is a framework of computational models at different\nlevels of abstraction that can be used in further development of concurrent\ncomputational models that deal with the problems inherent with concurrency."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5172v1", 
    "title": "Communicating Concurrent Functions", 
    "arxiv-id": "1111.5172v1", 
    "author": "Bob Diertens", 
    "publish": "2011-11-22T12:10:20Z", 
    "summary": "In this article we extend the framework of execution of concurrent functions\non different abstract levels from previous work with communication between the\nconcurrent functions. We classify the communications and identify problems that\ncan occur with these communications. We present solutions for the problems\nbased on encapsulation and abstraction to obtain correct behaviours. The result\nis that communication on a low level of abstraction in the form of shared\nmemory and message passing is dealt with on an higher level of abstraction."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.74.27", 
    "link": "http://arxiv.org/pdf/1111.5640v1", 
    "title": "A New Proposed Technique to Improve Software Regression Testing Cost", 
    "arxiv-id": "1111.5640v1", 
    "author": "Seifedine Kadry", 
    "publish": "2011-11-23T22:31:13Z", 
    "summary": "In this article, we describe the regression test process to test and verify\nthe changes made on software. A developed technique use the automation test\nbased on decision tree and test selection process in order to reduce the\ntesting cost is given. The developed technique is applied to a practical case\nand the result show its improvement."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.5721v1", 
    "title": "MAPSS, a Multi-Aspect Partner and Service Selection Method", 
    "arxiv-id": "1111.5721v1", 
    "author": "Willy Picard", 
    "publish": "2011-11-24T11:08:02Z", 
    "summary": "In Service-Oriented Virtual Organization Breeding Environments (SOVOBEs),\nservices performed by people, organizations and information systems are\ncomposed in potentially complex business processes performed by a set of\npartners. In a SOVOBE, the success of a virtual organization depends largely on\nthe partner and service selection process, which determines the composition of\nservices performed by the VO partners. In this paper requirements for a partner\nand service selection method for SOVOBEs are defined and a novel Multi-Aspect\nPartner and Service Selection method, MAPSS, is presented. The MAPSS method\nallows a VO planner to select appropriate services and partners based on their\ncompetences and their relations with other services/partners. The MAPSS method\nrelies on a genetic algorithm to select the most appropriate set of partners\nand services."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6730v1", 
    "title": "Minimizing the Risk of Architectural Decay by using Architecture-Centric   Evolution Process", 
    "arxiv-id": "1111.6730v1", 
    "author": "M. Aqeel Iqbal", 
    "publish": "2011-11-29T08:54:22Z", 
    "summary": "Software systems endure many noteworthy changes throughout their life-cycle\nin order to follow the evolution of the problem domains. Generally, the\nsoftware system architecture cannot follow the rapid evolution of a problem\ndomain which results in the discrepancies between the implemented and designed\narchitecture. Software architecture illustrates a system's structure and global\nproperties and consequently determines not only how the system should be\nconstructed but also leads its evolution. Architecture plays an important role\nto ensure that a system satisfies its business and mission goals during\nimplementation and evolution. However, the capabilities of the designed\narchitecture may possibly be lost when the implementation does not conform to\nthe designed architecture. Such a loss of consistency causes the risk of\narchitectural decay. The architectural decay can be avoided if architectural\nchanges are made as early as possible. The paper presents the Process Model for\nArchitecture-Centric Evolution which improves the quality of software systems\nthrough maintaining consistency between designed architecture and\nimplementation. It also increases architecture awareness of developers which\nassists in minimizing the risk of architectural decay. In the proposed approach\nconsistency checks are performed before and after the change implementation."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6858v1", 
    "title": "Workbook Structure Analysis - \"Coping with the Imperfect\"", 
    "arxiv-id": "1111.6858v1", 
    "author": "Ray Hooper", 
    "publish": "2011-11-29T15:41:13Z", 
    "summary": "This Paper summarises the operation of software developed for the analysis of\nworkbook structure. This comprises: the identification of layout in terms of\nfilled areas formed into \"Stripes\", the identification of all the Formula\nBlocks/Cells and the identification of Data Blocks/Cells referenced by those\nformulas. This development forms part of our FormulaDataSleuth toolset. It is\nessential for the initial \"Watching\" of an existing workbook and enables the\nworkbook to be subsequently managed and protected from damage."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6866v1", 
    "title": "Spreadsheets in Financial Departments: An Automated Analysis of 65,000   Spreadsheets using the Luminous Technology", 
    "arxiv-id": "1111.6866v1", 
    "author": "Shane Hayes", 
    "publish": "2011-11-29T15:59:44Z", 
    "summary": "Spreadsheet technology is a cornerstone of IT systems in most organisations.\nIt is often the glue that binds more structured transaction-based systems\ntogether. Financial operations are a case in point where spreadsheets fill the\ngaps left by dedicated accounting systems, particularly covering reporting and\nbusiness process operations. However, little is understood as to the nature of\nspreadsheet usage in organisations and the contents and structure of these\nspreadsheets as they relate to key business functions with few, if any,\ncomprehensive analyses of spreadsheet repositories in real organisations. As\nsuch this paper represents an important attempt at profiling real and\nsubstantial spreadsheet repositories.\n  Using the Luminous technology an analysis of 65,000 spreadsheets for the\nfinancial departments of both a government and a private commercial\norganisation was conducted. This provides an important insight into the nature\nand structure of these spreadsheets, the links between them, the existence and\nnature of macros and the level of repetitive processes performed through the\nspreadsheets. Furthermore it highlights the organisational dependence on\nspreadsheets and the range and number of spreadsheets dealt with by individuals\non a daily basis. In so doing, this paper prompts important questions that can\nframe future research in the domain."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6870v1", 
    "title": "Beyond The Desktop Spreadsheet", 
    "arxiv-id": "1111.6870v1", 
    "author": "Stephen McCrory", 
    "publish": "2011-11-29T16:06:26Z", 
    "summary": "Hypernumbers is a new commercial web-based spreadsheet. It addresses several\nrisk factors in deploying spreadsheets."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6872v1", 
    "title": "Effect of Range Naming Conventions on Reliability and Development Time   for Simple Spreadsheet Formulas", 
    "arxiv-id": "1111.6872v1", 
    "author": "Kevin McDaid", 
    "publish": "2011-11-29T16:18:22Z", 
    "summary": "Practitioners often argue that range names make spreadsheets easier to\nunderstand and use, akin to the role of good variable names in traditional\nprogramming languages, yet there is no supporting scientific evidence. The\nauthors previously published experiments that disproved this theory in relation\nto debugging, and now turn their focus to development. This paper presents the\nresults of two iterations of a new experiment, which measure the effect of\nrange names on the correctness of, and the time it takes to develop, simple\nsummation formulas. Our findings, supported by statistically significant\nresults, show that formulas developed by non-experts using range names are more\nlikely to contain errors and take longer to develop. Taking these findings with\nthe findings from previous experiments, we conclude that range names do not\nimprove the quality of spreadsheets developed by novice and intermediate users.\nThis paper is important in that it finds that the choice of naming convention\ncan have a significant impact on novice and intermediate users' performance in\nformula development, with less structured naming conventions resulting in\npoorer performance by users."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6878v1", 
    "title": "From Good Practices to Effective Policies for Preventing Errors in   Spreadsheets", 
    "arxiv-id": "1111.6878v1", 
    "author": "Daniel Kulesz", 
    "publish": "2011-11-29T16:28:56Z", 
    "summary": "Thanks to the enormous flexibility they provide, spreadsheets are considered\na priceless blessing by many end-users. Many spreadsheets, however, contain\nerrors which can lead to severe consequences in some cases. To manage these\nrisks, quality managers in companies are often asked to develop appropriate\npolicies for preventing spreadsheet errors. Good policies should specify rules\nwhich are based on \"known-good\" practices. While there are many proposals for\nsuch practices in literature written by practitioners and researchers, they are\noften not consistent with each other. Therefore no general agreement has been\nreached yet and no science-based \"golden rules\" have been published. This paper\nproposes an expert-based, retrospective approach to the identification of good\npractices for spreadsheets. It is based on an evaluation loop that\ncross-validates the findings of human domain experts against rules implemented\nin a semi-automated spreadsheet workbench, taking into account the context in\nwhich the spreadsheets are used."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6884v1", 
    "title": "A Platform for Spreadsheet Composition", 
    "arxiv-id": "1111.6884v1", 
    "author": "Michele Stecca", 
    "publish": "2011-11-29T16:36:59Z", 
    "summary": "A huge amount of data is everyday managed in large organizations in many\ncritical business sectors with the support of spreadsheet applications. The\nprocess of elaborating spreadsheet data is often performed in a distributed,\ncollaborative way, where many actors enter data belonging to their local\nbusiness domain to contribute to a global business view. The manual fusion of\nsuch data may lead to errors in copy-paste operations, loss of alignment and\ncoherency due to multiple spreadsheet copies in circulation, as well as loss of\ndata due to broken cross-spreadsheet links. In this paper we describe a\nmethodology, based on a Spreadsheet Composition Platform, which greatly reduces\nthese risks. The proposed platform seamlessly integrates the distributed\nspreadsheet elaboration, supports the commonly known spreadsheet tools for data\nprocessing and helps organizations to adopt a more controlled and secure\nenvironment for data fusion."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6895v1", 
    "title": "Breviz: Visualizing Spreadsheets using Dataflow Diagrams", 
    "arxiv-id": "1111.6895v1", 
    "author": "Arie van Deursen", 
    "publish": "2011-11-29T16:56:43Z", 
    "summary": "Spreadsheets are used extensively in industry, often for business critical\npurposes. In previous work we have analyzed the information needs of\nspreadsheet professionals and addressed their need for support with the\ntransition of a spreadsheet to a colleague with the generation of data flow\ndiagrams. In this paper we describe the application of these data flow diagrams\nfor the purpose of understanding a spreadsheet with three example cases. We\nfurthermore suggest an additional application of the data flow diagrams: the\nassessment of the quality of the spreadsheet's design."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6902v1", 
    "title": "Requirements for Automated Assessment of Spreadsheet Maintainability", 
    "arxiv-id": "1111.6902v1", 
    "author": "Miguel A. Ferreira", 
    "publish": "2011-11-29T17:07:59Z", 
    "summary": "The use of spreadsheets is widespread. Be it in business, finance,\nengineering or other areas, spreadsheets are created for their flexibility and\nease to quickly model a problem. Very often they evolve from simple prototypes\nto implementations of crucial business logic. Spreadsheets that play a crucial\nrole in an organization will naturally have a long lifespan and will be\nmaintained and evolved by several people. Therefore, it is important not only\nto look at their reliability, i.e., how well is the intended functionality\nimplemented, but also at their maintainability, i.e., how easy it is to\ndiagnose a spreadsheet for deficiencies and modify it without degrading its\nquality. In this position paper we argue for the need to create a model to\nestimate the maintainability of a spreadsheet based on (automated) measurement.\nWe propose to do so by applying a structured methodology that has already shown\nits value in the estimation of maintainability of software products. We also\nargue for the creation of a curated, community-contributed repository of\nspreadsheets."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6907v1", 
    "title": "Towards Evaluating the Quality of a Spreadsheet: The Case of the   Analytical Spreadsheet Model", 
    "arxiv-id": "1111.6907v1", 
    "author": "Johncharles Sander", 
    "publish": "2011-11-29T17:17:48Z", 
    "summary": "We consider the challenge of creating guidelines to evaluate the quality of a\nspreadsheet model. We suggest four principles. First, state the domain-the\nspreadsheets to which the guidelines apply. Second, distinguish between the\nprocess by which a spreadsheet is constructed from the resulting spreadsheet\nartifact. Third, guidelines should be written in terms of the artifact,\nindependent of the process. Fourth, the meaning of \"quality\" must be defined.\nWe illustrate these principles with an example. We define the domain of\n\"analytical spreadsheet models\", which are used in business, finance,\nengineering, and science. We propose for discussion a framework and terminology\nfor evaluating the quality of analytical spreadsheet models. This framework\ncategorizes and generalizes the findings of previous work on the more narrow\ndomain of financial spreadsheet models. We suggest that the ultimate goal is a\nset of guidelines for an evaluator, and a checklist for a developer."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6909v1", 
    "title": "In Search of a Taxonomy for Classifying Qualitative Spreadsheet Errors", 
    "arxiv-id": "1111.6909v1", 
    "author": "Kala Chand Seal", 
    "publish": "2011-11-29T17:27:27Z", 
    "summary": "Most organizations use large and complex spreadsheets that are embedded in\ntheir mission-critical processes and are used for decision-making purposes.\nIdentification of the various types of errors that can be present in these\nspreadsheets is, therefore, an important control that organizations can use to\ngovern their spreadsheets. In this paper, we propose a taxonomy for\ncategorizing qualitative errors in spreadsheet models that offers a framework\nfor evaluating the readiness of a spreadsheet model before it is released for\nuse by others in the organization. The classification was developed based on\ntypes of qualitative errors identified in the literature and errors committed\nby end-users in developing a spreadsheet model for Panko's (1996) \"Wall\nproblem\". Closer inspection of the errors reveals four logical groupings of the\nerrors creating four categories of qualitative errors. The usability and\nlimitations of the proposed taxonomy and areas for future extension are\ndiscussed."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1111.6917v1", 
    "title": "Spreadsheet on Cloud -- Framework for Learning and Health Management   System", 
    "arxiv-id": "1111.6917v1", 
    "author": "Manu Sheel Gupta", 
    "publish": "2011-11-29T17:54:07Z", 
    "summary": "Cloud Computing has caused a paradigm shift in the world of computing.\nSeveral use case scenarios have been floating around the programming world in\nrelation to this. Applications such as Spreadsheets have the capability to use\nthe Cloud framework to create complex web based applications. In our effort to\ndo the same, we have proposed a Spreadsheet on the cloud as the framework for\nbuilding new web applications, which will be useful in various scenarios,\nspecifically a School administration system and governance scenarios, such as\nHealth and Administration. This paper is a manifestation of this work, and\ncontains some use cases and architectures which can be used to realize these\nscenarios in the most efficient manner."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.0215v1", 
    "title": "A Framework for Automated and Certified Refinement Steps", 
    "arxiv-id": "1112.0215v1", 
    "author": "Shuling Wang", 
    "publish": "2011-12-01T15:43:48Z", 
    "summary": "The refinement calculus provides a methodology for transforming an abstract\nspecification into a concrete implementation, by following a succession of\nrefinement rules. These rules have been mechanized in theorem-provers, thus\nproviding a formal and rigorous way to prove that a given program refines\nanother one. In a previous work, we have extended this mechanization for\nobject-oriented programs, where the memory is represented as a graph, and we\nhave integrated our approach within the rCOS tool, a model-driven software\ndevelopment tool providing a refinement language. Hence, for any refinement\nstep, the tool automatically generates the corresponding proof obligations and\nthe user can manually discharge them, using a provided library of refinement\nlemmas. In this work, we propose an approach to automate the search of possible\nrefinement rules from a program to another, using the rewriting tool Maude.\nEach refinement rule in Maude is associated with the corresponding lemma in\nIsabelle, thus allowing the tool to automatically generate the Isabelle proof\nwhen a refinement rule can be automatically found. The user can add a new\nrefinement rule by providing the corresponding Maude rule and Isabelle lemma."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.1661v2", 
    "title": "Identification of the Risk Related to a Process on Hospital Emergency   Service: a Case Study", 
    "arxiv-id": "1112.1661v2", 
    "author": "Jos\u00e9 Tribolet", 
    "publish": "2011-12-07T18:41:07Z", 
    "summary": "This paper, framed in a vast investigation, describes the application of\ntechniques and methodologies in Organizational Engineering connected to the\nassociated risk to the processes developed in an Emergency Service of an\nimportant Portuguese Hospital. The transactions performed in an emergency\nservice and the consequent risk identification (negative behaviour associated\nto those transactions) is done based on static and dynamic models, developed\nduring the business modelling. Any non-trivial system is better portrayed\ntrough a small number of reasonably independent models. From this point of view\nit is important to look at the systems from a \"micro\" perspective, which allows\nus to analyse the system at the transaction level. All processes have some\nassociated risk (inherent risk). Its identification will be decisive for future\nanalysis and for the consequent decision over the need, or not, to study\ninternal control mechanisms. This decision will depend on the risk level that\nthe organization considers acceptable."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.1662v2", 
    "title": "Urgency/Emergency Health Processes' Modelling: A Case Study", 
    "arxiv-id": "1112.1662v2", 
    "author": "Jos\u00e9 Tribolet", 
    "publish": "2011-12-07T18:45:44Z", 
    "summary": "The growing complexity and sophistication of the organizational information\nsystems, and hospital ones particularly, render difficult their comprehension\nand, consequently, the implementation of control mechanisms that may assure, at\nall times, the auditability of the above mentioned systems, without having to\nuse models. This paper, framed in a wider investigation, aims to describe the\napplication of techniques and methodologies, in the sphere of action of\nOrganizational Engineering, in the modelling of business processes developed in\nthe main Operating Theatre of the Coimbra's University Hospital Emergency\nService, as a support for the implementation of an information system\narchitecture, using for that purpose the CEO framework, developed and suggested\nby the Centre for Organizational Engineering (CEO), based on the UML language."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.3877v1", 
    "title": "A Classical Fuzzy Approach for Software Effort Estimation on Machine   Learning Technique", 
    "arxiv-id": "1112.3877v1", 
    "author": "S. Sridhar", 
    "publish": "2011-12-16T16:22:55Z", 
    "summary": "Software Cost Estimation with resounding reliability,productivity and\ndevelopment effort is a challenging and onerous task. This has incited the\nsoftware community to give much needed thrust and delve into extensive research\nin software effort estimation for evolving sophisticated methods. Estimation by\nanalogy is one of the expedient techniques in software effort estimation field.\nHowever, the methodology utilized for the estimation of software effort by\nanalogy is not able to handle the categorical data in an explicit and precise\nmanner. A new approach has been developed in this paper to estimate software\neffort for projects represented by categorical or numerical data using\nreasoning by analogy and fuzzy approach. The existing historical data sets,\nanalyzed with fuzzy logic, produce accurate results in comparison to the data\nset analyzed with the earlier methodologies."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.4016v1", 
    "title": "The Study and Approach of Software Re-Engineering", 
    "arxiv-id": "1112.4016v1", 
    "author": "Phuc V. Nguyen", 
    "publish": "2011-12-17T03:29:02Z", 
    "summary": "The nature of software re-engineering is to improve or transform existing\nsoftware so it can be understood, controlled and reused as new software. Needs,\nthe necessity of re-engineering software has greatly increased. The system\nsoftware has become obsolete no longer used in architecture, platform they're\nrunning, stable and consistent they support the development and support needs\nchange. Software re-engineering is vital to restore and reuse the things\ninherent in the existing software, put the cost of software maintenance to the\nlowest in the control and establish a basis for the development of software in\nthe future."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-15961-9_39", 
    "link": "http://arxiv.org/pdf/1112.4017v1", 
    "title": "ITIL frameworks to ITD Company for improving capabilities in service   management", 
    "arxiv-id": "1112.4017v1", 
    "author": "Phuc V. Nguyen", 
    "publish": "2011-12-17T03:33:50Z", 
    "summary": "IT operates in dynamic environments with the need always to change and adapt.\nThere is a need to improve performance. Many gaps were found when we conduct\nthe IT audit and we tried to seek to close gaps in capabilities. One way to the\nclose these gaps is the adoption of good practices in wide industry use. There\nare several sources for good practices including public frameworks and\nstandards such as ITIL, COBIT, CMMI, eSCM-SP, PRINCE2, ISO 9000, ISO/IEC 20000\nand ISO/IEC 27001, etc. The paper propose ITIL frameworks to ITD Company for\nimproving capabilities in service management."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1112.4049v1", 
    "title": "An Adaptive Design Methodology for Reduction of Product Development Risk", 
    "arxiv-id": "1112.4049v1", 
    "author": "Dr. Ibrahim Khan", 
    "publish": "2011-12-17T11:48:17Z", 
    "summary": "Embedded systems interaction with environment inherently complicates\nunderstanding of requirements and their correct implementation. However,\nproduct uncertainty is highest during early stages of development. Design\nverification is an essential step in the development of any system, especially\nfor Embedded System. This paper introduces a novel adaptive design methodology,\nwhich incorporates step-wise prototyping and verification. With each adaptive\nstep product-realization level is enhanced while decreasing the level of\nproduct uncertainty, thereby reducing the overall costs. The back-bone of this\nframe-work is the development of Domain Specific Operational (DOP) Model and\nthe associated Verification Instrumentation for Test and Evaluation, developed\nbased on the DOP model. Together they generate functionally valid test-sequence\nfor carrying out prototype evaluation. With the help of a case study 'Multimode\nDetection Subsystem' the application of this method is sketched. The design\nmethodologies can be compared by defining and computing a generic performance\ncriterion like Average design-cycle Risk. For the case study, by computing\nAverage design-cycle Risk, it is shown that the adaptive method reduces the\nproduct development risk for a small increase in the total design cycle time."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1112.4271v3", 
    "title": "Refactoring Composite to Visitor and Inverse Transformation in Java", 
    "arxiv-id": "1112.4271v3", 
    "author": "Julien Cohen", 
    "publish": "2011-12-19T09:14:36Z", 
    "summary": "We describe how to use refactoring tools to transform a Java program\nconforming to the Composite design pattern into a program conforming to the\nVisitor design pattern with the same external behavior. We also describe the\ninverse transformation. We use the refactoring tool provided by IntelliJ IDEA."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1112.5774v1", 
    "title": "Dynamic Composition of Evolving Process Types", 
    "arxiv-id": "1112.5774v1", 
    "author": "Christian Attiogb\u00e9", 
    "publish": "2011-12-25T10:09:58Z", 
    "summary": "Classical approaches like process algebras or labelled transition systems\ndeal with static composition to model non-trivial concurrent or distributed\nsystems; this is not sufficient for systems with dynamic architecture and with\nvariable number of components. We introduce a method to guide the modelling and\nthe dynamic composition of processes to build large distributed systems with\ndynamic adhoc architecture. The modelling and the composition are based on an\nevent-based approach that favour the decoupling of the system components. The\ncomposition uses the sharing of abstract communication channels. The method is\nappropriate to deal with evolving processes (with mobility, mutation). The\nevent-B method is used for practical support. A fauna and its evolution are\nconsidered as a working system; this system presents some specificities, its\nbehaviour is not foreseeable, it has an adhoc (not statically fixed)\narchitecture."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.0357v1", 
    "title": "Empirical study of performance of data binding in ASP.NET web   applications", 
    "arxiv-id": "1201.0357v1", 
    "author": "Ivan Velinov", 
    "publish": "2012-01-01T15:14:51Z", 
    "summary": "Most developers use default properties of ASP.NET server controls when\ndeveloping web applications. ASP.NET web applications typically employ server\ncontrols to provide dynamic web pages, and data-bound server controls to\ndisplay and maintain database data. Though the default properties allow for\nfast creation of workable applications, creating a high-performance,\nmulti-user, and scalable web application requires careful configuring of server\ncontrols and their enhancement using custom-made code. In providing commonly\nrequired functionality in data-driven ASP.NET web applications such as paging,\nsorting and filtering, our empirical study evaluated the impact of various\ntechnical approaches: automatic data binding in web server controls; data\npaging and sorting on web server; paging and sorting on database server;\nindexed and non-indexed database columns; clustered vs. non-clustered indices.\nThe study observed significant performance differences between various\ntechnical approaches."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.0595v1", 
    "title": "Formalizing Traceability and Derivability in Software Product Lines", 
    "arxiv-id": "1201.0595v1", 
    "author": "Jean-Vivien Millo", 
    "publish": "2012-01-03T09:10:13Z", 
    "summary": "In the literature, the definition of product in a Software Product Line (SPL)\nis based upon the notion of consistency of the constraints, imposed by\nvariability and traceability relations on the elements of the SPL. In this\npaper, we contend that consistency does not model the natural semantics of the\nimplementability relation between problem and solution spaces correctly.\nTherefore, we define when a feature can be {\\em derived} from a set of\ncomponents . Using this, we define a product of the SPL by a <specification,\narchitecture> pair, where all the features in the specification are derived\nfrom the components in the architecture. This notion of derivability is\nformulated in a simple yet expressive, abstract model of a productline with\ntraceability relation. We then define a set of SPL analysis problems and show\nthat these problems can be encoded as Quantified Boolean Formulas. Then, QSAT\nsolvers like QUBE can be used to solve the analysis problems. We illustrate the\nmethodology on a small fragment of a realistic productline."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.0851v1", 
    "title": "Order Handling in Convergent Environments", 
    "arxiv-id": "1201.0851v1", 
    "author": "Toni Stojanovski", 
    "publish": "2012-01-04T09:18:26Z", 
    "summary": "The rapid development of IT&T technology had big impact on the traditional\ntelecommunications market, transforming it from monopolistic market to highly\ncompetitive high-tech market where new services are required to be created\nfrequently. This paper aims to describe a design approach that puts order\nmanagement process (as part of enterprise application integration) in function\nof rapid service creation. In the text we will present a framework for\ncollaborative order handling supporting convergent services. The design splits\nthe order handling processes in convergent environments in three business\nprocess groups: order capture, order management and order fulfillment. The\npaper establishes abstract framework for order handling and provides design\nguidelines for transaction handling implementation based on the checkpoint and\ninverse command strategy. The proposed design approach is based in a convergent\ntelecommunication environment. Same principles are applicable in solving\nproblems of collaboration in function of order processing in any given\nheterogeneous environment."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.0853v1", 
    "title": "Rapid Application Development Using Software Factories", 
    "arxiv-id": "1201.0853v1", 
    "author": "Tomislav Dzekov", 
    "publish": "2012-01-04T09:23:19Z", 
    "summary": "Software development is still based on manufactory production, and most of\nthe programming code is still hand-crafted. Software development is very far\naway from the ultimate goal of industrialization in software production,\nsomething which has been achieved long time ago in the other industries. The\nlack of software industrialization creates an inability to cope with fast and\nfrequent changes in user requirements, and causes cost and time inefficiencies\nduring their implementation. Analogous to what other industries had done long\ntime ago, industrialization of software development has been proposed using the\nconcept of software factories. We have accepted this vision about software\nfactories, and developed our own software factory which produces three-layered\nASP.NET web applications. In this paper we report about our experience with\nusing this approach in the process of software development, and present\ncomparative results on performances and deliverables in both traditional\ndevelopment and development using software factories."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.1481v1", 
    "title": "A MDA approach for defining WS-Policy semantic non-functional properties", 
    "arxiv-id": "1201.1481v1", 
    "author": "Ounsa Roudies", 
    "publish": "2012-01-06T19:44:34Z", 
    "summary": "A lot of works has been especially interested to the functional aspect of Web\nservices. Nevertheless, it is necessary to describe their non-functional\nproperties such as the security characteristics and the quality of service. The\nWS-Policy standard was recommended in 2007 to describe Web services policies\nincluding the non-functional properties. However, it doesn't provide any\ninformation of their meaning necessary for automatic processes. In this paper,\nwe propose a Model Driven Architecture approach founded on W3C standards to\ngenerate WSDL language based files including semantic policies. We use a\npackage of WSDL and WS-Policy profiles and transformations rules to generate\nWeb services interfaces files including policies. We extend a XML schema\nprofile according to SAWSDL standard to define semantic non-functional\nproperties domains. This work contributes to minimize the development cost of\nWeb services including semantic policies. Moreover, the generated services can\nbe automatically processed in discovery, selection and negotiation tasks."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.1611v1", 
    "title": "Identifying Clusters of Concepts in a Low Cohesive Class for Extract   Class Refactoring Using Metrics Supplemented Agglomerative Clustering   Technique", 
    "arxiv-id": "1201.1611v1", 
    "author": "K. Narendar Reddy", 
    "publish": "2012-01-08T06:45:05Z", 
    "summary": "Object oriented software with low cohesive classes can increase maintenance\ncost. Low cohesive classes are likely to be introduced into the software during\ninitial design due to deviation from design principles and during evolution due\nto software deterioration. Low cohesive class performs operations that should\nbe done by two or more classes. The low cohesive classes need to be identified\nand refactored using extract class refactoring to improve the cohesion. In this\nregard, two aspects are involved; the first one is to identify the low cohesive\nclasses and the second one is to identify the clusters of concepts in the low\ncohesive classes for extract class refactoring. In this paper, we propose\nmetrics supplemented agglomerative clustering technique for covering the above\ntwo aspects. The proposed metrics are validated using Weyuker's properties. The\napproach is applied successfully on two examples and on a case study."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijasuc.2011.2303", 
    "link": "http://arxiv.org/pdf/1201.1967v1", 
    "title": "Investigating the Awareness of Applying the Important Web Application   Development and Measurement Practices in Small Software Firms", 
    "arxiv-id": "1201.1967v1", 
    "author": "Moath Husni", 
    "publish": "2012-01-10T05:16:11Z", 
    "summary": "This paper aims to discuss the pilot study and analysis of the current\ndevelopment and measurement practices in Jordanian small software firms. It is\nconducted because most developers build web applications without using any\nspecific development method and don't know how to integrate the suitable\nmeasurements inside the process to improve and reduce defect, time and rework\nof the development life cycle. Furthermore the objectives of this pilot study\nare firstly; determine the real characteristics of small software firms in\nJordan. Secondly, investigate the current development and measurement\npractices. Thirdly, examine the need of new development methodology for\nbuilding web application in small software firms. Consequently, Pilot survey\nwas conducted in Jordanian small software firms. Descriptive statistics\nanalysis was used to rank the development and measurements methods according to\ntheir importance. This paper presents the data, analysis and finding based on\npilot survey. These actual findings of this survey will contribute to build new\nmethodology for developing web applications in small software firms taking to\naccount how to integrate the suitable measurement program to the whole\ndevelopment process and also will provide useful information to those who are\ndoing research in the same area."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.2031v1", 
    "title": "General Methodology for developing UML models from UI", 
    "arxiv-id": "1201.2031v1", 
    "author": "K. Rajani Kanth", 
    "publish": "2012-01-10T11:52:32Z", 
    "summary": "In recent past every discipline and every industry have their own methods of\ndeveloping products. It may be software development, mechanics, construction,\npsychology and so on. These demarcations work fine as long as the requirements\nare within one discipline. However, if the project extends over several\ndisciplines, interfaces have to be created and coordinated between the methods\nof these disciplines. Performance is an important quality aspect of Web\nServices because of their distributed nature. Predicting the performance of web\nservices during early stages of software development is significant. In\nIndustry, Prototype of these applications is developed during analysis phase of\nSoftware Development Life Cycle (SDLC). However, Performance models are\ngenerated from UML models. Methodologies for predicting the performance from\nUML models is available. Hence, In this paper, a methodology for developing Use\nCase model and Activity model from User Interface is presented. The methodology\nis illustrated with a case study on Amazon.com."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.2823v1", 
    "title": "Event Space Theory and Its Application", 
    "arxiv-id": "1201.2823v1", 
    "author": "Wang Weitao", 
    "publish": "2012-01-13T12:34:41Z", 
    "summary": "In this paper, the basic ideal of the Event Space Theory and Analyzing Events\nare expatiated on. Then it is suggested that how to set up event base library\nin developing application software. Based above the designing principle of\nfacing methodology. Finally, in order to explain how to apply the Event Space\nTheory in developing economic evaluation software, the software of \"sewage\ntreatment CAD\" in a national \"8th-Five Year Plan Research Project\" of PRC is\nused as an example. This software concerns economic effectiveness evaluation\nfor construction projects."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.3078v1", 
    "title": "Empirical Confirmation (and Refutation) of Presumptions on Software", 
    "arxiv-id": "1201.3078v1", 
    "author": "Dany Moshkovich", 
    "publish": "2012-01-15T10:18:01Z", 
    "summary": "Code metrics are easy to define, but not so easy to justify. It is hard to\nprove that a metric is valid, i.e., that measured numerical values imply\nanything on the vaguely defined, yet crucial software properties such as\ncomplexity and maintainability. This paper employs statistical analysis and\ntests to check some \"believable\" presumptions on the behavior of software and\nmetrics measured for this software. Among those are the reliability presumption\nimplicit in the application of any code metric, and the presumption that the\nmagnitude of change in a software artifact is correlated with changes to its\nversion number.\n  Putting a suite of 36 metrics to the trial, we confirm most of the\npresumptions. Unexpectedly, we show that a substantial portion of the\nreliability of some metrics can be observed even in random changes to\narchitecture. Another surprising result is that Boolean-valued metrics tend to\nflip their values more often in minor software version increments than in major\nincrements."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.3416v1", 
    "title": "Verifying Real-time Commit Protocols Using Dense-time Model Checking   Technology", 
    "arxiv-id": "1201.3416v1", 
    "author": "Terry Woodings", 
    "publish": "2012-01-17T03:31:50Z", 
    "summary": "The timed-based automata model, introduced by Alur and Dill, provides a\nuseful formalism for describing real-time systems. Over the last two decades,\nseveral dense-time model checking tools have been developed based on that\nmodel. The paper considers the verification of real-time distributed commit\nprotocols using dense-time model checking technology. More precisely, we model\nand verify the well-known timed two phase commit protocol in three different\nstate-of-the-art real-time model checkers: UPPAAL, Rabbit, and RED, and compare\nthe results."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.3929v1", 
    "title": "About Instruction Sequence Testing", 
    "arxiv-id": "1201.3929v1", 
    "author": "J. A. Bergstra", 
    "publish": "2012-01-18T21:06:18Z", 
    "summary": "Software testing is presented as a so-called theme within which different\nauthors and groups have defined different subjects each of these subjects\nhaving a different focus on testing. A uniform concept of software testing is\nnon-existent and the space of possible coherent perspectives on software\ntesting, each fitting within the theme, is viewed as being spanned by five\ndimensions, each dimension representing two opposite views with a variety of\nintermediate views in between.\n  Instruction sequences are used as a simple theoretical conceptualization of\ncomputer programs. A theory of instruction sequence testing may serve as a\nmodel for a theory of software testing. Instruction sequences testing is\nconsidered a new topic for which definitions may be freely contemplated without\nbeing restricted by existing views on software testing.\n  The problem of developing a theory of instruction sequence testing is posed.\nA survey is given of motivations and scenarios for developing a theory of\ninstruction sequence testing."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.3985v1", 
    "title": "Fault Localization for Java Programs using Probabilistic Program   Dependence Graph", 
    "arxiv-id": "1201.3985v1", 
    "author": "B. Giri Babu", 
    "publish": "2012-01-19T07:28:31Z", 
    "summary": "Fault localization is a process to find the location of faults. It determines\nthe root cause of the failure. It identifies the causes of abnormal behaviour\nof a faulty program. It identifies exactly where the bugs are. Existing fault\nlocalization techniques are Slice based technique, Program- Spectrum based\nTechnique, Statistics Based Technique, Program State Based Technique, Machine\nlearning based Technique and Similarity Based Technique. In the proposed method\nModel Based Fault Localization Technique is used, which is called Probabilistic\nProgram Dependence Graph . Probabilistic Program Dependence Graph (PPDG) is an\ninnovative model that scans the internal behaviour of the project. PPDG\nconstruction is enhanced by Program Dependence Graph (PDG). PDG is achieved by\nthe Control Flow Graph (CFG). The PPDG construction augments the structural\ndependences represented by a program dependence graph with estimates of\nstatistical dependences between node states, which are computed from the test\nset. The PPDG is based on the established framework of probabilistic graphical\nmodels. This work presents algorithms for constructing PPDGs and applying fault\nlocalization."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.4142v1", 
    "title": "Identifying Coordination Problems in Software Development: Finding   Mismatches between Software and Project Team Structures", 
    "arxiv-id": "1201.4142v1", 
    "author": "Kuldeep Kumar", 
    "publish": "2012-01-19T18:46:44Z", 
    "summary": "Today's dynamic and iterative development environment brings significant\nchallenges for software project management. In distributed project settings,\n\"management by walking around\" is no longer an option and project managers may\nmiss out on key project insights. The TESNA (TEchnical Social Network Analysis)\nmethod and tool aims to provide project managers both a method and a tool for\ngaining insights and taking corrective action. TESNA achieves this by analysing\na project's evolving social and technical network structures using data from\nmultiple sources, including CVS, email and chat repositories. Using pattern\ntheory, TESNA helps to identify areas where the current state of the project's\nsocial and technical networks conflicts with what patterns suggest. We refer to\nsuch a conflict as a Socio-Technical Structure Clash (STSC). In this paper we\nreport on our experience of using TESNA to identify STSCs in a corporate\nenvironment through the mining of software repositories. We find multiple\ninstances of three STSCs (Conway's Law, Code Ownership and Project\nCoordination) in many of the on-going development projects, thereby validating\nthe method and tool that we have developed."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2011.2401", 
    "link": "http://arxiv.org/pdf/1201.4500v1", 
    "title": "Requirements and the baseline plan", 
    "arxiv-id": "1201.4500v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2012-01-21T19:27:18Z", 
    "summary": "For each software project a plan is developed, according to a documented\nprocedure, that covers the software activities and commitments. The\nrequirements allocated to software form the basis for the software development\nplan. Estimates for critical computer resources are documented, reviewed, and\nagreed to. All affected groups and individuals understand the estimates and\nplans and commit to support them. Senior management reviews the estimates and\nplans before external commitments are made. Software risks associated with the\ncost, resources, schedule, and technical aspects of the project are identified\nand evaluated, and contingencies are documented. Planning and estimation data\nare collected for use in planning subsequent projects and for input in\nmanagement oversight review meetings."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1201.5230v1", 
    "title": "Invertible Program Restructurings for Continuing Modular Maintenance", 
    "arxiv-id": "1201.5230v1", 
    "author": "Akram Ajouli", 
    "publish": "2012-01-25T10:29:28Z", 
    "summary": "When one chooses a main axis of structural decompostion for a software, such\nas function- or data-oriented decompositions, the other axes become secondary,\nwhich can be harmful when one of these secondary axes becomes of main\nimportance. This is called the tyranny of the dominant decomposition. In the\ncontext of modular extension, this problem is known as the Expression Problem\nand has found many solutions, but few solutions have been proposed in a larger\ncontext of modular maintenance. We solve the tyranny of the dominant\ndecomposition in maintenance with invertible program transformations. We\nillustrate this on the typical Expression Problem example. We also report our\nexperiments with Java and Haskell programs and discuss the open problems with\nour approach."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1201.5735v1", 
    "title": "Deconcentration of Attention: Addressing the Complexity of Software   Engineering", 
    "arxiv-id": "1201.5735v1", 
    "author": "Igor Kusakov", 
    "publish": "2012-01-27T10:11:23Z", 
    "summary": "This article attempts to describe specific mental techniques that are related\nto resolving very complex tasks in software engineering. This subject may be\nfamiliar to some software specialists to different extents; however, there is\ncurrently no common consensus and popular terminology for this subject area. In\nthis article, the area is charted from a practical usability perspective.\n  This article also proposes to treat software engineering itself as research\non human thinking because software is meant to simulate thinking."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1201.6033v1", 
    "title": "Compact Symbolic Execution (technical report)", 
    "arxiv-id": "1201.6033v1", 
    "author": "Marek Trt\u00edk", 
    "publish": "2012-01-29T12:51:06Z", 
    "summary": "We present a generalisation of King's symbolic execution technique called\ncompact symbolic execution. It is based on a concept of templates: a template\nis a declarative parametric description of such a program part, generating\npaths in symbolic execution tree with regularities in program states along\nthem. Typical sources of these paths are program loops and recursive calls.\nUsing the templates we fold the corresponding paths into single vertices and\ntherefore considerably reduce size of the tree without loss of any information.\nThere are even programs for which compact symbolic execution trees are finite\neven though the classic symbolic execution trees are infinite."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1201.6078v2", 
    "title": "@tComment: Testing Javadoc Comments to Detect Comment-Code   Inconsistencies", 
    "arxiv-id": "1201.6078v2", 
    "author": "Gary T. Leavens", 
    "publish": "2012-01-29T20:40:52Z", 
    "summary": "This paper has been withdrawn by the author."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1201.6141v2", 
    "title": "Four Layered Approach to Non-Functional Requirements Analysis", 
    "arxiv-id": "1201.6141v2", 
    "author": "A. Ananda Rao", 
    "publish": "2012-01-30T09:39:39Z", 
    "summary": "Identification of non-functional requirements is important for successful\ndevelopment and deployment of the software product. The acceptance of the\nsoftware product by the customer depends on the non-functional requirements\nwhich are incorporated in the software. For this, we need to identify all the\nnon-functional requirements required by all stakeholders. In the literature not\nmany approaches are available for this purpose. Hence, we have proposed a four\nlayered analysis approach for identification of non-functional requirements.\nThe proposed layered approach has many advantages over non-layered approach. As\npart of this approach some rules are also proposed to be used in each layer.\nThe approach is applied successfully on two case studies. The identified\nnon-functional requirements are validated using a check list and in addition\nthe completeness of the identified non-requirements is computed using a metric."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.0652v1", 
    "title": "Agile Research", 
    "arxiv-id": "1202.0652v1", 
    "author": "Hamish Cunningham", 
    "publish": "2012-02-03T10:31:15Z", 
    "summary": "This paper discusses the application of agile software development methods in\nsoftware-based research environments."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.0723v2", 
    "title": "Assessment of OGC Web Processing Services for REST principles", 
    "arxiv-id": "1202.0723v2", 
    "author": "Joaqu\u00edn Huerta", 
    "publish": "2012-02-02T15:14:17Z", 
    "summary": "Recent distributed computing trends advocate the use of Representational\nState Transfer (REST) to alleviate the inherent complexity of the Web services\nstandards in building service-oriented web applications. In this paper we focus\non the particular case of geospatial services interfaced by the OGC Web\nProcessing Service (WPS) specification in order to assess whether WPS-based\ngeospatial services can be viewed from the architectural principles exposed in\nREST. Our concluding remarks suggest that the adoption of REST principles, to\nspecially harness the built-in mechanisms of the HTTP application protocol, may\nbe beneficial in scenarios where ad hoc composition of geoprocessing services\nare required, common for most non-expert users of geospatial information\ninfrastructures."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.0788v1", 
    "title": "STANSE: Bug-finding Framework for C Programs", 
    "arxiv-id": "1202.0788v1", 
    "author": "Marek Trt\u00edk", 
    "publish": "2012-02-03T17:47:27Z", 
    "summary": "STANSE is a free (available under the GPLv2 license) modular framework for\nfinding bugs in C programs using static analysis. Its two main design goals are\n1) ability to process large software projects like the Linux kernel and 2)\nextensibility with new bug-finding techniques with a minimal effort. Currently\nthere are four bug-finding algorithms implemented within STANSE:\nAutomatonChecker checks properties described in an automata-based formalism,\nThreadChecker detects deadlocks among multiple threads, LockChecker finds\nlocking errors based on statistics, and ReachabilityChecker looks for\nunreachable code. STANSE has been tested on the Linux kernel, where it has\nfound dozens of previously undiscovered bugs."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.1717v1", 
    "title": "Collaboration for enhancing the system development process in open   source diligence", 
    "arxiv-id": "1202.1717v1", 
    "author": "Murtaza Hussain Shaikh", 
    "publish": "2012-02-08T14:46:43Z", 
    "summary": "According to different opponents and commercial giants in software\nindustries, the open source style software development has enough capacity to\ncomplete successfully the large scale projects. But we have seen many flaws and\nloops in collaboration and handling of mega scale projects in open source\nenvironment. Perhaps the collaboration is a key of successful project\ndevelopment. In this article we have tries to identify different feasible and\nreliable solution to a better collaboration ways in the open source system\ndevelopment. Some of the issues also that are found in the development phase of\nthe open source have been identified and a proposed solution by explaining\nSuccessful communities such as GNU, the Apache Software Foundation, and Eclipse\nFoundation is discusses in this research article. It must be kept in mind that\nto improvement the collaboration in open source environment both the\ndevelopment community and the people should be more creative."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.1718v1", 
    "title": "A transformation approach for collaboration based requirement models", 
    "arxiv-id": "1202.1718v1", 
    "author": "Aicha Mokhtari", 
    "publish": "2012-02-08T14:48:18Z", 
    "summary": "Distributed software engineering is widely recognized as a complex task.\nAmong the inherent complexities is the process of obtaining a system design\nfrom its global requirement specification. This paper deals with such\ntransformation process and suggests an approach to derive the behavior of a\ngiven system components, in the form of distributed Finite State Machines, from\nthe global system requirements, in the form of an augmented UML Activity\nDiagrams notation. The process of the suggested approach is summarized in three\nsteps: the definition of the appropriate source Meta-Model (requirements\nMeta-Model), the definition of the target Design Meta-Model and the definition\nof the rules to govern the transformation during the derivation process. The\nderivation process transforms the global system requirements described as UML\ndiagram activities (extended with collaborations) to system roles behaviors\nrepresented as UML finite state machines. The approach is implemented using\nAtlas Transformation Language (ATL)."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.1953v1", 
    "title": "Arduino Tool: For Interactive Artwork Installations", 
    "arxiv-id": "1202.1953v1", 
    "author": "Murtaza Hussain Shaikh", 
    "publish": "2012-02-09T11:12:09Z", 
    "summary": "The emergence of the digital media and computational tools has widened the\ndoors for creativity. The cutting edge in the digital arts and role of new\ntechnologies can be explored for the possible creativity. This gives an\nopportunity to involve arts with technologies to make creative works. The\ninteractive artworks are often installed in the places where multiple people\ncan interact with the installation, which allows the art to achieve its purpose\nby allowing the people to observe and involve with the installation. The level\nof engagement of the audience depends on the various factors such as aesthetic\nsatisfaction, how the audience constructs meaning, pleasure and enjoyment. The\nmethod to evaluate these experiences is challenging as it depends on\nintegration between the artificial life and real life by means of human\ncomputer interaction. This research investigates \"How Adriano fits for creative\nand interactive artwork installations?\" using an artwork installation in the\ncampus of NTNU (Norwegian University of Science & Technology). The main focus\nof this investigation has been to get an overview on the intersection between\ninformation technology and Arts. This gives an opportunity to understand\nvarious attributes like creativity, cooperation and openness of processes\ninfluencing the creative Artworks. The artwork is combination of Adriano and\nother auxiliary components such as sensors, LED's and speakers."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSMR.2012.42", 
    "link": "http://arxiv.org/pdf/1202.2425v1", 
    "title": "The artifacts of component-based development", 
    "arxiv-id": "1202.2425v1", 
    "author": "Shaukat Ali Hayat", 
    "publish": "2012-02-11T09:58:44Z", 
    "summary": "Component based development idea was floated in a conference name \"Mass\nProduced Software Components\" in 1968 [1]. Since then engineering and\nscientific libraries are developed to reuse the previously developed functions.\nThis concept is now widely used in SW development as component based\ndevelopment (CBD). Component-based software engineering (CBSE) is used to\ndevelop/ assemble software from existing components [2]. Software developed\nusing components is called component ware [3]. This paper presents different\narchitectures of CBD such as ActiveX, common object request broker architecture\n(CORBA), remote method invocation (RMI) and simple object access protocol\n(SOAP). The overall objective of this paper is to support the practice of CBD\nby comparing its advantages and disadvantages. This paper also evaluates object\noriented process model to adapt it for CBD."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3104", 
    "link": "http://arxiv.org/pdf/1202.2427v1", 
    "title": "Autonomic html interface generator for web applications", 
    "arxiv-id": "1202.2427v1", 
    "author": "Mohammad Alwani", 
    "publish": "2012-02-11T10:37:13Z", 
    "summary": "Recent advances in computing systems have led to a new digital era in which\nevery area of life is nearly interrelated with information technology. However,\nwith the trend towards large-scale IT systems, a new challenge has emerged. The\ncomplexity of IT systems is becoming an obstacle that hampers the\nmanageability, operability, and maintainability of modern computing\ninfrastructures. Autonomic computing popped up to provide an answer to these\never-growing pitfalls. Fundamentally, autonomic systems are self-configuring,\nself-healing, self-optimizing, and self-protecting; hence, they can automate\nall complex IT processes without human intervention. This paper proposes an\nautonomic HTML web-interface generator based on XML Schema and Style Sheet\nspecifications for self-configuring graphical user interfaces of web\napplications. The goal of this autonomic generator is to automate the process\nof customizing GUI web-interfaces according to the ever-changing business\nrules, policies, and operating environment with the least IT labor involvement.\nThe conducted experiments showed a successful automation of web interfaces\ncustomization that dynamically self-adapts to keep with the always-changing\nbusiness requirements. Future research can improve upon the proposed solution\nso that it supports the selfconfiguring of not only web applications but also\ndesktop applications."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.2429v1", 
    "title": "Building sustainable ecosystem-oriented architectures", 
    "arxiv-id": "1202.2429v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-02-11T10:43:58Z", 
    "summary": "Currently, organizations are transforming their business processes into\ne-services and service-oriented architectures to improve coordination across\nsales, marketing, and partner channels, to build flexible and scalable systems,\nand to reduce integration-related maintenance and development costs. However,\nthis new paradigm is still fragile and lacks many features crucial for building\nsustainable and progressive computing infrastructures able to rapidly respond\nand adapt to the always-changing market and environmental business. This paper\nproposes a novel framework for building sustainable Ecosystem- Oriented\nArchitectures (EOA) using e-service models. The backbone of this framework is\nan ecosystem layer comprising several computing units whose aim is to deliver\nuniversal interoperability, transparent communication, automated management,\nself-integration, self-adaptation, and security to all the interconnected\nservices, components, and devices in the ecosystem. Overall, the proposed model\nseeks to deliver a comprehensive and a generic sustainable business IT model\nfor developing agile e-enterprises that are constantly up to new business\nconstraints, trends, and requirements. Future research can improve upon the\nproposed model so much so that it supports computational intelligence to help\nin decision making and problem solving."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.2498v1", 
    "title": "Survey-Based Analysis of the Proposed Component-Based Development   Process", 
    "arxiv-id": "1202.2498v1", 
    "author": "M. E. Sandhu", 
    "publish": "2012-02-12T08:38:28Z", 
    "summary": "The concept of component-based development (CBD) is widely practiced in\nsoftware (SW) development. CBD is based on reuse of the existing components\nwith the new ones. The objective of this paper is to propose a novel process\nmodel for CBD. Importance of repository has also been discussed. A survey has\nbeen conducted to evaluate the proposed model. The results of the survey show\nthat proposed process model can be efficiently implemented for CBD projects."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.2499v1", 
    "title": "Evaluation of the Improved XP Software Development Model", 
    "arxiv-id": "1202.2499v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2012-02-12T08:55:07Z", 
    "summary": "The concept of agile process models has attained great popularity in software\n(SW) development community in last few years. Agile models promote fast\ndevelopment. Fast development has certain drawbacks, such as weak documentation\nand performance for medium and large development projects. Fast development\nalso promotes use of agile process models in small-scale projects. This paper\nmodifies and evaluates Extreme Programming (XP) process model and proposes a\nnovel process model based on these modifications."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.2501v1", 
    "title": "An Improved XP Software Development Model", 
    "arxiv-id": "1202.2501v1", 
    "author": "S. A. Hussain", 
    "publish": "2012-02-12T08:59:21Z", 
    "summary": "The concept of agile process models has attained great popularity in software\n(SW) development community in last few years. Agile models promote fast\ndevelopment. Fast development has certain drawbacks, such as weak documentation\nand performance for medium and large development projects. Fast development\nalso promotes use of agile process models in small-scale projects. This paper\nmodifies and evaluates Extreme Programming (XP) process model and proposes a\nnovel process model based on these modifications."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2504v1", 
    "title": "Seamless Long Term Learning in Agile Teams for Sustainable Leadership", 
    "arxiv-id": "1202.2504v1", 
    "author": "Muhammad Kashif", 
    "publish": "2012-02-12T09:11:04Z", 
    "summary": "Seamless and continuous support for long term organizational learning needs\nis essential for long lasting progress of the organization. Agile process model\nprovides an excellent opportunity to cater that specific problem and also helps\nin motivation, satisfaction, coordination, presentation and technical skills\nenhancement of agile teams. This long term learning process makes organization\nto sustain their current successes and lead both organization and team members\nto successful and dynamic market leaders."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2506v1", 
    "title": "Improvement of Key Problems of Software Testing in Quality Assurance", 
    "arxiv-id": "1202.2506v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2012-02-12T09:17:23Z", 
    "summary": "Quality assurance makes sure the project will be completed based on the\npreviously approved specifications, standards and functionality. It is required\nwithout defects and possible problems. It monitors and tries to progress the\ndevelopment process from the start of the project. Software Quality Assurance\n(SQA) is the combination of the entire software development process, which\nincludes software design, coding, source code control, code review, change\nmanagement, configuration management and release management. In this paper we\ndescribe the solution for the key problems of software testing in quality\nassurance. The existing software practices have some problems such as testing\npractices, attitude of users and culture of organizations. All these tree\nproblems have some combined problems such as shortcuts in testing, reduction in\ntesting time, poor documentation etc. In this paper we are recommending\nstrategies to provide solution of the said problems mentioned above."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2508v1", 
    "title": "A Validation of the Proposed Component-Based Development Process", 
    "arxiv-id": "1202.2508v1", 
    "author": "M. E. Sandhu", 
    "publish": "2012-02-12T09:22:54Z", 
    "summary": "Component-based development (CBD) is a name, with which software development\nprofessionals are quite familiar. There are several models which have been\nproposed for CBD in last few years. They contain good features but there are\nsome improvement possibilities in them. The objective of this paper is to\npropose a process for CBD and to evaluate the effects of quality parameters on\nreusability. The validations of the proposed CBD model provide positive\nindication for software (SW) industry that it can be successfully implemented\nfor CBD projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2510v1", 
    "title": "A New Teaching Model For The Subject Of Software Project Management", 
    "arxiv-id": "1202.2510v1", 
    "author": "Asif Mehmood", 
    "publish": "2012-02-12T09:27:19Z", 
    "summary": "Software (SW) development is a very tough task which requires a skilled\nproject leader for its success. If the project leader is not skilled enough\nthen project may fail. In the real world of SW engineering 65% of the SW\nprojects fail to meet their objectives as in [1]. The main reason is lack of\ntraining of the project mangers. This extreme ratio of failure can be reduced\nby teaching SW project management (SPM) to the future project managers in the\npractical manner, so that they may be skillful enough to handle the project in\na better way. This paper intends to propose a model to be used to teach SPM to\nthe student of SW engineering to reduce the failure rate of projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2511v1", 
    "title": "A Step Forward To Component-based Software Cost Estimation in   Object-oriented Environment", 
    "arxiv-id": "1202.2511v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2012-02-12T09:34:14Z", 
    "summary": "Software cost estimation (SCE) of a project is pivotal to the acceptance or\nrejection of the development of software project. Various SCE techniques have\nbeen in practice with their own strengths and limitations. The latest of these\nis object-oriented one. Currently object-oriented approach for SCE is based on\nLine of Code (LOC), function points, functions and classes etc. Relatively less\nattention has been paid to the SCE in component-based software engineering\n(CBSE). So there is a pressing need to search parameters/variables that have a\nvital role for the SCE using CBSE which is taken up in this paper. This paper\nfurther looks at level of significance of all the parameters/variables thus\nsearched. The time is being used as an independent variable because time is a\nparameter which is almost, all previous in one. Therefore this approach may be\nin a way an alternate of all previous approaches. Infact the underlying\nresearch ultimately may lead towards SCE of complex systems, using CBSE, in a\nscientific, systematic and comprehensive way."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2513v1", 
    "title": "Empirical Evaluation of the Proposed eXScrum Model: Results of a Case   Study", 
    "arxiv-id": "1202.2513v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2012-02-12T10:24:43Z", 
    "summary": "Agile models promote fast development. XP and Scrum are the most widely used\nagile models. This paper investigates the phases of XP and Scrum models in\norder to identify their potentials and drawbacks. XP model has certain\ndrawbacks, such as not suitable for maintenance projects and poor performance\nfor medium and large-scale development projects. Scrum model has certain\nlimitations, such as lacked in engineering practices. Since, both XP and Scrum\nmodels contain good features and strengths but still there are improvement\npossibilities in these models. Majority of the software development companies\nare reluctant to switch from traditional methodologies to agile methodologies\nfor development of industrial projects. A fine integration, of software\nmanagement of the Scrum model and engineering practices of XP model, is very\nmuch required to accumulate the strengths and remove the limitations of both\nmodels. This is achieved by proposing an eXScrum model. The proposed model is\nvalidated by conducting a controlled case study. The results of case study show\nthat the proposed integrated eXScrum model enriches the potentials of both XP\nand Scrum models and eliminates their drawbacks."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2514v1", 
    "title": "A Comprehensive Study of Commonly Practiced Heavy & Light Weight   Software Methodologies", 
    "arxiv-id": "1202.2514v1", 
    "author": "Usman Ali Khan", 
    "publish": "2012-02-12T10:29:07Z", 
    "summary": "Software has been playing a key role in the development of modern society.\nSoftware industry has an option to choose suitable methodology/process model\nfor its current needs to provide solutions to give problems. Though some\ncompanies have their own customized methodology for developing their software\nbut majority agrees that software methodologies fall under two categories that\nare heavyweight and lightweight. Heavyweight methodologies (Waterfall Model,\nSpiral Model) are also known as the traditional methodologies, and their\nfocuses are detailed documentation, inclusive planning, and extroverted design.\nLightweight methodologies (XP, SCRUM) are, referred as agile methodologies.\nLight weight methodologies focused mainly on short iterative cycles, and rely\non the knowledge within a team. The aim of this paper is to describe the\ncharacteristics of popular heavyweight and lightweight methodologies that are\nwidely practiced in software industries. We have discussed the strengths and\nweakness of the selected models. Further we have discussed the strengths and\nweakness between the two opponent methodologies and some criteria is also\nillustrated that help project managers for the selection of suitable model for\ntheir projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICET.2009.5353140", 
    "link": "http://arxiv.org/pdf/1202.2515v1", 
    "title": "A Framework for Next Generation Mobile and Wireless Networks Application   Development using Hybrid Component Based Development Model", 
    "arxiv-id": "1202.2515v1", 
    "author": "Asif Irshad Khan", 
    "publish": "2012-02-12T10:38:47Z", 
    "summary": "The IP Multimedia Subsystems (IMS) that features in Next Generation Networks\n(NGN) offers the application developer (third party) abilities to map out\napplications over mobile telecommunication infrastructure. The IMS comes about\nwith APIs useful for mobile application developers to create applications to\nmeet end-users' demands and comply with the provider's infrastructure set up at\nthe same time. Session Initiation Protocol (SIP) is a signaling protocol for\nthis architecture. It is used for establishing sessions in IP network, making\nit an ideal candidate for supporting terminal mobility in to deliver the\nservices with improved Quality of Services (QOS). The realization of IMS's\nvirtues as far as software design is concerned is faced by lack of\nstandardizations and methodologies throughout application development process.\nIn this paper, we report on progress on ongoing research by our group toward\nputting together a platform as a testbed used for NGN application development.\nWe examine a novel component based development model used for SIP based mobile\napplications. The developed model is to be used as framework for general\npurpose application development over the testbed. We apply this model on MObile\nMass EXamination (MOMEX) system that is an application attracting the interest\nof educational authorities around the world due to its potential convenience."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.2516v1", 
    "title": "Novel Component Based Development Model For Sip-Based Mobile Application", 
    "arxiv-id": "1202.2516v1", 
    "author": "Asif Irshad Khan", 
    "publish": "2012-02-12T10:47:54Z", 
    "summary": "Universities and Institutions these days' deals with issues related to with\nassessment of large number of students. Various evaluation methods have been\nadopted by examiners in different institutions to examining the ability of an\nindividual, starting from manual means of using paper and pencil to electronic,\nfrom oral to written, practical to theoretical and many others. There is a need\nto expedite the process of examination in order to meet the increasing\nenrolment of students at the universities and institutes. Sip Based Mass Mobile\nExamination System (SiBMMES) expedites the examination process by automating\nvarious activities in an examination such as exam paper setting, Scheduling and\nallocating examination time and evaluation (auto-grading for objective\nquestions) etc. SiBMMES uses the IP Multimedia Subsystem (IMS) that is an IP\ncommunications framework providing an environment for the rapid development of\ninnovative and reusable services Session Initial Protocol (SIP) is a signalling\n(request-response) protocol for this architecture and it is used for\nestablishing sessions in an IP network, making it an ideal candidate for\nsupporting terminal mobility in the IMS to deliver the services, with the\nextended services available in IMS like open APIs, common network services,\nQuality of Services (QoS) like multiple sessions per call, Push to Talk etc\noften requiring multiple types of media (including voice, video, pictures, and\ntext). SiBMMES is an effective solution for mass education evaluation using\nmobile and web technology. In this paper, a novel hybrid component based\ndevelopment (CBD) model is proposed for SiBMMES."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.2572v2", 
    "title": "A Formal Approach for the Development of Service-Oriented Applications", 
    "arxiv-id": "1202.2572v2", 
    "author": "Ciprian Dobre", 
    "publish": "2012-02-12T21:06:31Z", 
    "summary": "Please cite this as \"Lorina Negreanu, Cristian Giumale, Alexandru Agache,\nMihnea Muraru, Matei Popovici, Ciprian Dobre, A Formal Approach for the\nDevelopment of Service-Oriented Applications, in Proc. of 18th International\nConference on Control Systems and Computer Science (CSCS-18), Bucharest,\nRomania, 2011, pp. 804-810, ISSN: 2066-4451, Politehnica Press\""
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.2731v1", 
    "title": "Risk Assessment Techniques and Survey Method for COTS Components", 
    "arxiv-id": "1202.2731v1", 
    "author": "Shalini Raghav", 
    "publish": "2012-02-13T13:55:15Z", 
    "summary": "The Rational Unified Process a software engineering process is gaining\npopularity nowadays. RUP delivers best software practices for component\nsoftware Development life cycle It supports component based software\ndevelopment. Risk is involved in every component development phase .neglecting\nthose risks sometimes hampers the software growth and leads to negative\noutcome. In Order to provide appropriate security and protection levels,\nidentifying various risks is very vital. Therefore Risk identification plays a\nvery crucial role in the component based software development This report\naddresses incorporation of component based software development cycle into RUP\nphases, assess several category of risk encountered in the component based\nsoftware. It also entails a survey method to identify the risk factor and\nevaluating the overall severity of the component software development in terms\nof the risk. Formula for determining risk prevention cost and finding the risk\nprobability is also been included. The overall goal of the paper is to provide\na theoretical foundation that facilitates a good understanding of risk in\nrelation to componentbased system development"
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.3466v1", 
    "title": "Novel Component-Based Development Model for SIP-Based Mobile Application   (1202)", 
    "arxiv-id": "1202.3466v1", 
    "author": "Asif Irshad Khan", 
    "publish": "2012-02-15T22:28:55Z", 
    "summary": "Universities and Institutions these days' deals with issues related to with\nassessment of large number of students. Various evaluation methods have been\nadopted by examiners in different institutions to examining the ability of an\nindividual, starting from manual means of using paper and pencil to electronic,\nfrom oral to written, practical to theoretical and many others. There is a need\nto expedite the process of examination in order to meet the increasing\nenrolment of students at the universities and institutes. Sip Based Mass Mobile\nExamination System (SiBMMES) expedites the examination process by automating\nvarious activities in an examination such as exam paper setting, Scheduling and\nallocating examination time and evaluation (auto-grading for objective\nquestions) etc. SiBMMES uses the IP Multimedia Subsystem (IMS) that is an IP\ncommunications framework providing an environment for the rapid development of\ninnovative and reusable services Session Initial Protocol (SIP) is a signalling\n(request-response) protocol for this architecture and it is used for\nestablishing sessions in an IP network, making it an ideal candidate for\nsupporting terminal mobility in the IMS to deliver the services, with the\nextended services available in IMS like open APIs, common network services,\nQuality of Services (QoS) like multiple sessions per call, Push to Talk etc\noften requiring multiple types of media (including voice, video, pictures, and\ntext). SiBMMES is an effective solution for mass education evaluation using\nmobile and web technology. In this paper, a novel hybrid component based\ndevelopment (CBD) model is proposed for SiBMMES. A Component based Hybrid Model\nis selected to the fact that IMS takes the concept of layered architecture one\nstep further by defining a horizontal architecture where service enablers and\ncommon functions can be reused for multiple applications."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.4099v1", 
    "title": "Web Services-Enhanced Agile Modeling and Integrating Business Processes", 
    "arxiv-id": "1202.4099v1", 
    "author": "Ounsa Roudi\u00e8s", 
    "publish": "2012-02-18T19:53:05Z", 
    "summary": "In a global business context with continuous changes, the enterprises have to\nenhance their operational efficiency, to react more quickly, to ensure the\nflexibility of their business processes, and to build new collaboration\npathways with external partners. To achieve this goal, they must use e-business\nmethods, mechanisms and techniques while capitalizing on the potential of new\ninformation and communication technologies. In this context, we propose a\nstandards, model and Web services-based approach for modeling and integrating\nagile enterprise business processes. The purpose is to benefit from Web\nservices characteristics to enhance the processes design and realize their\ndynamic integration. The choice of focusing on Web services is essentially\njustified by their broad adoption by enterprises as well as their capability to\nwarranty interoperability between both intra and inter-enterprises systems.\nThereby, we propose in this chapter a metamodel for describing business\nprocesses, and discuss their dynamic integration by addressing the Web services\ndiscovery issue. On the one hand, the proposed metamodel is in line with the\nW3C Web services standards, namely, WSDL, SAWSDL and WS-Policy. It considers\nthe use of BPMN standard to describe the behavioral aspect of business\nprocesses and completes their design using UML diagrams describing their\nfunctional, non-functional and semantic aspects. On other hand, our approach\nfor integrating processes is in line with BPEL standard recommended to\norchestrate Web services. To realize executable business processes, this\napproach recommends the use of semantic matching and selection mechanisms in\norder to produce agile systems."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.4527v1", 
    "title": "Study Paper on Test Case generation for GUI Based Testing", 
    "arxiv-id": "1202.4527v1", 
    "author": "Emi Retna", 
    "publish": "2012-02-21T05:12:13Z", 
    "summary": "With the advent of WWW and outburst in technology and software development,\ntesting the software became a major concern. Due to the importance of the\ntesting phase in a software development life cycle, testing has been divided\ninto graphical user interface (GUI) based testing, logical testing, integration\ntesting, etc.GUI Testing has become very important as it provides more\nsophisticated way to interact with the software. The complexity of testing GUI\nincreased over time. The testing needs to be performed in a way that it\nprovides effectiveness, efficiency, increased fault detection rate and good\npath coverage. To cover all use cases and to provide testing for all possible\n(success/failure) scenarios the length of the test sequence is considered\nimportant. Intent of this paper is to study some techniques used for test case\ngeneration and process for various GUI based software applications."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.4836v1", 
    "title": "Fault Based Techniques for Testing Boolean Expressions: A Survey", 
    "arxiv-id": "1202.4836v1", 
    "author": "S. Taruna", 
    "publish": "2012-02-22T06:43:05Z", 
    "summary": "Boolean expressions are major focus of specifications and they are very much\nprone to introduction of faults, this survey presents various fault based\ntesting techniques. It identifies that the techniques differ in their fault\ndetection capabilities and generation of test suite. The various techniques\nlike Cause effect graph, meaningful impact strategy, Branch Operator Strategy\n(BOR), BOR+MI, MUMCUT, Modified Condition/ Decision Coverage (MCDC) has been\nconsidered. This survey describes the basic algorithms and fault categories\nused by these strategies for evaluating their performance. Finally, it contains\nshort summaries of the papers that use Boolean expressions used to specify the\nrequirements for detecting faults. These techniques have been empirically\nevaluated by various researchers on a simplified safety related real time\ncontrol system."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3107", 
    "link": "http://arxiv.org/pdf/1202.5516v1", 
    "title": "Reusable Services from the neuGRID Project for Grid-Based Health   Applications", 
    "arxiv-id": "1202.5516v1", 
    "author": "the neuGRID Consortium", 
    "publish": "2012-02-24T18:02:14Z", 
    "summary": "By abstracting Grid middleware specific considerations from clinical research\napplications, re-usable services should be developed that will provide generic\nfunctionality aimed specifically at medical applications. In the scope of the\nneuGRID project, generic services are being designed and developed which will\nbe applied to satisfy the requirements of neuroscientists. These services will\nbring together sources of data and computing elements into a single view as far\nas applications are concerned, making it possible to cope with centralised,\ndistributed or hybrid data and provide native support for common medical file\nformats. Services will include querying, provenance, portal, anonymization and\npipeline services together with a 'glueing' service for connection to Grid\nservices. Thus lower-level services will hide the peculiarities of any specific\nGrid technology from upper layers, provide application independence and will\nenable the selection of 'fit-for-purpose' infrastructures. This paper outlines\nthe design strategy being followed in neuGRID using the glueing and pipeline\nservices as examples."
},{
    "category": "cs.SE", 
    "doi": "10.5121/csit.2012.2130", 
    "link": "http://arxiv.org/pdf/1202.5609v1", 
    "title": "A Framework Studio for Component Reusability", 
    "arxiv-id": "1202.5609v1", 
    "author": "Salman Abdul Moiz", 
    "publish": "2012-02-25T05:23:21Z", 
    "summary": "The deployment of a software product requires considerable amount of time and\neffort. In order to increase the productivity of the software products,\nreusability strategies were proposed in the literature. However effective reuse\nis still a challenging issue. This paper presents a framework studio for\neffective components reusability which provides the selection of components\nfrom framework studio and generation of source code based on stakeholders\nneeds. The framework studio is implemented using swings which are integrated\nonto the Net Beans IDE which help in faster generation of the source code."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80", 
    "link": "http://arxiv.org/pdf/1202.5826v1", 
    "title": "Proceedings 7th Workshop on Model-Based Testing", 
    "arxiv-id": "1202.5826v1", 
    "author": "Holger Schlingloff", 
    "publish": "2012-02-27T05:37:19Z", 
    "summary": "This volume contains the proceedings of the Seventh Workshop on Model-Based\nTesting (MBT 2012), which was held on 25 March, 2012 in Tallinn, Estonia, as a\nsatellite event of the European Joint Conferences on Theory and Practice of\nSoftware, ETAPS 2012.\n  The workshop is devoted to model-based testing of both software and hardware.\nModel-based testing uses models describing the required behavior of the system\nunder consideration to guide such efforts as test selection and test results\nevaluation. Testing validates the real system behavior against models and\nchecks that the implementation conforms to them, but is capable also to find\nerrors in the models themselves.\n  The first MBT workshop was held in 2004, in Barcelona. At that time MBT\nalready had become a hot topic, but the MBT workshop was the first event\ndevoted mostly to this topic. Since that time the area has generated enormous\nscientific interest, and today there are several specialized workshops and more\nbroad conferences on software and hardware design and quality assurance\ncovering model based testing. MBT has become one of the most powerful system\nanalysis tools, one of the latest hot topic related is applying MBT in security\nanalysis and testing. MBT workshop tries to keep up with current trends. In\n2012 \"industrial paper\" category was added to the program and two industrial\npapers were accepted by the program committee."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80", 
    "link": "http://arxiv.org/pdf/1202.5919v1", 
    "title": "FLOW-Methode - Methodenbeschreibung zur Anwendung von FLOW", 
    "arxiv-id": "1202.5919v1", 
    "author": "Kurt Schneider", 
    "publish": "2012-02-27T13:11:35Z", 
    "summary": "Information of many kinds is flowing in software projects and organizations.\nRequirements have to flow from the customer to the developers. Testers need to\nknow the requirements as well. Boundary conditions and design decisions have to\nbe at the right place at the right time. Information flow analysis with FLOW\nfacilitates modeling of mode and route of the flow of information and\nexperience independent of the development methodology. Experience often acts as\na control factor, because experienced developers can process and route\ninformation more efficiently. Therefore, experience needs to be at the right\nplace at the right time, too. However, most valuable experiences never get\ndocumented. Since information and experience is flowing in agile as well as in\ntraditional environments, the FLOW method does not distinguish between agile\nand traditional, but only between how the flows are shaped.\n  ----\n  In Softwareprojekten flie{\\ss}en vielerlei Informationen. Anforderungen\nm\\\"ussen vom Kunden zu den Entwicklern gelangen. Auch Tester m\\\"ussen die\nAnforderungen kennen. Randbedingungen und Entwurfsentscheidungen m\\\"ussen zur\nrechten Zeit am rechten Ort sein. Die Informationsflussanalyse mit FLOW\nerm\\\"oglicht es, unabh\\\"angig von der Entwicklungsmethode zu modellieren, wie\nund auf welchem Wege Informationen und Erfahrungen flie{\\ss}en. Erfahrungen\nspielen dabei oft die Rolle von Steuergr\\\"o{\\ss}en, denn erfahrene Mitarbeiter\nk\\\"onnen Informationen kompetenter bearbeiten und weiterleiten. Auch die\nErfahrungen m\\\"ussen in geeigneter Form zur rechten Zeit am rechten Ort sein.\nViele Erfahrungen werden aber nie dokumentiert. Da Informationen und\nErfahrungen sowohl in agilen als auch in traditionellen Umgebungen flie{\\ss}en\nm\\\"ussen, wird in FLOW ein Modell aufgebaut, das nicht nach agil, traditionell\noder anderen Bezeichnungen unterscheidet, sondern einzig danach, wie die\nFl\\\"usse gestaltet sind."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.1", 
    "link": "http://arxiv.org/pdf/1202.6118v1", 
    "title": "Model-Based Security Testing", 
    "arxiv-id": "1202.6118v1", 
    "author": "Martin Schneider", 
    "publish": "2012-02-28T05:33:02Z", 
    "summary": "Security testing aims at validating software system requirements related to\nsecurity properties like confidentiality, integrity, authentication,\nauthorization, availability, and non-repudiation. Although security testing\ntechniques are available for many years, there has been little approaches that\nallow for specification of test cases at a higher level of abstraction, for\nenabling guidance on test identification and specification as well as for\nautomated test generation.\n  Model-based security testing (MBST) is a relatively new field and especially\ndedicated to the systematic and efficient specification and documentation of\nsecurity test objectives, security test cases and test suites, as well as to\ntheir automated or semi-automated generation. In particular, the combination of\nsecurity modelling and test generation approaches is still a challenge in\nresearch and of high interest for industrial applications. MBST includes e.g.\nsecurity functional testing, model-based fuzzing, risk- and threat-oriented\ntesting, and the usage of security test patterns. This paper provides a survey\non MBST techniques and the related models as well as samples of new methods and\ntools that are under development in the European ITEA2-project DIAMONDS."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.2", 
    "link": "http://arxiv.org/pdf/1202.6119v1", 
    "title": "Reusing Test-Cases on Different Levels of Abstraction in a Model Based   Development Tool", 
    "arxiv-id": "1202.6119v1", 
    "author": "Daniel Ratiu", 
    "publish": "2012-02-28T05:33:10Z", 
    "summary": "Seamless model based development aims to use models during all phases of the\ndevelopment process of a system. During the development process in a\ncomponent-based approach, components of a system are described at qualitatively\ndiffering abstraction levels: during requirements engineering component models\nare rather abstract high-level and underspecified, while during implementation\nthe component models are rather concrete and fully specified in order to enable\ncode generation. An important issue that arises is assuring that the concrete\nmodels correspond to abstract models. In this paper, we propose a method to\nassure that concrete models for system components refine more abstract models\nfor the same components. In particular we advocate a framework for reusing\ntestcases at different abstraction levels. Our approach, even if it cannot\ncompletely prove the refinement, can be used to ensure confidence in the\ndevelopment process. In particular we are targeting the refinement of\nrequirements which are represented as very abstract models. Besides a formal\nmodel of our approach, we discuss our experiences with the development of an\nAdaptive Cruise Control (ACC) system in a model driven development process.\nThis uses extensions which we implemented for our model-based development tool\nand which are briefly presented in this paper."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.3", 
    "link": "http://arxiv.org/pdf/1202.6120v1", 
    "title": "Applying SMT Solvers to the Test Template Framework", 
    "arxiv-id": "1202.6120v1", 
    "author": "Claudia Frydman", 
    "publish": "2012-02-28T05:33:17Z", 
    "summary": "The Test Template Framework (TTF) is a model-based testing method for the Z\nnotation. In the TTF, test cases are generated from test specifications, which\nare predicates written in Z. In turn, the Z notation is based on first-order\nlogic with equality and Zermelo-Fraenkel set theory. In this way, a test case\nis a witness satisfying a formula in that theory. Satisfiability Modulo Theory\n(SMT) solvers are software tools that decide the satisfiability of arbitrary\nformulas in a large number of built-in logical theories and their combination.\nIn this paper, we present the first results of applying two SMT solvers, Yices\nand CVC3, as the engines to find test cases from TTF's test specifications. In\ndoing so, shallow embeddings of a significant portion of the Z notation into\nthe input languages of Yices and CVC3 are provided, given that they do not\ndirectly support Zermelo-Fraenkel set theory as defined in Z. Finally, the\nresults of applying these embeddings to a number of test specifications of\neight cases studies are analysed."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.4", 
    "link": "http://arxiv.org/pdf/1202.6121v1", 
    "title": "Exact Gap Computation for Code Coverage Metrics in ISO-C", 
    "arxiv-id": "1202.6121v1", 
    "author": "Christian Berg", 
    "publish": "2012-02-28T05:33:24Z", 
    "summary": "Test generation and test data selection are difficult tasks for model based\ntesting. Tests for a program can be meld to a test suite. A lot of research is\ndone to quantify the quality and improve a test suite. Code coverage metrics\nestimate the quality of a test suite. This quality is fine, if the code\ncoverage value is high or 100%. Unfortunately it might be impossible to achieve\n100% code coverage because of dead code for example. There is a gap between the\nfeasible and theoretical maximal possible code coverage value. Our review of\nthe research indicates, none of current research is concerned with exact gap\ncomputation. This paper presents a framework to compute such gaps exactly in an\nISO-C compatible semantic and similar languages. We describe an efficient\napproximation of the gap in all the other cases. Thus, a tester can decide if\nmore tests might be able or necessary to achieve better coverage."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.5", 
    "link": "http://arxiv.org/pdf/1202.6122v1", 
    "title": "Using Built-In Domain-Specific Modeling Support to Guide Model-Based   Test Generation", 
    "arxiv-id": "1202.6122v1", 
    "author": "Olli-Pekka Puolitaival", 
    "publish": "2012-02-28T05:33:32Z", 
    "summary": "We present a model-based testing approach to support automated test\ngeneration with domain-specific concepts. This includes a language expert who\nis an expert at building test models and domain experts who are experts in the\ndomain of the system under test. First, we provide a framework to support the\nlanguage expert in building test models using a full (Java) programming\nlanguage with the help of simple but powerful modeling elements of the\nframework. Second, based on the model built with this framework, the toolset\nautomatically forms a domain-specific modeling language that can be used to\nfurther constrain and guide test generation from these models by a domain\nexpert. This makes it possible to generate a large set of test cases covering\nthe full model, chosen (constrained) parts of the model, or manually define\nspecific test cases on top of the model while using concepts familiar to the\ndomain experts."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.7", 
    "link": "http://arxiv.org/pdf/1202.6123v1", 
    "title": "Towards Symbolic Model-Based Mutation Testing: Combining Reachability   and Refinement Checking", 
    "arxiv-id": "1202.6123v1", 
    "author": "Elisabeth J\u00f6bstl", 
    "publish": "2012-02-28T05:33:46Z", 
    "summary": "Model-based mutation testing uses altered test models to derive test cases\nthat are able to reveal whether a modelled fault has been implemented. This\nrequires conformance checking between the original and the mutated model. This\npaper presents an approach for symbolic conformance checking of action systems,\nwhich are well-suited to specify reactive systems. We also consider\nnondeterminism in our models. Hence, we do not check for equivalence, but for\nrefinement. We encode the transition relation as well as the conformance\nrelation as a constraint satisfaction problem and use a constraint solver in\nour reachability and refinement checking algorithms. Explicit conformance\nchecking techniques often face state space explosion. First experimental\nevaluations show that our approach has potential to outperform explicit\nconformance checkers."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.6", 
    "link": "http://arxiv.org/pdf/1202.6124v1", 
    "title": "Talking quiescence: a rigorous theory that supports parallel   composition, action hiding and determinisation", 
    "arxiv-id": "1202.6124v1", 
    "author": "Mari\u00eblle Stoelinga", 
    "publish": "2012-02-28T05:33:49Z", 
    "summary": "The notion of quiescence - the absence of outputs - is vital in both\nbehavioural modelling and testing theory. Although the need for quiescence was\nalready recognised in the 90s, it has only been treated as a second-class\ncitizen thus far. This paper moves quiescence into the foreground and\nintroduces the notion of quiescent transition systems (QTSs): an extension of\nregular input-output transition systems (IOTSs) in which quiescence is\nrepresented explicitly, via quiescent transitions. Four carefully crafted rules\non the use of quiescent transitions ensure that our QTSs naturally capture\nquiescent behaviour.\n  We present the building blocks for a comprehensive theory on QTSs supporting\nparallel composition, action hiding and determinisation. In particular, we\nprove that these operations preserve all the aforementioned rules.\nAdditionally, we provide a way to transform existing IOTSs into QTSs, allowing\neven IOTSs as input that already contain some quiescent transitions. As an\nimportant application, we show how our QTS framework simplifies the fundamental\nmodel-based testing theory formalised around ioco."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.8", 
    "link": "http://arxiv.org/pdf/1202.6125v1", 
    "title": "Rule-based Test Generation with Mind Maps", 
    "arxiv-id": "1202.6125v1", 
    "author": "Dimitry Polivaev", 
    "publish": "2012-02-28T05:33:53Z", 
    "summary": "This paper introduces basic concepts of rule based test generation with mind\nmaps, and reports experiences learned from industrial application of this\ntechnique in the domain of smart card testing by Giesecke & Devrient GmbH over\nthe last years. It describes the formalization of test selection criteria used\nby our test generator, our test generation architecture and test generation\nframework."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.9", 
    "link": "http://arxiv.org/pdf/1202.6126v1", 
    "title": "Constraint-Based Heuristic On-line Test Generation from   Non-deterministic I/O EFSMs", 
    "arxiv-id": "1202.6126v1", 
    "author": "Marko K\u00e4\u00e4ramees", 
    "publish": "2012-02-28T05:34:00Z", 
    "summary": "We are investigating on-line model-based test generation from\nnon-deterministic output-observable Input/Output Extended Finite State Machine\n(I/O EFSM) models of Systems Under Test (SUTs). We propose a novel\nconstraint-based heuristic approach (Heuristic Reactive Planning Tester (xRPT))\nfor on-line conformance testing non-deterministic SUTs. An indicative feature\nof xRPT is the capability of making reasonable decisions for achieving the test\ngoals in the on-line testing process by using the results of off-line bounded\nstatic reachability analysis based on the SUT model and test goal\nspecification. We present xRPT in detail and make performance comparison with\nother existing search strategies and approaches on examples with varying\ncomplexity."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1202.6127v1", 
    "title": "Model-Based Testing of Safety Critical Real-Time Control Logic Software", 
    "arxiv-id": "1202.6127v1", 
    "author": "Alexey Khoroshilov", 
    "publish": "2012-02-28T05:34:07Z", 
    "summary": "The paper presents the experience of the authors in model based testing of\nsafety critical real-time control logic software. It describes specifics of the\ncorresponding industrial settings and discusses technical details of usage of\nUniTESK model based testing technology in these settings. Finally, we discuss\npossible future directions of safety critical software development processes\nand a place of model based testing techniques in it."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1202.6600v1", 
    "title": "On the Role of Service Concept in IT", 
    "arxiv-id": "1202.6600v1", 
    "author": "Lucian Luca", 
    "publish": "2012-02-29T16:53:57Z", 
    "summary": "Hard times affecting world-wide economy have strong consequences and are\nchallenging IT departments from all sorts of enterprises. Expensive software\nprojects are replaced by component-based agile systems and paradigms like SOA,\nREST, cloud computing are the new buzz-words. Behind the canvas, the service\nconcept plays a central role, which we try to reveal"
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1202.6623v1", 
    "title": "Aspects of SOA: An Entry Point for Starters", 
    "arxiv-id": "1202.6623v1", 
    "author": "Qusay F. Hassan", 
    "publish": "2012-02-29T17:47:38Z", 
    "summary": "Because Service-Oriented Architecture (SOA) is one of the hottest topics that\nis currently gaining momentum, and the number of its adopters (both business\nand IT executives) is increasing in a tremendous manner, it is really a must to\nenlist important aspects related to it in order to allow these adopters to\nbetter understand the role that it can play in both software and business\nmarkets. These aspects varies from the definition of SOA and key components of\nit, different forms of support given by elite software vendors to it, its\nevolution history, the relationship between it and web services, the future\nexpectations about its uses and benefits in different organizations, the\nrelationship between SOA and Enterprise Application Integration (EAI), and\nvarious applications that can use it to overcome limitations related to other\ntraditional methods. Moreover, challenges that face SOA in software market\nshould be addressed and discussed in order to be able to see the big picture\nand to look for better solutions for them."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.0200v2", 
    "title": "An Interface using SOA Framework For Mediclaim Provider", 
    "arxiv-id": "1203.0200v2", 
    "author": "T. Bhuvaneswari", 
    "publish": "2012-03-01T14:36:04Z", 
    "summary": "SOA brought new opportunities for the long expected agility, reuse and the\nadaptive capability of information technology to the ever changing business\nrequirements and environments. The purpose of this paper is to describe the\nimplementation of Medical Insurance Claim Process Model using SOA. We adopt\nService Oriented Architecture (SOA) to reduce the complexity among systems and\nsolve data consistency problems among services. We choose n-tier and\nService-Oriented Architecture (SOA) as our system environment. This model can\nalso establish a potentially new innovative market branch for the insurance\nindustry."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.0400v1", 
    "title": "Bridging the Gap between Technical Heterogeneity of Context-Aware   Platforms: Experimenting a Service Based Connectivity between Adaptable   Android, WComp and OpenORB", 
    "arxiv-id": "1203.0400v1", 
    "author": "Sihem Cherif", 
    "publish": "2012-03-02T09:25:06Z", 
    "summary": "Many companies include in their Information Systems (IS) several\ncommunicating heterogeneous middleware according to their technical needs. The\nneed is the same when IS requires using context aware platforms for different\naims. Moreover, users may be mobile and want to receive and send services with\ntheir PDA that more often supports Android based Human Man Interface. In this\npaper, we show how we extend Android to make it adaptable and open. We also\npresent how we communicate between different heterogeneous context aware\nplatforms as WComp and OpenORB by using Android and Web Services. We introduce\na concrete case study to explain our approach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.1314v1", 
    "title": "Model based Software Develeopment: Issues & Challenges", 
    "arxiv-id": "1203.1314v1", 
    "author": "Mohammed Rizwanullah", 
    "publish": "2012-03-06T20:48:22Z", 
    "summary": "One of the goals of software design is to model a system in such a way that\nit is easily understandable. Nowadays the tendency for software development is\nchanging from manual coding to automatic code generation; it is becoming\nmodel-based. This is a response to the software crisis, in which the cost of\nhardware has decreased and conversely the cost of software development has\nincreased sharply. The methodologies that allowed this change are model-based,\nthus relieving the human from detailed coding. Still there is a long way to\nachieve this goal, but work is being done worldwide to achieve this objective.\nThis paper presents the drastic changes related to modeling and important\nchallenging issues and techniques that recur in MBSD."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.1328v1", 
    "title": "Performance Analysis of HR Portal Domain Components Extraction", 
    "arxiv-id": "1203.1328v1", 
    "author": "A. A. Moiz Qyser", 
    "publish": "2012-03-06T21:04:03Z", 
    "summary": "Extraction of components pertaining to a particular domain not only reduces\nthe cost but also helps in delivering a quality product. However, the\nadvantages of the Component Level Interaction's (CLI's) are not clearly\npresented. In the first part of the paper the design of HR Portal application\nis described. Later the results are simulated using the Netbeans Profiler tool\nwhich exposes and highlights the performance characteristics of component based\nsystem pertaining to HR domain."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.1717v1", 
    "title": "Requirements Engineering Methods: A Classification Framework and   Research Challenges", 
    "arxiv-id": "1203.1717v1", 
    "author": "Ivan Jureta", 
    "publish": "2012-03-08T09:30:58Z", 
    "summary": "Requirements Engineering Methods (REMs) support Requirements Engineering (RE)\ntasks, from elicitation, through modeling and analysis, to validation and\nevolution of requirements. Despite the growing interest to design, validate and\nteach REMs, it remains unclear what components REMs should have. A\nclassification framework for REMs is proposed. It distinguishes REMs based on\nthe domain-independent properties of their components. The classification\nframework is intended to facilitate (i) analysis, teaching and extension of\nexisting REMs, (ii) engineering and validation of new REMs, and (iii)\nidentifying research challenges in REM design. The framework should help\nclarify further the relations between REM and other concepts of interest in and\nto RE, including Requirements Problem and Solution, Requirements Modeling\nLanguage, and Formal Method."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.1760v1", 
    "title": "BPEL-RF: A formal framework for BPEL orchestrations integrating   distributed resources", 
    "arxiv-id": "1203.1760v1", 
    "author": "Gregorio D\u0131az", 
    "publish": "2012-03-08T11:53:19Z", 
    "summary": "Web service compositions are gaining attention to develop complex web systems\nby combination of existing services. Thus, there are many works that leverage\nthe advantages of this approach. However, there are only few works that use web\nservice compositions to manage distributed resources. In this paper, we then\npresent a formal model that combines orchestrations written in BPEL with\ndistributed resources, by using WSRF."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.2704v1", 
    "title": "A model and framework for reliable build systems", 
    "arxiv-id": "1203.2704v1", 
    "author": "George Necula", 
    "publish": "2012-03-13T03:28:55Z", 
    "summary": "Reliable and fast builds are essential for rapid turnaround during\ndevelopment and testing. Popular existing build systems rely on correct manual\nspecification of build dependencies, which can lead to invalid build outputs\nand nondeterminism. We outline the challenges of developing reliable build\nsystems and explore the design space for their implementation, with a focus on\nnon-distributed, incremental, parallel build systems. We define a general model\nfor resources accessed by build tasks and show its correspondence to the\nimplementation technique of minimum information libraries, APIs that return no\ninformation that the application doesn't plan to use. We also summarize\npreliminary experimental results from several prototype build managers."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.3085v1", 
    "title": "An Agile Method for E-Service Composition", 
    "arxiv-id": "1203.3085v1", 
    "author": "Seyyed Mohsen Hashemi", 
    "publish": "2012-03-14T13:52:42Z", 
    "summary": "Nowadays, application of Service Oriented Architecture is increasing rapidly;\nespecially since introduction of distributed electronic services on the web.\nSOA software has a modular manner and works as a collaboration of independent\nsoftware components. As a result, e-service approach is sufficient for software\nwith independent components, each of which may be developed by a different\ncompany. Such software components and their cooperation form a composite\nservice. Agile methodologies are the best candidate for developing small\nsoftware components. Composite services and its building blocks are small\npieces of software, making agile methodology a perfect fit for their\ndevelopment. In this paper, we introduce an agile method for service\ncomposition, inspired by agile patterns and practices. Therefore, across the\nagile manifesto, we can develop low cost, high quality composite services\nquickly using this method."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.3263v1", 
    "title": "Originator usage control with business process slicing", 
    "arxiv-id": "1203.3263v1", 
    "author": "Fr\u00e9d\u00e9rique Biennier", 
    "publish": "2012-03-15T03:49:52Z", 
    "summary": "Originator Control allows information providers to define the information\nre-dissemination condition. Combined with usage control policy, fine-grained\n'downstream usage control' can be achieved, which specifies what attributes the\ndownstream consumers should have and how data is used. This paper discusses\noriginator usage control, paying particular attention to enterprise-level\ndynamic business federations. Rather than 'pre-defining' the information\nre-dissemination paths, our business process slicing method 'capture' the asset\nderivation pattern, allowing to maintain originators' policies during the full\nlifecycle of assets in a collaborative context. First, we propose Service Call\nGraph (SCG), based on extending the System Dependency Graph, to describe\ndependencies among partners. When SCG (and corresponding 'service call tuple'\nlist) is built for a business process, it is analyzed to group partners into\nsub-contexts, according to their dependency relations. Originator usage control\ncan be achieved focusing on each sub-context, by examining downstream\nconsumers' security profiles with upstream asset providers' policies. Second,\nfor analyzing SCG, we propose two 'slicing' strategies, namely 'asset-based'\nand 'request-based' slicing, to deal with the scenarios of both\n'pre-processing' a business process scripts and 'on-the-fly' analyzing service\ncompositions. Last, our implementation work involves a 'context manager'\nservice for processing business processes defined in WS-BPEL. It can be\ncomposed with our former proposed policy negotiation and aggregation services\nto provide policy-based end-to-end security management. We also make\nexperiments based on processing the sample processes that come with\n'WS-BPEL2.0' specification."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.80.10", 
    "link": "http://arxiv.org/pdf/1203.5403v1", 
    "title": "Distributed, Cross-Platform, and Regression Testing Architecture for   Service-Oriented Architecture", 
    "arxiv-id": "1203.5403v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-03-24T10:49:06Z", 
    "summary": "As per leading IT experts, today's large enterprises are going through\nbusiness transformations. They are adopting service-based IT models such as SOA\nto develop their enterprise information systems and applications. In fact, SOA\nis an integration of loosely-coupled interoperable components, possibly built\nusing heterogeneous software technologies and hardware platforms. As a result,\ntraditional testing architectures are no more adequate for verifying and\nvalidating the quality of SOA systems and whether they are operating to\nspecifications. This paper first discusses the various state-of-the-art methods\nfor testing SOA applications, and then it proposes a novel automated,\ndistributed, cross-platform, and regression testing architecture for SOA\nsystems. The proposed testing architecture consists of several testing units\nwhich include test engine, test code generator, test case generator, test\nexecuter, and test monitor units. Experiments conducted showed that the\nproposed testing architecture managed to use parallel agents to test\nheterogeneous web services whose technologies were incompatible with the\ntesting framework. As future work, testing non-functional aspects of SOA\napplications are to be investigated so as to allow the testing of such\nproperties as performance, security, availability, and scalability."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICCITechn.2011.6164784", 
    "link": "http://arxiv.org/pdf/1203.5748v1", 
    "title": "Self-Healing by Means of Runtime Execution Profiling", 
    "arxiv-id": "1203.5748v1", 
    "author": "Jinsuk Baek", 
    "publish": "2012-03-26T18:07:54Z", 
    "summary": "A self-healing application brings itself into a stable state after a failure\nput the software into an unstable state. For such self-healing software\napplication, finding fix for a previously unseen fault is a grand challenge.\nAsking the user to provide fixes for every fault is bad for productivity,\nespecially when the users are non-savvy in technical aspect of computing. If\nfailure scenarios come into existence, the user wants the runtime environment\nto handle those situations autonomically. This paper presents a new technique\nof finding self-healing actions by matching a fault scenario to already\nestablished fault models. By profiling and capturing runtime parameters and\nexecution pathWays, stable execution models are established and later are used\nto match with an unstable execution scenario. Experimentation and results are\npresented that showed that even with additional overheads; this technique can\nprove beneficial for autonomically healing faults and reliving system\nadministrators from mundane troubleshooting situations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1203.6439v1", 
    "title": "An Analytical Approach for Project Managers in Effective Defect   Management in Software Process", 
    "arxiv-id": "1203.6439v1", 
    "author": "N. R. Shashi Kumar", 
    "publish": "2012-03-29T06:23:13Z", 
    "summary": "Defect estimation and prediction are some of the main modulating factors for\nthe success of software projects in any software industry. Maturity and\ncompetency of a project manager in efficient prediction and estimation of\nresource capabilities are one of the strategic driving forces towards the\ngeneration of high quality software. Currently, there are no estimation\ntechniques developed through empirical analysis to evaluate the decision\ncapability of a project manager towards resource allocation for effective\ndefect management. This paper brings out an empirical study carried out in a\nproduct based software organization. Our deep investigation on several projects\nthrows light on the impact of decision capability of project manager towards\naccomplishment of an aforementioned objective. The paper enables project\nmanagers to gain further awareness towards the significance of predictive\npositioning in resource allocation in order to develop high quality defect-free\nsoftware products. It also enhances the maturity level of the company and its\npersistence in the competitive atmosphere."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1203.6445v1", 
    "title": "Analysis of Test Efficiency during Software Development Process", 
    "arxiv-id": "1203.6445v1", 
    "author": "Pranesh Kumar Tiwari", 
    "publish": "2012-03-29T06:53:48Z", 
    "summary": "One of the prerequisites of any organization is an unvarying sustainability\nin the dynamic and competitive industrial environment. Development of high\nquality software is therefore an inevitable constraint of any software\nindustry. Defect management being one of the highly influencing factors for the\nproduction of high quality software, it is obligatory for the software\norganizations to orient them towards effective defect management. Since, the\ntime of software evolution, testing is deemed a promising technique of defect\nmanagement in all IT industries. This paper provides an empirical investigation\nof several projects through a case study comprising of four software companies\nhaving various production capabilities. The aim of this investigation is to\nanalyze the efficiency of test team during software development process. The\nstudy indicates very low-test efficiency at requirements analysis phase and\neven lesser test efficiency at design phase of software development.\nSubsequently, the study calls for a strong need to improve testing approaches\nusing techniques such as dynamic testing of design solutions in lieu of static\ntesting of design document. Dynamic testing techniques enhance the ability of\ndetection and elimination of design flaws right at the inception phase and\nthereby reduce the cost and time of rework. It further improves productivity,\nquality and sustainability of software industry."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.0042v1", 
    "title": "jpf-concurrent: An extension of Java PathFinder for java.util.concurrent", 
    "arxiv-id": "1205.0042v1", 
    "author": "Nastaran Shafiei", 
    "publish": "2012-04-30T22:08:39Z", 
    "summary": "One of the main challenges when verifying multi-threaded Java applications is\nthe state space explosion problem. Due to thread interleavings, the number of\nstates that the model checker has to verify can grow rapidly and impede the\nfeasibility of verification. In the Java language, the source of thread\ninterleavings can be the system under test as well as the Java Development Kit\n(JDK) itself. In our paper, we propose a method to minimize the state space\nexplosion problem for applications verified under the Java PathFinder (JPF)\nmodel checker. Our method is based on abstracting the state of the application\nto a smaller domain and implementing application behavior using the Model Java\nInterface (MJI) of JPF. To show the capabilities of our approach, we have\ncreated a JPF extension called jpf-concurrent which abstracts classes from the\nJava Concurrency Utilities. Several benchmarks proved the usefulness of our\napproach. In all cases, our implementation was faster than the JDK\nimplementation when running under the JPF model checker. Moreover, our\nimplementation led to significantly smaller state spaces."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.0104v1", 
    "title": "Migration of data for iKnow application at EURM - a case study", 
    "arxiv-id": "1205.0104v1", 
    "author": "Toni Stojanovski", 
    "publish": "2012-05-01T07:41:54Z", 
    "summary": "Software evolves. After many revisions and improvements software gets retired\nand replaced. When replacement takes place, one needs to migrate the data from\nthe old database into the new database, so the new application can replace the\nold application. Student administration application (SAA) currently used by\nEuropean University (EURM) has been outgrown by the university, and needs\nreplacement. iKnow application developed as part of the iKnow Tempus project is\nscheduled to replace the existing Student Administration application at EURM.\nThis paper describes the problems that were encountered while migrating the\ndata from the old databases of SAA to the new database designed for the iKnow\napplication. The problems were resolved using the well-known solutions typical\nfor an ETL process, since data migration can be considered as a type of ETL\nprocess. In this paper we describe the solutions for the problems that we\nencountered while migrating the data."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.0750v1", 
    "title": "Towards a new metamodel for the Task Flow Model of the Discovery Method", 
    "arxiv-id": "1205.0750v1", 
    "author": "Carlos Alberto Fernandez-y-Fernandez", 
    "publish": "2012-05-03T16:06:17Z", 
    "summary": "This paper presents our proposal for the evolution of the metamodel for the\nTask Algebra in the Task Flow model for the Discovery Method. The original Task\nAlgebra is based on simple and compound tasks structured using operators such\nas sequence, selection, and parallel composition. Recursion and encapsulation\nwere also considered. We propose additional characteristics to improve the\ncapabilities of the metamodel to represent accurately the Task Flow Model."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.0839v1", 
    "title": "Development of application for discovering and binding to published   geospatial processes in distributed environments", 
    "arxiv-id": "1205.0839v1", 
    "author": "Sergey M. Krasnopeyev", 
    "publish": "2012-05-04T00:47:53Z", 
    "summary": "Nowadays, society has recognized that the lack of access to spatial data and\ntools for their analysis is the limiting factor of economic development. It\ncame to the realization that without the single information space, which is\nimplemented in the form of spatial data infrastructures, a progressive business\ndevelopment is impossible. Spatial data infrastructures will support a variety\nof tasks, which requires the binding of geospatial information from multiple\nsources. In the last few years, the rate of progress in spatial data collection\nwas higher, than in management and analysis of data. Infrastructures allow the\naccumulated data to be available to large groups of users, and infrastructure\nof analysis allows the data to be effectively used for such tasks as municipal\nplanning, science research, etc. Moreover, free access to the information\nresources and instruments of analysis will serve as an additional impulse to\ndevelopment of application models in corresponding areas of expertise. The goal\nof this paper is to indicate possible solutions to the client-side problems of\nspatial data analysis in distributed environments, using the developing\napplication for data analysis as an example."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.0987v1", 
    "title": "Communication Analysis modelling techniques", 
    "arxiv-id": "1205.0987v1", 
    "author": "Marcela Ruiz", 
    "publish": "2012-05-04T15:42:30Z", 
    "summary": "This report describes and illustrates several modelling techniques proposed\nby Communication Analysis; namely Communicative Event Diagram, Message\nStructures and Event Specification Templates. The Communicative Event Diagram\nis a business process modelling technique that adopts a communicational\nperspective by focusing on communicative interactions when describing the\norganizational work practice, instead of focusing on physical activities1; at\nthis abstraction level, we refer to business activities as communicative\nevents. Message Structures is a technique based on structured text that allows\nspecifying the messages associated to communicative events. Event Specification\nTemplates are a means to organise the requirements concerning a communicative\nevent. This report can be useful to analysts and business process modellers in\ngeneral, since, according to our industrial experience, it is possible to apply\nmany Communication Analysis concepts, guidelines and criteria to other business\nprocess modelling notations such as BPMN. Also, Message Structures can\ncomplement business process models created with other notations different than\nCommunicative Event Diagram."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.1102v2", 
    "title": "Towards Ecology Inspired Software Engineering", 
    "arxiv-id": "1205.1102v2", 
    "author": "Martin Monperrus", 
    "publish": "2012-05-05T05:41:22Z", 
    "summary": "Ecosystems are complex and dynamic systems. Over billions of years, they have\ndeveloped advanced capabilities to provide stable functions, despite changes in\ntheir environment. In this paper, we argue that the laws of organization and\ndevelopment of ecosystems provide a solid and rich source of inspiration to lay\nthe foundations for novel software construction paradigms that provide\nstability as much as openness."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.3576v2", 
    "title": "Dexpler: Converting Android Dalvik Bytecode to Jimple for Static   Analysis with Soot", 
    "arxiv-id": "1205.3576v2", 
    "author": "Yves Le Traon", 
    "publish": "2012-05-16T06:55:00Z", 
    "summary": "This paper introduces Dexpler, a software package which converts Dalvik\nbytecode to Jimple. Dexpler is built on top of Dedexer and Soot. As Jimple is\nSoot's main internal rep- resentation of code, the Dalvik bytecode can be\nmanipu- lated with any Jimple based tool, for instance for performing point-to\nor flow analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4194v2", 
    "title": "Decision Taking as a Service", 
    "arxiv-id": "1205.4194v2", 
    "author": "Jan A. Bergstra", 
    "publish": "2012-05-18T16:46:28Z", 
    "summary": "Decision taking can be performed as a service to other parties and it is\namenable to outtasking rather than to outsourcing. Outtasking decision taking\nis compatible with selfsourcing of decision making activities carried out in\npreparation of decision taking. Decision taking as a service (DTaaS) is viewed\nas an instance of so-called decision casting. Preconditions for service casting\nare examined, and compliance of decision taking with these preconditions is\nconfirmed. Potential advantages and disadvantages of using decision taking as a\nservice are considered."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4261v1", 
    "title": "Deployment of software components: Application to Wireless System", 
    "arxiv-id": "1205.4261v1", 
    "author": "Bouzerita Mohamed", 
    "publish": "2012-05-18T20:48:55Z", 
    "summary": "The wide variety of wireless devices brings to design mobile applications as\na collection of interchangeable software components adapted to the deployment\nenvironment of the software. To ensure the proper functioning of the software\nassembly and make a real enforcement in case of failures, the introduction of\nconcepts, models and tools necessary for the administration of these components\nis crucial. This article proposes a method for deploying components in wireless\nsystems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4626v1", 
    "title": "Examining the Impact of Platform Properties on Quality Attributes", 
    "arxiv-id": "1205.4626v1", 
    "author": "T. V. Prabhakar", 
    "publish": "2012-05-21T15:03:11Z", 
    "summary": "We examine and bring out the architecturally significant characteristics of\nvarious virtualization and cloud oriented platforms. The impact of such\ncharacteristics on the ability of guest applications to achieve various quality\nattributes (QA) has also been determined by examining existing body of\narchitecture knowledge. We observe from our findings that efficiency, resource\nelasticity and security are among the most impacted QAs, and virtualization\nplatforms exhibit the maximum impact on various QAs."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4699v1", 
    "title": "Issues of Architectural Description Languages for Handling Dynamic   Reconfiguration", 
    "arxiv-id": "1205.4699v1", 
    "author": "Thais Batista", 
    "publish": "2012-05-21T19:20:01Z", 
    "summary": "Dynamic reconfiguration is the action of modifying a software system at\nruntime. Several works have been using architectural specification as the basis\nfor dynamic reconfiguration. Indeed ADLs (architecture description languages)\nlet architects describe the elements that could be reconfigured as well as the\nset of constraints to which the system must conform during reconfiguration. In\nthis work, we investigate the ADL literature in order to illustrate how\nreconfiguration is supported in four well-known ADLs: pi-ADL, ACME, C2SADL and\nDynamic Wright. From this review, we conclude that none of these ADLs: (i)\naddresses the issue of consistently reconfiguring both instances and types;\n(ii) takes into account the behaviour of architectural elements during\nreconfiguration; and (iii) provides support for assessing reconfiguration,\ne.g., verifying the transition against properties."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4928v1", 
    "title": "Grey-box GUI Testing: Efficient Generation of Event Sequences", 
    "arxiv-id": "1205.4928v1", 
    "author": "Martin Sch\u00e4f", 
    "publish": "2012-05-22T14:33:41Z", 
    "summary": "Graphical user interfaces (GUIs), due to their event driven nature, present a\npotentially unbounded space of all possible ways to interact with software.\nDuring testing it becomes necessary to effectively sample this space. In this\npaper we develop algorithms that sample the GUI's input space by only\ngenerating sequences that (1) are allowed by the GUI's structure, and (2) chain\ntogether only those events that have data dependencies between their event\nhandlers. We create a new abstraction, called an event-dependency graph (EDG)\nof the GUI, that captures data dependencies between event handler code. We\ndevelop a mapping between EDGs and an existing black-box user-level model of\nthe GUI's workflow, called an event-flow graph (EFG). We have implemented\nautomated EDG construction in a tool that analyzes the bytecode of each event\nhandler. We evaluate our \"grey-box\" approach using four open-source\napplications and compare it with the current state-of-the-art EFG approach. Our\nresults show that using the EDG reduces the number of test cases while still\nachieving at least the same coverage. Furthermore, we were able to detect 2 new\nbugs in the subject applications."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.4951v2", 
    "title": "Speculative Symbolic Execution", 
    "arxiv-id": "1205.4951v2", 
    "author": "Ji Wang", 
    "publish": "2012-05-22T15:40:00Z", 
    "summary": "Symbolic execution is an effective path oriented and constraint based program\nanalysis technique. Recently, there is a significant development in the\nresearch and application of symbolic execution. However, symbolic execution\nstill suffers from the scalability problem in practice, especially when applied\nto large-scale or very complex programs. In this paper, we propose a new\nfashion of symbolic execution, named Speculative Symbolic Execution (SSE), to\nspeed up symbolic execution by reducing the invocation times of constraint\nsolver. In SSE, when encountering a branch statement, the search procedure may\nspeculatively explore the branch without regard to the feasibility. Constraint\nsolver is invoked only when the speculated branches are accumulated to a\nspecified number. In addition, we present a key optimization technique that\nenhances SSE greatly. We have implemented SSE and the optimization technique on\nSymbolic Pathfinder (SPF). Experimental results on six programs show that, our\nmethod can reduce the invocation times of constraint solver by 21% to 49% (with\nan average of 30%), and save the search time from 23.6% to 43.6% (with an\naverage of 30%)."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.5106v2", 
    "title": "OTS/CafeOBJ2JML: An attempt to combine Design By Contract with   Behavioral Specifications", 
    "arxiv-id": "1205.5106v2", 
    "author": "Panayiotis Frangos", 
    "publish": "2012-05-23T07:17:42Z", 
    "summary": "Design by Constract (DBC) has influenced the development of formal\nspecification languages that allow the mix of specification and implementation\ncode, like Eiffel, the Java Modeling Language (JML) and Spec#. Meanwhile\nalgebraic specification languages have been developing independently and offer\nfull support for specification and verification of design for large and complex\nsystems in a mathematical rigorous way. However there is no guarantee that the\nfinal implementation will comply to the specification. In this paper we\nproposed the use of the latter for the specification and verification of the\nsystems design and then by presenting a translation between the two, the use of\nthe former to ensure that the implementation respects the specification and\nthus enjoy the verified properties."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.5373v2", 
    "title": "An Adaptive XP-based approach to Agile Development", 
    "arxiv-id": "1205.5373v2", 
    "author": "Lian Luo", 
    "publish": "2012-05-24T08:55:38Z", 
    "summary": "Software design is gradually becoming open, distributed, pervasive, and\nconnected. It is a sad statistical fact that software projects are\nscientifically fragile and tend to fail more than other engineering fields.\nAgile development is a philosophy. And agile methods are processes that support\nthe agile philosophy. XP places a strong emphasis on technical practices in\naddition to the more common teamwork and structural practices. In this paper,\nwe elaborate how XP practices can be used to thinking, collaborating,\nreleasing, planning, developing. And the state that make your team and\norganization more successful."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.5615v1", 
    "title": "A Literature Review of Code Clone Analysis to Improve Software   Maintenance Process", 
    "arxiv-id": "1205.5615v1", 
    "author": "Salah Uddin Ahmed", 
    "publish": "2012-05-25T05:28:29Z", 
    "summary": "Software systems are getting more complex as the system grows where\nmaintaining such system is a primary concern for the industry. Code clone is\none of the factors making software maintenance more difficult. It is a process\nof replicating code blocks by copy-and-paste that is common in software\ndevelopment. In the beginning stage of the project, developers find it easy and\ntime consuming though it has crucial drawbacks in the long run. There are two\ntypes of researchers where some researchers think clones lead to additional\nchanges during maintenance phase, in later stage increase the overall\nmaintenance effort. On the other hand, some researchers think that cloned codes\nare more stable than non cloned codes. In this study, we discussed Code Clones\nand different ideas, methods, clone detection tools, related research on code\nclone, case study."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.5783v1", 
    "title": "Model Driven Mutation Applied to Adaptative Systems Testing", 
    "arxiv-id": "1205.5783v1", 
    "author": "Yves Le Traon", 
    "publish": "2012-05-25T19:26:29Z", 
    "summary": "Dynamically Adaptive Systems modify their behav- ior and structure in\nresponse to changes in their surrounding environment and according to an\nadaptation logic. Critical sys- tems increasingly incorporate dynamic\nadaptation capabilities; examples include disaster relief and space exploration\nsystems. In this paper, we focus on mutation testing of the adaptation logic.\nWe propose a fault model for adaptation logics that classifies faults into\nenvironmental completeness and adaptation correct- ness. Since there are\nseveral adaptation logic languages relying on the same underlying concepts, the\nfault model is expressed independently from specific adaptation languages.\nTaking benefit from model-driven engineering technology, we express these\ncommon concepts in a metamodel and define the operational semantics of mutation\noperators at this level. Mutation is applied on model elements and model\ntransformations are used to propagate these changes to a given adaptation\npolicy in the chosen formalism. Preliminary results on an adaptive web server\nhighlight the difficulty of killing mutants for adaptive systems, and thus the\ndifficulty of generating efficient tests."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.6162v1", 
    "title": "Current Web Application Development and Measurement Practices for Small   Software Firms", 
    "arxiv-id": "1205.6162v1", 
    "author": "Moath Husni", 
    "publish": "2012-05-28T17:05:39Z", 
    "summary": "This paper discusses issues on current development and measurement practices\nthat were identified from a pilot study conducted on Jordanian small software\nfirms. The study was to investigate whether developers follow development and\nmeasurement best practices in web applications development. The analysis was\nconducted in two stages: first, grouping the development and measurement\npractices using variable clustering, and second, identifying the acceptance\ndegree. Mean interval was used to determine the degree of acceptance.\nHierarchal clustering was used to group the development and measurement\npractices. The actual findings of this survey will be used for building a new\nmethodology for developing web applications in small software firms."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MySEC.2011.6140669", 
    "link": "http://arxiv.org/pdf/1205.6177v1", 
    "title": "Decision Taking versus Action Determination", 
    "arxiv-id": "1205.6177v1", 
    "author": "Jan A. Bergstra", 
    "publish": "2012-05-28T18:27:29Z", 
    "summary": "Decision taking is discussed in the context of the role it may play for\nvarious types of agents, and it is contrasted with action determination. Some\nremarks are made about the role of decision taking and action determination in\nthe ongoing debate concerning the reverse polder development of the hertogin\nHedwige polder."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ASE.2011.6100076", 
    "link": "http://arxiv.org/pdf/1205.6361v1", 
    "title": "Querying Source Code with Natural Language", 
    "arxiv-id": "1205.6361v1", 
    "author": "Mira Mezini", 
    "publish": "2012-05-29T13:38:15Z", 
    "summary": "One common task of developing or maintaining software is searching the source\ncode for information like specific method calls or write accesses to certain\nfields. This kind of information is required to correctly implement new\nfeatures and to solve bugs. This paper presents an approach for querying source\ncode with natural language."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s10664-011-9186-4", 
    "link": "http://arxiv.org/pdf/1205.6363v1", 
    "title": "What Should Developers Be Aware Of? An Empirical Study on the Directives   of API Documentation", 
    "arxiv-id": "1205.6363v1", 
    "author": "Mira Mezini", 
    "publish": "2012-05-29T13:39:36Z", 
    "summary": "Application Programming Interfaces (API) are exposed to developers in order\nto reuse software libraries. API directives are natural-language statements in\nAPI documentation that make developers aware of constraints and guidelines\nrelated to the usage of an API. This paper presents the design and the results\nof an empirical study on the directives of API documentation of object-oriented\nlibraries. Its main contribution is to propose and extensively discuss a\ntaxonomy of 23 kinds of API directives."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s10664-011-9186-4", 
    "link": "http://arxiv.org/pdf/1205.6399v1", 
    "title": "Forming Teams for Teaching Programming based on Static Code Analysis", 
    "arxiv-id": "1205.6399v1", 
    "author": "Clifton Clunie", 
    "publish": "2012-05-29T15:45:58Z", 
    "summary": "The use of team for teaching programming can be effective in the classroom\nbecause it helps students to generate and acquire new knowledge in less time,\nbut these groups to be formed without taking into account some respects, may\ncause an adverse effect on the teaching-learning process. This paper proposes a\ntool for the formation of team based on the semantics of source code (SOFORG).\nThis semantics is based on metrics extracted from the preferences, styles and\ngood programming practices. All this is achieved through a static analysis of\ncode that each student develops. In this way, you will have a record of\nstudents with the information extracted; it evaluates the best formation of\nteams in a given course. The team's formations are based on programming styles,\nskills, pair programming or with leader."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1205.6440v1", 
    "title": "Monitoring Software Reliability using Statistical Process Control An   Ordered Statistics Approach", 
    "arxiv-id": "1205.6440v1", 
    "author": "R. R. L. Kantham", 
    "publish": "2012-05-29T18:10:25Z", 
    "summary": "The nature and complexity of software have changed significantly in the last\nfew decades. With the easy availability of computing power, deeper and broader\napplications are made. It has been extremely necessary to produce good quality\nsoftware with high precession of reliability right in the first place. Olden\nday's software errors and bugs were fixed at a later stage in the software\ndevelopment. Today to produce high quality reliable software and to keep a\nspecific time schedule is a big challenge. To cope up the challenge many\nconcepts, methodology and practices of software engineering have been evolved\nfor developing reliable software. Better methods of controlling the process of\nsoftware production are underway. One of such methods to assess the software\nreliability is using control charts. In this paper we proposed an NHPP based\ncontrol mechanism by using order statistics with cumulative quantity between\nobservations of failure data using mean value function of exponential\ndistribution."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1205.6594v1", 
    "title": "Towards a better understanding of testing if conditionals", 
    "arxiv-id": "1205.6594v1", 
    "author": "Tanay Kanti Paul", 
    "publish": "2012-05-30T09:11:27Z", 
    "summary": "Fault based testing is a technique in which test cases are chosen to reveal\ncertain classes of faults. At present, testing professionals use their personal\nexperience to select testing methods for fault classes considered the most\nlikely to be present. However, there is little empirical evidence available in\nthe open literature to support these intuitions. By examining the source code\nchanges when faults were fixed in seven open source software artifacts, we have\nclassified bug fix patterns into fault classes, and recorded the relative\nfrequencies of the identified fault classes. This paper reports our findings\nrelated to \"if-conditional\" fixes. We have classified the \"if-conditional\"\nfixes into fourteen fault classes and calculated their frequencies. We found\nthe most common fault class related to changes within a single \"atom\". The next\nmost common fault was the omission of an \"atom\". We analysed these results in\nthe context of Boolean specification testing."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1205.6677v1", 
    "title": "Cross-View of Testing Techniques Toward Improving Web-Based Application   Testing", 
    "arxiv-id": "1205.6677v1", 
    "author": "Sherif Mazen", 
    "publish": "2012-05-30T13:45:32Z", 
    "summary": "Web Applications (WA's) failures may lead to collapse of the institutions,\ntherefore the importance of good quality WA's is increasing over the time.\nTesting is one of the best quality metrics that decide whether WA's are\nreliable or not. WA's testing approaches suffer from the lack of proper\ncoverage of WA's functional requirements testing. On the other hand some\napproaches produce test cases that already cover WA's testing but they also\nproduce a great number of irrelevant test cases. This research analyzed the\nmain testing approaches for WA's and GUI applications. Also we have an overview\nof Test-Driven Development and its effects on the current development. The\nspecification of good testing approach that satisfies the proper testing is\nthen presented."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1205.6904v1", 
    "title": "A Simulation Model for the Waterfall Software Development Life Cycle", 
    "arxiv-id": "1205.6904v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-05-31T07:35:04Z", 
    "summary": "Software development life cycle or SDLC for short is a methodology for\ndesigning, building, and maintaining information and industrial systems. So\nfar, there exist many SDLC models, one of which is the Waterfall model which\ncomprises five phases to be completed sequentially in order to develop a\nsoftware solution. However, SDLC of software systems has always encountered\nproblems and limitations that resulted in significant budget overruns, late or\nsuspended deliveries, and dissatisfied clients. The major reason for these\ndeficiencies is that project directors are not wisely assigning the required\nnumber of workers and resources on the various activities of the SDLC.\nConsequently, some SDLC phases with insufficient resources may be delayed;\nwhile, others with excess resources may be idled, leading to a bottleneck\nbetween the arrival and delivery of projects and to a failure in delivering an\noperational product on time and within budget. This paper proposes a simulation\nmodel for the Waterfall development process using the Simphony.NET simulation\ntool whose role is to assist project managers in determining how to achieve the\nmaximum productivity with the minimum number of expenses, workers, and hours.\nIt helps maximizing the utilization of development processes by keeping all\nemployees and resources busy all the time to keep pace with the arrival of\nprojects and to decrease waste and idle time. As future work, other SDLC models\nsuch as spiral and incremental are to be simulated, giving project executives\nthe choice to use a diversity of software development methodologies."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.0122v1", 
    "title": "ACME vs PDDL: support for dynamic reconfiguration of software   architectures", 
    "arxiv-id": "1206.0122v1", 
    "author": "J\u00e9r\u00e9my Buisson", 
    "publish": "2012-06-01T08:19:16Z", 
    "summary": "On the one hand, ACME is a language designed in the late 90s as an\ninterchange format for software architectures. The need for recon guration at\nruntime has led to extend the language with speci c support in Plastik. On the\nother hand, PDDL is a predicative language for the description of planning\nproblems. It has been designed in the AI community for the International\nPlanning Competition of the ICAPS conferences. Several related works have\nalready proposed to encode software architectures into PDDL. Existing planning\nalgorithms can then be used in order to generate automatically a plan that\nupdates an architecture to another one, i.e., the program of a recon guration.\nIn this paper, we improve the encoding in PDDL. Noticeably we propose how to\nencode ADL types and constraints in the PDDL representation. That way, we can\nstatically check our design and express PDDL constraints in order to ensure\nthat the generated plan never goes through any bad or inconsistent\narchitecture, not even temporarily."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.0361v1", 
    "title": "Defect Management Using Depth of Inspection and the Inspection   Performance Metric", 
    "arxiv-id": "1206.0361v1", 
    "author": "V Suma", 
    "publish": "2012-06-02T10:35:05Z", 
    "summary": "Advancement in fundamental engineering aspects of software development\nenables IT enterprises to develop a more cost effective and better quality\nproduct through aptly organized defect management strategies. Inspection\ncontinues to be the most effective and efficient technique of defect\nmanagement. To have an appropriate measurement of the inspection process, the\nprocess metric, Depth of Inspection (DI) and the people metric, Inspection\nPerformance Metric (IPM) are introduced. The introduction of these pair of\nmetrics can yield valuable information from a company in relation to the\ninspection process."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.0373v1", 
    "title": "Generation and Optimization of Test cases for Object-Oriented Software   Using State Chart Diagram", 
    "arxiv-id": "1206.0373v1", 
    "author": "Durga Prasad Mohapatra", 
    "publish": "2012-06-02T12:47:40Z", 
    "summary": "The process of testing any software system is an enormous task which is time\nconsuming and costly. The time and required effort to do sufficient testing\ngrow, as the size and complexity of the software grows, which may cause overrun\nof the project budget, delay in the development of software system or some test\ncases may not be covered. During SDLC (software development life cycle),\ngenerally the software testing phase takes around 40-70% of the time and cost.\nState-based testing is frequently used in software testing. Test data\ngeneration is one of the key issues in software testing. A properly generated\ntest suite may not only locate the errors in a software system, but also help\nin reducing the high cost associated with software testing. It is often desired\nthat test data in the form of test sequences within a test suite can be\nautomatically generated to achieve required test coverage. This paper proposes\nan optimization approach to test data generation for the state-based software\ntesting. In this paper, first state transition graph is derived from state\nchart diagram. Then, all the required information are extracted from the state\nchart diagram. Then, test cases are generated. Lastly, a set of test cases are\nminimized by calculating the node coverage for each test case. It is also\ndetermined that which test cases are covered by other test cases. The advantage\nof our test generation technique is that it optimizes test coverage by\nminimizing time and cost. The proposed test data generation scheme generates\ntest cases which satisfy transition path coverage criteria, path coverage\ncriteria and action coverage criteria. A case study on Automatic Ticket Machine\n(ATM) has been presented to illustrate our approach."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.0603v1", 
    "title": "The COMICS Tool - Computing Minimal Counterexamples for Discrete-time   Markov Chains", 
    "arxiv-id": "1206.0603v1", 
    "author": "Bernd Becker", 
    "publish": "2012-06-04T12:58:35Z", 
    "summary": "This report presents the tool COMICS, which performs model checking and\ngenerates counterexamples for DTMCs. For an input DTMC, COMICS computes an\nabstract system that carries the model checking information and uses this\nresult to compute a critical subsystem, which induces a counterexample. This\nabstract subsystem can be refined and concretized hierarchically. The tool\ncomes with a command-line version as well as a graphical user interface that\nallows the user to interactively influence the refinement process of the\ncounterexample."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.0788v1", 
    "title": "Timed Test Case Generation Using Labeled Prioritized Time Petri Nets", 
    "arxiv-id": "1206.0788v1", 
    "author": "Abdelkader Adla", 
    "publish": "2012-06-04T23:02:43Z", 
    "summary": "Model-based testing of software and hardware systems uses behavioral and\nformal models of the systems. The paper presents a technique for model-based\nblack-box conformance testing of real-time systems using Labeled Prioritized\nTime Petri Nets (LPrTPN). The Timed Input/Output Conformance (tioco) relation,\nwhich takes environment assumptions into account, serves as reference to decide\nof implementation correctness. Test suites are derived automatically from a\nLPrTPN made up of two concurrent sub-nets that respectively specify the system\nunder test and its environment. The result is optimal in the sense that test\ncases have the shortest possible accumulated time to be executed. Test cases\nselection combines test purposes and structural coverage criteria associated\nwith the model. A test purpose or a coverage criterion is specified in a SE-LTL\nformula. The TIme Petri Net Analyzer TINA has been extended to support\nconcurrent composed subnets. Automatic generation of time-optimal test suites\nwith the Tina toolbox combines the model checker selt and the path analyzer\nplan. selt outputs a sequence that satisfies the logic formula. plan computes\nthe fastest execution of this sequence which will be transformed in a test\ncases suite."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.4120v1", 
    "title": "Experience on Re-engineering Applying with Software Product Line", 
    "arxiv-id": "1206.4120v1", 
    "author": "Waraporn Jirapanthong", 
    "publish": "2012-06-19T04:10:53Z", 
    "summary": "In this paper, we present our experience based on a reengineering project.\nThe software project is to re-engineer the original system of a company to\nanswer the new requirements and changed business functions. Reengineering is a\nprocess that involves not only the software system, but also underlying\nbusiness model. Particularly, the new business model is designed along with new\ntechnologies to support the new system. This paper presents our experience that\napplies with software product line approach to develop the new system\nsupporting original business functions and new ones."
},{
    "category": "cs.SE", 
    "doi": "10.5120/3917-5515", 
    "link": "http://arxiv.org/pdf/1206.4477v1", 
    "title": "Modeling Languages: metrics and assessing tools", 
    "arxiv-id": "1206.4477v1", 
    "author": "Pedro Rangel Henriques", 
    "publish": "2012-06-20T12:38:48Z", 
    "summary": "Any traditional engineering field has metrics to rigorously assess the\nquality of their products. Engineers know that the output must satisfy the\nrequirements, must comply with the production and market rules, and must be\ncompetitive.\n  Professionals in the new field of software engineering started a few years\nago to define metrics to appraise their product: individual programs and\nsoftware systems. This concern motivates the need to assess not only the\noutcome but also the process and tools employed in its development. In this\ncontext, assessing the quality of programming languages is a legitimate\nobjective; in a similar way, it makes sense to be concerned with models and\nmodeling approaches, as more and more people start the software development\nprocess by a modeling phase.\n  In this paper we introduce and motivate the assessment of models quality in\nthe Software Development cycle. After the general discussion of this topic, we\nfocus the attention on the most popular modeling language -- the UML --\npresenting metrics. Through a Case-Study, we present and explore two tools. To\nconclude we identify what is still lacking in the tools side."
},{
    "category": "cs.SE", 
    "doi": "10.4230/OASIcs.SLATE.2012.185", 
    "link": "http://arxiv.org/pdf/1206.5104v1", 
    "title": "Automatic Test Generation for Space", 
    "arxiv-id": "1206.5104v1", 
    "author": "Pedro Rangel Henriques", 
    "publish": "2012-06-22T10:38:07Z", 
    "summary": "The European Space Agency (ESA) uses an engine to perform tests in the Ground\nSegment infrastructure, specially the Operational Simulator. This engine uses\nmany different tools to ensure the development of regression testing\ninfrastructure and these tests perform black-box testing to the C++ simulator\nimplementation. VST (VisionSpace Technologies) is one of the companies that\nprovides these services to ESA and they need a tool to infer automatically\ntests from the existing C++ code, instead of writing manually scripts to\nperform tests. With this motivation in mind, this paper explores automatic\ntesting approaches and tools in order to propose a system that satisfies VST\nneeds."
},{
    "category": "cs.SE", 
    "doi": "10.4230/OASIcs.SLATE.2012.185", 
    "link": "http://arxiv.org/pdf/1206.5166v1", 
    "title": "Linking Quality Attributes and Constraints with Architectural Decisions", 
    "arxiv-id": "1206.5166v1", 
    "author": "Xavier Franch", 
    "publish": "2012-06-22T14:43:57Z", 
    "summary": "Quality attributes and constraints are among the main drivers of\narchitectural decision making. The quality attributes are improved or damaged\nby the architectural decisions, while restrictions directly include or exclude\nparts of the architecture (for example, the logical components or\ntechnologies). We can determine the impact of a decision of architecture in\nsoftware quality, or which parts of the architecture are affected by a\nconstraint, but the difficult problem is whether we are respecting the quality\nrequirements (requirements on quality attributes) and constraints with all the\narchitectural decisions made. Currently, the common practice is that architects\nuse their own experience to design architectures that meet the quality\nrequirements and restrictions, but at the end, especially for the crucial\ndecisions, the architect has to deal with complex trade-offs between quality\nattributes and juggle possible incompatibilities raised by the constraints. In\nthis paper we present Quark, a computer-aided method to support architects in\nsoftware architecture decision making."
},{
    "category": "cs.SE", 
    "doi": "10.4230/OASIcs.SLATE.2012.185", 
    "link": "http://arxiv.org/pdf/1206.5430v1", 
    "title": "Home Healthcare Process: Challenges and Open Issues", 
    "arxiv-id": "1206.5430v1", 
    "author": "N. Smith-Guerin", 
    "publish": "2012-06-23T19:16:20Z", 
    "summary": "Home healthcare is part of the most critical research and development\nhealthcare areas. The objective is to decentralize healthcare, leading to a\nshift from in-hospital care to more advanced home healthcare, while improving\nefficiency, individualisation, equity and quality of healthcare delivery and\nlimiting financial resources. In this paper, we adopt a process approach to\ntackle the home healthcare domain in order to highlight the importance of\norganisational aspects in the success of an ICT-home healthcare project. Such\nprojects should be supported by an automated system, called in this paper, Home\nHealthcare support system. We examine HH processes from two selected\nperspectives (complexity and dynamics) to illustrate the requirements of a HH\nsupport system. We advocate that satisfying these requirements is part of the\nmost important challenges in the home healthcare research domain and we propose\nfirst track of solutions by attempting to benefit from past experiences in 3\nprocess research communities."
},{
    "category": "cs.SE", 
    "doi": "10.4230/OASIcs.SLATE.2012.185", 
    "link": "http://arxiv.org/pdf/1208.0044v1", 
    "title": "Maintenance de l'outil Wr2fdr de traduction de Wright vers CSP", 
    "arxiv-id": "1208.0044v1", 
    "author": "Mouti Hammami", 
    "publish": "2012-07-09T15:58:44Z", 
    "summary": "The use of formal ADL like Wright is critically dependent on the tools that\nare made available to architects. The Wr2fdr tools accompanying the formal\nWright ADL provides translation to Wright to CSP. Wr2fdr automates four\nstandard properties concerning consistency Connectors (properties 2 and 3),\nComponent (a property 1) and Configuration Management (Property 8) Wright using\nthe model checker FDR. After conducting an audit activity of this tool, we were\nable to correct errors related to both properties 2 and 3. In addition, we\nproposed an implementation of both properties 1 and 8. Finally, we added the\ntool Wr2fdr with a semantic analyzer of Wright."
},{
    "category": "cs.SE", 
    "doi": "10.4230/OASIcs.SLATE.2012.185", 
    "link": "http://arxiv.org/pdf/1208.0592v2", 
    "title": "Debugging Invariant Issues in Pseudo Embedded Program: an Analytical   Approach", 
    "arxiv-id": "1208.0592v2", 
    "author": "Banibrata Bag", 
    "publish": "2012-08-02T17:03:00Z", 
    "summary": "Debugging is an unavoidable and most crucial aspect of software development\nlife cycle. Especially when it comes the turn of embedded one. Due to the\nrequirements of low code size and less resource consumption, the embedded\nsoftwares need to be upgraded all the time involving obvious change of code\nduring development phase. This leads the huge risk of intrusion of bugs into\nthe code at production time. In this paper we propose an approach of debugging\nembedded program in pseudo format, incorporating invariant analysis. Our\nmethodology works on top of Daikon, a popular invariant analyzer. We have\nexperimented with a simplified code snippet [1], used during debugging a\nreported error in BusyBox which is a de-facto standard for Linux in embedded\nsystems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/TECHSYM.2011.5783857", 
    "link": "http://arxiv.org/pdf/1208.0594v1", 
    "title": "Debugging Memory Issues In Embedded Linux: A Case Study", 
    "arxiv-id": "1208.0594v1", 
    "author": "Ansuman Banerjee", 
    "publish": "2012-08-02T17:04:05Z", 
    "summary": "Debugging denotes the process of detecting root causes of unexpected\nobservable behaviors in programs, such as a program crash, an unexpected output\nvalue being produced or an assertion violation. Debugging of program errors is\na difficult task and often takes a significant amount of time in the software\ndevelopment life cycle. In the context of embedded software, the probability of\nbugs is quite high. Due to requirements of low code size and less resource\nconsumption, embedded softwares typically do away with a lot of sanity checks\nduring development time. This leads to high chance of errors being uncovered in\nthe production code at run time. In this paper we propose a methodology for\ndebugging errors in BusyBox, a de-facto standard for Linux in embedded systems.\nOur methodology works on top of Valgrind, a popular memory error detector and\nDaikon, an invariant analyzer. We have experimented with two published errors\nin BusyBox and report our findings in this paper."
},{
    "category": "cs.SE", 
    "doi": "10.1109/TECHSYM.2011.5783857", 
    "link": "http://arxiv.org/pdf/1208.1906v1", 
    "title": "Batch Spreadsheet for C Programmers", 
    "arxiv-id": "1208.1906v1", 
    "author": "Richard Perry", 
    "publish": "2012-08-09T13:38:26Z", 
    "summary": "A computing environment is proposed, based on batch spreadsheet processing,\nwhich produces a spreadsheet display from plain text input files of commands,\nsimilar to the way documents are created using LaTeX. In this environment,\nbesides the usual spreadsheet rows and columns of cells, variables can be\ndefined and are stored in a separate symbol table. Cell and symbol formulas may\ncontain cycles, and cycles which converge can be used to implement iterative\nalgorithms. Formulas are specified using the syntax of the C programming\nlanguage, and all of C's numeric operators are supported, with operators such\nas ++, +=, etc. being implicitly cyclic. User-defined functions can be written\nin C and are accessed using a dynamic link library. The environment can be\ncombined with a GUI front-end processor to enable easier interaction and\ngraphics including plotting."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.2265v1", 
    "title": "Minimal TestCase Generation for Object-Oriented Software with State   Charts", 
    "arxiv-id": "1208.2265v1", 
    "author": "Durga Prasad Mohapatra", 
    "publish": "2012-08-10T04:39:26Z", 
    "summary": "Today statecharts are a de facto standard in industry for modeling system\nbehavior. Test data generation is one of the key issues in software testing.\nThis paper proposes an reduction approach to test data generation for the\nstate-based software testing. In this paper, first state transition graph is\nderived from state chart diagram. Then, all the required information are\nextracted from the state chart diagram. Then, test cases are generated. Lastly,\na set of test cases are minimized by calculating the node coverage for each\ntest case. It is also determined that which test cases are covered by other\ntest cases. The advantage of our test generation technique is that it optimizes\ntest coverage by minimizing time and cost. The present test data generation\nscheme generates test cases which satisfy transition path coverage criteria,\npath coverage criteria and action coverage criteria. A case study on Railway\nTicket Vending Machine (RTVM) has been presented to illustrate our approach."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.2460v1", 
    "title": "Decision Taking for Selling Thread Startup", 
    "arxiv-id": "1208.2460v1", 
    "author": "Jan A. Bergstra", 
    "publish": "2012-08-12T20:01:57Z", 
    "summary": "Decision Taking is discussed in the context of the role it may play for a\nselling agent in a search market, in particular for agents involved in the sale\nof valuable and relatively unique items, such as a dwelling, a second hand car,\nor a second hand recreational vessel.\n  Detailed connections are made between the architecture of decision making\nprocesses and a sample of software technology based concepts including\ninstruction sequences, multi-threading, and thread algebra.\n  Ample attention is paid to the initialization or startup of a thread\ndedicated to achieving a given objective, and to corresponding decision taking.\nAs an application, the selling of an item is taken as an objective to be\nachieved by running a thread that was designed for that purpose."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.2563v1", 
    "title": "Towards a Formalization of the OSGi Component Framework", 
    "arxiv-id": "1208.2563v1", 
    "author": "Jan Olaf Blech", 
    "publish": "2012-08-13T12:22:58Z", 
    "summary": "We present a formalization of the OSGi component framework. Our formalization\nis intended to be used as a basis for describing behavior of OSGi based\nsystems. Furthermore, we describe specification formalisms for describing\nproperties of OSGi based systems. One application is its use for behavioral\ntypes. Potential uses comprise the derivation of runtime monitors, checking\ncompatibility of component composition, discovering components using brokerage\nservices and checking the compatibility of implementation artifacts towards a\nspecification."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.3337v4", 
    "title": "What Good Are Strong Specifications?", 
    "arxiv-id": "1208.3337v4", 
    "author": "Bertrand Meyer", 
    "publish": "2012-08-16T10:37:21Z", 
    "summary": "Experience with lightweight formal methods suggests that programmers are\nwilling to write specification if it brings tangible benefits to their usual\ndevelopment activities. This paper considers stronger specifications and\nstudies whether they can be deployed as an incremental practice that brings\nadditional benefits without being unacceptably expensive. We introduce a\nmethodology that extends Design by Contract to write strong specifications of\nfunctional properties in the form of preconditions, postconditions, and\ninvariants. The methodology aims at being palatable to developers who are not\nfluent in formal techniques but are comfortable with writing simple\nspecifications. We evaluate the cost and the benefits of using strong\nspecifications by applying the methodology to testing data structure\nimplementations written in Eiffel and C#. In our extensive experiments, testing\nagainst strong specifications detects twice as many bugs as standard contracts,\nwith a reasonable overhead in terms of annotation burden and run-time\nperformance while testing. In the wide spectrum of formal techniques for\nsoftware quality, testing against strong specifications lies in a \"sweet spot\"\nwith a favorable benefit to effort ratio."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.3340v2", 
    "title": "Concurrent Models for Object Execution", 
    "arxiv-id": "1208.3340v2", 
    "author": "Bob Diertens", 
    "publish": "2012-08-16T11:01:57Z", 
    "summary": "In previous work we developed a framework of computational models for the\nconcurrent execution of functions on different levels of abstraction. It shows\nthat the traditional sequential execution of function is just a possible\nimplementation of an abstract computational model that allows for the\nconcurrent execution of function. We use this framework as base for the\ndevelopment of abstract computational models that allow for the concurrent\nexecution of objects."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.3887v1", 
    "title": "BPM, Agile, and Virtualization Combine to Create Effective Solutions", 
    "arxiv-id": "1208.3887v1", 
    "author": "Robert Hyer", 
    "publish": "2012-08-19T20:09:06Z", 
    "summary": "The rate of change in business and government is accelerating. A number of\ntechniques for addressing that change have emerged independently to provide for\nautomated solutions in this environment. This paper will examine three of the\nmost popular of these technologies-business process management, the agile\nsoftware development movement, and infrastructure virtualization-to expose the\ncommonalities in these approaches and how, when used together, their combined\neffect results in rapidly deployed, more successful solutions."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3404", 
    "link": "http://arxiv.org/pdf/1208.3890v1", 
    "title": "Significance of Rapid Solutions Development to Business Process   Management", 
    "arxiv-id": "1208.3890v1", 
    "author": "Steve Kruba", 
    "publish": "2012-08-19T20:33:45Z", 
    "summary": "Business process management (BPM) is moving from a niche market into the\nmainstream. One of the factors leading to this transformation is the emergence\nof very powerful rapid solutions development tools for creating BPM solutions\n(BPM RSD). It has been widely recognized that this facility is important for\nachieving benefits quickly. Similar benefits are attributed to the agile\nsoftware movement, but BPM RSD differs in that the objective is to reduce the\nneed for custom software development. As the BPM RSD features of some of the\ncurrent business process management suites (BPMS) products have matured,\nadditional benefits have emerged that fundamentally change the way we approach\nsolutions in this space."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1208.4635v1", 
    "title": "A Case Study on Formal Verification of Self-Adaptive Behaviors in a   Decentralized System", 
    "arxiv-id": "1208.4635v1", 
    "author": "Danny Weyns", 
    "publish": "2012-08-22T22:01:53Z", 
    "summary": "Self-adaptation is a promising approach to manage the complexity of modern\nsoftware systems. A self-adaptive system is able to adapt autonomously to\ninternal dynamics and changing conditions in the environment to achieve\nparticular quality goals. Our particular interest is in decentralized\nself-adaptive systems, in which central control of adaptation is not an option.\nOne important challenge in self-adaptive systems, in particular those with\ndecentralized control of adaptation, is to provide guarantees about the\nintended runtime qualities. In this paper, we present a case study in which we\nuse model checking to verify behavioral properties of a decentralized\nself-adaptive system. Concretely, we contribute with a formalized architecture\nmodel of a decentralized traffic monitoring system and prove a number of\nself-adaptation properties for flexibility and robustness. To model the main\nprocesses in the system we use timed automata, and for the specification of the\nrequired properties we use timed computation tree logic. We use the Uppaal tool\nto specify the system and verify the flexibility and robustness properties."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1208.5195v1", 
    "title": "Decreasing defect rate of test cases by designing and analysis for   recursive modules of a program structure: Improvement in test cases", 
    "arxiv-id": "1208.5195v1", 
    "author": "Ihsan Ullah", 
    "publish": "2012-08-26T06:55:07Z", 
    "summary": "Designing and analysis of test cases is a challenging tasks for tester roles\nespecially those who are related to test the structure of program. Recently,\nProgrammers are showing valuable trend towards the implementation of recursive\nmodules in a program structure. In testing phase of software development life\ncycle, test cases help the tester to test the structure and flow of program.\nThe implementation of well designed test cases for a program leads to reduce\nthe defect rate and efforts needed for corrective maintenance. In this paper,\nauthor proposed a strategy to design and analyze the test cases for a program\nstructure of recursive modules. This strategy will definitely leads to\nvalidation of program structure besides reducing the defect rate and corrective\nmaintenance efforts."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1208.6264v1", 
    "title": "The Boost.Build System", 
    "arxiv-id": "1208.6264v1", 
    "author": "Vladimir Prus", 
    "publish": "2012-08-30T18:54:02Z", 
    "summary": "Boost.Build is a new build system with unique approach to portability. This\npaper discusses the underlying requirements, the key design decisions, and the\nlessons learned during several years of development. We also review other\ncontemporary build systems, and why they fail to meet the same requirements."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1208.6408v2", 
    "title": "Java Source-code Clustering: Unifying Syntactic and Semantic Features", 
    "arxiv-id": "1208.6408v2", 
    "author": "Janardan Misra", 
    "publish": "2012-08-31T07:20:35Z", 
    "summary": "This is a companion draft to the paper 'Software Clustering: Unifying\nSyntactic and Semantic Features', 19th Working Conference on Reverse\nEngineering (WCRE 2012). It discusses software clustering process in detail,\nwhich appeared in the paper in an abridged form. It also contains certain\nadditional process steps which were not covered in the WCRE paper. The\nclustering process is described for applications with Java source-code.\nHowever, as argued in the WCRE paper, it can be seamlessly adapted to many\nother programming paradigms."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1209.1327v1", 
    "title": "The frog and the octopus: a conceptual model of software development", 
    "arxiv-id": "1209.1327v1", 
    "author": "Philippe Kruchten", 
    "publish": "2012-09-06T16:08:34Z", 
    "summary": "We propose a conceptual model of software development that encompasses all\napproaches: traditional or agile, light and heavy, for large and small\ndevelopment efforts. The model identifies both the common aspects in all\nsoftware development, i.e., elements found in some form or another in each and\nevery software development project (Intent, Product, People, Work, Time,\nQuality, Risk, Cost, Value), as well as the variable part, i.e., the main\nfactors that cause the very wide variations we can find in the software\ndevelopment world (Size, Age, Criticality, Architecture stability, Business\nmodel, Governance, Rate of change, Geographic distribution). We show how the\nmodel can be used as an explanatory theory of software development, as a tool\nfor analysis of practices, techniques, processes, as the basis for curriculum\ndesign or for software process adoption and improvement, and to support\nempirical research on software development methods. This model is also proposed\nas a way to depolarize the debate on agile methods versus the\nrest-of-the-world: a unified model."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.91.4", 
    "link": "http://arxiv.org/pdf/1209.1551v1", 
    "title": "The Meaning of Requirements and Adaptation", 
    "arxiv-id": "1209.1551v1", 
    "author": "Amit K. Chopra", 
    "publish": "2012-09-07T14:30:28Z", 
    "summary": "The traditional understanding of stakeholders requirements is that they\nexpress desirable relationships among phenomena in the relevant environment.\nHistorically, software engineering research has tended to focus more on the\nproblems of modeling requirements and deriving specifications given\nrequirements, and much less on the meaning of a requirement itself. I introduce\nnew concepts that elucidate the meaning of requirements, namely, the designated\nset and the falsifiability of requirements.\n  By relying on these concepts, I (i) show that the adaptive requirements\napproaches, which constitute a lively and growing field in RE, are\nfundamentally flawed, (ii) give a sufficient characterization of vague\nrequirements, and (iii) make the connection between requirements modeling and\nthe Zave and Jackson sense of engineering. I support my claims with examples\nand an extensive discussion of the related literature. Finally, I show how\nadaptation can be framed in terms of Zave and Jackson's ontology."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.94", 
    "link": "http://arxiv.org/pdf/1209.1699v1", 
    "title": "Proceedings Sixth Workshop on Formal Languages and Analysis of   Contract-Oriented Software", 
    "arxiv-id": "1209.1699v1", 
    "author": "Anders P. Ravn", 
    "publish": "2012-09-08T09:16:58Z", 
    "summary": "The ability to negotiate contracts for a wide range of aspects and to provide\nservices conforming to them is a most pressing need in service-oriented\narchitectures. High-level models of contracts are making their way into the\narea, but application developers are still left to their own devices when it\ncomes to writing code that will comply with a contract concluded before service\nprovision. At the programming language level, contracts appear as separate\nconcerns that crosscut through application logic. Therefore there is a need for\ncontract analysis tools that extract abstracted models from applications so\nthey become amenable to formal reasoning using formal language techniques.\n  Since its inception, the aim of of FLACOS has been that of bringing together\nresearchers and practitioners working on language- or application-based\nsolutions to these problems through the formalization of contracts, the design\nof appropriate abstraction mechanisms, and tools and techniques for analysis of\ncontracts, and analysis, testing and monitoring of conformance to contracts by\napplications."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.94", 
    "link": "http://arxiv.org/pdf/1209.2382v1", 
    "title": "Raw Report on the Model Checking Contest at Petri Nets 2012", 
    "arxiv-id": "1209.2382v1", 
    "author": "K. Wolf", 
    "publish": "2012-09-08T06:39:01Z", 
    "summary": "This article presents the results of the Model Checking Contest held at Petri\nNets 2012 in Hambourg. This contest aimed at a fair and experimental evaluation\nof the performances of model checking techniques applied to Petri nets. This is\nthe second edition after a successful one in 2011.\n  The participating tools were compared on several examinations (state space\ngeneration and evaluation of several types of formulae - structural,\nreachability, LTL, CTL) run on a set of common models (Place/Transition and\nSymmetric Petri nets).\n  After a short overview of the contest, this paper provides the raw results\nfrom the context, model per model and examination per examination."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmit.2012.4303", 
    "link": "http://arxiv.org/pdf/1209.2516v1", 
    "title": "Efficient Indicators to Evaluate the Status of Software Development   Effort Estimation inside the Organizations", 
    "arxiv-id": "1209.2516v1", 
    "author": "Roliana Ibrahim", 
    "publish": "2012-09-12T08:09:51Z", 
    "summary": "Development effort is an undeniable part of the project management which\nconsiderably influences the success of project. Inaccurate and unreliable\nestimation of effort can easily lead to the failure of project. Due to the\nspecial specifications, accurate estimation of effort in the software projects\nis a vital management activity that must be carefully done to avoid from the\nunforeseen results. However numerous effort estimation methods have been\nproposed in this field, the accuracy of estimates is not satisfying and the\nattempts continue to improve the performance of estimation methods. Prior\nresearches conducted in this area have focused on numerical and quantitative\napproaches and there are a few research works that investigate the root\nproblems and issues behind the inaccurate effort estimation of software\ndevelopment effort. In this paper, a framework is proposed to evaluate and\ninvestigate the situation of an organization in terms of effort estimation. The\nproposed framework includes various indicators which cover the critical issues\nin field of software development effort estimation. Since the capabilities and\nshortages of organizations for effort estimation are not the same, the proposed\nindicators can lead to have a systematic approach in which the strengths and\nweaknesses of organizations in field of effort estimation are discovered."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmit.2012.4303", 
    "link": "http://arxiv.org/pdf/1209.2553v1", 
    "title": "Optimization of fuzzy analogy in software cost estimation using   linguistic variables", 
    "arxiv-id": "1209.2553v1", 
    "author": "S. Sridhar", 
    "publish": "2012-09-12T10:35:01Z", 
    "summary": "One of the most important objectives of software engineering community has\nbeen the increase of useful models that beneficially explain the development of\nlife cycle and precisely calculate the effort of software cost estimation. In\nanalogy concept, there is deficiency in handling the datasets containing\ncategorical variables though there are innumerable methods to estimate the\ncost. Due to the nature of software engineering domain, generally project\nattributes are often measured in terms of linguistic values such as very low,\nlow, high and very high. The imprecise nature of such value represents the\nuncertainty and vagueness in their elucidation. However, there is no efficient\nmethod that can directly deal with the categorical variables and tolerate such\nimprecision and uncertainty without taking the classical intervals and numeric\nvalue approaches. In this paper, a new approach for optimization based on fuzzy\nlogic, linguistic quantifiers and analogy based reasoning is proposed to\nimprove the performance of the effort in software project when they are\ndescribed in either numerical or categorical data. The performance of this\nproposed method exemplifies a pragmatic validation based on the historical NASA\ndataset. The results were analyzed using the prediction criterion and indicates\nthat the proposed method can produce more explainable results than other\nmachine learning methods."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.2659v1", 
    "title": "Reliability improvement with PSP of Web-based software application", 
    "arxiv-id": "1209.2659v1", 
    "author": "Pedro Mej\u00eda-Alvarez", 
    "publish": "2012-09-12T16:45:20Z", 
    "summary": "In diverse industrial and academic environments, the quality of the software\nhas been evaluated using different analytic studies. The contribution of the\npresent work is focused on the development of a methodology in order to improve\nthe evaluation and analysis of the reliability of web-based software\napplications. The Personal Software Process (PSP) was introduced in our\nmethodology for improving the quality of the process and the product. The\nEvaluation + Improvement (Ei) process is performed in our methodology to\nevaluate and improve the quality of the software system. We tested our\nmethodology in a web-based software system and used statistical modeling theory\nfor the analysis and evaluation of the reliability. The behavior of the system\nunder ideal conditions was evaluated and compared against the operation of the\nsystem executing under real conditions. The results obtained demonstrated the\neffectiveness and applicability of our methodology."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.3517v1", 
    "title": "Measuring Spreadsheet Formula Understandability", 
    "arxiv-id": "1209.3517v1", 
    "author": "Arie van Deursen", 
    "publish": "2012-09-16T20:23:47Z", 
    "summary": "Spreadsheets are widely used in industry, because they are flexible and easy\nto use. Sometimes they are even used for business-critical applications. It is\nhowever difficult for spreadsheet users to correctly assess the quality of\nspreadsheets, especially with respect to their understandability.\nUnderstandability of spreadsheets is important, since spreadsheets often have a\nlong lifespan, during which they are used by several users.\n  In this paper, we establish a set of spreadsheet understandability metrics.\nWe start by studying related work and interviewing 40 spreadsheet professionals\nto obtain a set of characteristics that might contribute to understandability\nproblems in spreadsheets. Based on those characteristics we subsequently\ndetermine a number of understandability metrics. To evaluate the usefulness of\nour metrics, we conducted a series of experiments in which professional\nspreadsheet users performed a number of small maintenance tasks on a set of\nspreadsheets from the EUSES spreadsheet corpus. We subsequently calculate the\ncorrelation between the metrics and the performance of subjects on these tasks.\n  The results clearly indicate that the number of ranges, the nesting depth and\nthe presence of conditional operations in formulas significantly increase the\ndifficulty of understanding a spreadsheet."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.3518v1", 
    "title": "On the Interpretation of Spreadsheets within their Environment", 
    "arxiv-id": "1209.3518v1", 
    "author": "Stephen Allen", 
    "publish": "2012-09-16T20:42:37Z", 
    "summary": "A demonstration in MS Excel to show how users can connect their spreadsheet\nmodels to the external environment that the model represents. We employ indexes\nto generate a list of relevant evidence that is hyperlinked to the context in\nwhich the evidence is discussed. The hyperlinks between the index and the\ncontextual discussion have their own specific presentational identity. We\ncontend that these presentational differences aid the integrity and\nunderstanding of complex models. Where models are complex, separate individual\nresults can lead to contradictory conclusions. The demonstration includes a\nmethodology for interpreting the analyses within a workbook and presenting them\nin the form of a standard written report."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.3878v1", 
    "title": "Content Management in Ruby on Rails", 
    "arxiv-id": "1209.3878v1", 
    "author": "Joaqu\u00edn Salvach\u00faa", 
    "publish": "2012-09-18T09:01:24Z", 
    "summary": "Web development is currently driven by model-view-controller (MVC)\nframeworks. How has content management adapted to this scenario? This paper\nreviews content management features in Ruby on Rails framework and its most\npopular plug-ins. These features are distributed among the different layers of\nthe MVC architecture."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.4453v1", 
    "title": "A Metric for the Activeness of a Distributed Object Oriented Component   Library", 
    "arxiv-id": "1209.4453v1", 
    "author": "Deepak Kumar Sharma", 
    "publish": "2012-09-20T08:27:46Z", 
    "summary": "This paper makes an attempt to analyze the Activeness of a Distributed Object\nOriented Component Library and develops a software metric called Distributed\nComponent Activeness Quotient which is defined as the degree of readiness of a\nDOOCL. The advantages of the DCAQ include a possible comparison between various\nDOOCLs leading to selection of the best DOOCL for use during the development\ntask, and providing a measure for gauging the usefulness of the DOOCL as\nindicated by the value of the DCAQ. The disadvantage of the DCAQ is that it may\nhave some error because of its subjective and random nature. The Stability of a\nDOOCL is another characteristic which is indicated by the DCAQ. The greater the\nvalue of the DCAQ, greater will be the stability of the corresponding DOOCL."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.4633v1", 
    "title": "A Metric For The Activeness Of An Object-Oriented Component Library", 
    "arxiv-id": "1209.4633v1", 
    "author": "Nitin Bhardwaj", 
    "publish": "2012-09-20T08:37:25Z", 
    "summary": "In this paper, an attempt has been made to analyze the Activeness of an\nObject Oriented Component Library and develop a special type of software metric\ncalled Component Activeness Quotient which is defined as the degree of\nreadiness of an OOCL. The advantages of the CAQ include a possible comparison\nbetween various OOCLs leading to selection of the best OOCL for use during the\ndevelopment task, and Stability of the software can be gauged as indicated by\nthe value of the CAQ. The disadvantage of the CAQ is that it may have some\nerror because of its subjective and random nature. The paper also tries to\nimprovise the calculation of the Activeness Quotient. The extreme case of a\nsoftware organization having an RQ greater than 1 and MQ equal to 0 was not\nhandled by the method of taking an average of RQ and MQ to calculate the AQ.\nThe improvisation is that the AQ must be equal to a product of MQ and RQ and\nthis is mentioned in the Appendix."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.4634v1", 
    "title": "A Metric for the Activeness of a Class", 
    "arxiv-id": "1209.4634v1", 
    "author": "T V Prasad", 
    "publish": "2012-09-20T14:20:50Z", 
    "summary": "In this paper, the authors propose a software metric called Class Activeness\nMetric which helps to determine the level of accessibility of the members of a\nclass when it is instantiated as objects. Object interactions need to be\nstraight forward as far as possible as complexity in these interactions can\nlead to time delays in accessing members not just confusing inheritance\nhierarchies. For object interactions to be non-complex, the classes must be\ndesigned well so that they are easily accessible. This necessitates the\ndevelopment of a metric for gauging the quality of design of a class. This\nmetric is the Class Activeness Metric."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.4635v1", 
    "title": "DolNet: A Division Of Labour Based Distributed Object Oriented Software   Process Model", 
    "arxiv-id": "1209.4635v1", 
    "author": "Deepak Kumar Sharma", 
    "publish": "2012-09-20T14:39:45Z", 
    "summary": "Distributed Software Development today is in its childhood and not too\nwidespread as a method of developing software in the global IT Industry. In\nthis context, Petrinets are a mathematical model for describing distributed\nsystems theoretically, whereas AttNets are one of their offshoots. But\ndevelopment of true distributed software is limited to network operating\nsystems majorly. Software that runs on many machines with separate programs for\neach machine, are very few. This paper introduces and defines Distributed\nObject Oriented Software Engineering DOOSE as a new field in software\nengineering. The paper further gives a Distributed Object Oriented Software\nProcess Model DOOSPM, called the DolNet, which describes how work may be done\nby a software development organization while working on Distributed Object\nOriented DOO Projects."
},{
    "category": "cs.SE", 
    "doi": "10.5121/cseij.2012.2402", 
    "link": "http://arxiv.org/pdf/1209.5257v1", 
    "title": "A Graphical Tool for Testing Timed Systems based on Meta- Modeling and   Graph Grammars", 
    "arxiv-id": "1209.5257v1", 
    "author": "Djamel-Eddine Saidouni", 
    "publish": "2012-09-24T13:17:25Z", 
    "summary": "The test is one of the approaches commonly used for validating systems to\nensure qualitative and quantitative implementation requirements. In this paper,\nwe interest in formal testing using graph transformation, thus we propose an\napproach for translating a Durational Actions Timed Automata model (DATA*) with\na high number of states into a timed refusals region graph (TRRG) for creating\na canonical tester and generating test cases using graph transformation.\nThough, our approach allows to generate automatically a visual modeling tool\nfor DATA*, TRRG and the canonical tester. The cost of building a visual\nmodeling tool from scratch is prohibitive. Meta- modeling approach is useful to\ndeal with this problem since it allows the modeling of the formalisms\nthemselves, by means of graph grammars. The meta-modeling tool AToM3 is used."
},{
    "category": "cs.SE", 
    "doi": "10.1109/WCRE.2008.45", 
    "link": "http://arxiv.org/pdf/1209.5490v1", 
    "title": "Consistent Layout for Thematic Software Maps", 
    "arxiv-id": "1209.5490v1", 
    "author": "Oscar Nierstrasz", 
    "publish": "2012-09-25T04:19:04Z", 
    "summary": "Software visualizations can provide a concise overview of a complex software\nsystem. Unfortunately, since software has no physical shape, there is no\n\"natural\" mapping of software to a two-dimensional space. As a consequence most\nvisualizations tend to use a layout in which position and distance have no\nmeaning, and consequently layout typical diverges from one visualization to\nanother. We propose a consistent layout for software maps in which the position\nof a software artifact reflects its \\emph{vocabulary}, and distance corresponds\nto similarity of vocabulary. We use Latent Semantic Indexing (LSI) to map\nsoftware artifacts to a vector space, and then use Multidimensional Scaling\n(MDS) to map this vector space down to two dimensions. The resulting consistent\nlayout allows us to develop a variety of thematic software maps that express\nvery different aspects of software while making it easy to compare them. The\napproach is especially suitable for comparing views of evolving software, since\nthe vocabulary of software artifacts tends to be stable over time."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1209.5573v1", 
    "title": "Defect Management Strategies in Software Development", 
    "arxiv-id": "1209.5573v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2012-09-25T10:51:01Z", 
    "summary": "Software is a unique entity that has laid a strong impact on all other fields\neither related or not related to software. These include medical, scientific,\nbusiness, educational, defence, transport, telecommunication to name a few.\nState-of-the-art professional domain activities demands the development of high\nquality software. High quality software attributes to a defect-free product,\nwhich is competent of producing predictable results and remains deliverable\nwithin time and cost constraints. It should be manageable with minimum\ninterferences. It should also be maintainable, dependable, understandable and\nefficient. Thus, a systematic approach towards high quality software\ndevelopment is required due to increased competitiveness in today's business\nworld, technological advances, hardware complexity and frequently changing\nbusiness requirements."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1209.5800v1", 
    "title": "Lessons Learned from Evaluating MDE Abstractions in an Industry Case   Study", 
    "arxiv-id": "1209.5800v1", 
    "author": "Gail C. Murphy", 
    "publish": "2012-09-26T00:27:33Z", 
    "summary": "In a recent empirical study we found that evaluating abstractions of\nModel-Driven Engineering (MDE) is not as straight forward as it might seem. In\nthis paper, we report on the challenges that we as researchers faced when we\nconducted the aforementioned field study. In our study we found that modeling\nhappens within a complex ecosystem of different people working in different\nroles. An empirical evaluation should thus mind the ecosystem, that is, focus\non both technical and human factors. In the following, we present and discuss\nfive lessons learnt from our recent work."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1209.6163v1", 
    "title": "From Functions to Object-Orientation by Abstraction", 
    "arxiv-id": "1209.6163v1", 
    "author": "Bob Diertens", 
    "publish": "2012-09-27T08:53:56Z", 
    "summary": "In previous work we developed a framework of computational models for\nfunction and object execution. The models on an higher level of abstraction in\nthis framework allow for concurrent execution of functions and objects. We show\nthat the computational model for object execution complies with the\nfundamentals of object-orientation."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1209.6466v1", 
    "title": "Four-Step Approach Model of Inspection (FAMI) for Effective Defect   Management in Software Development", 
    "arxiv-id": "1209.6466v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2012-09-28T09:55:55Z", 
    "summary": "IT industry should inculcate effective defect management on a continual basis\nto deploy nearly a zerodefect product to their customers. Inspection is one of\nthe most imperative and effective strategies of defect management.\nNevertheless, existing defect management strategies in leading software\nindustries are successful to deliver a maximum of 96% defect-free product. An\nempirical study of various projects across several service-based and\nproduct-based industries proves the above affirmations. This paper provides an\nenhanced approach of inspection through a Four-Step Approach Model of\nInspection (FAMI). FAMI consists of i) integration of Inspection Life Cycle in\nV-model of software development, ii) implementation of process metric Depth of\nInspection (DI), iii) implementation of people metric Inspection Performance\nMetric (IPM), iv) application of Bayesian probability approach for selection of\nappropriate values of inspection affecting parameters to achieve the desirable\nDI. The managers of software houses can make use of P2 metric as a benchmarking\ntool for the projects in order to improve the in-house defect management\nprocess. Implementation of FAMI in software industries reflects a continual\nprocess improvement and leads to the development of nearly a zero-defect\nproduct through effective defect management."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1210.1179v1", 
    "title": "Extending OWL-S for the Composition of Web Services Generated With a   Legacy Application Wrapper", 
    "arxiv-id": "1210.1179v1", 
    "author": "Bernard Gibaud", 
    "publish": "2012-10-01T12:26:40Z", 
    "summary": "Despite numerous efforts by various developers, web service composition is\nstill a difficult problem to tackle. Lot of progressive research has been made\non the development of suitable standards. These researches help to alleviate\nand overcome some of the web services composition issues. However, the legacy\napplication wrappers generate nonstandard WSDL which hinder the progress.\nIndeed, in addition to their lack of semantics, WSDLs have sometimes different\nshapes because they are adapted to circumvent some technical implementation\naspect. In this paper, we propose a method for the semi automatic composition\nof web services in the context of the NeuroLOG project. In this project the\nreuse of processing tools relies on a legacy application wrapper called jGASW.\nThe paper describes the extensions to OWL-S in order to introduce and enable\nthe composition of web services generated using the jGASW wrapper and also to\nimplement consistency checks regarding these services."
},{
    "category": "cs.SE", 
    "doi": "10.5772/7407", 
    "link": "http://arxiv.org/pdf/1210.1291v1", 
    "title": "Graphical Visualization of Risk Assessment for Effective Risk Management   during Software Development Process", 
    "arxiv-id": "1210.1291v1", 
    "author": "Suma. V", 
    "publish": "2012-10-04T04:13:57Z", 
    "summary": "Success of any IT industry depends on the success rate of their projects,\nwhich in turn depends on several factors such as cost, time, and availability\nof resources. These factors formulate the risk areas, which needs to be\naddressed in a proactive way. The rudimentary objective of risk management is\nto circumvent the possibility of their occurrence by identifying the risks,\npreparing the contingency plans and mitigation plans in order to reduce the\nconsequences of the risks. Hence, effective risk management becomes one of the\nimperative challenges in any organization, which if deemed in an apt way\nassures the continued sustainability of the organization in the high-end\ncompetitive environment. This paper provides visualization of risk assessment\nthrough a graphical model. Further, the matrix representation of the risk\nassessment aids the project personnel to identify all the risks, comprehend\ntheir frequency and probability of their occurrence. In addition, the graphical\nmodel enables one to analyze the impact of identified risks and henceforth to\nassign their priorities. This mode of representation of risk assessment factors\nhelps the organization in accurate prediction of success rate of the project."
},{
    "category": "cs.SE", 
    "doi": "10.5120/7834-1132", 
    "link": "http://arxiv.org/pdf/1210.2506v2", 
    "title": "Enabling Reusability in Agile Software Development", 
    "arxiv-id": "1210.2506v2", 
    "author": "Inderveer Chana", 
    "publish": "2012-10-09T07:16:54Z", 
    "summary": "Software Engineering Discipline is constantly achieving momentum from past\ntwo decades. In last decade, remarkable progress has been observed. New process\nmodels that are introduced from time to time in order to keep pace with\nmultidimensional demands of the industry. New software development paradigms\nare finding its place in industry such as Agile Software Development, Reuse\nbased Development and Component based Development. But different software\ndevelopment models fail to satisfy many needs of software industry. As aim of\nall the process models is same, i.e., to get quality product, reduce time of\ndevelopment, productivity improvement and reduction in cost. Still, no single\nprocess model is complete in itself. Software industry is moving towards Agile\nSoftware Development. Agile development does not obviously fit well for\nbuilding reusable artifacts. However, with careful attention, and important\nmodifications made to agile processes, it may be possible to successfully adapt\nand put on agile methods to development of reusable objects. The model being\nproposed here combines the features of Agile Software Development and\nreusability."
},{
    "category": "cs.SE", 
    "doi": "10.5120/7834-1132", 
    "link": "http://arxiv.org/pdf/1210.3320v1", 
    "title": "Contemporary Semantic Web Service Frameworks: An Overview and   Comparisons", 
    "arxiv-id": "1210.3320v1", 
    "author": "Norbik Bashah Idris", 
    "publish": "2012-10-11T18:57:40Z", 
    "summary": "The growing proliferation of distributed information systems, allows\norganizations to offer their business processes to a worldwide audience through\nWeb services. Semantic Web services have emerged as a means to achieve the\nvision of automatic discovery, selection, composition, and invocation of Web\nservices by encoding the specifications of these software components in an\nunambiguous and machine-interpretable form. Several frameworks have been\ndevised as enabling technologies for Semantic Web services. In this paper, we\nsurvey the prominent Semantic Web service frameworks. In addition, a set of\ncriteria is identified and the discussed frameworks are evaluated and compared\nwith respect to these criteria. Knowing the strengths and weaknesses of the\nSemantic Web service frameworks can help researchers to utilize the most\nappropriate one according to their needs."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.3604v1", 
    "title": "User-Centric Optimization for Constraint Web Service Composition using a   Fuzzy-guided Genetic Algorithm System", 
    "arxiv-id": "1210.3604v1", 
    "author": "Mohsen Hashemi", 
    "publish": "2012-10-12T19:08:27Z", 
    "summary": "Service-Oriented Applications (SOA) are being regarded as the main pragmatic\nsolution for distributed environments. In such systems, however each service\nresponds the user request independently, it is essential to compose them for\ndelivering a compound value-added service. Since, there may be a number of\ncompositions to create the requested service, it is important to find one which\nits properties are close to user's desires and meet some non-functional\nconstraints and optimize criteria such as overall cost or response time. In\nthis paper, a user-centric approach is presented for evaluating the service\ncompositions which attempts to obtain the user desires. This approach uses\nfuzzy logic in order to inference based on quality criteria ranked by user and\nGenetic Algorithms to optimize the QoS-aware composition problem. Results show\nthat the Fuzzy-based Genetic algorithm system enables user to participate in\nthe process of web service composition easier and more efficient."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.3640v1", 
    "title": "Conducting Verification And Validation Of Multi- Agent Systems", 
    "arxiv-id": "1210.3640v1", 
    "author": "Nedhal Al Saiyd", 
    "publish": "2012-10-12T21:09:46Z", 
    "summary": "Verification and Validation (V&V) is a series of activities, technical and\nmanagerial, which performed by system tester not the system developer in order\nto improve the system quality, system reliability and assure that product\nsatisfies the users operational needs. Verification is the assurance that the\nproducts of a particular development phase are consistent with the requirements\nof that phase and preceding phase(s), while validation is the assurance that\nthe final product meets system requirements. an outside agency can be used to\nperformed V&V, which is indicate by Independent V&V, or IV&V, or by a group\nwithin the organization but not the developer, referred to as Internal V&V. Use\nof V&V often accompanies testing, can improve quality assurance, and can reduce\nrisk. This paper putting guidelines for performing V&V of Multi-Agent Systems\n(MAS)."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.3756v1", 
    "title": "A Conceptual Framework to Analyze Enterprise Business Solutions from a   Software Architecture Perspective", 
    "arxiv-id": "1210.3756v1", 
    "author": "Basem Y. Alkazemi", 
    "publish": "2012-10-14T04:10:32Z", 
    "summary": "The architectural aspects of software systems are not always explicitly\nexposed to customers when a product is presented to them by software vendors.\nTherefore, customers might be put at a major risk if new emerging business\nneeds come to light that require modification of some of the core business\nprocesses within their organizations. So they might need to replace their\nexisting systems or re-architect old ones to comply with new architectural\nstandards. This paper describes a proposed framework that helps organizations\nto build a comprehensive view of their system architecture prior to dealing\nwith vendors. Consequently, every organization can have a reference model that\nfacilitates negotiation and communication with software vendors. The paper\napplies the proposed framework to an organization in the region of Saudi Arabia\nto validate its applicability and generates an architectural design for their\nsoftware systems."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.3758v1", 
    "title": "On verification of software components", 
    "arxiv-id": "1210.3758v1", 
    "author": "Basem Y. Alkazemi", 
    "publish": "2012-10-14T04:30:17Z", 
    "summary": "Utilizing third party software components in the development of new systems\nbecame somewhat unfavourable approach among many organizations nowadays. This\nreluctance is primarily built due to the lack of support to verify the quality\nattributes of software components in order to avoid potential mismatches with\nsystems requirements. This paper presents an approach to overcome this problem\nby providing a tool support to check component compatibility to a specification\nprovided by developers. So, components compatibility can be checked and\ndevelopers can verify components that match their quality attributes prior of\nintegrating them into their system."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.3858v1", 
    "title": "Application of classical compilation techniques for syntactic and   semantic analysis of specification written in Object Z", 
    "arxiv-id": "1210.3858v1", 
    "author": "Kais Haddar", 
    "publish": "2012-10-15T00:00:20Z", 
    "summary": "Building a parser for a formal specification language such as Object Z is not\nan easy task. Indeed, it requires a double competence both in the compilation\nfield than in the field of formal specification. In this paper, we first\npresent some tools for analyzing specifications written in Z and Object Z by\nshowing the characteristics of each. Then, we identify some common semantic\nconstraints in Object Z. Finally, we propose an approach for building a parser\nfor Object Z based on the conventional techniques of compilation."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.4686v1", 
    "title": "Black-Box Verification for GUI Applications", 
    "arxiv-id": "1210.4686v1", 
    "author": "Andreas Podelski", 
    "publish": "2012-10-17T10:20:39Z", 
    "summary": "In black-box testing of GUI applications (a form of system testing), a\ndynamic analysis of the GUI application is used to infer a black-box model; the\nblack-box model is then used to derive test cases for the test of the GUI\napplication. In this paper, we propose to supplement the test with the\nverification of the black-box model. We present a method that can give a\nguarantee of the absence of faults, i.e., the correctness of all test cases of\nthe black-box model. The black-model allows us to formulate a parametrized\nverification problem. As we will show, it also allows us to circumvent the\nstatic analysis of the GUI tool kit. We have implemented our approach;\npreliminary experiments indicate its practical potential."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwsc.2012.3301", 
    "link": "http://arxiv.org/pdf/1210.5262v1", 
    "title": "Hands-Off Spreadsheets", 
    "arxiv-id": "1210.5262v1", 
    "author": "Colin A. Kerr", 
    "publish": "2012-10-18T21:08:15Z", 
    "summary": "The wealth of functionality in the Excel software package means it can go\nbeyond use as a static evaluator of predefined cell formulae, to be used\nactively in manipulating and transforming data. Due to human error it is\nimpossible to ensure a process like this is always error free, and frequently\nthe sequence of actions is recorded only in the operator's head. If done\nregularly by highly paid staff it will be expensive. This paper applies to\nthose spreadsheets which involve significant operator intervention, describes a\nmethod that has been used to improve reliability and efficiency, and reports on\nhow it has worked in practice."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.6115v1", 
    "title": "Analyzing Consistency of Behavioral REST Web Service Interfaces", 
    "arxiv-id": "1210.6115v1", 
    "author": "Ivan Porres", 
    "publish": "2012-10-23T02:55:21Z", 
    "summary": "REST web services can offer complex operations that do more than just simply\ncreating, retrieving, updating and deleting information from a database. We\nhave proposed an approach to design the interfaces of behavioral REST web\nservices by defining a resource and a behavioral model using UML. In this paper\nwe discuss the consistency between the resource and behavioral models that\nrepresent service states using state invariants. The state invariants are\ndefined as predicates over resources and describe what are the valid state\nconfigurations of a behavioral model. If a state invariant is unsatisfiable\nthen there is no valid state configuration containing the state and there is no\nservice that can implement the service interface. We also show how we can use\nreasoning tools to determine the consistency between these design models."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.6636v1", 
    "title": "Informaticology: combining Computer Science, Data Science, and Fiction   Science", 
    "arxiv-id": "1210.6636v1", 
    "author": "Jan A. Bergstra", 
    "publish": "2012-10-24T19:24:59Z", 
    "summary": "Motivated by an intention to remedy current complications with Dutch\nterminology concerning informatics, the term informaticology is positioned to\ndenote an academic counterpart of informatics where informatics is conceived of\nas a container for a coherent family of practical disciplines ranging from\ncomputer engineering and software engineering to network technology, data\ncenter management, information technology, and information management in a\nbroad sense.\n  Informaticology escapes from the limitations of instrumental objectives and\nthe perspective of usage that both restrict the scope of informatics. That is\nachieved by including fiction science in informaticology and by ranking fiction\nscience on equal terms with computer science and data science, and framing (the\nstudy of) game design, evelopment, assessment and distribution, ranging from\nserious gaming to entertainment gaming, as a chapter of fiction science. A\nsuggestion for the scope of fiction science is specified in some detail.\n  In order to illustrate the coherence of informaticology thus conceived, a\npotential application of fiction to the ontology of instruction sequences and\nto software quality assessment is sketched, thereby highlighting a possible\nrole of fiction (science) within informaticology but outside gaming."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.6815v2", 
    "title": "Formally Checking Large Data Sets in the Railways", 
    "arxiv-id": "1210.6815v2", 
    "author": "Michael Leuschel", 
    "publish": "2012-10-25T12:38:19Z", 
    "summary": "This article presents industrial experience of validating large data sets\nagainst specification written using the B / Event-B mathematical language and\nthe ProB model checker."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7030v1", 
    "title": "Lessons Learned/Sharing the Experience of Developing a Metro System Case   Study", 
    "arxiv-id": "1210.7030v1", 
    "author": "Renato Silva", 
    "publish": "2012-10-26T00:50:43Z", 
    "summary": "In this document we share the experiences gained throughout the development\nof a metro system case study. The model is constructed in Event-B using its\nrespective tool set, the Rodin platform. Starting from requirements, adding\nmore details to the model in a stepwise manner through refinement, we identify\nsome keys points and available plugins necessary for modelling large systems\n(requirement engineering, decomposition, generic instantiation, among others),\nwhich ones are lacking plus strengths and weaknesses of the tool."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7032v1", 
    "title": "Dependability-Explicit Engineering with Event-B: Overview of Recent   Achievements", 
    "arxiv-id": "1210.7032v1", 
    "author": "Elena Troubitsyna", 
    "publish": "2012-10-26T01:03:59Z", 
    "summary": "Event-B has been actively used within the EU Deploy project to model\ndependable systems from various application domains. As a result, we have\ncreated a number of formal approaches to explicitly reason about dependability\nin the refinement process. In this paper we overview the work on formal\nengineering of dependable systems carried out in the Deploy project. We outline\nour approaches to integrating safety analysis into the development process,\nmodelling fault tolerant systems and probabilistic dependability evaluation. We\ndiscuss achievements and challenges in development of dependable systems within\nthe Event-B framework."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7034v1", 
    "title": "Building on the DEPLOY Legacy: Code Generation and Simulation", 
    "arxiv-id": "1210.7034v1", 
    "author": "John Colley", 
    "publish": "2012-10-26T01:11:39Z", 
    "summary": "The RODIN, and DEPLOY projects laid solid foundations for further\ntheoretical, and practical (methodological and tooling) advances with Event-B.\nOur current interest is the co-simulation of cyber-physical systems using\nEvent-B. Using this approach we aim to simulate various features of the\nenvironment separately, in order to exercise deployable code. This paper has\ntwo contributions, the first is the extension of the code generation work of\nDEPLOY, where we add the ability to generate code from Event-B state-machine\ndiagrams. The second describes how we may use code, generated from\nstate-machines, to simulate the environment, and simulate concurrently\nexecuting state-machines, in a single task. We show how we can instrument the\ncode to guide the simulation, by controlling the relative rate that\nnon-deterministic transitions are traversed in the simulation."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7035v1", 
    "title": "Development of Fault Tolerant MAS with Cooperative Error Recovery by   Refinement in Event-B", 
    "arxiv-id": "1210.7035v1", 
    "author": "Linas Laibinis", 
    "publish": "2012-10-26T01:11:59Z", 
    "summary": "Designing fault tolerance mechanisms for multi-agent systems is a notoriously\ndifficult task. In this paper we present an approach to formal development of a\nfault tolerant multi-agent system by refinement in Event-B. We demonstrate how\nto formally specify cooperative error recovery and dynamic reconfiguration in\nEvent-B. Moreover, we discuss how to express and verify essential properties of\na fault tolerant multi-agent system while refining it. The approach is\nillustrated by a case study - a multi-robotic system."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7036v1", 
    "title": "Towards Refinement Strategy Planning for Event-B", 
    "arxiv-id": "1210.7036v1", 
    "author": "Shinichi Honiden", 
    "publish": "2012-10-26T01:12:26Z", 
    "summary": "Event-B is a formal approach oriented to system modeling and analysis. It\nsupports refinement mechanism that enables stepwise modeling and verification\nof a system. By using refinement, the complexity of verification can be spread\nand mitigated. In common development using Event-B, a specification written in\na natural language is examined before modeling in order to plan the modeling\nand refinement strategy. After that, starting from a simple abstract model,\nconcrete models in several different abstraction levels are constructed by\ngradually introducing complex structures and concepts. Although users of\nEvent-B have to plan how to abstract the specification for the construction of\neach model, guidelines for such a planning have not been suggested.\nSpecifically, some elements in a model often require that other elements are\nincluded in the model because of semantics constraints of Event-B. As such\nrequirements introduces many elements at once, non-experts of Event-B often\nmake refinement rough though rough refinement does not mitigate the complexity\nof verification well. In response to the problem, a method is proposed to plan\nwhat models are constructed in each abstraction level. The method calculates\nplans that mitigate the complexity well considering the semantics constraints\nof Event-B and the relationships between elements in a system."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7039v1", 
    "title": "Formal Data Validation with Event-B", 
    "arxiv-id": "1210.7039v1", 
    "author": "Marielle Doche-Petit", 
    "publish": "2012-10-26T01:16:06Z", 
    "summary": "This article presents a verification and validation activity performed in an\nindustrial context, to validate configuration data of a metro CBTC system by\ncreating a formal B model of these configuration data and of their properties.\nA double tool chain is used to safely check whether a certain given input of\nconfiguration data fulfill its properties. One tool is based on some Rodin and\nopen source plug-ins and the other tool is based on ProB."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7138v1", 
    "title": "Legacy Software Restructuring: Analyzing a Concrete Case", 
    "arxiv-id": "1210.7138v1", 
    "author": "Jannik Laval", 
    "publish": "2012-10-26T13:20:00Z", 
    "summary": "Software re-modularization is an old preoccupation of reverse engineering\nresearch. The advantages of a well structured or modularized system are well\nknown. Yet after so much time and efforts, the field seems unable to come up\nwith solutions that make a clear difference in practice. Recently, some\nresearchers started to question whether some basic assumptions of the field\nwere not overrated. The main one consists in evaluating the\nhigh-cohesion/low-coupling dogma with metrics of unknown relevance. In this\npaper, we study a real structuring case (on the Eclipse platform) to try to\nbetter understand if (some) existing metrics would have helped the software\nengineers in the task. Results show that the cohesion and coupling metrics used\nin the experiment did not behave as expected and would probably not have helped\nthe maintainers reach there goal. We also measured another possible\nrestructuring which is to decrease the number of cyclic dependencies between\nmodules. Again, the results did not meet expectations."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.7283v1", 
    "title": "Abstract Data Types in Event-B - An Application of Generic Instantiation", 
    "arxiv-id": "1210.7283v1", 
    "author": "Naoto Sato", 
    "publish": "2012-10-27T03:01:38Z", 
    "summary": "Integrating formal methods into industrial practice is a challenging task.\nOften, different kinds of expertise are required within the same development.\nOn the one hand, there are domain engineers who have specific knowledge of the\nsystem under development. On the other hand, there are formal methods experts\nwho have experience in rigorously specifying and reasoning about formal\nsystems. Coordination between these groups is important for taking advantage of\ntheir expertise. In this paper, we describe our approach of using generic\ninstantiation to facilitate this coordination. In particular, generic\ninstantiation enables a separation of concerns between the different parties\ninvolved in developing formal systems."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.98.8", 
    "link": "http://arxiv.org/pdf/1210.8011v1", 
    "title": "Reusability Framework for Cloud Computing", 
    "arxiv-id": "1210.8011v1", 
    "author": "Rishideep Singh", 
    "publish": "2012-10-30T13:57:19Z", 
    "summary": "Cloud based development is a challenging task for several software\nengineering projects, especially for those which needs development with\nreusability. Present time of cloud computing is allowing new professional\nmodels for using the software development. The expected upcoming trend of\ncomputing is assumed to be this cloud computing because of speed of application\ndeployment, shorter time to market, and lower cost of operation. Until Cloud Co\nmputing Reusability Model is considered a fundamental capability, the speed of\ndeveloping services is very slow. Th is paper spreads cloud computing with\ncomponent based development named Cloud Co mputing Reusability Model (CCR) and\nenable reusability in cloud computing. In this paper Cloud Co mputing\nReusability Model has been proposed. The model has been validated by Cloudsim\nan d experimental result shows that reusability based cloud computing approach\nis effective in minimizing cost and time to market."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.0592v1", 
    "title": "Requirements of a Recovery Solution for Failure of Composite Web   Services", 
    "arxiv-id": "1211.0592v1", 
    "author": "Sameem Abdul Kareem", 
    "publish": "2012-11-03T04:44:15Z", 
    "summary": "Web services are building blocks of interoperable systems. Composing Web\nservices makes the processes capable of doing complex tasks. Composite services\nmay fail during their execution which can be diagnosed by a mediator. The\nmediator adapts the structure so that the failure is recovered. Moreover,\nfuture executions should avoid the situation or organize a strategy to repair\nthe structure with a minimum delay. In this paper the failure reasons of a\ncomposite service are reviewed. Furthermore, the requirements of a solution for\nrecovery of a system from a failure are investigated."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.0713v1", 
    "title": "From user requirements to UML class diagram", 
    "arxiv-id": "1211.0713v1", 
    "author": "Wahiba Ben Abdessalem", 
    "publish": "2012-11-04T19:35:11Z", 
    "summary": "The transition from user requirements to UML diagrams is a difficult task for\nthe designer especially when he handles large texts expressing these needs.\nModeling class Diagram must be performed frequently, even during the\ndevelopment of a simple application. This paper proposes an approach to\nfacilitate class diagram extraction from textual requirements using NLP\ntechniques and domain ontology."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.1136v1", 
    "title": "Estimation of Effort in Software Cost Analysis for Heterogenous Dataset   using Fuzzy Analogy", 
    "arxiv-id": "1211.1136v1", 
    "author": "S. Sridhar", 
    "publish": "2012-11-06T08:15:30Z", 
    "summary": "One of the significant objectives of software engineering community is to use\neffective and useful models for precise calculation of effort in software cost\nestimation. The existing techniques cannot handle the dataset having\ncategorical variables efficiently including the commonly used analogy method.\nAlso, the project attributes of cost estimation are measured in terms of\nlinguistic values whose imprecision leads to confusion and ambiguity while\nexplaining the process. There are no definite set of models which can\nefficiently handle the dataset having categorical variables and endure the\nmajor hindrances such as imprecision and uncertainty without taking the\nclassical intervals and numeric value approaches. In this paper, a new approach\nbased on fuzzy logic, linguistic quantifiers and analogy based reasoning is\nproposed to enhance the performance of the effort estimation in software\nprojects dealing with numerical and categorical data. The performance of this\nproposed method illustrates that there is a realistic validation of the results\nwhile using historical heterogeneous dataset. The results were analyzed using\nthe Mean Magnitude Relative Error (MMRE) and indicates that the proposed method\ncan produce more explicable results than the methods which are in vogue."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.2259v1", 
    "title": "Proceedings: Workshop on the experience of and advances in developing   dependable systems in Event-B (DS-Event-B 2012)", 
    "arxiv-id": "1211.2259v1", 
    "author": "Alexander Romanovsky", 
    "publish": "2012-11-09T22:02:27Z", 
    "summary": "These proceedings include papers presented at the Workshop on \"The experience\nof and advances in developing dependable systems in Event-B\" held on November\n13, 2012 as part of the ICFEM 2012 (Kyoto, Japan)."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.2620v2", 
    "title": "Context-Driven Elicitation of Default Requirements: an Empirical   Validation", 
    "arxiv-id": "1211.2620v2", 
    "author": "St\u00e9phane Faulkner", 
    "publish": "2012-11-12T14:04:38Z", 
    "summary": "In Requirements Engineering, requirements elicitation aims the acquisition of\ninformation from the stakeholders of a system-to-be. An important task during\nelicitation is to identify and render explicit the stakeholders' implicit\nassumptions about the system-to-be and its environment. Purpose of doing so is\nto identify omissions in, and conflicts between requirements. This paper offers\na conceptual framework for the identification and documentation of default\nrequirements that stakeholders may be using. The framework is relevant for\npractice, as it forms a check-list for types of questions to use during\nelicitation. An empirical validation is described, and guidelines for\nelicitation are drawn."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.2708v1", 
    "title": "Extending Refusal Testing by Stochastic Refusals for Testing   Non-deterministic Systems", 
    "arxiv-id": "1211.2708v1", 
    "author": "Djamel-Eddine Saidouni", 
    "publish": "2012-11-12T17:38:55Z", 
    "summary": "Testing is a validation activity used to check the system's correctness with\nrespect to the specification. In this context,test based on refusals is studied\nin theory and tools are effectively constructed. This paper addresses,a formal\ntesting based on stochastic refusals graphs (SRG) in order to test stochastic\nsystem represented by maximality-based labeled stochastic transition systems\n(MLSTS). First, we propose a framework to generate SRGs from MLSTSs. Second,we\npresent a new technique to generate automatically a canonical tester from\nstochastic refusal graph and conformance relation confSRG. Finally,\nimplementation is proposed and the application of our approach is shown by an\nexample."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.2858v1", 
    "title": "Fault Localization Using Textual Similarities", 
    "arxiv-id": "1211.2858v1", 
    "author": "Westley Weimer", 
    "publish": "2012-11-13T00:37:38Z", 
    "summary": "Maintenance is a dominant component of software cost, and localizing reported\ndefects is a significant component of maintenance. We propose a scalable\napproach that leverages the natural language present in both defect reports and\nsource code to identify files that are potentially related to the defect in\nquestion. Our technique is language-independent and does not require test\ncases. The approach represents reports and code as separate structured\ndocuments and ranks source files based on a document similarity metric that\nleverages inter-document relationships.\n  We evaluate the fault-localization accuracy of our method against both\nlightweight baseline techniques and also reported results from state-of-the-art\ntools. In an empirical evaluation of 5345 historical defects from programs\ntotaling 6.5 million lines of code, our approach reduced the number of files\ninspected per defect by over 91%. Additionally, we qualitatively and\nquantitatively examine the utility of the textual and surface features used by\nour approach."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3402", 
    "link": "http://arxiv.org/pdf/1211.3229v1", 
    "title": "Context-Awareness for Service Oriented Systems", 
    "arxiv-id": "1211.3229v1", 
    "author": "Abdelaziz Kriouile", 
    "publish": "2012-11-14T07:50:12Z", 
    "summary": "Today, service oriented systems need to be enhanced to sense and react to\nusers context in order to provide a better user experience. To meet this\nrequirement, Context-Aware Services (CAS) have emerged as an underling design\nand development paradigm for the development of context-aware systems. The\nfundamental challenges for such systems development are context-awareness\nmanagement and service adaptation to the users context. To cope with such\nrequirements, we propose a well designed architecture, named ACAS, to support\nthe development of Context-Aware Service Oriented Systems (CASOS). This\narchitecture relies on a set of context-awareness and CAS specifications and\nmetamodels to enhance a core service, in service oriented systems, to be\ncontext-aware. This enhancement is fulfilled by the Aspect Adaptations Weaver\n(A2W) which, based on the Aspect Paradigm (AP) concepts, considers the services\nadaptations as aspects."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2480362.2480590", 
    "link": "http://arxiv.org/pdf/1211.3257v1", 
    "title": "The Search for the Laws of Automatic Random Testing", 
    "arxiv-id": "1211.3257v1", 
    "author": "Yi Wei", 
    "publish": "2012-11-14T10:05:43Z", 
    "summary": "Can one estimate the number of remaining faults in a software system? A\ncredible estimation technique would be immensely useful to project managers as\nwell as customers. It would also be of theoretical interest, as a general law\nof software engineering. We investigate possible answers in the context of\nautomated random testing, a method that is increasingly accepted as an\neffective way to discover faults. Our experimental results, derived from\nbest-fit analysis of a variety of mathematical functions, based on a large\nnumber of automated tests of library code equipped with automated oracles in\nthe form of contracts, suggest a poly-logarithmic law. Although further\nconfirmation remains necessary on different code bases and testing techniques,\nwe argue that understanding the laws of testing may bring significant benefits\nfor estimating the number of detectable faults and comparing different projects\nand practices."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2480362.2480590", 
    "link": "http://arxiv.org/pdf/1211.4347v3", 
    "title": "How many software engineering professionals hold this certificate?", 
    "arxiv-id": "1211.4347v3", 
    "author": "Fedor Dzerzhinskiy", 
    "publish": "2012-11-19T09:54:21Z", 
    "summary": "Estimates of quantity of the certificates issued during 10 years of existence\nof the professionals certification program in the area of software engineering\nimplemented by one of the leading professional associations are presented. The\nestimates have been obtained by way of processing certificate records openly\naccessible at the certification program Web-site. Comparison of these estimates\nand the known facts about evolution of the certification program indicates that\nas of the present day this evolution has not led to a large scale issuance of\nthese certificates. But the same estimates, possibly, indicate that the meaning\nof these certificates differs from what is usually highlighted, and their real\nvalue is much greater. Also these estimates can be viewed, besides all else, as\nreflecting an outcome of a decade long experimental verification of the known\nidea about \"software engineering as a mature engineering profession,\" and they\npossibly show that this idea deserves partial revision.\n  Keywords: software engineering certification, actual results vs.\nexpectations, software engineering profession."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2506375", 
    "link": "http://arxiv.org/pdf/1211.4470v4", 
    "title": "Loop invariants: analysis, classification, and examples", 
    "arxiv-id": "1211.4470v4", 
    "author": "Sergey Velder", 
    "publish": "2012-11-19T15:50:53Z", 
    "summary": "Software verification has emerged as a key concern for ensuring the continued\nprogress of information technology. Full verification generally requires, as a\ncrucial step, equipping each loop with a \"loop invariant\". Beyond their role in\nverification, loop invariants help program understanding by providing\nfundamental insights into the nature of algorithms. In practice, finding sound\nand useful invariants remains a challenge. Fortunately, many invariants seem\nintuitively to exhibit a common flavor. Understanding these fundamental\ninvariant patterns could therefore provide help for understanding and verifying\na large variety of programs.\n  We performed a systematic identification, validation, and classification of\nloop invariants over a range of fundamental algorithms from diverse areas of\ncomputer science. This article analyzes the patterns, as uncovered in this\nstudy, governing how invariants are derived from postconditions; it proposes a\ntaxonomy of invariants according to these patterns, and presents its\napplication to the algorithms reviewed. The discussion also shows the need for\nhigh-level specifications based on \"domain theory\". It describes how the\ninvariants and the corresponding algorithms have been mechanically verified\nusing an automated program prover; the proof source files are available. The\ncontributions also include suggestions for invariant inference and for\nmodel-based specification."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_17", 
    "link": "http://arxiv.org/pdf/1211.4775v6", 
    "title": "Contracts in Practice", 
    "arxiv-id": "1211.4775v6", 
    "author": "Bertrand Meyer", 
    "publish": "2012-11-20T15:26:17Z", 
    "summary": "Contracts are a form of lightweight formal specification embedded in the\nprogram text. Being executable parts of the code, they encourage programmers to\ndevote proper attention to specifications, and help maintain consistency\nbetween specification and implementation as the program evolves. The present\nstudy investigates how contracts are used in the practice of software\ndevelopment. Based on an extensive empirical analysis of 21 contract-equipped\nEiffel, C#, and Java projects totaling more than 260 million lines of code over\n7700 revisions, it explores, among other questions: 1) which kinds of contract\nelements (preconditions, postconditions, class invariants) are used more often;\n2) how contracts evolve over time; 3) the relationship between implementation\nchanges and contract changes; and 4) the role of inheritance in the process. It\nhas found, among other results, that: the percentage of program elements that\ninclude contracts is above 33% for most projects and tends to be stable over\ntime; there is no strong preference for a certain type of contract element;\ncontracts are quite stable compared to implementations; and inheritance does\nnot significantly affect qualitative trends of contract usage."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3401", 
    "link": "http://arxiv.org/pdf/1211.4867v1", 
    "title": "Adaptation of Web services to the context based on workflow: Approach   for self-adaptation of service-oriented architectures to the context", 
    "arxiv-id": "1211.4867v1", 
    "author": "Jalel Akaichi", 
    "publish": "2012-11-20T10:38:45Z", 
    "summary": "The emergence of Web services in the information space, as well as the\nadvanced technology of SOA, give tremendous opportunities for users in an\nambient space or distant, empowerment and organizations in various fields\napplication, such as geolocation, E-learning, healthcare, digital government,\netc.. In fact, Web services are a solution for the integration of distributed\ninformation systems, autonomous, heterogeneous and self-adaptable to the\ncontext. However, as Web services can evolve in a dynamic environment in a\nwell-defined context and according to events automatically, such as time,\ntemperature, location, authentication, etc.. We are interested in improving\ntheir SOA to empower the Web services to be self adaptive contexts. In this\npaper, we propose a new trend of self adaptability of Web services context.\nThen applying these requirements in the architecture of the platform of\nadaptability to context WComp, by integrating the workflow. Our work is\nillustrated by a case study of authentication."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2012.3401", 
    "link": "http://arxiv.org/pdf/1211.5451v1", 
    "title": "Bypassing the Combinatorial Explosion: Using Similarity to Generate and   Prioritize T-wise Test Suites for Large Software Product Lines", 
    "arxiv-id": "1211.5451v1", 
    "author": "Yves Le Traon", 
    "publish": "2012-11-23T09:47:12Z", 
    "summary": "Software Product Lines (SPLs) are families of products whose commonalities\nand variability can be captured by Feature Models (FMs). T-wise testing aims at\nfinding errors triggered by all interactions amongst t features, thus reducing\ndrastically the number of products to test. T-wise testing approaches for SPLs\nare limited to small values of t -- which miss faulty interactions -- or\nlimited by the size of the FM. Furthermore, they neither prioritize the\nproducts to test nor provide means to finely control the generation process.\nThis paper offers (a) a search-based approach capable of generating products\nfor large SPLs, forming a scalable and flexible alternative to current\ntechniques and (b) prioritization algorithms for any set of products.\nExperiments conducted on 124 FMs (including large FMs such as the Linux kernel)\ndemonstrate the feasibility and the practicality of our approach."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.6191v1", 
    "title": "CTGEN - a Unit Test Generator for C", 
    "arxiv-id": "1211.6191v1", 
    "author": "Jan Peleska", 
    "publish": "2012-11-27T02:36:52Z", 
    "summary": "We present a new unit test generator for C code, CTGEN. It generates test\ndata for C1 structural coverage and functional coverage based on\npre-/post-condition specifications or internal assertions. The generator\nsupports automated stub generation, and data to be returned by the stub to the\nunit under test (UUT) may be specified by means of constraints. The typical\napplication field for CTGEN is embedded systems testing; therefore the tool can\ncope with the typical aliasing problems present in low-level C, including\npointer arithmetics, structures and unions. CTGEN creates complete test\nprocedures which are ready to be compiled and run against the UUT. In this\npaper we describe the main features of CTGEN, their technical realisation, and\nwe elaborate on its performance in comparison to a list of competing test\ngeneration tools. Since 2011, CTGEN is used in industrial scale test campaigns\nfor embedded systems code in the automotive domain."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.6258v2", 
    "title": "Modelling the Strategic Alignment of Software Requirements using Goal   Graphs", 
    "arxiv-id": "1211.6258v2", 
    "author": "Badr Haque", 
    "publish": "2012-11-27T10:18:00Z", 
    "summary": "This paper builds on existing Goal Oriented Requirements Engineering (GORE)\nresearch by presenting a methodology with a supporting tool for analysing and\ndemonstrating the alignment between software requirements and business\nobjectives. Current GORE methodologies can be used to relate business goals to\nsoftware goals through goal abstraction in goal graphs. However, we argue that\nunless the extent of goal-goal contribution is quantified with verifiable\nmetrics and confidence levels, goal graphs are not sufficient for demonstrating\nthe strategic alignment of software requirements. We introduce our methodology\nusing an example software project from Rolls-Royce. We conclude that our\nmethodology can improve requirements by making the relationships to business\nproblems explicit, thereby disambiguating a requirement's underlying purpose\nand value."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.6322v2", 
    "title": "Metamodel Instance Generation: A systematic literature review", 
    "arxiv-id": "1211.6322v2", 
    "author": "James F. Power", 
    "publish": "2012-11-27T15:08:52Z", 
    "summary": "Modelling and thus metamodelling have become increasingly important in\nSoftware Engineering through the use of Model Driven Engineering. In this paper\nwe present a systematic literature review of instance generation techniques for\nmetamodels, i.e. the process of automatically generating models from a given\nmetamodel. We start by presenting a set of research questions that our review\nis intended to answer. We then identify the main topics that are related to\nmetamodel instance generation techniques, and use these to initiate our\nliterature search. This search resulted in the identification of 34 key papers\nin the area, and each of these is reviewed here and discussed in detail. The\noutcome is that we are able to identify a knowledge gap in this field, and we\noffer suggestions as to some potential directions for future research."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.6370v1", 
    "title": "Increasing the failure recovery probability of atomic replacement   approaches", 
    "arxiv-id": "1211.6370v1", 
    "author": "Sameem Abdul Kareem", 
    "publish": "2012-11-24T06:38:07Z", 
    "summary": "Web processes are made up of services as their units of functionality. The\nservices are represented as a graph and compose a synergy of service. The\ncomposite service is prone to failure due to various causes. However, the\nend-user should receive a smooth and non-interrupted execution. Atomic\nreplacement of a failed Web service to recover the system is a straightforward\napproach. Nevertheless, finding a similar service is not reliable. In order to\nincrease the probability of the recovery of a failed composite service, a set\nof services is replaced with another similar set."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.6781v1", 
    "title": "User Defined Spreadsheet Functions in Excel", 
    "arxiv-id": "1211.6781v1", 
    "author": "Dermot Balson", 
    "publish": "2012-11-28T23:25:12Z", 
    "summary": "Creating user defined functions (UDFs) is a powerful method to improve the\nquality of computer applications, in particular spreadsheets. However, the only\ndirect way to use UDFs in spreadsheets is to switch from the functional and\ndeclarative style of spreadsheet formulas to the imperative VBA, which creates\na high entry barrier even for proficient spreadsheet users. It has been\nproposed to extend Excel by UDFs declared by a spreadsheet: user defined\nspreadsheet functions (UDSFs). In this paper we present a method to create a\nlimited form of UDSFs in Excel without any use of VBA. Calls to those UDSFs\nutilize what-if data tables to execute the same part of a worksheet several\ntimes, thus turning it into a reusable function definition."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.7100v1", 
    "title": "Governance of Spreadsheets through Spreadsheet Change Reviews", 
    "arxiv-id": "1211.7100v1", 
    "author": "Joost Visser", 
    "publish": "2012-11-29T21:48:16Z", 
    "summary": "We present a pragmatic method for management of risks that arise due to\nspreadsheet use in large organizations. We combine peer-review, tool-assisted\nevaluation and other pre-existing approaches into a single organization-wide\napproach that reduces spreadsheet risk without overly restricting spreadsheet\nuse. The method was developed in the course of several spreadsheet evaluation\nassignments for a corporate customer. Our method addresses a number of issues\npertinent to spreadsheet risks that were raised by the Sarbanes-Oxley act."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.7104v1", 
    "title": "Investigating Effects of Common Spreadsheet Design Practices on   Correctness and Maintainability", 
    "arxiv-id": "1211.7104v1", 
    "author": "Sebastian Zitzelsberger", 
    "publish": "2012-11-29T22:09:37Z", 
    "summary": "Spreadsheets are software programs which are typically created by end-users\nand often used for business-critical tasks. Many studies indicate that errors\nin spreadsheets are very common. Thus, a number of vendors offer auditing tools\nwhich promise to detect errors by checking spreadsheets against so-called Best\nPractices such as \"Don't put constants in fomulae\". Unfortunately, it is\nlargely unknown which Best Practices have which actual effects on which\nspreadsheet quality aspects in which settings.\n  We have conducted a controlled experiment with 42 subjects to investigate the\nquestion whether observance of three commonly suggested Best Practices is\ncorrelated with desired positive effects regarding correctness and\nmaintainability: \"Do not put constants in formulae\", \"keep formula complexity\nlow\" and \"refer to the left and above\". The experiment was carried out in two\nphases which covered the creation of new and the modification of existing\nspreadsheets. It was evaluated using a novel construction kit for spreadsheet\nauditing tools called Spreadsheet Inspection Framework.\n  The experiment produced a small sample of directly comparable spreadsheets\nwhich all try to solve the same task. Our analysis of the obtained spreadsheets\nindicates that the correctness of \"bottom-line\" results is not affected by the\nobservance of the three Best Practices. However, initially correct spreadsheets\nwith high observance of these Best Practices tend to be the ones whose later\nmodifications yield the most correct results."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.102.9", 
    "link": "http://arxiv.org/pdf/1211.7345v1", 
    "title": "JooFlux : modification de code \u00e0 chaud et injection d'aspects   directement dans une JVM 7", 
    "arxiv-id": "1211.7345v1", 
    "author": "Fr\u00e9d\u00e9ric Le Mou\u00ebl", 
    "publish": "2012-11-30T19:24:42Z", 
    "summary": "Changing functional and non-functional software implementation at runtime is\nuseful and even sometimes critical both in development and production\nenvironments. JooFlux is a JVM agent that allows both the dynamic replacement\nof method implementations and the application of aspect advices. It works by\ndoing bytecode transformation to take advantage of the new invokedynamic\ninstruction added in Java SE 7 to help implementing dynamic languages for the\nJVM. JooFlux can be managed using a JMX agent so as to operate dynamic\nmodifications at runtime, without resorting to a dedicated domain-specific\nlanguage. We compared JooFlux with existing AOP platforms and dynamic\nlanguages. Results demonstrate that JooFlux performances are close to the Java\nones --- with most of the time a marginal overhead, and sometimes a gain ---\nwhere AOP platforms and dynamic languages present significant overheads. This\npaves the way for interesting future evolutions and applications of JooFlux."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9349-3675", 
    "link": "http://arxiv.org/pdf/1212.0312v1", 
    "title": "Software Reuse in Medical Database for Cardiac Patients using Pearson   Family Equations", 
    "arxiv-id": "1212.0312v1", 
    "author": "M. H. M. Krishna Prasad", 
    "publish": "2012-12-03T08:45:18Z", 
    "summary": "Software reuse is a subfield of software engineering that is used to adopt\nthe existing software for similar purposes. Reuse Metrics determine the extent\nto which an existing software component is reused in new software with an\nobjective to minimize the errors and cost of the new project. In this paper,\nmedical database related to cardiology is considered. The Pearson Type I\nDistribution is used to calculate the probability density function (pdf) and\nthereby utilizing it for clustering the data. Further, coupling methodology is\nused to bring out the similarity of the new patient data by comparing it with\nthe existing data. By this, the concerned treatment to be followed for the new\npatient is deduced by comparing with that of the previous patients case\nhistory. The metrics proposed by Chidamber and Kemerer are utilized for this\npurpose. This model will be useful for the medical field through software,\nparticularly in remote areas."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9349-3675", 
    "link": "http://arxiv.org/pdf/1212.1762v1", 
    "title": "A Change Support Model for Distributed Collaborative Work", 
    "arxiv-id": "1212.1762v1", 
    "author": "Koichiro Ochimizu", 
    "publish": "2012-12-08T06:31:55Z", 
    "summary": "Distributed collaborative software development tends to make artifacts and\ndecisions inconsistent and uncertain. We try to solve this problem by providing\nan information repository to reflect the state of works precisely, by managing\nthe states of artifacts/products made through collaborative work, and the\nstates of decisions made through communications. In this paper, we propose\nmodels and a tool to construct the artifact-related part of the information\nrepository, and explain the way to use the repository to resolve\ninconsistencies caused by concurrent changes of artifacts. We first show the\nmodel and the tool to generate the dependency relationships among UML model\nelements as content of the information repository. Next, we present the model\nand the method to generate change support workflows from the information\nrepository. These workflows give us the way to efficiently modify the\nchange-related artifacts for each change request. Finally, we define\ninconsistency patterns that enable us to be aware of the possibility of\ninconsistency occurrences. By combining this mechanism with version control\nsystems, we can make changes safely. Our models and tool are useful in the\nmaintenance phase to perform changes safely and efficiently."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9349-3675", 
    "link": "http://arxiv.org/pdf/1212.1796v1", 
    "title": "On Extracting Unit Tests from Interactive Programming Sessions", 
    "arxiv-id": "1212.1796v1", 
    "author": "Adrian Kuhn", 
    "publish": "2012-12-08T14:21:04Z", 
    "summary": "Software engineering methodologies propose that developers should capture\ntheir efforts in ensuring that programs run correctly in repeatable and\nautomated artifacts, such as unit tests. However, when looking at developer\nactivities on a spectrum from exploratory testing to scripted testing we find\nthat many engineering activities include bursts of exploratory testing. In this\npaper we propose to leverage these exploratory testing bursts by automatically\nextracting scripted tests from a recording of these sessions. In order to do\nso, we wiretap the development environment so we can record all program input,\nall user-issued functions calls, and all program output of an exploratory\ntesting session. We propose to then use machine learning (i.e. clustering) to\nextract scripted test cases from these recordings in real-time. We outline two\nearly-stage prototypes, one for a static and one for a dynamic language. And we\noutline how this idea fits into the bigger research direction of programming by\nexample."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9349-3675", 
    "link": "http://arxiv.org/pdf/1212.3060v1", 
    "title": "A use case driven approach for system level testing", 
    "arxiv-id": "1212.3060v1", 
    "author": "Zahid Hussain Qaisar", 
    "publish": "2012-12-13T06:07:22Z", 
    "summary": "Use case scenarios are created during the analysis phase to specify software\nsystem requirements and can also be used for creating system level test cases.\nUsing use cases to get system tests has several benefits including test design\nat early stages of software development life cycle that reduces over all\ndevelopment cost of the system. Current approaches for system testing using use\ncases involve functional details and does not include guards as passing\ncriteria i.e. use of class diagram that seem to be difficult at very initial\nlevel which lead the need of specification based testing without involving\nfunctional details. In this paper, we proposed a technique for system testing\ndirectly derived from the specification without involving functional details.\nWe utilize initial and post conditions applied as guards at each level of the\nuse cases that enables us generation of formalized test cases and makes it\npossible to generate test cases for each flow of the system. We used use case\nscenarios to generate system level test cases, whereas system sequence diagram\nis being used to bridge the gap between the test objective and test cases,\nderived from the specification of the system. Since, a state chart derived from\nthe combination of sequence diagrams can model the entire behavior of the\nsystem.Generated test cases can be employed and executed to state chart in\norder to capture behavior of the system with the state change.All these steps\nenable us to systematically refine the specification to achieve the goals of\nsystem testing at early development stages."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9349-3675", 
    "link": "http://arxiv.org/pdf/1212.3067v1", 
    "title": "The application of cause effect graph for the college placement process", 
    "arxiv-id": "1212.3067v1", 
    "author": "Namrata Ojha", 
    "publish": "2012-12-13T07:06:29Z", 
    "summary": "This paper presents a case study on the application of cause effect graph for\nrepresenting the college placement process. This paper begins with giving a\nbrief overview of the college placement process which will serve as the basis\nfor developing the cause effect graph and the decision table for the same in a\nsystematic manner. Finally, it concludes with the design of test cases thus\ngiving a complete and clear representation about the application of\ncause-effect graph in the software testing domain."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.3248v1", 
    "title": "A survey of service oriented architecture systems testing", 
    "arxiv-id": "1212.3248v1", 
    "author": "Ebrahim Shamsoddin-Motlagh", 
    "publish": "2012-12-13T18:15:26Z", 
    "summary": "Service oriented architecture (SOA) is one of the latest software\narchitectures. This architecture is created in direction of the business\nrequirements and removed the gap between softwares and businesses. The software\ntesting is the rising cost of activities in development software. SOA has\ndifferent specifications and features proportion of the other software\narchitectures. First this paper reviews SOA testing challenges and existing\nsolution(s) for those challenges. Then that reports a survey of recent research\nto SOA systems testing, that covers both functional and non-functional testing.\nThose are presented for different levels of functional testing, including unit,\nintegration, and regression testing."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.4246v1", 
    "title": "Sysml Knowledge base for Designing Dependable Complex System", 
    "arxiv-id": "1212.4246v1", 
    "author": "Nabil Sadou", 
    "publish": "2012-12-18T07:09:30Z", 
    "summary": "The work presented in this paper is part of a proposed framework as complete\nand rigorous as possible for the design of complex systems. The methodological\nframework used is System Engineering, which is a methodological approach to\ncontrol the design of complex systems. The practices of this approach are\ntranscribed in standards, realized by methods and supported by tools. In our\ncase, the standard EIA-632 was adopted. Specifically, to deal with the\ndependability of these complex systems and to improve the processes dealing\nwith dependability, we have defined a global approach. This approach\nincorporates the consideration of dependability in system engineering\nprocesses. The work presented in this paper supports and complements the\noverall approach: it is the proposal of an information model based on the SysML\nlanguage, allowing the requirements management, including safety requirements"
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.4258v1", 
    "title": "Compositional Verification of Evolving Software Product Lines", 
    "arxiv-id": "1212.4258v1", 
    "author": "Ganesh Khandu Narwane", 
    "publish": "2012-12-18T08:01:54Z", 
    "summary": "This paper presents a novel approach to the design verification of Software\nProduct Lines(SPL). The proposed approach assumes that the requirements and\ndesigns are modeled as finite state machines with variability information. The\nvariability information at the requirement and design levels are expressed\ndifferently and at different levels of abstraction. Also the proposed approach\nsupports verification of SPL in which new features and variability may be added\nincrementally. Given the design and requirements of an SPL, the proposed design\nverification method ensures that every product at the design level behaviorally\nconforms to a product at the requirement level. The conformance procedure is\ncompositional in the sense that the verification of an entire SPL consisting of\nmultiple features is reduced to the verification of the individual features.\nThe method has been implemented and demonstrated in a prototype tool SPLEnD\n(SPL Engine for Design Verification) on a couple of fairly large case studies."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.4959v1", 
    "title": "D'Event-B vers UML/OCL en passant par UML/EM-OCL", 
    "arxiv-id": "1212.4959v1", 
    "author": "Imen Sayar", 
    "publish": "2012-12-20T09:48:12Z", 
    "summary": "To overcome the limitations of both approaches classical and formal for the\ndevelopment of complex software, we proposed a hybrid approach combining the\nformal approach (Event-B) and the classical approach (UML/OCL). Upstream phases\nof our approach include: Rewriting the requirements document, Refinement\nstrategy, Abstract specification and Horizontal refinement. We have shown the\nfeasibility of our approach on a case study: An Electronic Hotel Key System\n(SCEH). The problem of transition from the formal (Event-B) to the semi-formal\n(UML/OCL) is processed through our extension to OCL (EM-OCL)."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.5089v1", 
    "title": "Dead code elimination based pointer analysis for multithreaded programs", 
    "arxiv-id": "1212.5089v1", 
    "author": "Mohamed A. El-Zawawy", 
    "publish": "2012-12-20T15:38:10Z", 
    "summary": "This paper presents a new approach for optimizing multitheaded programs with\npointer constructs. The approach has applications in the area of certified code\n(proof-carrying code) where a justification or a proof for the correctness of\neach optimization is required. The optimization meant here is that of dead code\nelimination.\n  Towards optimizing multithreaded programs the paper presents a new\noperational semantics for parallel constructs like join-fork constructs,\nparallel loops, and conditionally spawned threads. The paper also presents a\nnovel type system for flow-sensitive pointer analysis of multithreaded\nprograms. This type system is extended to obtain a new type system for\nlive-variables analysis of multithreaded programs. The live-variables type\nsystem is extended to build the third novel type system, proposed in this\npaper, which carries the optimization of dead code elimination. The\njustification mentioned above takes the form of type derivation in our\napproach."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2012.3602", 
    "link": "http://arxiv.org/pdf/1212.5204v1", 
    "title": "FReD: Automated Debugging via Binary Search through a Process Lifetime", 
    "arxiv-id": "1212.5204v1", 
    "author": "Gene Cooperman", 
    "publish": "2012-12-20T19:39:10Z", 
    "summary": "Reversible debuggers have been developed at least since 1970. Such a feature\nis useful when the cause of a bug is close in time to the bug manifestation.\nWhen the cause is far back in time, one resorts to setting appropriate\nbreakpoints in the debugger and beginning a new debugging session. For these\ncases when the cause of a bug is far in time from its manifestation, bug\ndiagnosis requires a series of debugging sessions with which to narrow down the\ncause of the bug.\n  For such \"difficult\" bugs, this work presents an automated tool to search\nthrough the process lifetime and locate the cause. As an example, the bug could\nbe related to a program invariant failing. A binary search through the process\nlifetime suffices, since the invariant expression is true at the beginning of\nthe program execution, and false when the bug is encountered. An algorithm for\nsuch a binary search is presented within the FReD (Fast Reversible Debugger)\nsoftware. It is based on the ability to checkpoint, restart and\ndeterministically replay the multiple processes of a debugging session. It is\nbased on GDB (a debugger), DMTCP (for checkpoint-restart), and a custom\ndeterministic record-replay plugin for DMTCP.\n  FReD supports complex, real-world multithreaded programs, such as MySQL and\nFirefox. Further, the binary search is robust. It operates on multi-threaded\nprograms, and takes advantage of multi-core architectures during replay."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39031-9_3", 
    "link": "http://arxiv.org/pdf/1212.5491v1", 
    "title": "Concurrent object-oriented development with behavioral design patterns", 
    "arxiv-id": "1212.5491v1", 
    "author": "Hassan Gomaa", 
    "publish": "2012-12-21T15:33:16Z", 
    "summary": "The development of concurrent applications is challenging because of the\ncomplexity of concurrent designs and the hazards of concurrent programming.\nArchitectural modeling using the Unified Modeling Language (UML) can support\nthe development process, but the problem of mapping the model to a concurrent\nimplementation remains. This paper addresses this problem by defining a scheme\nto map concurrent UML designs to a concurrent object-oriented program. Using\nthe COMET method for the architectural design of concurrent object-oriented\nsystems, each component and connector is annotated with a stereotype indicating\nits behavioral design pattern. For each of these patterns, a reference\nimplementation is provided using SCOOP, a concurrent object-oriented\nprogramming model. We evaluate this development process using a case study of\nan ATM system, obtaining a fully functional implementation based on the\nsystematic mapping of the individual patterns. Given the strong execution\nguarantees of the SCOOP model, which is free of data races by construction,\nthis development method eliminates a source of intricate concurrent programming\nerrors."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39031-9_3", 
    "link": "http://arxiv.org/pdf/1212.6041v1", 
    "title": "XML parser GUI using .NET Technology", 
    "arxiv-id": "1212.6041v1", 
    "author": "Jimbo Claver", 
    "publish": "2012-12-25T13:03:03Z", 
    "summary": "The purpose of this paper is to implement software that can save time,\neffort, and facilitate XML and XSL programming. The XML parser helps the\nprogrammer to determine whether the XML document is Well-formed or not, by\nspecifying if any the positions of the errors."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39031-9_3", 
    "link": "http://arxiv.org/pdf/1302.1153v1", 
    "title": "On the need for optimization of the software development processes in   short-term projects", 
    "arxiv-id": "1302.1153v1", 
    "author": "Jorge Rafael Aguilar Cisneros", 
    "publish": "2013-02-05T19:06:06Z", 
    "summary": "Nowadays, most of the software development projects in Mexico are short-term\nprojects (micro and small projects); for this reason, in this paper we are\npresenting a research proposal with the goal of identifying the elements\ncontributing to their success or failure. With this research, we are trying to\nidentify and propose techniques and tools that would contribute in the\nsuccessful outcome of these projects."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2013.4106", 
    "link": "http://arxiv.org/pdf/1302.1355v1", 
    "title": "Characterizing and Evaluating The Impact of Software Interface Clones", 
    "arxiv-id": "1302.1355v1", 
    "author": "Osama Shata", 
    "publish": "2013-02-06T13:16:44Z", 
    "summary": "Software Interfaces are meant to describe contracts governing interactions\nbetween logic modules. Interfaces, if well designed, significantly reduce\nsoftware complexity and ease maintainability . However, as software evolves,\nthe organization and the quality of software interfaces gradually deteriorate.\nAs a consequence, this often leads to increased development cost, lower code\nquality and reduced reusability . Code clones are one of the most known bad\nsmells in source code. This design defect may occur in interfaces by\nduplicating method/API declarations in several interfaces. Such interfaces are\nsimilar from the point of view of public services/APIs they specify, thus they\nindicate a bad organization of application services. In this paper, we\ncharacterize the interface clone design defect and illustrate it via examples\ntaken from real-world open source software applications. We conduct an\nempirical study covering nine real-world open source software applications to\nquantify the presence of interface clones and evaluate their impact on\ninterface design quality . The results of the empirical study show that\ninterface clones are widely present in software interfaces. They also show that\nthe presence of interface clones may cause a degradation of interface cohesion\nand indicate a considerable presence of code clones at implementations level."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.1393v1", 
    "title": "Semantic integration process of business components to support   information system designers", 
    "arxiv-id": "1302.1393v1", 
    "author": "Abderrahim Sekkaki", 
    "publish": "2013-02-06T15:01:33Z", 
    "summary": "The present work is inscribed within the intersection of two scientific\nthematic: the engineering by reuse of components and ontologies alignment. The\nintegration of Business Components (BC) is a research problem that has been\nidentified in the field of engineering by reuse. Our proposal aims to provide\nassistance to designers of information systems in the integration phase. It is\na process guided by domain ontology to provide semantic integration of BC. This\nprocess allows the detection and resolution of semantic conflicts naming type\nencountered in the process of integration of BC."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.1591v1", 
    "title": "Automatically Mining Program Build Information via Signature Matching", 
    "arxiv-id": "1302.1591v1", 
    "author": "Charng-Da Lu", 
    "publish": "2013-02-06T21:45:34Z", 
    "summary": "Program build information, such as compilers and libraries used, is vitally\nimportant in an auditing and benchmarking framework for HPC systems. We have\ndeveloped a tool to automatically extract this information using\nsignature-based detection, a common strategy employed by anti-virus software to\nsearch for known patterns of data within the program binaries. We formulate the\npatterns from various \"features\" embedded in the program binaries, and the\nexperiment shows that our tool can successfully identify many different\ncompilers, libraries, and their versions."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.1912v1", 
    "title": "Testing and Evaluation of Service Oriented Systems", 
    "arxiv-id": "1302.1912v1", 
    "author": "Ashim Raj Singla", 
    "publish": "2013-02-08T00:02:19Z", 
    "summary": "Evaluation of service oriented system has been a challenge, though there are\nlarge number of evaluation metrics exist but none of them is efficient to\nevaluate these systems effectively.This paper discusses the different testing\ntools and evaluation methods available for SOA and summarizes their limitation\nand support in context of service oriented architectures."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.2193v1", 
    "title": "Circumstantial-Evidence-Based Judgment for Software Effort Estimation", 
    "arxiv-id": "1302.2193v1", 
    "author": "He Zhang", 
    "publish": "2013-02-09T04:53:29Z", 
    "summary": "Expert judgment for software effort estimation is oriented toward direct\nevidences that refer to actual effort of similar projects or activities through\nexperts' experiences. However, the availability of direct evidences implies the\nrequirement of suitable experts together with past data. The\ncircumstantial-evidence-based judgment proposed in this paper focuses on the\ndevelopment experiences deposited in human knowledge, and can then be used to\nqualitatively estimate implementation effort of different proposals of a new\nproject by rational inference. To demonstrate the process of\ncircumstantial-evidence-based judgment, this paper adopts propositional\nlearning theory based diagnostic reasoning to infer and compare different\neffort estimates when implementing a Web service composition project with some\ndifferent techniques and contexts. The exemplar shows our proposed work can\nhelp determine effort tradeoff before project implementation. Overall,\ncircumstantial-evidence-based judgment is not an alternative but complementary\nto expert judgment so as to facilitate and improve software effort estimation."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.2657v1", 
    "title": "Metrics for Assessing The Design of Software Interfaces", 
    "arxiv-id": "1302.2657v1", 
    "author": "Osama Shata", 
    "publish": "2013-02-11T22:41:25Z", 
    "summary": "Recent studies have largely investigated the detection of class design\nanomalies. They proposed a large set of metrics that help in detecting those\nanomalies and in predicting the quality of class design. While those studies\nand the proposed metrics are valuable, they do not address the particularities\nof software interfaces. Interfaces define the contracts that spell out how\nsoftware modules and logic units interact with each other. This paper proposes\na list of design defects related to interfaces: shared similarity between\ninterfaces, interface clones and redundancy in interface hierarchy. We identify\nand describe those design defects through real examples, taken from well-known\nJava applications. Then we define three metrics that help in automatically\nestimating the interface design quality, regarding the proposed design\nanomalies, and identify refactoring candidates. We investigate our metrics and\nshow their usefulness through an empirical study conducted on three large Java\napplications."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.2747v1", 
    "title": "Effective factors in agile transformation process from change management   perspective", 
    "arxiv-id": "1302.2747v1", 
    "author": "Abu Bakar Md. Sultan", 
    "publish": "2013-02-12T10:09:13Z", 
    "summary": "After introducing agile approach in 2001, several agile methods were founded\nover the last decade. Agile values such as customer collaboration, embracing\nchanges, iteration and frequent delivery, continuous integration, etc. motivate\nall software stakeholders to use these methods in their projects. The main\nissue is that for using these methods instead of traditional methods in\nsoftware development, companies should change their approach from traditional\nto agile. This change is a fundamental and critical mutation. Several studies\nhave been done for investigating of barriers, challenges and issues in agile\nmovement process and also in how to use agile methods in companies. The main\nissue is altering attitude from traditional to agile approach. We believe that\nbefore managing agile transformation process, its related factors should be\nstudied in deep. This study focuses on different dimensions of changing\napproach to agile from change management perspective. These factors are how to\nbeing agile, method selection and awareness of challenges and issues. These\nfundamental factors encompass many items for agile movement and adoption\nprocess. However these factors may change in different organization, but they\nshould be studied in deep before any action plan for designing a change\nstrategy. The main contribution of this paper is introducing and these factors\nand discuss on them deeply."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijwest.2013.4104", 
    "link": "http://arxiv.org/pdf/1302.2748v1", 
    "title": "A Systematic Literature Review on relationship between agile methods and   Open Source Software Development methodology", 
    "arxiv-id": "1302.2748v1", 
    "author": "Abu Bakar Md Sultan", 
    "publish": "2013-02-12T10:09:42Z", 
    "summary": "Agile software development methods (ASD) and open source software development\nmethods (OSSD) are two different approaches which were introduced in last\ndecade and both of them have their fanatical advocators. Yet, it seems that\nrelation and interface between ASD and OSSD is a fertile area and few rigorous\nstudies have been done in this matter. Major goal of this study was assessment\nof the relation and integration of ASD and OSSD. Analyzing of collected data\nshows that ASD and OSSD are able to support each other. Some practices in one\nof them are useful in the other. Another finding is that however there are some\ncase studies using ASD and OSSD simultaneously, but there is not enough\nevidence about comprehensive integration of them."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.scico.2014.11.007", 
    "link": "http://arxiv.org/pdf/1302.4061v1", 
    "title": "The Sensemaking-Coevolution-Implementation Theory of Software Design", 
    "arxiv-id": "1302.4061v1", 
    "author": "Paul Ralph", 
    "publish": "2013-02-17T12:09:46Z", 
    "summary": "Understanding software design practice is critical to understanding modern\ninformation systems development. New developments in empirical software\nengineering, information systems design science and the interdisciplinary\ndesign literature combined with recent advances in process theory and\ntestability have created a situation ripe for innovation. Consequently, this\npaper utilizes these breakthroughs to formulate a process theory of software\ndesign practice: Sensemaking-Coevolution-Implementation Theory explains how\ncomplex software systems are created by collocated software development teams\nin organizations. It posits that an independent agent (design team) creates a\nsoftware system by alternating between three activities: organizing their\nperceptions about the context, mutually refining their understandings of the\ncontext and design space, and manifesting their understanding of the design\nspace in a technological artifact. This theory development paper defines and\nillustrates Sensemaking-Coevolution-Implementation Theory, grounds its concepts\nand relationships in existing literature, conceptually evaluates the theory and\nsituates it in the broader context of information systems development."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.108", 
    "link": "http://arxiv.org/pdf/1302.4780v1", 
    "title": "Proceedings 10th International Workshop on Formal Engineering Approaches   to Software Components and Architectures", 
    "arxiv-id": "1302.4780v1", 
    "author": "Jan Kofro\u0148", 
    "publish": "2013-02-20T00:34:13Z", 
    "summary": "These are the proceedings of the 10th International Workshop on Formal\nEngineering approaches to Software Components and Architectures (FESCA). The\nworkshop was held on March 23, 2013 in Rome (Italy) as a satellite event to the\nEuropean Joint Conference on Theory and Practice of Software (ETAPS'13).\n  The aim of the FESCA workshop is to bring together both young and senior\nresearchers from formal methods, software engineering, and industry interested\nin the development and application of formal modelling approaches as well as\nassociated analysis and reasoning techniques with practical benefits for\ncomponent-based software engineering.\n  FESCA aims to address the open question of how formal methods can be applied\neffectively to these new contexts and challenges. FESCA is interested in both\nthe development and application of formal methods in component-based\ndevelopment and tries to cross-fertilize their research and application."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.108.1", 
    "link": "http://arxiv.org/pdf/1302.5169v1", 
    "title": "Extensible Technology-Agnostic Runtime Verification", 
    "arxiv-id": "1302.5169v1", 
    "author": "Gordon J. Pace", 
    "publish": "2013-02-21T03:58:47Z", 
    "summary": "With numerous specialised technologies available to industry, it has become\nincreasingly frequent for computer systems to be composed of heterogeneous\ncomponents built over, and using, different technologies and languages. While\nthis enables developers to use the appropriate technologies for specific\ncontexts, it becomes more challenging to ensure the correctness of the overall\nsystem. In this paper we propose a framework to enable extensible technology\nagnostic runtime verification and we present an extension of polyLarva, a\nruntime-verification tool able to handle the monitoring of\nheterogeneous-component systems. The approach is then applied to a case study\nof a component-based artefact using different technologies, namely C and Java."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.108.2", 
    "link": "http://arxiv.org/pdf/1302.5170v1", 
    "title": "Sequence Diagram Test Case Specification and Virtual Integration   Analysis using Timed-Arc Petri Nets", 
    "arxiv-id": "1302.5170v1", 
    "author": "Peter Battram", 
    "publish": "2013-02-21T03:59:02Z", 
    "summary": "In this paper, we formally define Test Case Sequence Diagrams (TCSD) as an\neasy-to-use means to specify test cases for components including timing\nconstraints. These test cases are modeled using the UML2 syntax and can be\nspecified by standard UML-modeling-tools. In a component-based design an early\nidentification of errors can be achieved by a virtual integration of components\nbefore the actual system is build. We define such a procedure which integrates\nthe individual test cases of the components according to the interconnections\nof a given architecture and checks if all specified communication sequences are\nconsistent. Therefore, we formally define the transformation of TCSD into\ntimed-arc Petri nets and a process for the combination of these nets. The\napplicability of our approach is demonstrated on an avionic use case from the\nARP4761 standard."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.108.2", 
    "link": "http://arxiv.org/pdf/1302.5454v1", 
    "title": "Statistical Approach for Predicting Factors of Mood Method for Object   Oriented", 
    "arxiv-id": "1302.5454v1", 
    "author": "Fawzi Altaani", 
    "publish": "2013-02-22T00:03:48Z", 
    "summary": "Object oriented design is becoming more popular in software development and\nobject oriented design metrics which is an essential part of software\nenvironment. The main goal in this paper is to predict factors of MOOD method\nfor OO using a statistical approach. Therefore, linear regression model is used\nto find the relationship between factors of MOOD method and their influences on\nOO software measurements. Fortunately, through this process a prediction could\nbe made for the line of code (LOC), number of classes (NOC), number of methods\n(NOM), and number of attributes (NOA). These measurements permit designers to\naccess the software early in process, making changes that will reduce\ncomplexity and improve the continuing capability of the design."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.108.2", 
    "link": "http://arxiv.org/pdf/1302.5788v1", 
    "title": "Unified Modeling Language for Describing Business Value Chain Activities", 
    "arxiv-id": "1302.5788v1", 
    "author": "Ashim Raj Singla", 
    "publish": "2013-02-23T10:56:05Z", 
    "summary": "With the market competition aggravating, it becomes necessary for market\nplayers to adopt a business model which can adopt dynamic business changes. Any\nenterprise has the possibility to win in the competition only when it forms the\nstrategic alliance with the upstream and downstream enterprise. This paper\narticulates a way of using unified modelling language (UML) to develop business\nvalue chain activities for any enterprise to develop dynamic, adhoc and agile\nbusiness model. The results show that the UML is useful in the development of\ninformation systems and is independent of any programming language."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111", 
    "link": "http://arxiv.org/pdf/1303.0379v1", 
    "title": "Proceedings Eighth Workshop on Model-Based Testing", 
    "arxiv-id": "1303.0379v1", 
    "author": "Holger Schlingloff", 
    "publish": "2013-03-02T11:53:08Z", 
    "summary": "This volume contains the proceedings of the Eighth Workshop on Model-Based\nTesting (MBT 2013), which was held on March 17, 2013 in Rome, Italy, as a\nsatellite event of the European Joint Conferences on Theory and Practice of\nSoftware, ETAPS 2013.\n  The workshop is devoted to model-based testing of both software and hardware.\nModel-based testing uses models describing the required behavior of the system\nunder consideration to guide such efforts as test selection and test results\nevaluation. Testing validates the real system behavior against models and\nchecks that the implementation conforms to them, but is capable also to find\nerrors in the models themselves.\n  The first MBT workshop was held in 2004, in Barcelona. At that time MBT\nalready had become a hot topic, but the MBT workshop was the first event\ndevoted mostly to this domain. Since that time the area has generated enormous\nscientific interest, and today there are several specialized workshops and more\nbroad conferences on software and hardware design and quality assurance\ncovering model based testing. MBT has become one of the most powerful system\nanalysis tools, one of the latest cutting-edge topics related is applying MBT\nin security analysis and testing. MBT workshop tries to keep up with current\ntrends."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111", 
    "link": "http://arxiv.org/pdf/1303.0395v1", 
    "title": "ICT System Design & Implementation Using Wireless Sensors to Support   Elderly In-home Assistance", 
    "arxiv-id": "1303.0395v1", 
    "author": "Stefan Plank", 
    "publish": "2013-03-02T16:08:18Z", 
    "summary": "Around the globe the number of older people in relation to the rest is\nconstantly growing. As a result, medical and care facilities cannot handle the\ngrowing number of patients. Therefore, elderly in-home assistance gets more\nattention an importance. Due to issues regarding memory, physical strength and\nreduced self-assessment, old people face a lot of challenges in accomplishing\ntheir activities of daily living. This thesis is meant to address these\nproblems by analysing the required infrastructure of a home-care facility as\nwell as the arising issues regarding used components, especially wireless\nsensors. After the analysis, a prototype of a home-care system is designed and\nimplemented. Furthermore, the issue of energy consumption of the used wireless\nsensor node is addressed by modifying the intelligence of the used sensor.\nAfter that, the design and components of the prototype used for the energy\nconsumption analysis is explained, together with the programming structure of\nthe sensor nodes used in this thesis. Thereupon, the results are of the\nsimulations are discussed and compared with the authors' expectations. Finally\nthe overall outcomes of the thesis are analysed and summed up, followed by a\nshort outlook of further possible improvements and developments."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.1", 
    "link": "http://arxiv.org/pdf/1303.1006v1", 
    "title": "Industrial-Strength Model-Based Testing - State of the Art and Current   Challenges", 
    "arxiv-id": "1303.1006v1", 
    "author": "Jan Peleska", 
    "publish": "2013-03-05T12:18:09Z", 
    "summary": "As of today, model-based testing (MBT) is considered as leading-edge\ntechnology in industry. We sketch the different MBT variants that - according\nto our experience - are currently applied in practice, with special emphasis on\nthe avionic, railway and automotive domains. The key factors for successful\nindustrial-scale application of MBT are described, both from a scientific and a\nmanagerial point of view. With respect to the former view, we describe the\ntechniques for automated test case, test data and test procedure generation for\nconcurrent reactive real-time systems which are considered as the most\nimportant enablers for MBT in practice. With respect to the latter view, our\nexperience with introducing MBT approaches in testing teams are sketched.\nFinally, the most challenging open scientific problems whose solutions are\nbound to improve the acceptance and effectiveness of MBT in industry are\ndiscussed."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.3", 
    "link": "http://arxiv.org/pdf/1303.1007v1", 
    "title": "Towards the Usage of MBT at ETSI", 
    "arxiv-id": "1303.1007v1", 
    "author": "Milan Zoric", 
    "publish": "2013-03-05T12:18:14Z", 
    "summary": "In 2012 the Specialists Task Force (STF) 442 appointed by the European\nTelcommunication Standards Institute (ETSI) explored the possibilities of using\nModel Based Testing (MBT) for test development in standardization. STF 442\nperformed two case studies and developed an MBT-methodology for ETSI. The case\nstudies were based on the ETSI-standards GeoNetworking protocol (ETSI TS 102\n636) and the Diameter-based Rx protocol (ETSI TS 129 214). Models have been\ndeveloped for parts of both standards and four different MBT-tools have been\nemployed for generating test cases from the models. The case studies were\nsuccessful in the sense that all the tools were able to produce the test suites\nhaving the same test adequacy as the corresponding manually developed\nconformance test suites. The MBT-methodology developed by STF 442 is based on\nthe experiences with the case studies. It focusses on integrating MBT into the\nsophisticated standardization process at ETSI. This paper summarizes the\nresults of the STF 442 work."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.4", 
    "link": "http://arxiv.org/pdf/1303.1008v1", 
    "title": "Testing Java implementations of algebraic specifications", 
    "arxiv-id": "1303.1008v1", 
    "author": "Filipe Lu\u00eds", 
    "publish": "2013-03-05T12:18:23Z", 
    "summary": "In this paper we focus on exploiting a specification and the structures that\nsatisfy it, to obtain a means of comparing implemented and expected behaviours\nand find the origin of faults in implementations. We present an approach to the\ncreation of tests that are based on those specification-compliant structures,\nand to the interpretation of those tests' results leading to the discovery of\nthe method responsible for an eventual test failure. Results of comparative\nexperiments with a tool implementing this approach are presented."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.6", 
    "link": "http://arxiv.org/pdf/1303.1010v1", 
    "title": "Runtime Verification Based on Executable Models: On-the-Fly Matching of   Timed Traces", 
    "arxiv-id": "1303.1010v1", 
    "author": "Alexander Kamkin", 
    "publish": "2013-03-05T12:18:37Z", 
    "summary": "Runtime verification is checking whether a system execution satisfies or\nviolates a given correctness property. A procedure that automatically, and\ntypically on the fly, verifies conformance of the system's behavior to the\nspecified property is called a monitor. Nowadays, a variety of formalisms are\nused to express properties on observed behavior of computer systems, and a lot\nof methods have been proposed to construct monitors. However, it is a frequent\nsituation when advanced formalisms and methods are not needed, because an\nexecutable model of the system is available. The original purpose and structure\nof the model are out of importance; rather what is required is that the system\nand its model have similar sets of interfaces. In this case, monitoring is\ncarried out as follows. Two \"black boxes\", the system and its reference model,\nare executed in parallel and stimulated with the same input sequences; the\nmonitor dynamically captures their output traces and tries to match them. The\nmain problem is that a model is usually more abstract than the real system,\nboth in terms of functionality and timing. Therefore, trace-to-trace matching\nis not straightforward and allows the system to produce events in different\norder or even miss some of them. The paper studies on-the-fly conformance\nrelations for timed systems (i.e., systems whose inputs and outputs are\ndistributed along the time axis). It also suggests a practice-oriented\nmethodology for creating and configuring monitors for timed systems based on\nexecutable models. The methodology has been successfully applied to a number of\nindustrial projects of simulation-based hardware verification."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.7", 
    "link": "http://arxiv.org/pdf/1303.1011v1", 
    "title": "Top-Down and Bottom-Up Approach for Model-Based Testing of Product Lines", 
    "arxiv-id": "1303.1011v1", 
    "author": "Hartmut Lackner", 
    "publish": "2013-03-05T12:18:52Z", 
    "summary": "Systems tend to become more and more complex. This has a direct impact on\nsystem engineering processes. Two of the most important phases in these\nprocesses are requirements engineering and quality assurance. Two significant\ncomplexity drivers located in these phases are the growing number of product\nvariants that have to be integrated into the requirements engineering and the\never growing effort for manual test design. There are modeling techniques to\ndeal with both complexity drivers like, e.g., feature modeling and model-based\ntest design. Their combination, however, has been seldom the focus of\ninvestigation. In this paper, we present two approaches to combine feature\nmodeling and model-based testing as an efficient quality assurance technique\nfor product lines. We present the corresponding difficulties and approaches to\novercome them. All explanations are supported by an example of an online shop\nproduct line."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.111.7", 
    "link": "http://arxiv.org/pdf/1303.1238v1", 
    "title": "Toward the Integration of Traditional and Agile Approaches", 
    "arxiv-id": "1303.1238v1", 
    "author": "Stephen C-Y. Lu", 
    "publish": "2013-03-06T02:06:59Z", 
    "summary": "The agile approach uses continuous delivery, instead of distinct procedure,\nto work closer with customers and to respond faster requirement changes. All of\nthese are against the traditional plan driven approach. Due to agile method's\ncharacteristics and its success in the real world practices, a number of\ndiscussions regarding the differences between agile and traditional approaches\nemerged recently and many studies intended to integrate both methods to\nsynthesize the benefits from these two sides. However, this type of research\noften concludes from observations of a development activity or surveys after a\nproject. To provide a more objective supportive evidence of comparing these two\napproaches, our research analyzes the source codes, logs, and notes. We argue\nthat the agile and traditional approaches share common characteristics, which\ncan be considered as the glue for integrating both methods. In our study, we\ncollect all the submissions from the version control repository, and meeting\nnotes and discussions. By applying our suggested analysis method, we illustrate\nthe shared properties between agile and traditional approaches; thus, different\ndevelopment phases, like implementation and test, can still be identified in\nagile development history. This result not only provides a positive result for\nour hypothesis but also offers a suggestion for a better integration."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.1971v1", 
    "title": "Improving the management of cost and scope in software projects using   agile practices", 
    "arxiv-id": "1303.1971v1", 
    "author": "Enio J\u00fanior Seidel", 
    "publish": "2013-03-08T12:28:44Z", 
    "summary": "While organizations want to develop software products with reduced cost and\nflexible scope, stories about the applicability of agile practices to improve\nproject development and performance in the software industry are scarce and\nfocused on specific methodologies such as Scrum and XP. Given these facts, this\npaper aims to investigate, through practitioners' perceptions of value, which\nagile practices are being used to improve two performance criteria for software\nprojects-cost and scope. Using a multivariate statistical technique known as\nExploratory Factor Analysis (EFA), the results suggest that the use of agile\npractices can be represented in factors which describe different applications\nin software development process to improve cost and scope. Also, we conclude\nthat some agile practices should be used together in order to get better\nefficiency on cost and scope in four development aspects: improving (a) team\nabilities, (b)management of requirements, (c) quality of the code developed,\nand (d) delivery of software on-budget and on-time."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.2554v1", 
    "title": "Artifact Lifecycle Discovery", 
    "arxiv-id": "1303.2554v1", 
    "author": "Marlon Dumas", 
    "publish": "2013-03-11T15:51:28Z", 
    "summary": "Artifact-centric modeling is a promising approach for modeling business\nprocesses based on the so-called business artifacts - key entities driving the\ncompany's operations and whose lifecycles define the overall business process.\nWhile artifact-centric modeling shows significant advantages, the overwhelming\nmajority of existing process mining methods cannot be applied (directly) as\nthey are tailored to discover monolithic process models. This paper addresses\nthe problem by proposing a chain of methods that can be applied to discover\nartifact lifecycle models in Guard-Stage-Milestone notation. We decompose the\nproblem in such a way that a wide range of existing (non-artifact-centric)\nprocess discovery and analysis methods can be reused in a flexible manner. The\nmethods presented in this paper are implemented as software plug-ins for ProM,\na generic open-source framework and architecture for implementing process\nmining tools."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.2646v1", 
    "title": "Work Issues in Software Engineering", 
    "arxiv-id": "1303.2646v1", 
    "author": "Jeremy Leipzig", 
    "publish": "2013-03-11T19:55:14Z", 
    "summary": "Using data from a web-based survey of software developers, the author\nattempts to determine root causes of \"death march\" projects and excessive work\nhours in the software industry in relation to company practices and management.\nSpecial emphasis is placed on the factor of business/technical supervisor\nbackground. An analysis of variance revealed significant differences between\nthese supervisor groups with regard to a \"Pointy-Haired Boss\" (PHB) sentiment\nindex. This difference, combined with correlations between the PHB index and\nthe endpoints of project failure and use of software engineering practices,\nindicate some disparity in the suitability of businessbackground supervisors to\nmanage software development projects compared with their technical-background\ncounterparts. Other survey data points to improved project management skills as\nthe biggest necessity for supervisors in the business-background group."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.2784v1", 
    "title": "Using State Infection Conditions to Detect Equivalent Mutants and Speed   up Mutation Analysis", 
    "arxiv-id": "1303.2784v1", 
    "author": "Gordon Fraser", 
    "publish": "2013-03-12T06:18:16Z", 
    "summary": "Mutation analysis evaluates test suites and testing techniques by measuring\nhow well they detect seeded defects (mutants). Even though well established in\nresearch, mutation analysis is rarely used in practice due to scalability\nproblems --- there are multiple mutations per code statement leading to a large\nnumber of mutants, and hence executions of the test suite. In addition, the use\nof mutation to improve test suites is futile for mutants that are equivalent,\nwhich means that there exists no test case that distinguishes them from the\noriginal program.\n  This paper introduces two optimizations based on state infection conditions,\ni.e., conditions that determine for a test execution whether the same execution\non a mutant would lead to a different state. First, redundant test execution\ncan be avoided by monitoring state infection conditions, leading to an overall\nperformance improvement. Second, state infection conditions can aid in\nidentifying equivalent mutants, thus guiding efforts to improve test suites."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.2966v1", 
    "title": "Automatic instantiation of abstract tests on specific configurations for   large critical control systems", 
    "arxiv-id": "1303.2966v1", 
    "author": "Antonio Orazzo", 
    "publish": "2013-03-12T17:47:01Z", 
    "summary": "Computer-based control systems have grown in size, complexity, distribution\nand criticality. In this paper a methodology is presented to perform an\nabstract testing of such large control systems in an efficient way: an abstract\ntest is specified directly from system functional requirements and has to be\ninstantiated in more test runs to cover a specific configuration, comprising\nany number of control entities (sensors, actuators and logic processes). Such a\nprocess is usually performed by hand for each installation of the control\nsystem, requiring a considerable time effort and being an error prone\nverification activity. To automate a safe passage from abstract tests, related\nto the so called generic software application, to any specific installation, an\nalgorithm is provided, starting from a reference architecture and a state-based\nbehavioural model of the control software. The presented approach has been\napplied to a railway interlocking system, demonstrating its feasibility and\neffectiveness in several years of testing experience."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5104", 
    "link": "http://arxiv.org/pdf/1303.4056v1", 
    "title": "A weaving process to define requirements for Cooperative Information   System", 
    "arxiv-id": "1303.4056v1", 
    "author": "Pierre Jean Charrel", 
    "publish": "2013-03-17T12:13:55Z", 
    "summary": "The development of a Cooperative Information System (CIS) becomes more and\nmore complex, new challenges arise for managing this complexity. So, the aspect\nparadigm is regarded as a promising software development technique which can\nreduce the complexity and cost of developing large software systems. This\nopportunity can be used to develop a CIS able to support the interconnection of\norganizations information systems in order to ensure a common global service\nand to support the tempo of change in the business world that is increasing at\nan exponential level. We previously proposed an approach named AspeCiS (An\nAspect-oriented Approach to Develop a Cooperative Information System) to\ndevelop a Cooperative Information System from existing Information Systems by\nusing their artifacts such as existing requirements, and design. In this\napproach we have studied how to elicit CIS Requirements called Cooperative\nRequirements in AspeCiS. In this paper we propose a weaving process to define\nthese requirements by reusing existing requirements and new aspectual\nrequirements that we define to modify these requirements in order to be reused."
},{
    "category": "cs.SE", 
    "doi": "10.4236/jsea.2013.68050", 
    "link": "http://arxiv.org/pdf/1303.4761v1", 
    "title": "IEC 61499 vs. 61131: A Comparison Based on Misperceptions", 
    "arxiv-id": "1303.4761v1", 
    "author": "Kleanthis Thramboulidis", 
    "publish": "2013-03-19T20:42:40Z", 
    "summary": "IEC 61131 has been widely accepted in the industrial automation domain.\nHowever, it is claimed that the standard does not address today the new\nrequirements of complex industrial systems, which include among others,\nportability, interoperability, increased reusability and distribution. To\naddress these restrictions, IEC has initiated the task of developing IEC 61499,\nwhich is presented as a mature technology to enable intelligent automation in\nvarious domains. This standard was not accepted by industry even though it is\nhighly promoted by the academic community. In this paper, a comparison between\nthe two standards is presented. We argue that IEC 61499 has been promoted by\nacademy based on unsubstantiated claims on its main features, i.e.,\nreusability, portability, interoperability, event-driven execution. A number of\nmisperceptions are presented and discussed. Based on this, it is claimed that\nIEC 61499 does not provide a solid framework for the next generation of\nindustrial automation systems."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.5541v1", 
    "title": "Lowering the Barrier to Reuse through Test-Driven Search", 
    "arxiv-id": "1303.5541v1", 
    "author": "Colin Atkinson", 
    "publish": "2013-03-22T08:31:49Z", 
    "summary": "Dedicated software search engines that index open source software\nrepositories or in-house software assets significantly enhance the chance of\nfinding software components suitable for reuse. However, they still leave the\nwork of evaluating and testing components to the developer. To significantly\nchange the risk-cost-benefit tradeoff involved in software reuse, search\nengines need to be supported by user friendly environments that deliver code\nsearch functionality non-intrusively right to developers' fingertips."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.5874v1", 
    "title": "BIMS: Biomedical Information Management System", 
    "arxiv-id": "1303.5874v1", 
    "author": "Jes\u00fas Bisbal", 
    "publish": "2013-03-23T18:54:02Z", 
    "summary": "In this paper, we present BIMS (Biomedical Information Management System).\nBIMS is a software architecture designed to provide a flexible computational\nframework to manage the information needs of a wide range of biomedical\nresearch projects. The main goal is to facilitate the clinicians' job in data\nentry, and researcher's tasks in data management, in high data quality\nbiomedical research projects. The BIMS architecture has been designed following\nthe two-level modeling paradigm, a promising methodology to model rich and\ndynamic information environments. In addition, a functional implementation of\nBIMS architecture has been developed as a web-based application. The result is\na highly flexible web application which allows modeling and managing large\namounts of heterogeneous biomedical data sets, both textual as well as visual\n(medical images) information."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.5926v1", 
    "title": "STC: Semantic Taxonomical Clustering for Service Category Learning", 
    "arxiv-id": "1303.5926v1", 
    "author": "Yugyung Lee", 
    "publish": "2013-03-24T08:30:44Z", 
    "summary": "Service discovery is one of the key problems that has been widely researched\nin the area of Service Oriented Architecture (SOA) based systems. Service\ncategory learning is a technique for efficiently facilitating service\ndiscovery. Most approaches for service category learning are based on suitable\nsimilarity distance measures using thresholds. Threshold selection is\nessentially difficult and often leads to unsatisfactory accuracy. In this\npaper, we have proposed a self-organizing based clustering algorithm called\nSemantic Taxonomical Clustering (STC) for taxonomically organizing services\nwith self-organizing information and knowledge. We have tested the STC\nalgorithm on both randomly generated data and the standard OWL-S TC dataset. We\nhave observed promising results both in terms of classification accuracy and\nruntime performance compared to existing approaches."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.5938v1", 
    "title": "The Two Paradigms of Software Design", 
    "arxiv-id": "1303.5938v1", 
    "author": "Paul Ralph", 
    "publish": "2013-03-24T11:21:16Z", 
    "summary": "The dominant view of design in information systems and software engineering,\nthe Rational Design Paradigm, views software development as a methodical,\nplan-centered, approximately rational process of optimizing a design candidate\nfor known constraints and objectives. This paper synthesizes an Alternative\nDesign Paradigm, which views software development as an amethodical,\nimprovisational, emotional process of simultaneously framing the problem and\nbuilding artifacts to address it. These conflicting paradigms are\nmanifestations of a deeper philosophical conflict between rationalism and\nempiricism. The paper clarifies the nature, components and assumptions of each\nparadigm and explores the implications of the paradigmatic conflict for\nresearch, practice and education."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.5950v1", 
    "title": "High Quality Requirement Engineering and Applying Priority Based Tools   for QoS Standardization in Web Service Architecture", 
    "arxiv-id": "1303.5950v1", 
    "author": "C. Dinesh", 
    "publish": "2013-03-24T14:23:36Z", 
    "summary": "Even though there are more development to improving the Quality of Service\nand requirement engineering in web services yet there is a big scarcity for its\nrelated standardization in day to day progress leading to vast needs in its\narea. Also in web service environment it always has been a big challenge to\nraise the standard of Quality of Service in requirement engineering analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1303.7379v1", 
    "title": "Control Explicit---Data Symbolic Model Checking: An Introduction", 
    "arxiv-id": "1303.7379v1", 
    "author": "Petr Bauch", 
    "publish": "2013-03-29T12:40:54Z", 
    "summary": "A comprehensive verification of parallel software imposes three crucial\nrequirements on the procedure that implements it. Apart from accepting real\ncode as program input and temporal formulae as specification input, the\nverification should be exhaustive, with respect to both control and data flows.\nThis paper is concerned with the third requirement, proposing to combine\nexplicit model checking to handle the control with symbolic set representations\nto handle the data. The combination of explicit and symbolic approaches is\nfirst investigated theoretically and we report the requirements on the symbolic\nrepresentation and the changes to the model checking process the combination\nentails. The feasibility and efficiency of the combination is demonstrated on a\ncase study using the DVE modelling language and we report a marked improvement\nin scalability compared to previous solutions. The results described in this\npaper show the potential to meet all three requirements for automatic\nverification in a single procedure combining explicit model checking with\nsymbolic set representations."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2009.5070015", 
    "link": "http://arxiv.org/pdf/1308.0818v1", 
    "title": "Effects of Individual Success on Globally Distributed Team Performance", 
    "arxiv-id": "1308.0818v1", 
    "author": "Onur Y\u0131lmaz", 
    "publish": "2013-08-04T15:49:30Z", 
    "summary": "Necessity of different competencies with high level of knowledge makes it\ninevitable that software development is a team work. With the today's\ntechnology, teams can communicate both synchronously and asynchronously using\ndifferent online collaboration tools throughout the world. Researches indicate\nthat there are many factors that affect the team success and in this paper,\neffect of individual success on globally distributed team performance will be\nanalyzed. Student team projects undertaken by other researchers will be used to\nanalyze collected data and conclusions will be drawn for further analysis."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.0938v1", 
    "title": "Handling Parallelism in a Concurrency Model", 
    "arxiv-id": "1308.0938v1", 
    "author": "Bertrand Meyer", 
    "publish": "2013-08-05T11:13:59Z", 
    "summary": "Programming models for concurrency are optimized for dealing with\nnondeterminism, for example to handle asynchronously arriving events. To shield\nthe developer from data race errors effectively, such models may prevent shared\naccess to data altogether. However, this restriction also makes them unsuitable\nfor applications that require data parallelism. We present a library-based\napproach for permitting parallel access to arrays while preserving the safety\nguarantees of the original model. When applied to SCOOP, an object-oriented\nconcurrency model, the approach exhibits a negligible performance overhead\ncompared to ordinary threaded implementations of two parallel benchmark\nprograms."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.1901v1", 
    "title": "Distributed Object Store Principles of Operation The Case for   Intelligent Storage", 
    "arxiv-id": "1308.1901v1", 
    "author": "Robert Primmer", 
    "publish": "2013-08-08T16:33:24Z", 
    "summary": "In this paper we look at the growth of distributed object stores (DOS) and\nexamine the underlying mechanisms that guide their use and development. Our\nfocus is on the fundamental principles of operation that define this class of\nsystem, how it has evolved, and where it is heading as new markets expand\nbeyond the use originally presented. We conclude by speculating about how\nobject stores as a class must evolve to meet the more demanding requirements of\nfuture applications."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.2468v1", 
    "title": "Path Conditions Help to Locate and Localize Faults from Programs", 
    "arxiv-id": "1308.2468v1", 
    "author": "Ayaz Keerio", 
    "publish": "2013-08-12T06:01:14Z", 
    "summary": "Precisely and automatically detection of faults in programs, is a software\nengineering dream. Every effort in this regard takes us one step closer to\nrealizing it. Many efforts have been taken from the people of these areas on\ntesting, verification and debugging. We are proposing such effort for the\nresearch community of this domain is using path conditions to generate a\nminimal set of PLOFC (possible lines of faulty code). It's a run time method\nthat will effectively bring the minimal possible set of faulty lines of code\nthrough the help of path conditions and some heuristics involved. In this paper\nwe are generating possible fault locations from programs using path conditions\nwhich can put positive impact on the static analysis of programs. Further we\ndiscuss the basic ideas regarding path conditions, the theory, and first\nanalysis results. This work is based on a previous work that uses the variable\ndependences for fault detection. We showed some examples to the applicable of\nthis idea and can be useful for software verification."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.2876v1", 
    "title": "Work Breakdown Structure: A Tool for Software Project Scope Verification", 
    "arxiv-id": "1308.2876v1", 
    "author": "Robert T. Hans", 
    "publish": "2013-08-13T14:23:28Z", 
    "summary": "Software project scope verification is a very important process in project\nscope management and it needs to be performed properly and thoroughly so as to\navoid project rework and scope creep. Moreover, software scope verification is\ncrucial in the process of delivering exactly what the customer requested and\nminimizing project scope changes. Well defined software scope eases the process\nof scope verification and contributes to project success. Furthermore, a\ndeliverable-oriented WBS provides a road map to a well defined software scope\nof work. It is on the basis of this that this paper extends the use of\ndeliverable-oriented WBS to that of scope verification process. This paper\nargues that a deliverable-oriented WBS is a tool for software scope\nverification."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.3320v1", 
    "title": "Improving the Testability of Object-oriented Software during Testing and   Debugging Processes", 
    "arxiv-id": "1308.3320v1", 
    "author": "V. B. Singh", 
    "publish": "2013-08-15T06:56:26Z", 
    "summary": "Testability is the probability whether tests will detect a fault, given that\na fault in the program exists. How efficiently the faults will be uncovered\ndepends upon the testability of the software. Various researchers have proposed\nqualitative and quantitative techniques to improve and measure the testability\nof software. In literature, a plethora of reliability growth models have been\nused to assess and measure the quantitative quality assessment of software\nduring testing and operational phase. The knowledge about failure distribution\nand their complexity can improve the testability of software. Testing effort\nallocation can be made easy by knowing the failure distribution and complexity\nof faults, and this will ease the process of revealing faults from the\nsoftware. As a result, the testability of the software will be improved. The\nparameters of the model along with the proportion of faults of different\ncomplexity to be removed from the software have been presented in the paper .We\nhave used failure data of two object oriented software developed under open\nsource environment namely MySQL for python and Squirrel SQL Client for\nestimation purpose"
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-39955-8_4", 
    "link": "http://arxiv.org/pdf/1308.4045v1", 
    "title": "Efficient Leverage of Symbolic ATG Tools to Advanced Coverage Criteria", 
    "arxiv-id": "1308.4045v1", 
    "author": "Fran\u00e7ois Cheynier", 
    "publish": "2013-08-19T14:53:37Z", 
    "summary": "Automatic test data generation (ATG) is a major topic in software\nengineering. In this paper, we seek to bridge the gap between the coverage\ncriteria supported by symbolic ATG tools and the most advanced coverage\ncriteria found in the literature. We define a new testing criterion, label\ncoverage, and prove it to be both expressive and amenable to efficient\nautomation. We propose several innovative techniques resulting in an effective\nblack-box support for label coverage, while a direct approach induces an\nexponential blow-up of the search space. Initial experiments show that ATG for\nlabel coverage can be achieved at a reasonable cost and that our optimisations\nyield very significant savings."
},{
    "category": "cs.SE", 
    "doi": "10.7287/peerj.preprints.51v1", 
    "link": "http://arxiv.org/pdf/1308.4978v1", 
    "title": "Traverse the landscape of the mind by walking: an exploration of a new   brainstorming practice", 
    "arxiv-id": "1308.4978v1", 
    "author": "Pekka Abrahamsson", 
    "publish": "2013-08-22T20:00:21Z", 
    "summary": "Group brainstorming is a well-known idea generation technique, which plays a\nkey role in software development processes. Despite this, the relevant\nliterature has had little to offer in advancing our understanding of the\neffectiveness of group brainstorming sessions. In this paper we present a\nresearch-in-progress on brainstorming while walking, which is a practice built\nupon the relationship between thinking and walking. The objective is to better\nunderstand how to conduct group brainstorming effectively. We compared two\nbrainstorming sessions, one performed during a mountain walk, the other\ntraditionally in a room. Three preliminary findings are obtained: walking can\nlead to an effective idea generation session; brainstorming while walking can\nencourage team members to participate in and contribute to the session in an\nequal manner; and it can help a team to maintain sustainable mental energy. Our\nstudy opens up an avenue for future exploration of effective group\nbrainstorming practices."
},{
    "category": "cs.SE", 
    "doi": "10.7287/peerj.preprints.51v1", 
    "link": "http://arxiv.org/pdf/1308.6413v1", 
    "title": "Development of a language and its enacting engine for the unified   discovery of heterogeneous services", 
    "arxiv-id": "1308.6413v1", 
    "author": "Michael Pantazoglou", 
    "publish": "2013-08-29T10:04:30Z", 
    "summary": "Service orientation fosters a high-level model for distributed applications\ndevelopment, which is based on the discovery, composition and reuse of existing\nsoftware services. However, the heterogeneity among current service-oriented\ntechnologies renders the important task of service discovery tedious and\nineffective. This dissertation proposes a new approach to address this\nchallenge. Specifically, it contributes a framework supporting the unified\ndiscovery of heterogeneous services, with a focus on web, peer-to-peer, and\ngrid services. The framework comprises a service query language and its\nenacting service discovery engine. Overall, the proposed solution is\ncharacterized by generality and flexibility, which are ensured by appropriate\nabstractions, extension points, and their sup- porting mechanisms. The\nviability, performance, and effectiveness of the proposed framework are\ndemonstrated by experimental measurements."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.0369v2", 
    "title": "Case study: Class diagram restructuring", 
    "arxiv-id": "1309.0369v2", 
    "author": "S. Kolahdouz Rahimi", 
    "publish": "2013-09-02T11:39:37Z", 
    "summary": "This case study is an update-in-place refactoring transformation on UML class\ndiagrams. Its aim is to remove clones of attributes from a class diagram, and\nto identify new classes which abstract groups of classes that share common data\nfeatures.\n  It is used as one of a general collection of transformations (such as the\nremoval of redundant inheritance, or multiple inheritance) which aim to improve\nthe quality of a specification or design level class diagram.\n  The transformation is a typical example of a model refactoring, and\nillustrates the issues involved in such transformations."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.0456v1", 
    "title": "The Harmony Platform", 
    "arxiv-id": "1309.0456v1", 
    "author": "Xavier Blanc", 
    "publish": "2013-09-02T16:26:41Z", 
    "summary": "According to Wikipedia, The Mining Software Repositories (MSR) field analyzes\nthe rich data available in software repositories, such as version control\nrepositories, mailing list archives, bug tracking systems, issue tracking\nsystems, etc. to uncover interesting and actionable information about software\nsystems, projects and software engineering. The MSR field has received a great\ndeal of attention and has now its own research conference :\nhttp://www.msrconf.org/. However performing MSR studies is still a technical\nchallenge. Indeed, data sources (such as version control system or bug tracking\nsystems) are highly heterogeneous. Moreover performing a study on a lot of data\nsources is very expensive in terms of execution time. Surprisingly, there are\nnot so many tools able to help researchers in their MSR quests. This is why we\ncreated the Harmony platform, as a mean to assist researchers in performing MSR\nstudies."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.0897v2", 
    "title": "Dynamics of Open-Source Software Developer's Commit Behavior: An   Empirical Investigation of Subversion", 
    "arxiv-id": "1309.0897v2", 
    "author": "Youwei Xu", 
    "publish": "2013-09-04T02:14:41Z", 
    "summary": "Commit is an important operation of revision control for open-source software\n(OSS). Recent research has been pursued to explore the statistical laws of such\nan operation, but few of those papers conduct empirical investigations on\ncommit interval (i.e., the waiting time between two consecutive commits). In\nthis paper, we investigated software developer's collective and individual\ncommit behavior in terms of the distribution of commit intervals, and found\nthat 1) the data sets of project-level commit interval within both the\nlifecycle and each release of the projects analyzed roughly follow power-law\ndistributions; and 2) lifecycle- and release-level collective commit interval\non class files can also be best fitted with power laws. These findings reveal\nsome general (collective) collaborative development patterns of OSS projects,\ne.g., most of the waiting times between two consecutive commits to a central\nrepository are short, but only a few of them experience a long duration of\nwaiting. Then, the implications of what we found for OSS research were\noutlined, which could provide an insight into understanding OSS development\nprocesses better based on software developers' historical commit behavior."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.1232v1", 
    "title": "Bug Tracking and Reporting System", 
    "arxiv-id": "1309.1232v1", 
    "author": "S. Aarthi", 
    "publish": "2013-09-05T04:37:16Z", 
    "summary": "This is the world of information. The ever growing field Information\nTechnology has its many advanced notable features which made it what it was now\ntoday. In this world, the information has to be processed, clearly distributed\nand must be efficiently reachable to the end users intended for that. Otherwise\nwe know it lead to disastrous situations. The other coin of the same phase is\nit is absolutely necessary to know any bugs that are hither to faced by the end\nusers. The project Bug Tracking and Reporting System aims to provide the\nsolution for that. The Bug Tracker can be made from any two types. The first\none being the system side, the other being the services side. Our project deals\nwith the second one. The paper is wholly dedicated to tracking the bugs that\nare hither by arise. The administrator maintains the master details regarding\nto the bugs id, bugs type, bugs description, bugs severity, bugs status, user\ndetails. The administrator too has the authority to update the master details\nof severity level, status level, etc, modules of the paper. The administrator\nadds the users and assign them responsibility of completing the paper. Finally\non analyzing the paper assigned to the particular user, the administrator can\ntrack the bugs, and it is automatically added to the tables containing the\nbugs, by order of severity and status. The administrator can know the\ninformation in tact the various paper s assigned to various users, their bug\ntracking status, their description etc in the form of reports from time to\ntime."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.1640v3", 
    "title": "Towards a Software Product Sustainability Model", 
    "arxiv-id": "1309.1640v3", 
    "author": "Manuel F. Bertoa", 
    "publish": "2013-09-06T13:52:08Z", 
    "summary": "The necessity to adapt current products and services into a way of working\nenvironmentally friendly is already a social and economic demand. Although the\nGreenIT can be considered a mature discipline, software sustainability, both in\nits process and its use, has not begun to be a topic of interest until the last\nfew years. In this sense we think is fundamental to define what we consider\nthat is software sustainability and how to evaluate it properly."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.1796v1", 
    "title": "VisIt: Experiences with Sustainable Software", 
    "arxiv-id": "1309.1796v1", 
    "author": "Hank Childs", 
    "publish": "2013-09-07T00:16:52Z", 
    "summary": "The success of the VisIt visualization system has been wholly dependent upon\nthe culture and practices of software development that have fostered its\nwelcome by users and embrace by developers and researchers. In the following\npaper, we, the founding developers and designers of VisIt, summarize some of\nthe major efforts, both successful and unsuccessful, that we have undertaken in\nthe last thirteen years to foster community, encourage research, create a\nsustainable open-source development model, measure impact, and support\nproduction software. We also provide commentary about the career paths that our\ndevelopment work has engendered."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.1806v1", 
    "title": "Software Engineering as Instrumentation for the Long Tail of Scientific   Software", 
    "arxiv-id": "1309.1806v1", 
    "author": "Hilmar Lapp", 
    "publish": "2013-09-07T02:31:43Z", 
    "summary": "The vast majority of the long tail of scientific software, the myriads of\ntools that implement the many analysis and visualization methods for different\nscientific fields, is highly specialized, purpose-built for a research project,\nand has to rely on community uptake and reuse for its continued development and\nmaintenance. Although uptake cannot be controlled over even guaranteed, some of\nthe key factors that influence whether new users or developers decide to adopt\nan existing tool or start a new one are about how easy or difficult it is to\nuse or enhance a tool for a purpose for which it was not originally designed.\nThe science of software engineering has produced techniques and practices that\nwould reduce or remove a variety of barriers to community uptake of software,\nbut for a variety of reasons employing trained software engineers as part of\nthe development of long tail scientific software has proven to be challenging.\nAs a consequence, community uptake of long tail tools is often far more\ndifficult than it would need to be, even though opportunities for reuse abound.\nWe discuss likely reasons why employing software engineering in the long tail\nis challenging, and propose that many of those obstacles could be addressed in\nthe form of a cross-cutting non-profit center of excellence that makes software\nengineering broadly accessible as a shared service, conceptually and in its\neffect similar to shared instrumentation."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.135.2", 
    "link": "http://arxiv.org/pdf/1309.1817v1", 
    "title": "Initial Findings from a Study of Best Practices and Models for   Cyberinfrastructure Software Sustainability", 
    "arxiv-id": "1309.1817v1", 
    "author": "Von Welch", 
    "publish": "2013-09-07T04:07:05Z", 
    "summary": "We present a set of common themes and recommendations extracted from in depth\ninterviews with the leaders of 12 distinct cyberinfrastructure software\nprojects. These interviews were conducted as part of a larger study to identify\nand elucidate the best practices and management models that lead to\nsustainability for cyberinfrastructure software. Respondents in a formal survey\nof cyberinfrastructure users identified these projects as good examples of\nsustained software initiatives. While there is clearly no single method or plan\nthat will guarantee sustainability for all projects, we can draw general\nguidance from these exemplars. This paper presents the common themes, ideas,\nand recommendations that emerged from those interviews."
},{
    "category": "cs.SE", 
    "doi": "10.1002/rcs.1415", 
    "link": "http://arxiv.org/pdf/1309.1863v1", 
    "title": "Integration of the OpenIGTLink Network Protocol for Image-Guided Therapy   with the Medical Platform MeVisLab", 
    "arxiv-id": "1309.1863v1", 
    "author": "William M. Wells III", 
    "publish": "2013-09-07T12:59:28Z", 
    "summary": "We present the integration of the OpenIGTLink network protocol for\nimage-guided therapy (IGT) with the medical prototyping platform MeVisLab.\nOpenIGTLink is a new, open, simple and extensible network communication\nprotocol for IGT. The protocol provides a standardized mechanism to connect\nhardware and software by the transfer of coordinate transforms, images, and\nstatus messages. MeVisLab is a framework for the development of image\nprocessing algorithms and visualization and interaction methods, with a focus\non medical imaging. The integration of OpenIGTLink into MeVisLab has been\nrealized by developing a software module using the C++ programming language. As\na result, researchers using MeVisLab can interface their software to hardware\ndevices that already support the OpenIGTLink protocol, such as the NDI Aurora\nmagnetic tracking system. In addition, the OpenIGTLink module can also be used\nto communicate directly with Slicer, a free, open source software package for\nvisualization and image analysis. The integration has been tested with tracker\nclients available online and a real tracking system."
},{
    "category": "cs.SE", 
    "doi": "10.5120/11995-7879", 
    "link": "http://arxiv.org/pdf/1309.2404v1", 
    "title": "Implementation of Function Point Analysis in Measuring The Volume   Estimation of Software System in Object Oriented and Structural Model of   Academic System", 
    "arxiv-id": "1309.2404v1", 
    "author": "Dian Pratiwi", 
    "publish": "2013-09-10T08:01:35Z", 
    "summary": "In the software development required a fidelity and accuracy in determining\nthe size or value of the software to fit the operation is executed. Various\nmethods of calculation has been widely applied to estimate the size, and one of\nthem is by using the method of Function Point Analysis (FPA). The method is\nthen applied by author to measure the complexity of an academic information\nsystem by using the two modeling approaches, namely object oriented and\nstructural models. Measurements in this paper consists of several stages,\nnamely describing the information system that will be built into the UML models\nand structured. Then the model is analyzed by calculating Crude Function Points\n(CRP), Relative Complexity Adjustment Factor (RCAF), and then calculate its\nfunction point. From the result of a calculation using the FPA to the academic\nsystem software development, FP values of object oriented model obtained for\n174,64 and the FP value of structured models for 180,93. The result of function\npoint that will be used by developers in determining the price and cost of\nsoftware systems to be built."
},{
    "category": "cs.SE", 
    "doi": "10.5120/11995-7879", 
    "link": "http://arxiv.org/pdf/1309.2485v1", 
    "title": "Model Checking Contest @ Petri Nets, Report on the 2013 edition", 
    "arxiv-id": "1309.2485v1", 
    "author": "Karsten Wolf", 
    "publish": "2013-09-10T12:44:12Z", 
    "summary": "This document presents the results of the Model Checking Contest held at\nPetri Nets 2013 in Milano. This contest aimed at a fair and experimental\nevaluation of the performances of model checking techniques applied to Petri\nnets. This is the third edition after two successful editions in 2011 and 2012.\n  The participating tools were compared on several examinations (state space\ngeneration and evaluation of several types of formul{\\ae} -- reachability, LTL,\nCTL for various classes of atomic propositions) run on a set of common models\n(Place/Transition and Symmetric Petri nets).\n  After a short overview of the contest, this paper provides the raw results\nfrom the contest, model per model and examination per examination. An HTML\nversion of this report is also provided (http://mcc.lip6.fr)."
},{
    "category": "cs.SE", 
    "doi": "10.5120/11995-7879", 
    "link": "http://arxiv.org/pdf/1309.2489v3", 
    "title": "Refinement in the Function-Behaviour-Structure Framework", 
    "arxiv-id": "1309.2489v3", 
    "author": "Bob Diertens", 
    "publish": "2013-09-10T12:56:28Z", 
    "summary": "We introduce refinement in the function-behaviour-structure framework for\ndesign, as described by John Gero, in order to deal with complexity. We do this\nby connecting the frameworks for the design of two models, one the refinement\nof the other. The result is a framework for the design of an object that\nsupports levels of abstraction in the design. This framework can easily be\nextended for the design of an object on more than two levels of abstraction."
},{
    "category": "cs.SE", 
    "doi": "10.14806/ej.19.B.727", 
    "link": "http://arxiv.org/pdf/1309.2787v4", 
    "title": "Taverna Mobile: Taverna workflows on Android", 
    "arxiv-id": "1309.2787v4", 
    "author": "Carole Goble", 
    "publish": "2013-09-11T11:19:24Z", 
    "summary": "Researchers are often on the move, say at conferences or projects meetings,\nand as workflows are becoming ubiquitous in the scientific process, having\naccess to scientific workflows from a mobile device would be a significant\nadvantage. We therefore have developed Taverna Mobile, an application for\nAndroid phones which allows browsing of existing workflows, executing them, and\nreviewing the results.\n  Taverna Mobile does not aim to reproduce the full experience of building\nworkflows in the Taverna Workbench, rather it focuses on tasks we have deemed\nrelevant to a scientist that is not at her desk. For instance, when visiting a\nconference she might hear about someone's workflow, which she can quickly\nlocate and mark for later exploration. When in the biology lab, faced with\nupdated scientific data, the scientist can rerun her own workflow with new\ninputs. While commuting, she can monitor the status of a long-running job."
},{
    "category": "cs.SE", 
    "doi": "10.14806/ej.19.B.727", 
    "link": "http://arxiv.org/pdf/1309.3052v2", 
    "title": "Robust Dynamic Selection of Tested Modules in Software Testing for   Maximizing Delivered Reliability", 
    "arxiv-id": "1309.3052v2", 
    "author": "Kai-Yuan Cai", 
    "publish": "2013-09-12T07:54:36Z", 
    "summary": "Software testing is aimed to improve the delivered reliability of the users.\nDelivered reliability is the reliability of using the software after it is\ndelivered to the users. Usually the software consists of many modules. Thus,\nthe delivered reliability is dependent on the operational profile which\nspecifies how the users will use these modules as well as the defect number\nremaining in each module. Therefore, a good testing policy should take the\noperational profile into account and dynamically select tested modules\naccording to the current state of the software during the testing process. This\npaper discusses how to dynamically select tested modules in order to maximize\ndelivered reliability by formulating the selection problem as a dynamic\nprogramming problem. As the testing process is performed only once, risk must\nbe considered during the testing process, which is described by the tester's\nutility function in this paper. Besides, since usually the tester has no\naccurate estimate of the operational profile, by employing robust optimization\ntechnique, we analysis the selection problem in the worst case, given the\nuncertainty set of operational profile. By numerical examples, we show the\nnecessity of maximizing delivered reliability directly and using robust\noptimization technique when the tester has no clear idea of the operational\nprofile. Moreover, it is shown that the risk averse behavior of the tester has\na major influence on the delivered reliability."
},{
    "category": "cs.SE", 
    "doi": "10.14806/ej.19.B.727", 
    "link": "http://arxiv.org/pdf/1309.3235v1", 
    "title": "Transformations between Composite and Visitor implementations in Java", 
    "arxiv-id": "1309.3235v1", 
    "author": "Jean-Claude Royer", 
    "publish": "2013-09-12T18:30:32Z", 
    "summary": "Basic automated refactoring operations can be chained to perform complex\nstructure transformations. This is useful for recovering the initial\narchitecture of a source code which has been degenerated with successive\nevolutions during its maintenance lifetime. This is also useful for changing\nthe structure of a program so that a maintenance task at hand becomes modular\nwhen it would be initially crosscutting. We focus on programs structured\naccording to Composite and Visitor design patterns, which have dual properties\nwith respect to modularity. We consider a refactoring-based round-trip\ntransformation between these two structures and we study how that\ntransformation is impacted by four variations in the implementation of these\npatterns. We validate that study by computing the smallest preconditions for\nthe resulting transformations. We also automate the transformation and apply it\nto JHotDraw, where the studied variations occur."
},{
    "category": "cs.SE", 
    "doi": "10.14806/ej.19.B.727", 
    "link": "http://arxiv.org/pdf/1309.3730v1", 
    "title": "Automatically Extracting Instances of Code Change Patterns with AST   Analysis", 
    "arxiv-id": "1309.3730v1", 
    "author": "Martin Monperrus", 
    "publish": "2013-09-15T05:52:17Z", 
    "summary": "A code change pattern represents a kind of recurrent modification in\nsoftware. For instance, a known code change pattern consists of the change of\nthe conditional expression of an if statement. Previous work has identified\ndifferent change patterns. Complementary to the identification and definition\nof change patterns, the automatic extraction of pattern instances is essential\nto measure their empirical importance. For example, it enables one to count and\ncompare the number of conditional expression changes in the history of\ndifferent projects. In this paper we present a novel approach for search\npatterns instances from software history. Our technique is based on the\nanalysis of Abstract Syntax Trees (AST) files within a given commit. We\nvalidate our approach by counting instances of 18 change patterns in 6\nopen-source Java projects."
},{
    "category": "cs.SE", 
    "doi": "10.14806/ej.19.B.727", 
    "link": "http://arxiv.org/pdf/1309.4645v1", 
    "title": "Measuring the Success of Software Process Improvement: The Dimensions", 
    "arxiv-id": "1309.4645v1", 
    "author": "Pekka Abrahamsson", 
    "publish": "2013-09-18T13:54:34Z", 
    "summary": "Quality managers, change agents and researchers are often troubled in\ndefining and demonstrating the level of success achieved in software process\nimprovement (SPI) initiatives. So far, there exist only few frameworks for\nidentifying the level of success achieved in SPI. Analysis shows that these\nframeworks do not provide a comprehensive view from all relevant stakeholders\ninvolved in SPI. Early results from an ongoing research effort to discover and\noperationalise success dimensions are reported. Adapted from the project\nmanagement literature it is suggested that five dimensions characterise the\nlevel of success achieved in SPI: (1) project efficiency, (2) impact on the\nprocess user, (3) business success, (4) direct operational success and (5)\nprocess improvement fit. Results from an empirical analysis are reported where\n23 change agents evaluated the relative level of importance of each dimension.\nEarly results indicate that change agents valued the process user satisfaction\nthe most and the process improvement fit the least. This finding confirms the\nneed of having various stakeholders and dimensions acknowledged in a framework\nthat is used to measure the overall success of an SPI initiative."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9602-4227", 
    "link": "http://arxiv.org/pdf/1309.5688v1", 
    "title": "Revised Modularity Index to Measure Modularity of OSS Projects with Case   Study of Freemind", 
    "arxiv-id": "1309.5688v1", 
    "author": "Daniel Jahja Surjawan", 
    "publish": "2013-09-23T03:22:03Z", 
    "summary": "Open Source Software (OSS) Projects are gaining popularity worldwide. Studies\nby many researchers show that the important key success factor is modularity of\nthe source code. This paper presents the revised Modularity Index which is a\nsoftware metrics to measure the modularity level of a javabased OSS Projects.\nTo show its effectiveness in analyzing OSS Project, the Modularity Index and\nits supporting software metrics are then used to analyze the evolution of\nFreemind mind mapping OSS Project. The analysis using Modularity Index and its\nsupporting metrics shows the strength and weaknesses of the Freemind OSS\nProjects."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9602-4227", 
    "link": "http://arxiv.org/pdf/1309.5689v1", 
    "title": "Modularity Index Metrics for Java-Based Open Source Software Projects", 
    "arxiv-id": "1309.5689v1", 
    "author": "Khabib Mustofa", 
    "publish": "2013-09-23T03:28:07Z", 
    "summary": "Open Source Software (OSS) Projects are gaining popularity these days, and\nthey become alternatives in building software system. Despite many failures in\nthese projects, there are some success stories with one of the identified\nsuccess factors is modularity. This paper presents the first quantitative\nsoftware metrics to measure modularity level of Java-based OSS Projects called\nModularity Index. This software metrics is formulated by analyzing modularity\ntraits such as size, complexity, cohesion, and coupling of 59 Java-based OSS\nProjects from sourceforge.net using SONAR tool. These OSS Projects are selected\nsince they have been downloaded more than 100K times and believed to have the\nrequired modularity trait to be successful. The software metrics related to\nmodularity in class, package and system level of these projects are extracted\nand analyzed. The similarities found are then analyzed to determine the class\nquality, package quality, and then combined with system architecture measure to\nformulate the Modularity Index. The case study of measuring Modularity Index\nduring the evolution of JFreeChart project has shown that this software metrics\nis able to identify strengths and potential problems of the project."
},{
    "category": "cs.SE", 
    "doi": "10.5120/9602-4227", 
    "link": "http://arxiv.org/pdf/1309.7341v1", 
    "title": "OntoMaven: Maven-based Ontology Development and Management of   Distributed Ontology Repositories", 
    "arxiv-id": "1309.7341v1", 
    "author": "Adrian Paschke", 
    "publish": "2013-09-27T19:48:05Z", 
    "summary": "In collaborative agile ontology development projects support for modular\nreuse of ontologies from large existing remote repositories, ontology project\nlife cycle management, and transitive dependency management are important\nneeds. The Apache Maven approach has proven its success in distributed\ncollaborative Software Engineering by its widespread adoption. The contribution\nof this paper is a new design artifact called OntoMaven. OntoMaven adopts the\nMaven-based development methodology and adapts its concepts to knowledge\nengineering for Maven-based ontology development and management of ontology\nartifacts in distributed ontology repositories."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSIT.2013.6588778", 
    "link": "http://arxiv.org/pdf/1309.7950v1", 
    "title": "Software Interfaces: On The Impact of Interface Design Anomalies", 
    "arxiv-id": "1309.7950v1", 
    "author": "Abdelkarim Erradi", 
    "publish": "2013-09-30T18:32:57Z", 
    "summary": "Interfaces are recognized as an important mechanism to define contracts\ngoverning interactions between semi-independent software modules. Well-designed\ninterfaces significantly reduce software complexity and ease maintainability by\nfostering modularization, hiding implementation details and minimizing the\nimpact caused by changes in the software implementation. However, designing\ngood interfaces is not a trivial task. The presence of interface design defects\noften yield increased development cost, lower code quality and reduced\ndevelopment productivity. Despite their importance, currently there are only a\nfew research efforts that investigate the quality of interface design. In this\npaper, we identify and characterize common interface design anomalies and\nillustrate them via examples taken from well-known open source applications. In\norder to quantify the presence of interface design anomalies and estimate their\nimpact on the interface design quality, as well on the software quality\nattributes, such as maintainability, we conduct an empirical study covering 9\nopen source projects. Building on our empirical results, we develop a set of\nrecommendations to improve interface design."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CSIT.2013.6588778", 
    "link": "http://arxiv.org/pdf/1311.0228v1", 
    "title": "Improving Software Developer's Competence: Is the Personal Software   Process Working?", 
    "arxiv-id": "1311.0228v1", 
    "author": "Jouni Lappalainen", 
    "publish": "2013-11-01T17:23:06Z", 
    "summary": "Emerging agile software development methods are people oriented development\napproaches to be used by the software industry. The personal software process\n(PSP) is an accepted method for improving the capabilities of a single software\nengineer. Five original hypotheses regarding the impact of the PSP to\nindividual performance are tested. Data is obtained from 58 computer science\nstudents in three university courses on the master level, which were held in\ntwo different educational institutions in Finland and Denmark. Statistical data\ntreatment shows that the use of PSP did not improve size and time estimation\nskills but that the productivity did not decrease and the resulting product\nquality was improved. The implications of these findings are briefly addressed."
},{
    "category": "cs.SE", 
    "doi": "10.4236/jsea.2012.59081", 
    "link": "http://arxiv.org/pdf/1311.1197v1", 
    "title": "Software Reuse in Cardiology Related Medical Database Using K-Means   Clustering Technique", 
    "arxiv-id": "1311.1197v1", 
    "author": "M. H. M. Krishna Prasad", 
    "publish": "2013-11-05T20:59:13Z", 
    "summary": "Software technology based on reuse is identified as a process of designing\nsoftware for the reuse purpose. The software reuse is a process in which the\nexisting software is used to build new software. A metric is a quantitative\nindicator of an attribute of an item or thing. Reusability is the likelihood\nfor a segment of source code that can be used again to add new functionalities\nwith slight or no modification. A lot of research has been projected using\nreusability in reducing code, domain, requirements, design etc., but very\nlittle work is reported using software reuse in medical domain. An attempt is\nmade to bridge the gap in this direction, using the concepts of clustering and\nclassifying the data based on the distance measures. In this paper cardiologic\ndatabase is considered for study. The developed model will be useful for\nDoctors or Paramedics to find out the patients level in the cardiologic\ndisease, deduce the medicines required in seconds and propose them to the\npatient. In order to measure the reusability K means clustering algorithm is\nused."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2015.09.003", 
    "link": "http://arxiv.org/pdf/1311.1322v2", 
    "title": "Modelling Families of Business Process Variants: A Decomposition Driven   Method", 
    "arxiv-id": "1311.1322v2", 
    "author": "Raimundas Matulevi\u010dius", 
    "publish": "2013-11-06T09:38:41Z", 
    "summary": "Business processes usually do not exist as singular entities that can be\nmanaged in isolation, but rather as families of business process variants. When\nmodelling such families of variants, analysts are confronted with the choice\nbetween modelling each variant separately, or modelling multiple or all\nvariants in a single model. Modelling each variant separately leads to a\nproliferation of models that share common parts, resulting in redundancies and\ninconsistencies. Meanwhile, modelling all variants together leads to less but\nmore complex models, thus hindering on comprehensibility. This paper introduces\na method for modelling families of process variants that addresses this\ntrade-off. The key tenet of the method is to alternate between steps of\ndecomposition (breaking down processes into sub-processes) and deciding which\nparts should be modelled together and which ones should be modelled separately.\nWe have applied the method to two case studies: one concerning the\nconsolidation of ex-isting process models, and another dealing with green-field\nprocess discovery. In both cases, the method produced fewer models with respect\nto the baseline and reduced duplicity by up to 50% without significant impact\non complexity."
},{
    "category": "cs.SE", 
    "doi": "10.1109/CHASE.2013.6614747!", 
    "link": "http://arxiv.org/pdf/1311.1323v1", 
    "title": "How Does Kanban Impact Communication and Collaboration in Software   Engineering Teams?", 
    "arxiv-id": "1311.1323v1", 
    "author": "J\u00fcrgen M\u00fcnch", 
    "publish": "2013-11-06T09:40:37Z", 
    "summary": "Highly iterative development processes such as Kanban have gained significant\nimportance in industry. However, the impact of such processes on team\ncollaboration and communication is widely unknown. In this paper, we analyze\nhow the Kanban process aids software team's behaviours -- in particular,\ncommunication and collaboration. The team under study developed a mobile\npayment software product in six iterations over seven weeks. The data were\ncollected by a questionnaire, repeated at the end of each iteration. The\nresults indicate that Kanban has a positive effect at the beginning to get the\nteam working together to identify and coordinate the work. Later phases, when\nthe team members have established good rapport among them, the importance for\nfacilitating team collaboration could not be shown. Results also indicate that\nKanban helps team members to collectively identify and surface the missing\ntasks to keep the pace of the development harmonized across the whole team,\nresulting into increased collaboration. Besides presenting the study and the\nresults, the article gives an outlook on future work."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSEW.2013.8", 
    "link": "http://arxiv.org/pdf/1311.1334v1", 
    "title": "Onboarding in Open Source Software Projects: A Preliminary Analysis", 
    "arxiv-id": "1311.1334v1", 
    "author": "J\u00fcrgen M\u00fcnch", 
    "publish": "2013-11-06T10:09:04Z", 
    "summary": "Nowadays, many software projects are partially or completely open-source\nbased. There is an increasing need for companies to participate in open-source\nsoftware (OSS) projects, e.g., in order to benefit from open source ecosystems.\nOSS projects introduce particular challenges that have to be understood in\norder to gain the benefits. One such challenge is getting newcomers onboard\ninto the projects effectively. Similar challenges may be present in other\nself-organised, virtual team environments. In this paper we present preliminary\nobservations and results of in-progress research that studies the process of\nonboarding into virtual OSS teams. The study is based on a program created and\nconceived at Stanford University in conjunction with Facebook's Education\nModernization program. It involves the collaboration of more than a dozen\ninternational universities and nine open source projects. More than 120\nstudents participated in 2013. The students have been introduced to and\nsupported by mentors experienced in the participating OSS projects. Our\nfindings indicate that mentoring is an important factor for effective\nonboarding in OSS projects, promoting cohesion within distributed teams and\nmaintaining an appropriate pace."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSEW.2013.8", 
    "link": "http://arxiv.org/pdf/1311.1343v1", 
    "title": "Verification for Reliable Product Lines", 
    "arxiv-id": "1311.1343v1", 
    "author": "Axel Legay", 
    "publish": "2013-11-06T10:36:15Z", 
    "summary": "Many product lines are critical, and therefore reliability is a vital part of\ntheir requirements. Reliability is a probabilistic property. We therefore\npropose a model for feature-aware discrete-time Markov chains as a basis for\nverifying probabilistic properties of product lines, including reliability. We\ncompare three verification techniques: The enumerative technique uses PRISM, a\nstate-of-the-art symbolic probabilistic model checker, on each product. The\nparametric technique exploits our recent advances in parametric model checking.\nFinally, we propose a new bounded technique that performs a single bounded\nverification for the whole product line, and thus takes advantage of the common\nbehaviours of the product line. Experimental results confirm the advantages of\nthe last two techniques."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SEAA.2013.62", 
    "link": "http://arxiv.org/pdf/1311.1618v1", 
    "title": "Experiences and Insights from Applying GQM+Strategies in a Systems   Product Development Organisation", 
    "arxiv-id": "1311.1618v1", 
    "author": "Jari Partanen", 
    "publish": "2013-11-07T09:48:28Z", 
    "summary": "Aligning software-related activities with corporate strategies and goals is\nincreasingly important for several reasons such as increasing the customer\nsatisfaction in software-based products and services. Several approaches have\nbeen proposed to create such an alignment. GQM+Strategies is an approach that\napplies measurement principles to link goals and strategies on different levels\nof an organisation. In this paper, we describe experiences from applying\nGQM+Strategies to elicit, link, and align the goals of an integrated systems\nproduct development organisation across multiple organisational levels. We\nprovide insights into how GQM+Strategies was applied during a five- month\nperiod. The paper presents the enacted application process and main lessons\nlearnt. In addition, related approaches are described and an outlook on future\nwork is given."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.1698v1", 
    "title": "A Survey on the State of the Practice in Distributed Software   Development: Criteria for Task Allocation", 
    "arxiv-id": "1311.1698v1", 
    "author": "Dieter Rombach", 
    "publish": "2013-11-05T13:24:14Z", 
    "summary": "The allocation of tasks can be seen as a success-critical management activity\nin distributed development projects. However, such task allocation is still one\nof the major challenges in global software development due to an insufficient\nunderstanding of the criteria that influence task allocation decisions. This\narticle presents a qualitative study aimed at identifying and understanding\nsuch criteria that are used in practice. Based on interviews with managers from\nselected software development organizations, criteria currently applied in\nindustry are identified. One important result is, for instance, that the\nsourcing strategy and the type of software to be developed have a significant\neffect on the applied criteria. The article presents the goals, design, and\nresults of the study as well as an overview of related and future work."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.1729v1", 
    "title": "Managing Requirement Elicitation Issues Using Step-Wise Refinement Model", 
    "arxiv-id": "1311.1729v1", 
    "author": "Sakthi Kumaresh", 
    "publish": "2013-11-07T16:14:50Z", 
    "summary": "In this paper, a Step-wise Refinement model is proposed to elicit\nrequirements in a more effective manner."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.1895v2", 
    "title": "Mining Crash Fix Patterns", 
    "arxiv-id": "1311.1895v2", 
    "author": "Ning Chen", 
    "publish": "2013-11-08T08:18:23Z", 
    "summary": "During the life cycle of software development, developers have to fix\ndifferent kinds of bugs reported by testers or end users. The efficiency and\neffectiveness of fixing bugs have a huge impact on the reliability of the\nsoftware as well as the productivity of the development team. Software\ncompanies usually spend a large amount of money and human resources on the\ntesting and bug fixing departments. As a result, a better and more reliable way\nto fix bugs is highly desired by them. In order to achieve such goal, in depth\nstudies on the characteristics of bug fixes from well maintained, highly\npopular software projects are necessary. In this paper, we study the bug fixing\nhistories extracted from the Eclipse project, a well maintained, highly popular\nopen source project. After analyzing more than 36,000 bugs that belongs to\nthree major kinds of exception types, we are able to reveal some common fix\ntypes that are frequently used to fix certain kinds of program exceptions. Our\nanalysis shows that almost all of the exceptions that belong to a certain\nexception can be fixed by less than ten fix types. Our result implies that most\nof the bugs in software projects can be and should be fixed by only a few\ncommon fix patterns."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.2412v1", 
    "title": "PLOS/Mozilla Scientific Code Review Pilot: Summary of Findings", 
    "arxiv-id": "1311.2412v1", 
    "author": "Greg Wilson", 
    "publish": "2013-11-11T10:42:35Z", 
    "summary": "PLOS and Mozilla conducted a month-long pilot study in which professional\ndevelopers performed code reviews on software associated with papers published\nin PLOS Computational Biology. While the developers felt the reviews were\nlimited by (a) lack of familiarity with the domain and (b) lack of two-way\ncontact with authors, the scientists appreciated the reviews, and both sides\nwere enthusiastic about repeating the experiment."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.2535v1", 
    "title": "Issues about the Adoption of Formal Methods for Dependable Composition   of Web Services", 
    "arxiv-id": "1311.2535v1", 
    "author": "Michele Ciavotta", 
    "publish": "2013-11-11T19:16:40Z", 
    "summary": "Web Services provide interoperable mechanisms for describing, locating and\ninvoking services over the Internet; composition further enables to build\ncomplex services out of simpler ones for complex B2B applications. While\ncurrent studies on these topics are mostly focused - from the technical\nviewpoint - on standards and protocols, this paper investigates the adoption of\nformal methods, especially for composition. We logically classify and analyze\nthree different (but interconnected) kinds of important issues towards this\ngoal, namely foundations, verification and extensions. The aim of this work is\nto individuate the proper questions on the adoption of formal methods for\ndependable composition of Web Services, not necessarily to find the optimal\nanswers. Nevertheless, we still try to propose some tentative answers based on\nour proposal for a composition calculus, which we hope can animate a proper\ndiscussion."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2009.12", 
    "link": "http://arxiv.org/pdf/1311.2968v1", 
    "title": "An Experience based Evaluation Process for ERP bids", 
    "arxiv-id": "1311.2968v1", 
    "author": "Abdul Jawad Chaudhry", 
    "publish": "2013-11-12T21:56:40Z", 
    "summary": "Enterprise Resource Planning ERP systems integrate information across an\nentire organization that automate core activities such as finance accounting,\nhuman resources, manufacturing, production and supply chain management etc. to\nfacilitate an integrated centralized system and rapid decision making resulting\nin cost reduction, greater planning, and increased control. Many organizations\nare updating their current management information systems with ERP systems.\nThis is not a trivial task. They have to identify the organizations objectives\nand satisfy a myriad of stakeholders. They have to understand what business\nprocesses they have, how they can be improved, and what particular systems\nwould best suit their needs. They have to understand how an ERP system is\nbuilt, it involves the modification of an existing system with its own set of\nbusiness rules. Deciding what to ask for and how to select the best option is a\nvery complex operation and there is limited experience with this type of\ncontracting in organizations. In this paper we discuss a particular experience\nwith contracting out an ERP system, provide some lessons learned, and offer\nsuggestions in how the RFP and bid selection processes could have been\nimproved."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-01680-6_30", 
    "link": "http://arxiv.org/pdf/1311.3026v1", 
    "title": "Incrementally Introducing Process Model Rationale Support in an   Organization", 
    "arxiv-id": "1311.3026v1", 
    "author": "William E. Riddle", 
    "publish": "2013-11-13T06:32:38Z", 
    "summary": "Popular process models such as the Rational Unified Process or the V-Modell\nXT are by nature large and complex. Each time that a new release is published\nsoftware development organizations are confronted with the big challenge of\nunderstanding the rationale behind the new release and the extent to which it\naffects them. Usually, there is no information about what has changed or most\nimportantly why. This is because of the lack of a flexible approach that\nsupports organizations responsible for evolving such large process models in\ndocumenting their decisions and that reflects the extent of the capabilities to\nwhich they can provide this information. This paper describes an approach to\nincrementally deploying rationale support as needed to match an organisation's\nneeds, the capabilities and interests of the organisation's process engineering\nteams, and the organisation's willingness to support the effort required for\nthe collection and application of the rationale information."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5509", 
    "link": "http://arxiv.org/pdf/1311.3243v1", 
    "title": "Systems Variability Modeling: A Textual Model Mixing Class and Feature   Concepts", 
    "arxiv-id": "1311.3243v1", 
    "author": "Mohammad H. Alomari", 
    "publish": "2013-11-13T18:31:11Z", 
    "summary": "System reuse and cost are very important in software product line design\narea. Developers goal is to increase system reuse and decreasing cost and\nefforts for building components from scratch for each software configuration.\nThis can be reached by developing Software Product Line (SPL). To handle SPL\nengineering process, several approaches with several techniques were developed.\nOne of these approaches is called separated approach. It requires separating\nthe commonalities and variability for system components to allow configuration\nselection based on user defined features. Textual notation-based approaches\nhave been used for their formal syntax and semantics to represent system\nfeatures and implementations. But these approaches are still weak in mixing\nfeatures (conceptual level) and classes (physical level) that guarantee smooth\nand automatic configuration generation for software releases. The absence of\nmethodology supporting the mixing process is a real weakness. In this paper, we\nenhanced SPL reuse by introducing some meta-features, classified according to\ntheir functions. As a first consequence, mixing class and feature concepts is\nsupported in a simple way using class interfaces and inherent features for\nsmooth move from feature model to class model. And as a second consequence, the\nmixing process is supported by a textual design and implementation methodology,\nmixing class and feature models by combining their concepts in a single\nlanguage. The supported configuration generation process is simple, coherent,\nand complete."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5509", 
    "link": "http://arxiv.org/pdf/1311.3414v1", 
    "title": "Mining Software Repair Models for Reasoning on the Search Space of   Automated Program Fixing", 
    "arxiv-id": "1311.3414v1", 
    "author": "Martin Monperrus", 
    "publish": "2013-11-14T08:28:45Z", 
    "summary": "This paper is about understanding the nature of bug fixing by analyzing\nthousands of bug fix transactions of software repositories. It then places this\nlearned knowledge in the context of automated program repair. We give extensive\nempirical results on the nature of human bug fixes at a large scale and a fine\ngranularity with abstract syntax tree differencing. We set up mathematical\nreasoning on the search space of automated repair and the time to navigate\nthrough it. By applying our method on 14 repositories of Java software and\n89,993 versioning transactions, we show that not all probabilistic repair\nmodels are equivalent."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2013.5509", 
    "link": "http://arxiv.org/pdf/1311.3523v3", 
    "title": "First Workshop on Sustainable Software for Science: Practice and   Experiences (WSSSPE): Submission and Peer-Review Process, and Results", 
    "arxiv-id": "1311.3523v3", 
    "author": "David Proctor", 
    "publish": "2013-11-14T15:00:25Z", 
    "summary": "This technical report discusses the submission and peer-review process used\nby the First Workshop on on Sustainable Software for Science: Practice and\nExperiences (WSSSPE) and the results of that process. It is intended to record\nboth this alternative model as well as the papers associated with the workshop\nthat resulted from that process."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.1", 
    "link": "http://arxiv.org/pdf/1311.3626v1", 
    "title": "Verification and Validation Issues in Systems of Systems", 
    "arxiv-id": "1311.3626v1", 
    "author": "Eric Honour", 
    "publish": "2013-11-14T19:40:19Z", 
    "summary": "The cutting edge in systems development today is in the area of \"systems of\nsystems\" (SoS) large networks of inter-related systems that are developed and\nmanaged separately, but that also perform collective activities. Such large\nsystems typically involve constituent systems operating with different life\ncycles, often with uncoordinated evolution. The result is an ever-changing SoS\nin which adaptation and evolution replace the older engineering paradigm of\n\"development\". This short paper presents key thoughts about verification and\nvalidation in this environment. Classic verification and validation methods\nrely on having (a) a basis of proof, in requirements and in operational\nscenarios, and (b) a known system configuration to be proven. However, with\nconstant SoS evolution, management of both requirements and system\nconfigurations are problematic. Often, it is impossible to maintain a valid set\nof requirements for the SoS due to the ongoing changes in the constituent\nsystems. Frequently, it is even difficult to maintain a vision of the SoS\noperational use as users find new ways to adapt the SoS. These features of the\nSoS result in significant challenges for system proof. In addition to\ndiscussing the issues, the paper also indicates some of the solutions that are\ncurrently used to prove the SoS."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.2", 
    "link": "http://arxiv.org/pdf/1311.3627v1", 
    "title": "Variability and Evolution in Systems of Systems", 
    "arxiv-id": "1311.3627v1", 
    "author": "Goetz Botterweck", 
    "publish": "2013-11-14T19:40:33Z", 
    "summary": "In this position paper (1) we discuss two particular aspects of Systems of\nSystems, i.e., variability and evolution. (2) We argue that concepts from\nProduct Line Engineering and Software Evolution are relevant to Systems of\nSystems Engineering. (3) Conversely, concepts from Systems of Systems\nEngineering can be helpful in Product Line Engineering and Software Evolution.\nHence, we argue that an exchange of concepts between the disciplines would be\nbeneficial."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.3", 
    "link": "http://arxiv.org/pdf/1311.3628v1", 
    "title": "Systems of Systems Modeled by a Hierarchical Part-Whole State-Based   Formalism", 
    "arxiv-id": "1311.3628v1", 
    "author": "Luca Pazzi", 
    "publish": "2013-11-14T19:40:41Z", 
    "summary": "The paper presents an explicit state-based modeling approach aimed at\nmodeling Systems of Systems behavior. The approach allows to specify and verify\nincrementally safety and liveness rules without using model checking\ntechniques. The state-based approach allows moreover to use the system behavior\ndirectly as an interface, greatly improving the effectiveness of the recursive\ncomposition needed when assembling Systems of Systems. Such systems are, at the\nsame time, both parts and wholes, thus giving a formal characterization to the\nnotion of Holon."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.4", 
    "link": "http://arxiv.org/pdf/1311.3629v1", 
    "title": "System-of-Systems Complexity", 
    "arxiv-id": "1311.3629v1", 
    "author": "Hermann Kopetz", 
    "publish": "2013-11-14T19:40:50Z", 
    "summary": "The global availability of communication services makes it possible to\ninterconnect independently developed systems, called constituent systems, to\nprovide new synergistic services and more efficient economic processes. The\ncharacteristics of these new Systems-of-Systems are qualitatively different\nfrom the classic monolithic systems. In the first part of this presentation we\nelaborate on these differences, particularly with respect to the autonomy of\nthe constituent systems, to dependability, continuous evolution, and emergence.\nIn the second part we look at a SoS from the point of view of cognitive\ncomplexity. Cognitive complexity is seen as a relation between a model of an\nSoS and the observer. In order to understand the behavior of a large SoS we\nhave to generate models of adequate simplicity, i.e, of a cognitive complexity\nthat can be handled by the limited capabilities of the human mind. We will\ndiscuss the importance of properly specifying and placing the relied-upon\nmessage interfaces between the constituent systems that form an open SoS and\ndiscuss simplification strategies that help to reduce the cognitive complexity."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.5", 
    "link": "http://arxiv.org/pdf/1311.3630v1", 
    "title": "Challenges for modelling and analysis in embedded systems and   systems-of-systems design", 
    "arxiv-id": "1311.3630v1", 
    "author": "Boudewijn R. Haverkort", 
    "publish": "2013-11-14T19:41:00Z", 
    "summary": "Over the last decade we have witnessed an increasing use of data processing\nin embedded systems. Where in the past the data processing was limited (if\npresent at all) to the handling of a small number of \"on-off control signals\",\nmore recently much more complex sensory data is being captured, processed and\nused to improve system performance and dependability. The advent of\nsystems-of-systems aggravates the use of more and more data, for instance, by\nbringing together data from several independent sources, allowing, in\nprinciple, for even better performing systems. However, this ever stronger\ndata-orientation brings along several challenges in system design, both\ntechnically and organisationally, and also forces manufacturers to think beyond\ntheir traditional field of expertise. In this short paper, I will address these\nnew design challenges, through a number of examples. The paper finishes with\nconcrete challenges for supporting tools and techniques for system design in\nthis new context."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.6", 
    "link": "http://arxiv.org/pdf/1311.3631v1", 
    "title": "Contracts and Behavioral Patterns for SoS: The EU IP DANSE approach", 
    "arxiv-id": "1311.3631v1", 
    "author": "Axel Legay", 
    "publish": "2013-11-14T19:41:12Z", 
    "summary": "This paper presents some of the results of the first year of DANSE, one of\nthe first EU IP projects dedicated to SoS. Concretely, we offer a tool chain\nthat allows to specify SoS and SoS requirements at high level, and analyse them\nusing powerful toolsets coming from the formal verification area. At the high\nlevel, we use UPDM, the system model provided by the british army as well as a\nnew type of contract based on behavioral patterns. At low level, we rely on a\npowerful simulation toolset combined with recent advances from the area of\nstatistical model checking. The approach has been applied to a case study\ndeveloped at EADS Innovation Works."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.133.7", 
    "link": "http://arxiv.org/pdf/1311.3632v1", 
    "title": "SoS contract verification using statistical model checking", 
    "arxiv-id": "1311.3632v1", 
    "author": "Alexandre Arnold", 
    "publish": "2013-11-14T19:41:25Z", 
    "summary": "Exhaustive formal verification for systems of systems (SoS) is impractical\nand cannot be applied on a large scale. In this paper we propose to use\nstatistical model checking for efficient verification of SoS. We address three\nrelevant aspects for systems of systems: 1) the model of the SoS, which\nincludes stochastic aspects; 2) the formalization of the SoS requirements in\nthe form of contracts; 3) the tool-chain to support statistical model checking\nfor SoS. We adapt the SMC technique for application to heterogeneous SoS. We\nextend the UPDM/SysML specification language to express the SoS requirements\nthat the implemented strategies over the SoS must satisfy. The requirements are\nspecified with a new contract language specifically designed for SoS, targeting\na high-level English- pattern language, but relying on an accurate semantics\ngiven by the standard temporal logics. The contracts are verified against the\nUPDM/SysML specification using the Statistical Model Checker (SMC) PLASMA\ncombined with the simulation engine DESYRE, which integrates heterogeneous\nbehavioral models through the functional mock-up interface (FMI) standard. The\ntool-chain allows computing an estimation of the satisfiability of the\ncontracts by the SoS. The results help the system architect to trade-off\ndifferent solutions to guide the evolution of the SoS."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.3798v1", 
    "title": "Integrating Inspection and Test Processes Based on Context-Specific   Assumptions", 
    "arxiv-id": "1311.3798v1", 
    "author": "Bernd Freimut", 
    "publish": "2013-11-15T10:31:46Z", 
    "summary": "Inspections and testing are two of the most commonly performed software\nquality assurance processes today. Typically, these processes are applied in\nisolation, which, however, fails to exploit the benefits of systematically\ncombining and integrating them. In consequence, tests are not focused based on\nearly defect detection data. Expected benefits of such process integration\ninclude higher defect detection rates or reduced quality assurance effort.\nMoreover, when conducting testing without any prior information regarding the\nsystem's quality, it is often unclear how to focus testing. A systematic\nintegration of inspection and testing processes requires context-specific\nknowledge about the relationships between inspections and testing. This\nknowledge is typically not available and needs to be empirically identified and\nvalidated. Often, context-specific assumptions can be seen as a starting point\nfor generating such knowledge. Based on the In2Test approach, which uses\ninspection data to focus testing, we present in this article how knowledge\nabout the relationship between inspections and testing can be gained,\ndocumented, and evolved in an analytical or empirical manner. In addition, this\narticle gives an overview of related work and highlights future research\ndirections."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.4176v3", 
    "title": "ComReg: A Complex Network Approach to Prioritize Test Cases for   Regression Testing", 
    "arxiv-id": "1311.4176v3", 
    "author": "Jacob Chakareski", 
    "publish": "2013-11-17T16:15:03Z", 
    "summary": "Regression testing is performed to provide confidence that changes in a part\nof software do not affect other parts of the software. An execution of all\nexisting test cases is the best way to re-establish this confidence. However,\nregression testing is an expensive process---there might be insufficient\nresources (e.g., time, workforce) to allow for the re-execution of all test\ncases. Regression test prioritization techniques attempt to re-order a\nregression test suite based on some criteria so that highest priority test\ncases are executed earlier.\n  In this study, we want to prioritize test cases for regression testing based\non the dependency network of faults. In software testing, it is common that\nsome faults are consequences of other faults (leading faults). Moreover,\ndependent faults can be removed if and only if the leading faults have been\nremoved. Our goal is to prioritize test cases so that test cases that exposed\nleading faults (the most central faults in the fault dependency network) in the\nsystem testing phase, are executed first in regression testing.\n  We present ComReg, a test case prioritization technique based on the\ndependency network of faults. We model a fault dependency network as a directed\ngraph and identify leading faults to prioritize test cases for regression\ntesting. We use a centrality aggregation technique which considers six network\nrepresentative centrality metrics to identify leading faults in the fault\ndependency network. We also discuss the use of fault communities to select an\narbitrary percentage of the test cases from a prioritized regression test\nsuite. We conduct a case study that evaluates the effectiveness and\napplicability of the proposed method."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.4615v1", 
    "title": "A Notion of Dynamic Interface for Depth-Bounded Object-Oriented Packages", 
    "arxiv-id": "1311.4615v1", 
    "author": "Damien Zufferey", 
    "publish": "2013-11-19T03:21:57Z", 
    "summary": "Programmers using software components have to follow protocols that specify\nwhen it is legal to call particular methods with particular arguments. For\nexample, one cannot use an iterator over a set once the set has been changed\ndirectly or through another iterator. We formalize the notion of dynamic\npackage interfaces (DPI), which generalize state-machine interfaces for single\nobjects, and give an algorithm to statically compute a sound abstraction of a\nDPI. States of a DPI represent (unbounded) sets of heap configurations and\nedges represent the effects of method calls on the heap. We introduce a novel\nheap abstract domain based on depth-bounded systems to deal with potentially\nunboundedly many objects and the references among them. We have implemented our\nalgorithm and show that it is effective in computing representations of common\npatterns of package usage, such as relationships between viewer and label,\ncontainer and iterator, and JDBC statements and cursors."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.4934v2", 
    "title": "Dynamic Package Interfaces - Extended Version", 
    "arxiv-id": "1311.4934v2", 
    "author": "Damien Zufferey", 
    "publish": "2013-11-20T01:58:30Z", 
    "summary": "A hallmark of object-oriented programming is the ability to perform\ncomputation through a set of interacting objects. A common manifestation of\nthis style is the notion of a package, which groups a set of commonly used\nclasses together. A challenge in using a package is to ensure that a client\nfollows the implicit protocol of the package when calling its methods.\nViolations of the protocol can cause a runtime error or latent invariant\nviolations. These protocols can extend across different, potentially\nunboundedly many, objects, and are specified informally in the documentation.\nAs a result, ensuring that a client does not violate the protocol is hard.\n  We introduce dynamic package interfaces (DPI), a formalism to explicitly\ncapture the protocol of a package. The DPI of a package is a finite set of\nrules that together specify how any set of interacting objects of the package\ncan evolve through method calls and under what conditions an error can happen.\nWe have developed a dynamic tool that automatically computes an approximation\nof the DPI of a package, given a set of abstraction predicates. A key property\nof DPI is that the unbounded number of configurations of objects of a package\nare summarized finitely in an abstract domain. This uses the observation that\nmany packages behave monotonically: the semantics of a method call over a\nconfiguration does not essentially change if more objects are added to the\nconfiguration. We have exploited monotonicity and have devised heuristics to\nobtain succinct yet general DPIs. We have used our tool to compute DPIs for\nseveral commonly used Java packages with complex protocols, such as JDBC,\nHashSet, and ArrayList."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.5270v2", 
    "title": "Automated Feature Identification in Web Applications", 
    "arxiv-id": "1311.5270v2", 
    "author": "Pekka Abrahamsson", 
    "publish": "2013-11-21T00:03:26Z", 
    "summary": "Market-driven software intensive product development companies have been more\nand more experiencing the problem of feature expansion over time. Product\nmanagers face the challenge of identifying and locating the high value features\nin an application and weeding out the ones of low value from the next releases.\nCurrently, there are few methods and tools that deal with feature\nidentification and they address the problem only partially. Therefore, there is\nan urgent need of methods and tools that would enable systematic feature\nreduction to resolve issues resulting from feature creep. This paper presents\nan approach and an associated tool to automate feature identification for web\napplications. For empirical validation, a multiple case study was conducted\nusing three well known web applications: Youtube, Google and BBC. The results\nindicate that there is a good potential for automating feature identification\nin web applications."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.5587v1", 
    "title": "Dynamic Integration of ALM Tools for Agile Software Development", 
    "arxiv-id": "1311.5587v1", 
    "author": "J\u00f6rg L\u00e4ssig", 
    "publish": "2013-11-21T21:27:49Z", 
    "summary": "The paper describes the need for and goals of tool-integration within\nsoftware development processes. In particular we focus on agile software\ndevelopment but are not limited to. The integration of tools and data between\nthe different domains of the process is essential for an efficient, effective\nand customized software development. We describe what the next steps in the\npursuit of integration are and how major goals can be achieved. Beyond\ntheoretical and architectural considerations we describe the prototypical\nimplementation of an open platform approach. The paper introduces platform apps\nand a functionality store as general concepts to make apps and their\nfunctionalities available to the community. We describe the implementation of\nthe approach and how it can be practically utilized. The description is based\non one major use case and further steps are motivated by various other\nexamples."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.6145v1", 
    "title": "Towards a Formalism-Based Toolkit for Automotive Applications", 
    "arxiv-id": "1311.6145v1", 
    "author": "Manuel Mazzara", 
    "publish": "2013-11-24T17:02:17Z", 
    "summary": "The success of a number of projects has been shown to be significantly\nimproved by the use of a formalism. However, there remains an open issue: to\nwhat extent can a development process based on a singular formal notation and\nmethod succeed. The majority of approaches demonstrate a low level of\nflexibility by attempting to use a single notation to express all of the\ndifferent aspects encountered in software development. Often, these approaches\nleave a number of scalability issues open. We prefer a more eclectic approach.\nIn our experience, the use of a formalism-based toolkit with adequate notations\nfor each development phase is a viable solution. Following this principle, any\nspecific notation is used only where and when it is really suitable and not\nnecessarily over the entire software lifecycle. The approach explored in this\narticle is perhaps slowly emerging in practice - we hope to accelerate its\nadoption. However, the major challenge is still finding the best way to\ninstantiate it for each specific application scenario. In this work, we\ndescribe a development process and method for automotive applications which\nconsists of five phases. The process recognizes the need for having adequate\n(and tailored) notations (Problem Frames, Requirements State Machine Language,\nand Event-B) for each development phase as well as direct traceability between\nthe documents produced during each phase. This allows for a stepwise\nverification/validation of the system under development. The ideas for the\nformal development method have evolved over two significant case studies\ncarried out in the DEPLOY project."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1569", 
    "link": "http://arxiv.org/pdf/1311.6221v1", 
    "title": "The Effects of GQM+Strategies on Organizational Alignment", 
    "arxiv-id": "1311.6221v1", 
    "author": "Jari Partanen", 
    "publish": "2013-11-25T07:42:08Z", 
    "summary": "The increasing role of software for developing products and services requires\nthat organizations align their software-related activities with high-level\nbusiness goals. In practice, this alignment is very difficult and only little\nsystematic support is available. GQM+Strategies is a method that aims at\naligning organizational goals, strategies, and measurements at all levels of an\norganization in a seamless way. This article describes a case study of applying\nGQM+Strategies in a globally op- erating industrial R&D organization developing\nspecial-purpose device products for B2B customers. The study analyzes how\nGQM+Strategies has helped clarify and harmonize the goal set of the\norganization. Results of the study indicate improved alignment and integration\nof different goals. In addition, the method helped to make the initially\ninformal goal-setting more transparent and consequently enabled revising it\nwhile new, more important goals were discovered and comprehended. Moreover,\nseveral elements affecting the achievement of goals as well as impediments were\nidentified."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MC.2010.108", 
    "link": "http://arxiv.org/pdf/1311.6224v1", 
    "title": "Linking Software Development and Business Strategy Through Measurement", 
    "arxiv-id": "1311.6224v1", 
    "author": "Adam Trendowicz", 
    "publish": "2013-11-25T08:11:56Z", 
    "summary": "Most of today's products and services are software-based. Organizations that\ndevelop software want to maintain and improve their competitiveness by\ncontrolling software-related risks. To do this, they need to align their\nbusiness goals with software development strategies and translate them into\nquantitative project management. There is also an increasing need to justify\ncost and resources for software and system development and other IT services by\ndemonstrating their impact on an organisation's higher-level goals. For both,\nlinking business goals and software-related efforts in an organization is\nnecessary. However, this is a challenging task, and there is a lack of methods\naddressing this gap. The GQM+Strategies approach effectively links goals and\nstrategies on all levels of an organization by means of goal-oriented\nmeasurement. The approach is based on rationales for deciding about options\nwhen operationalizing goals and for evaluating the success of strategies with\nrespect to goals."
},{
    "category": "cs.SE", 
    "doi": "10.1109/MC.2010.108", 
    "link": "http://arxiv.org/pdf/1311.6249v1", 
    "title": "Distributed-Pair Programming can work well and is not just Distributed   Pair-Programming", 
    "arxiv-id": "1311.6249v1", 
    "author": "Stephan Salinger", 
    "publish": "2013-11-25T10:13:49Z", 
    "summary": "Background: Distributed Pair Programming can be performed via screensharing\nor via a distributed IDE. The latter offers the freedom of concurrent editing\n(which may be helpful or damaging) and has even more awareness deficits than\nscreen sharing. Objective: Characterize how competent distributed pair\nprogrammers may handle this additional freedom and these additional awareness\ndeficits and characterize the impacts on the pair programming process. Method:\nA revelatory case study, based on direct observation of a single, highly\ncompetent distributed pair of industrial software developers during a 3-day\ncollaboration. We use recordings of these sessions and conceptualize the\nphenomena seen. Results: 1. Skilled pairs may bridge the awareness deficits\nwithout visible obstruction of the overall process. 2. Skilled pairs may use\nthe additional editing freedom in a useful limited fashion, resulting in\npotentially better fluency of the process than local pair programming.\nConclusion: When applied skillfully in an appropriate context, distributed-pair\nprogramming can (not will!) work at least as well as local pair programming."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-27213-4_12", 
    "link": "http://arxiv.org/pdf/1311.6264v1", 
    "title": "Inspection and Test Process Integration Based on Explicit Test   Prioritization Strategies", 
    "arxiv-id": "1311.6264v1", 
    "author": "Robert Eschbach", 
    "publish": "2013-11-25T11:14:07Z", 
    "summary": "Today's software quality assurance techniques are often applied in isolation.\nConsequently, synergies resulting from systematically integrating different\nquality assurance activities are often not exploited. Such combinations promise\nbenefits, such as a reduction in quality assurance effort or higher defect\ndetection rates. The integration of inspection and testing, for instance, can\nbe used to guide testing activities. For example, testing activities can be\nfocused on defect-prone parts based upon inspection results. Existing\napproaches for predicting defect-prone parts do not make systematic use of the\nresults from inspections. This article gives an overview of an integrated\ninspection and testing approach, and presents a preliminary case study aiming\nat verifying a study design for evaluating the approach. First results from\nthis preliminary case study indicate that synergies resulting from the\nintegration of inspection and testing might exist, and show a trend that\ntesting activities could be guided based on inspection results."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_35", 
    "link": "http://arxiv.org/pdf/1311.6329v2", 
    "title": "Flexible Invariants Through Semantic Collaboration", 
    "arxiv-id": "1311.6329v2", 
    "author": "Bertrand Meyer", 
    "publish": "2013-11-25T15:16:35Z", 
    "summary": "Modular reasoning about class invariants is challenging in the presence of\ndependencies among collaborating objects that need to maintain global\nconsistency. This paper presents semantic collaboration: a novel methodology to\nspecify and reason about class invariants of sequential object-oriented\nprograms, which models dependencies between collaborating objects by semantic\nmeans. Combined with a simple ownership mechanism and useful default schemes,\nsemantic collaboration achieves the flexibility necessary to reason about\ncomplicated inter-object dependencies but requires limited annotation burden\nwhen applied to standard specification patterns. The methodology is implemented\nin AutoProof, our program verifier for the Eiffel programming language (but it\nis applicable to any language supporting some form of representation\ninvariants). An evaluation on several challenge problems proposed in the\nliterature demonstrates that it can handle a variety of idiomatic collaboration\npatterns, and is more widely applicable than the existing invariant\nmethodologies."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_35", 
    "link": "http://arxiv.org/pdf/1311.6606v1", 
    "title": "Random Grammar-based Testing for Covering All Non-Terminals", 
    "arxiv-id": "1311.6606v1", 
    "author": "Olga Kouchnarenko", 
    "publish": "2013-11-26T09:57:51Z", 
    "summary": "In the context of software testing, generating complex data inputs is\nfrequently performed using a grammar-based specification. For combinatorial\nreasons, an exhaustive generation of the data -- of a given size -- is\npractically impossible, and most approaches are either based on random\ntechniques or on coverage criteria. In this paper, we show how to combine these\ntwo techniques by biasing the random generation in order to optimise the\nprobability of satisfying a coverage criterion."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_35", 
    "link": "http://arxiv.org/pdf/1311.6659v1", 
    "title": "MARTE Profile-based MDA approach for semantic NFP-aware Web services", 
    "arxiv-id": "1311.6659v1", 
    "author": "Ounsa Roudies", 
    "publish": "2013-11-26T13:29:24Z", 
    "summary": "Non Functional Properties (NFPs) such as security, quality of service and\nbusiness related properties enhance the service description and provide\nnecessary information about the fitness of its behaviour. These properties have\nbecome crucial criteria for efficient selection and composition of Web\nservices. However, they belong to different domains, are complex, change\nfrequently and have to be semantically described. The W3C standard\nWSPolicy,recommended to describe these properties doesn t define standardized\nspecifications that cover all NFPs domains. Moreover, it doesn t provide an\neasy manner to express them independently of domains, and doesn t support their\nsemantic. This paper proposes a Model driven approach to describe and\nautomatically generate enriched Web services including semantic NFPs. It\nexplores both the use of the OMG Profile for Modelling and Analysis of\nReal-Time Embedded Systems (MARTE) and the W3C standards. Mapping rules, from\nNFPs profile to WS-Policy and SAWSDL files, transforms NFPs into policies\nassociated with WSDL elements."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_35", 
    "link": "http://arxiv.org/pdf/1311.6933v1", 
    "title": "Attaining High-performing Software Teams with Agile and Lean Practices:   An Empirical Case Study", 
    "arxiv-id": "1311.6933v1", 
    "author": "J\u00fcrgen M\u00fcnch", 
    "publish": "2013-11-27T11:17:43Z", 
    "summary": "This paper presents an empirical study on how self- organized software teams\ncould attain high performance using agile and lean practices. In particular,\nthe paper qualitatively examines characteristics of high performance and self-\norganization in one project team. The case under study is a customer-driven\nstudent project, carried out to develop an alpha-version prototype. The paper\nalso studies how certain agile software practices aid in initialising\nself-organization in the team. The main results indicate that self-organization\nas supported by certain Agile and Lean practices helps teams in achieving\nhigher performance."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-319-06410-9_35", 
    "link": "http://arxiv.org/pdf/1311.7313v2", 
    "title": "Improving CASA Runtime Performance by Exploiting Basic Feature Model   Analysis", 
    "arxiv-id": "1311.7313v2", 
    "author": "Alexander Egyed", 
    "publish": "2013-11-28T13:28:25Z", 
    "summary": "In Software Product Line Engineering (SPLE) families of systems are designed,\nrather than developing the individual systems independently. Combinatorial\nInteraction Testing has proven to be effective for testing in the context of\nSPLE, where a representative subset of products is chosen for testing in place\nof the complete family. Such a subset of products can be determined by\ncomputing a so called t-wise Covering Array (tCA), whose computation is\nNP-complete. Recently, reduction rules that exploit basic feature model\nanalysis have been proposed that reduce the number of elements that need to be\nconsidered during the computation of tCAs for Software Product Lines (SPLs). We\napplied these rules to CASA, a simulated annealing algorithm for tCA generation\nfor SPLs. We evaluated the adapted version of CASA using 133 publicly available\nfeature models and could record on average a speedup of $61.8\\%$ of median\nexecution time, while at the same time preserving the coverage of the generated\narray."
},{
    "category": "cs.SE", 
    "doi": "10.7321/jscse.v3.n3.19", 
    "link": "http://arxiv.org/pdf/1402.0157v1", 
    "title": "UML Artifacts Reuse: State of the Art", 
    "arxiv-id": "1402.0157v1", 
    "author": "Moataz A. Ahmed", 
    "publish": "2014-02-02T08:40:45Z", 
    "summary": "The benefits that can be derived from reusing software include accelerated\ndevelopment, reduced cost, reduced risk and effective use of specialists. Reuse\nof software artifacts during the initial stages of software development\nincreases reuse benefits, because it allows subsequent reuse of later stage\nartifacts derived from earlier artifacts. UML is the de facto modeling language\nused by software developers during the initial stages of software development\nsuch as requirements engineering, architectural and detailed design. This\nsurvey analyzes previous works on UML artifacts reuse. The analysis considers\nfour perspectives: retrieval method, artifact support, tool support and\nexperiments performed. As an outcome of the analysis, some suggestions for\nfuture work on UML artifacts reuse are also provided"
},{
    "category": "cs.SE", 
    "doi": "10.7321/jscse.v3.n3.25", 
    "link": "http://arxiv.org/pdf/1402.0160v1", 
    "title": "A framework for reuse of multi-view UML artifacts", 
    "arxiv-id": "1402.0160v1", 
    "author": "Moataz Ahmed", 
    "publish": "2014-02-02T08:45:47Z", 
    "summary": "Software is typically modeled from different viewpoints such as structural\nview, behavioral view and functional view. Few existing works can be considered\nas applying multi-view retrieval approaches. A number of important issues\nregarding mapping of entities during multi-view retrieval of UML models is\nidentified in this study. In response, we describe a framework for reusing UML\nartifacts, and discuss how our retrieval approach tackles the identified\nissues."
},{
    "category": "cs.SE", 
    "doi": "10.7321/jscse.v3.n3.25", 
    "link": "http://arxiv.org/pdf/1402.0292v1", 
    "title": "GQM+Strategies: A Comprehensive Methodology for Aligning Business   Strategies with Software Measurement", 
    "arxiv-id": "1402.0292v1", 
    "author": "Adam Trendowicz", 
    "publish": "2014-02-03T06:32:53Z", 
    "summary": "In software-intensive organizations, an organizational management system will\nnot guarantee organizational success unless the business strategy can be\ntranslated into a set of operational software goals. The Goal Question Metric\n(GQM) approach has proven itself useful in a variety of industrial settings to\nsupport quantitative software project management. However, it does not address\nlinking software measurement goals to higher-level goals of the organization in\nwhich the software is being developed. This linkage is important, as it helps\nto justify software measurement efforts and allows measurement data to\ncontribute to higher-level decisions. In this paper, we propose a\nGQM+Strategies(R) measurement approach that builds on the GQM approach to plan\nand implement software measurement. GQM+Strategies(R) provides mechanisms for\nexplicitly linking software measurement goals to higher-level goals for the\nsoftware organization, and further to goals and strategies at the level of the\nentire business. The proposed method is illustrated in the context of an\nexample application of the method."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-05290-3_34", 
    "link": "http://arxiv.org/pdf/1402.0294v1", 
    "title": "Systematic Task Allocation Evaluation in Distributed Software   Development", 
    "arxiv-id": "1402.0294v1", 
    "author": "Ansgar Lamersdorf", 
    "publish": "2014-02-03T07:01:50Z", 
    "summary": "Systematic task allocation to different development sites in global software\nde- velopment projects can open business and engineering perspectives and help\nto reduce risks and problems inherent in distributed development. Relying only\non a single evaluation criterion such as development cost when distributing\ntasks to development sites has shown to be very risky and often does not lead\nto successful solutions in the long run. Task allocation in global software\nprojects is challenging due to a multitude of impact factors and constraints.\nSystematic allocation decisions require the ability to evaluate and compare\ntask allocation alternatives and to effectively establish customized task\nallocation practices in an organization. In this article, we present a\ncustomizable process for task allocation evaluation that is based on results\nfrom a systematic interview study with practitioners. In this process, the\nrelevant criteria for evaluating task allocation alternatives are derived by\napplying principles from goal-oriented measurement. In addition, the\ncustomization of the process is demonstrated, related work and limitations are\nsketched, and an outlook on future work is given."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICIAFS.2008.4783956", 
    "link": "http://arxiv.org/pdf/1402.0670v1", 
    "title": "Axis2UNO: Web Services Enabled Openoffice.org", 
    "arxiv-id": "1402.0670v1", 
    "author": "S. Radhakrishnan", 
    "publish": "2014-02-04T09:31:51Z", 
    "summary": "Openoffice.org is a popular, free and open source office product. This\nproduct is used by millions of people and developed, maintained and extended by\nthousands of developers worldwide. Playing a dominant role in the web, web\nservices technology is serving millions of people every day. Axis2 is one of\nthe most popular, free and open source web service engines. The framework\npresented in this paper, Axis2UNO, a combination of such two technologies is\ncapable of making a new era in office environment. Two other attempts to\nenhance web services functionality in office products are Excel Web Services\nand UNO Web Service Proxy. Excel Web Services is combined with Microsoft\nSharePoint technology and exposes information sharing in a different\nperspective within the proprietary Microsoft office products. UNO Web Service\nProxy is implemented with Java Web Services Developer Pack and enables basic\nweb services related functionality in Openoffice.org. However, the work\npresented here is the first one to combine Openoffice.org and Axis2 and we\nexpect it to outperform the other efforts with the community involvement and\nfeature richness in those products."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICIAFS.2008.4783956", 
    "link": "http://arxiv.org/pdf/1402.0780v1", 
    "title": "Model performance indicators ERP systems", 
    "arxiv-id": "1402.0780v1", 
    "author": "Masoud Rafighi", 
    "publish": "2014-02-04T15:59:48Z", 
    "summary": "Implementation process ERP is complex and expensive process. Typically always\nbe faced with many failures. Successfully implemented in an organization has\nmany challenges. Organizations in the deployment and success of the system\ndepends on several factors.One of the key factors in the successful deployment\nof systems methodology is the implementation process. Methodology has several\nindicators for successful implementation of ERP systems, we have examined. And\nindicators for each of the methodologies have identified. The proposed method\nis also an important indicator of the success of security controls and\nindicators to be monitored and controlled."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICIAFS.2008.4783956", 
    "link": "http://arxiv.org/pdf/1402.0997v1", 
    "title": "Engineering Adaptive Digital Investigations using Forensics Requirements", 
    "arxiv-id": "1402.0997v1", 
    "author": "Bashar Nuseibeh", 
    "publish": "2014-02-05T10:19:25Z", 
    "summary": "A digital forensic investigation aims to collect and analyse the evidence\nnecessary to demonstrate a potential hypothesis of a digital crime. Despite the\navailability of several digital forensics tools, investigators still approach\neach crime case from scratch, postulating potential hypotheses and analysing\nlarge volumes of data. This paper proposes to explicitly model forensic\nrequirements in order to engineer software systems that are forensic-ready and\nguide the activities of a digital investigation. Forensic requirements relate\nsome speculative hypotheses of a crime to the evidence that should be collected\nand analysed in a crime scene. In contrast to existing approaches, we propose\nto perform proactive activities to preserve important - potentially ephemeral -\nevidence, depending on the risk of a crime to take place. Once an investigation\nstarts, the evidence collected proactively is analysed to assess if some of the\nspeculative hypotheses of a crime hold and what further evidence is necessary\nto support them. For each hypothesis that is satisfied, a structured argument\nis generated to demonstrate how the evidence collected supports that\nhypothesis. Our evaluation results suggest that the approach provides correct\ninvestigative findings and reduces significantly the amount of evidence to be\ncollected and the hypotheses to be analysed."
},{
    "category": "cs.SE", 
    "doi": "10.7321/jscse.v3.n3.20", 
    "link": "http://arxiv.org/pdf/1402.1012v1", 
    "title": "Sequencing Participatory Action Research and i* Modeling Framework in   Capturing Multiple Roles Requirements", 
    "arxiv-id": "1402.1012v1", 
    "author": "Ariza Nordin", 
    "publish": "2014-02-05T11:12:20Z", 
    "summary": "This paper presents the conceptual framework for sequencing of Participatory\nAction Research (PAR) methodology with the implementation of i* modeling\nframework in capturing multiple roles requirements. There are multiple roles\ninvolved in the development of information system, thus it involves with\ndifference users requirements and preferences, context as well as the demands\nwhich become a challenge in development of system. This is due to these roles\nwhere information of the project monitoring is perceived in accordance to their\nrole and domain. In the development of information systems, requirement\nengineering is a vital methodology. Requirement engineering (RE) consists of\nseveral phases which elicitation is a crucial phase in RE since it requires\nresearcher to gather the requirement from the users. Methods of eliciting\nrequirements are now more co-operative. Based on the preliminary study of\nconstruction-based in Malaysia, evidence of dynamic requirements has been\nobserved according to the environments, economic, technology and manpower\ninvolved in the construction project. An adaptive design for project monitoring\nis needed which allow the physical system to self-adapt in response to the\nchanging environments. Adaptive design requires selecting the right techniques\nof requirements elicitation. The conceptual framework defined shall be used to\nelicit requirements from a local construction company."
},{
    "category": "cs.SE", 
    "doi": "10.1109/SUITE.2012.6225476", 
    "link": "http://arxiv.org/pdf/1402.1188v1", 
    "title": "On Designing Better Tools for Learning APIs", 
    "arxiv-id": "1402.1188v1", 
    "author": "Robert DeLine", 
    "publish": "2014-02-05T21:29:28Z", 
    "summary": "Modern software development requires a large investment in learning\napplication programming interfaces (APIs). Recent research found that the\nlearning materials themselves are often inadequate: developers struggle to find\nanswers beyond simple usage scenarios. Solving these problems requires a large\ninvestment in tool and search engine development. To understand where further\ninvestment would be most useful, we ran a study with 19 professional developers\nto understand what a solution might look like, free of technical constraints.\nIn this paper, we report on design implications of tools for API learning,\ngrounded in the reality of the professional developers themselves. The\nreoccurring themes in the participants' feedback were trustworthiness,\nconfidentiality, information overload and the need for code examples as\nfirst-class documentation artifacts."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2008.15", 
    "link": "http://arxiv.org/pdf/1402.1563v1", 
    "title": "Towards a Multi-criteria Development Distribution Model: An Analysis of   Existing Task Distribution Approaches", 
    "arxiv-id": "1402.1563v1", 
    "author": "Dieter Rombach", 
    "publish": "2014-02-07T07:16:12Z", 
    "summary": "Distributing development tasks in the context of global software development\nbears both many risks and many opportunities. Nowadays, distributed development\nis often driven by only a few factors or even just a single factor such as\nworkforce costs. Risks and other relevant factors such as workforce\ncapabilities, the innovation potential of different regions, or cultural\nfactors are often not recognized sufficiently. This could be improved by using\nempirically-based multi-criteria distribution models. Currently, there is a\nlack of such decision models for distributing software development work. This\narticle focuses on mechanisms for such decision support. First, requirements\nfor a distribution model are formulated based on needs identified from\npractice. Then, distribution models from different domains are surveyed,\ncompared, and analyzed in terms of suitability. Finally, research questions and\ndirections for future work are given."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2008.15", 
    "link": "http://arxiv.org/pdf/1402.1925v1", 
    "title": "Survey on software testing techniques in cloud computing", 
    "arxiv-id": "1402.1925v1", 
    "author": "Malathi. A", 
    "publish": "2014-02-09T07:59:32Z", 
    "summary": "Cloud computing is the next stage of the internet evolution. It relies on\nsharing of resources to achieve coherence on a network. It is emerged as new\ncomputing standard that impacts several different research fields, including\nsoftware testing. There are various software techniques used for testing\napplication. It not only changes the way of obtaining computing resources but\nalso changes the way of managing and delivering computing services,\ntechnologies and solutions, meanwhile it causes new issues, challenges and\nneeds in software testing. Software testing in cloud can reduce the need for\nhardware and software resources and offer a flexible and efficient alternative\nto the traditional software testing process. This paper provides an overview\nregarding trends, oppurtunities, challenges, issues, and needs in cloud testing\nand cloud based application."
},{
    "category": "cs.SE", 
    "doi": "10.2478/amcs-2014-0069", 
    "link": "http://arxiv.org/pdf/1402.1978v3", 
    "title": "A System for Deduction-based Formal Verification of Workflow-oriented   Software Models", 
    "arxiv-id": "1402.1978v3", 
    "author": "Radoslaw Klimek", 
    "publish": "2014-02-09T19:09:59Z", 
    "summary": "The work concerns formal verification of workflow-oriented software models\nusing deductive approach. The formal correctness of a model's behaviour is\nconsidered. Manually building logical specifications, which are considered as a\nset of temporal logic formulas, seems to be the significant obstacle for an\ninexperienced user when applying the deductive approach. A system, and its\narchitecture, for the deduction-based verification of workflow-oriented models\nis proposed. The process of inference is based on the semantic tableaux method\nwhich has some advantages when compared to traditional deduction strategies.\nThe algorithm for an automatic generation of logical specifications is\nproposed. The generation procedure is based on the predefined workflow patterns\nfor BPMN, which is a standard and dominant notation for the modeling of\nbusiness processes. The main idea for the approach is to consider patterns,\ndefined in terms of temporal logic,as a kind of (logical) primitives which\nenable the transformation of models to temporal logic formulas constituting a\nlogical specification. Automation of the generation process is crucial for\nbridging the gap between intuitiveness of the deductive reasoning and the\ndifficulty of its practical application in the case when logical specifications\nare built manually. This approach has gone some way towards supporting,\nhopefully enhancing our understanding of, the deduction-based formal\nverification of workflow-oriented models."
},{
    "category": "cs.SE", 
    "doi": "10.2478/amcs-2014-0069", 
    "link": "http://arxiv.org/pdf/1402.1985v1", 
    "title": "Generating Logical Specifications from Requirements Models for   Deduction-based Formal Verification", 
    "arxiv-id": "1402.1985v1", 
    "author": "Radoslaw Klimek", 
    "publish": "2014-02-09T20:22:34Z", 
    "summary": "The work concerns automatic generation of logical specifications from\nrequirements models. Logical specifications obtained in such a way can be\nsubjected to formal verification using deductive reasoning. Formal verification\nconcerns correctness of a model behaviour. Reliability of the requirements\nengineering is essential for all phases of software development processes.\nDeductive reasoning is an important alternative among other formal methods.\nHowever, logical specifications, considered as sets of temporal logic formulas,\nare difficult to specify manually by inexperienced users and this fact can be\nregarded as a significant obstacle to practical use of deduction-based\nverification tools. A method of building requirements models using some UML\ndiagrams, including their logical specifications, is presented step by step.\nOrganizing activity diagrams into predefined workflow patterns enables\nautomated extraction of logical specifications. The crucial aspect of the\npresented approach is integrating the requirements engineering phase and the\nautomatic generation of logical specifications. A system of the deduction-based\nverification is proposed. The reasoning process could be based on the semantic\ntableaux method. A simple yet illustrative example of the requirements\nelicitation and verification is provided."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2144v1", 
    "title": "A Framework for Enhancing Performance and Handling Run-Time Uncertainty   in Self-Adaptive Systems", 
    "arxiv-id": "1402.2144v1", 
    "author": "Mohammed Abufouda", 
    "publish": "2014-02-10T13:49:53Z", 
    "summary": "Self-adaptivity allows software systems to autonomously adjust their behavior\nduring run-time to reduce the cost complexities caused by manual maintenance.\nIn this paper, a framework for building an external adaptation engine for\nself-adaptive software systems is proposed. In order to improve the quality of\nself-adaptive software systems, this research addresses two challenges in\nself-adaptive software systems. The first challenge is to provide better\nperformance of the adaptation engine by managing the complexity of the\nadaptation space efficiently and the second challenge is handling run-time\nuncertainty that hinders the adaptation process. This research utilizes\nCase-based Reasoning as an adaptation engine along with utility functions for\nrealizing the managed system's requirements."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2271v1", 
    "title": "An Optimized Semantic Web Service Composition Method Based on Clustering   and Ant Colony Algorithm", 
    "arxiv-id": "1402.2271v1", 
    "author": "Mehrdad Jalali", 
    "publish": "2014-02-10T20:59:22Z", 
    "summary": "In today's Web, Web Services are created and updated on the fly. For\nanswering complex needs of users, the construction of new web services based on\nexisting ones is required. It has received a great attention from different\ncommunities. This problem is known as web services composition. However, it is\none of big challenge problems of recent years in a distributed and dynamic\nenvironment. Web services can be composed manually but it is a time consuming\ntask. The automatic web service composition is one of the key features for\nfuture the semantic web. The various approaches in field of web service\ncompositions proposed by the researchers. In this paper, we propose a novel\narchitecture for semantic web service composition using clustering and Ant\ncolony algorithm."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2372v1", 
    "title": "Design Patterns as Quality Influencing Factor in Object Oriented Design   Approach", 
    "arxiv-id": "1402.2372v1", 
    "author": "Vasanth Kumar. H", 
    "publish": "2014-02-11T04:58:01Z", 
    "summary": "Object Oriented Design methodology is an emerging software development\napproach for complex systems with huge set of requirements. Unlike procedural\napproach, it captures the requirements as a set of data rather than services,\nencapsulated as a single entity. The success such a project relies on major\nfactors like design patterns framework, key principles, metric standards and\nbest practices adapted by the industry. The patterns are key structures for\nrecursive problem bits in the problem domain. The combination of design\npatterns forms a framework which suits the problem statement in hand. The\npattern includes static design and dynamic behavior of different types of\nentities which can be mapped as a functional diagram with cardinalities between\nthem. The degree of cardinality represents the coupling factor which the\nindustry perceives and measures for software design quality. The organization\nspecific design principles and rich repository of on-the-shelf patterns are the\nmajor design-quality-influencing-factor contribute to software success. These\nare the asset of an industry to deliver a quality product to sustain itself in\nthe competitive market."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2373v1", 
    "title": "Visualization of Object Oriented Modeling from the Perspective of Set   theory", 
    "arxiv-id": "1402.2373v1", 
    "author": "Suma. V", 
    "publish": "2014-02-11T05:05:03Z", 
    "summary": "Language is a medium for communication of our thoughts. Natural language is\ntoo wide to conceive and formulate the thoughts and ideas in a precise way. As\nscience and technology grows, the necessity of languages arouses through which\nthe thoughts are expressed in a better manner. Set Theory is such a\nmathematical language for expressing the thought of interest in a realistic\nway. It is well suited for presenting object oriented solution model, since\nthis implementation methodology analyzes and modulates the requirements in a\nrealistic way. Since the design flaws are one of the factors for software\nfailure, industries are focusing on minimizing the design defects through\nbetter solution modeling techniques and quality assessment practices. The\nObject Oriented (OO) solution space can be visualized using the language of Set\ntheory with which the design architecture of modules can be well defined. It\nprovides a strong base to quantify the relationships within and between the\nmodules, which is a mode for measuring the complexity of solution design of any\nsoftware projects. This paper provides a visualization of OO modeling from the\nperspective of Set theory. Thereby, it paves the path for the designers to\neffectively design the application which is one of the challenges of a project\ndevelopment. Further, this mode of visualization enables one to effectively\nmeasure and controls the design complexity leading towards reducing the design\nflaws and enhanced software quality."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2374v1", 
    "title": "Factors Modulating Software Design Quality", 
    "arxiv-id": "1402.2374v1", 
    "author": "Suma. V", 
    "publish": "2014-02-11T05:12:18Z", 
    "summary": "Object oriented approach is one of the popular software development approach\nfor managing complex systems with massive set of requirements. Unlike\nprocedural approach, this approach captures the requirements as set of data\nrather than services. Further, class is considered as a key unit of the\nsolution-domain with data and services wrapped together, representing\narchitectural design of a basic module. Thus, system complexity is directly\nrelated to the number of modules and the degree of interaction between them.\nThis could be mapped as a functional diagram with cardinalities between the\nmodules. However, complexity is always a threat to quality at each stage of\nsoftware development. Design phase is therefore one of the core influencing\nphases during development that selects the right architecture based on the\nproblem statement which is bound to be measured for quality. Hence, software\nindustries adapts several organization- specific principles, domain-specific\npatterns, metric standards and best practices to improve and measure the\nquality of both process and product. The paper highlights the factors which\ninfluence the overall design quality and metrics implication in improving the\nquality of final product. It also presents the solution domain as an\ninterdependent layered architecture which has a greater impact on concluding\nthe quality of the end product. This approach of design is a unique\ncontribution to the domain of Object Oriented approach of software development.\nIt also focuses on design metrics which ensures the implementation of right\nchoice of design towards the retention of quality of the product."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2375v1", 
    "title": "Significance of Coupling and Cohesion on Design Quality", 
    "arxiv-id": "1402.2375v1", 
    "author": "Suma. V", 
    "publish": "2014-02-11T05:19:56Z", 
    "summary": "In recent years, the complexity of the software is increasing due to\nautomation of every segment of application. Software is nowhere remained as\none-time development product since its architectural dimension is increasing\nwith addition of new requirements over a short duration. Object Oriented\nDevelopment (OOD) methodology is a popular development approach for such\nsystems which perceives and models the requirements as real world entities.\nClasses and Objects logically represent the entities in the solution space and\nquality of the software is directly depending on the design quality of these\nlogical entities. Cohesion and Coupling (C&C) are two major design decisive\nfactors in OOD which impacts the design of a class and dependency between them\nin complex software. It is also most significant to measure C&C for software to\ncontrol the complexity level as requirements increases. Several metrics are in\npractice to quantify C&C which plays a major role in measuring the design\nquality. The software industries are focusing on increasing and measuring the\nquality of the product through quality design to continue their market image in\nthe competitive world. As a part of our research, this paper highlights on the\nimpact of C&C on design quality of a complex system and its measures to\nquantify the overall quality of software."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2376v1", 
    "title": "Prediction of Human Performance Capability during Software Development   using Classification", 
    "arxiv-id": "1402.2376v1", 
    "author": "Suma V", 
    "publish": "2014-02-11T05:30:26Z", 
    "summary": "The quality of human capital is crucial for software companies to maintain\ncompetitive advantages in knowledge economy era. Software companies recognize\nsuperior talent as a business advantage. They increasingly recognize the\ncritical linkage between effective talent and business success. However,\nsoftware companies suffering from high turnover rates often find it hard to\nrecruit the right talents. There is an urgent need to develop a personnel\nselection mechanism to find the talents who are the most suitable for their\nsoftware projects. Data mining techniques assures exploring the information\nfrom the historical projects depending on which the project manager can make\ndecisions for producing high quality software. This study aims to fill the gap\nby developing a data mining framework based on decision tree and association\nrules to refocus on criteria for personnel selection. An empirical study was\nconducted in a software company to support their hiring decision for project\nmembers. The results demonstrated that there is a need to refocus on selection\ncriteria for quality objectives. Better selection criteria was identified by\npatterns obtained from data mining models by integrating knowledge from\nsoftware project database and authors research techniques."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2377v1", 
    "title": "Empirical Study on Selection of Team Members for Software Projects -   Data Mining Approach", 
    "arxiv-id": "1402.2377v1", 
    "author": "Suma. V", 
    "publish": "2014-02-11T05:39:50Z", 
    "summary": "One of the essential requisites of any software industry is the development\nof customer satisfied products. However, accomplishing the aforesaid business\nobjective depends upon the depth of quality of product that is engineered in\nthe organization. Thus, generation of high quality depends upon process, which\nis in turn depends upon the people. Existing scenario in IT industries demands\na requirement for deploying the right personnel for achieving desirable quality\nin the product through the existing process. The goal of this paper is to\nidentify the criteria which will be used in industrial practice to select\nmembers of a software project team, and to look for relationships between these\ncriteria and project success. Using semi-structured interviews and qualitative\nmethods for data analysis and synthesis, a set of team building criteria was\nidentified from project managers in industry. The findings show that the\nconsistent use of the set of criteria correlated significantly with project\nsuccess, and the criteria related to human factors present strong correlations\nwith software quality and thereby project success. This knowledge enables\ndecision making for project managers in allocation of right personnel to\nrealize desired level."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2379v1", 
    "title": "Enhancing Human Aspect of Software Engineering using Bayesian Classifier", 
    "arxiv-id": "1402.2379v1", 
    "author": "Suma V", 
    "publish": "2014-02-11T05:44:48Z", 
    "summary": "IT industries in current scenario have to struggle effectively in terms of\ncost, quality, service or innovation for their subsistence in the global\nmarket. Due to the swift transformation of technology, software industries owe\nto manage a large set of data having precious information hidden. Data mining\ntechnique enables one to effectively cope with this hidden information where it\ncan be applied to code optimization, fault prediction and other domains which\nmodulates the success nature of software projects. Additionally, the efficiency\nof the product developed further depends upon the quality of the project\npersonnel. The position of the paper therefore is to explore potentials of\nproject personnel in terms of their competency and skill set and its influence\non quality of project. The above mentioned objective is accomplished using a\nBayesian classifier in order to capture the pattern of human performance. By\nthis means, the hidden and valuable knowledge discovered in the related\ndatabases will be summarized in the statistical structure. This mode of\npredictive study enables the project managers to reduce the failure ratio to a\nsignificant level and improve the performance of the project using the right\nchoice of project personnel."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5106", 
    "link": "http://arxiv.org/pdf/1402.2389v1", 
    "title": "Supporting Process Maturation with the Enhanced CoBRA Method", 
    "arxiv-id": "1402.2389v1", 
    "author": "J\u00fcrgen M\u00fcnch", 
    "publish": "2014-02-11T07:52:16Z", 
    "summary": "Cost estimation is a very crucial field for software developing companies. In\nthe context of learning organizations, estimation applicability and accuracy\nare not the only acceptance criteria. The contribution of an estimation\ntechnique to the understanding and maturing of related organizational processes\n(such as identification of cost and productivity factors, measurement, data\nvalidation, model validation, model maintenance) has recently been gaining\nincreasing importance. Yet, most of the proposed cost modeling approaches\nprovide software engineers with hardly any assistance in supporting related\nprocesses. Insufficient support is provided for validating created cost models\n(including underlying data collection processes) or, if valid models are\nobtained, for applying them to achieve an organization's objectives such as\nimproved productivity or reduced schedule. This paper presents an enhancement\nof the CoBRA(R) cost modeling method by systematically including additional\nquantitative methods into iterative analysis-feedback cycles. Applied at Oki\nElectric Industry Co., Ltd., Japan, the CoBRA(R) method contributed to the\nachievement of the following objectives, including: (1) maturation of existing\nmeasurement processes, (2) increased expertise of Oki software project decision\nmakers regarding cost-related software processes, and, finally, (3) reduction\nof initial estimation error from an initial 120% down to 14%."
},{
    "category": "cs.SE", 
    "doi": "10.5121/csit.2014.4117", 
    "link": "http://arxiv.org/pdf/1402.2611v1", 
    "title": "Quality-aware Approach for Engineering Self-adaptive Software Systems", 
    "arxiv-id": "1402.2611v1", 
    "author": "Mohammed Abufouda", 
    "publish": "2014-02-11T19:49:26Z", 
    "summary": "Self-adaptivity allows software systems to autonomously adjust their behavior\nduring run-time to reduce the cost complexities caused by manual maintenance.\nIn this paper, an approach for building an external adaptation engine for\nself-adaptive software systems is proposed. In order to improve the quality of\nself-adaptive software systems, this research addresses two challenges in\nself-adaptive software systems. The first challenge is managing the complexity\nof the adaptation space efficiently and the second is handling the run-time\nuncertainty that hinders the adaptation process. This research utilizes\nCase-based Reasoning as an adaptation engine along with utility functions for\nrealizing the managed system's requirements and handling uncertainty."
},{
    "category": "cs.SE", 
    "doi": "10.5121/csit.2014.4117", 
    "link": "http://arxiv.org/pdf/1402.3107v1", 
    "title": "Rigorous Description Of Design Components Functionality: An Approach   Based Contract", 
    "arxiv-id": "1402.3107v1", 
    "author": "Zitouni Abdelhafid", 
    "publish": "2014-02-13T12:25:34Z", 
    "summary": "Current models for software components have made component-based software\nengineering practical. However, these models are limited in the sense that\ntheir support for the characterization/specification of design components\nprimarily deals with syntactic issues. To avoid mismatch and misuse of\ncomponents, more comprehensive specification of software components is\nrequired, In this paper, we present a contract-based approach to analyze and\nmodel the both aspects (functional and non-functional) properties of design\ncomponents and their composition in order to detect and correct composition\nerrors. This approach permits to characterize the structural, interface and\nbehavioural aspects of design component. To enable this we present a pattern\ncontract language that captures the structural and behavioral requirements\nassociated with a range of patterns, as well as the system properties that are\nguaranteed as a result. In addition, we propose the use of the LOTOS language\nas an ADL for formalizing these aspects. We illustrate the approach by applying\nit to a standard design"
},{
    "category": "cs.SE", 
    "doi": "10.5121/csit.2014.4117", 
    "link": "http://arxiv.org/pdf/1402.3821v1", 
    "title": "Efficient and Generalized Decentralized Monitoring of Regular Languages", 
    "arxiv-id": "1402.3821v1", 
    "author": "Yli\u00e8s Falcone", 
    "publish": "2014-02-16T17:49:57Z", 
    "summary": "The main contribution of this paper is an efficient and generalized\ndecentralized monitoring algorithm allowing to detect satisfaction or violation\nof any regular specification by local monitors alone in a system without\ncentral observation point. Our algorithm does not assume any form of\nsynchronization between system events and communication of monitors, uses state\nmachines as underlying mechanism for efficiency, and tries to keep the number\nand size of messages exchanged between monitors to a minimum. We provide a full\nimplementation of the algorithm with an open-source benchmark to evaluate its\nefficiency in terms of number, size of exchanged messages, and delay induced by\ncommunication between monitors. Experimental results demonstrate the\neffectiveness of our algorithm which outperforms the previous most general one\nalong several (new) monitoring metrics."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.infsof.2014.11.006", 
    "link": "http://arxiv.org/pdf/1402.3873v4", 
    "title": "An Empirical Study on Software Defect Prediction with a Simplified   Metric Set", 
    "arxiv-id": "1402.3873v4", 
    "author": "Yutao Ma", 
    "publish": "2014-02-17T02:29:16Z", 
    "summary": "Software defect prediction plays a crucial role in estimating the most\ndefect-prone components of software, and a large number of studies have pursued\nimproving prediction accuracy within a project or across projects. However, the\nrules for making an appropriate decision between within- and cross-project\ndefect prediction when available historical data are insufficient remain\nunclear. The objective of this work is to validate the feasibility of the\npredictor built with a simplified metric set for software defect prediction in\ndifferent scenarios, and to investigate practical guidelines for the choice of\ntraining data, classifier and metric subset of a given project. First, based on\nsix typical classifiers, we constructed three types of predictors using the\nsize of software metric set in three scenarios. Then, we validated the\nacceptable performance of the predictor based on Top-k metrics in terms of\nstatistical methods. Finally, we attempted to minimize the Top-k metric subset\nby removing redundant metrics, and we tested the stability of such a minimum\nmetric subset with one-way ANOVA tests. The experimental results indicate that\n(1) the choice of training data should depend on the specific requirement of\nprediction accuracy; (2) the predictor built with a simplified metric set works\nwell and is very useful in case limited resources are supplied; (3) simple\nclassifiers (e.g., Naive Bayes) also tend to perform well when using a\nsimplified metric set for defect prediction; and (4) in several cases, the\nminimum metric subset can be identified to facilitate the procedure of general\ndefect prediction with acceptable loss of prediction precision in practice. The\nguideline for choosing a suitable simplified metric set in different scenarios\nis presented in Table 12."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.infsof.2014.11.006", 
    "link": "http://arxiv.org/pdf/1402.3920v2", 
    "title": "A Framework for the Implementation of Industrial Automation Systems   Based on PLCs", 
    "arxiv-id": "1402.3920v2", 
    "author": "Kleanthis Thramboulidis", 
    "publish": "2014-02-17T07:59:49Z", 
    "summary": "Industrial automation systems (IASs) are traditionally developed using a\nsequential approach where the automation software, which is commonly based on\nthe IEC 61131 languages, is developed when the design and in many cases the\nimplementation of mechanical parts have been completed. However, it is claimed\nthat this approach does not lead to the optimal system design and that the IEC\n61131 does not meet new challenges in this domain. In this paper, a system\nengineering process based on the new version of IEC 61131, which supports\nObject Orientation, is presented. SysML and UML are utilized to introduce a\nhigher layer of abstraction in the design space of IAS and Internet of Things\n(IoT) is considered as an enabling technology for the integration of Cyber and\nCyber-physical components of the system, bringing into the industrial\nautomation domain the benefits of these technologies."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.infsof.2014.11.006", 
    "link": "http://arxiv.org/pdf/1402.3937v1", 
    "title": "A Hybrid Modified Semantic Matching Algorithm Based on Instances   Detection With Case Study on Renewable Energy", 
    "arxiv-id": "1402.3937v1", 
    "author": "Ahmad Khader Haboush", 
    "publish": "2014-02-17T09:29:02Z", 
    "summary": "This Matching input keywords with historical or information domain is an\nimportant point in modern computations in order to find the best match\ninformation domain for specific input queries. Matching algorithms represents\nhot area of researches in computer science and artificial intelligence. In the\narea of text matching, it is more reliable to study semantics of the pattern\nand query in terms of semantic matching. This paper improves the semantic\nmatching results between input queries and information ontology domain. The\ncontributed algorithm is a hybrid technique that is based on matching extracted\ninstances from booth, the queries and in information domain. The instances\nextraction algorithm that is presented in this paper are contributed which is\nbased on mathematical and statistical analysis of objects with respect to each\nother and also with respect to marked objects. The instances that are instances\nfrom the queries and information domain are subjected to semantic matching to\nfind the best match, match percentage, and to improve the decision making\nprocess. An application case was studied in this paper which is related to\nrenewable energy, where the input queries represents the customer requirements\ninput and the knowledge domain is renewable energy vendors profiles. The\ncomparison was made with most known recent matching researches."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.infsof.2014.11.006", 
    "link": "http://arxiv.org/pdf/1402.3944v1", 
    "title": "Context-driven Software Project Estimation", 
    "arxiv-id": "1402.3944v1", 
    "author": "Jens Heidrich", 
    "publish": "2014-02-17T09:53:10Z", 
    "summary": "Using quantitative data from past projects for software project estimation\nrequires context knowledge that characterizes its origin and indicates its\napplicability for future use. This article sketches the SPRINT I technique for\nproject planning and controlling. The underlying prediction mechanism is based\non the identification of similar past projects and the building of so-called\nclusters with typical data curves. The article focuses on how to characterize\nthese clusters with context knowledge and how to use context information from\nactual projects for prediction. The SPRINT approach is tool-supported and first\nevaluations have been conducted."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.infsof.2014.11.006", 
    "link": "http://arxiv.org/pdf/1402.4164v1", 
    "title": "Fine-grained Patches for Java Software Upgrades", 
    "arxiv-id": "1402.4164v1", 
    "author": "Eduardo R. B. Marques", 
    "publish": "2014-02-17T22:05:42Z", 
    "summary": "We present a novel methodology for deriving fine-grained patches of Java\nsoftware. We consider an abstract-syntax tree (AST) representation of Java\nclasses compiled to the Java Virtual Machine (JVM) format, and a difference\nanalysis over the AST representation to derive patches. The AST representation\ndefines an appropriate abstraction level for analyzing differences, yielding\ncompact patches that correlate modularly to actual source code changes. The\napproach contrasts to other common, coarse-grained approaches, like plain\nbinary differences, which may easily lead to disproportionately large patches.\nWe present the main traits of the methodology, a prototype tool called aspa\nthat implements it, and a case-study analysis on the use of aspa to derive\npatches for the Java 2 SE API. The case-study results illustrate that aspa\npatches have a significantly smaller size than patches derived by binary\ndifferencing tools."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2635868.2635883", 
    "link": "http://arxiv.org/pdf/1402.4182v3", 
    "title": "Learning Natural Coding Conventions", 
    "arxiv-id": "1402.4182v3", 
    "author": "Charles Sutton", 
    "publish": "2014-02-17T23:40:01Z", 
    "summary": "Every programmer has a characteristic style, ranging from preferences about\nidentifier naming to preferences about object relationships and design\npatterns. Coding conventions define a consistent syntactic style, fostering\nreadability and hence maintainability. When collaborating, programmers strive\nto obey a project's coding conventions. However, one third of reviews of\nchanges contain feedback about coding conventions, indicating that programmers\ndo not always follow them and that project members care deeply about adherence.\nUnfortunately, programmers are often unaware of coding conventions because\ninferring them requires a global view, one that aggregates the many local\ndecisions programmers make and identifies emergent consensus on style. We\npresent NATURALIZE, a framework that learns the style of a codebase, and\nsuggests revisions to improve stylistic consistency. NATURALIZE builds on\nrecent work in applying statistical natural language processing to source code.\nWe apply NATURALIZE to suggest natural identifier names and formatting\nconventions. We present four tools focused on ensuring natural code during\ndevelopment and release management, including code review. NATURALIZE achieves\n94% accuracy in its top suggestions for identifier names and can even transfer\nknowledge about conventions across projects, leveraging a corpus of 10,968 open\nsource projects. We used NATURALIZE to generate 18 patches for 5 open source\nprojects: 14 were accepted."
},{
    "category": "cs.SE", 
    "doi": "10.1002/spip.199", 
    "link": "http://arxiv.org/pdf/1402.4280v1", 
    "title": "Guided Support for Collaborative Modeling, Enactment and Simulation of   Software Development Processes", 
    "arxiv-id": "1402.4280v1", 
    "author": "J\u00fcrgen M\u00fcnch", 
    "publish": "2014-02-18T10:36:02Z", 
    "summary": "Recently, the awareness of the importance of distributed software development\nhas been growing in the software engineering community. Economic constraints,\nmore and more outsourcing of development activities, and the increasing spatial\ndistribution of companies come along with challenges of how to organize\ndistributed development.\n  In this article, we reason that a common process understanding is mandatory\nfor successful distributed development. Integrated process planning, guidance\nand enactment are seen as enabling technologies to reach a unique process view.\n  We sketch a synthesis of the software process modeling environment SPEARMINT\nand the XCHIPS system for web-based process support. Hereby, planners and\ndevelopers are provided with collaborative planning and enactment support and\nadvanced process guidance via electronic process guides (EPGs). We describe the\nusage of this integrated environment by using a case study for the development\nof a learning system."
},{
    "category": "cs.SE", 
    "doi": "10.1002/spip.199", 
    "link": "http://arxiv.org/pdf/1402.4320v1", 
    "title": "Turning Time from Enemy into an Ally Using the Pomodoro Technique", 
    "arxiv-id": "1402.4320v1", 
    "author": "Michael Lane", 
    "publish": "2014-02-18T13:02:00Z", 
    "summary": "Time is one of the most important factors dominating agile software\ndevelopment processes in distributed settings. Effective time management helps\nagile teams to plan and monitor the work to be performed, and create and\nmaintain a fast yet sustainable pace. The Pomodoro Technique is one promising\ntime management technique. Its application and adaptation in Sourcesense Milan\nTeam surfaced various benefits, challenges and implications for distributed\nagile software development. Lessons learnt from the experiences of Sourcesense\nMilan Team can be useful for other distributed agile teams to turn time from\nenemy into an ally."
},{
    "category": "cs.SE", 
    "doi": "10.1007/3-540-36209-6_30", 
    "link": "http://arxiv.org/pdf/1402.4597v1", 
    "title": "Empirically Driven Design of Software Development Processes for Wireless   Internet Services", 
    "arxiv-id": "1402.4597v1", 
    "author": "Gino Palladino", 
    "publish": "2014-02-19T09:21:14Z", 
    "summary": "The development of software for wireless services on the Internet is a\nchallenging task due to the extreme time-to-market pressure, the newness of the\napplication domain, and the quick evolution of the technical infrastructure.\nNevertheless, developing software of a predetermined quality in a predictable\nfashion can only be achieved with systematic development processes and the use\nof engineering principles. Thus, systematic development processes for this\ndomain are needed urgently. This article presents a method for the design of an\nadaptable software development process based on existing practices from related\ndomains, industrial piloting, and expert knowledge. First results of the\napplication of the method for the wireless Internet services domain are\ndescribed. The benefit for the reader is twofold: the article describes a\nvalidated method on how to gain process knowledge for an upcoming field fast\nand incrementally. Furthermore, first results of the process design for the\nwireless Internet services domain are given."
},{
    "category": "cs.SE", 
    "doi": "10.1007/3-540-36209-6_30", 
    "link": "http://arxiv.org/pdf/1402.5121v3", 
    "title": "Challenges in Selecting Software to be Reused", 
    "arxiv-id": "1402.5121v3", 
    "author": "Daniel S. Katz", 
    "publish": "2014-02-18T13:42:21Z", 
    "summary": "This is a position paper for Sharing, Re-Use and Circulation of Resources in\nCooperative Scientific Work, a CSCW'14 workshop. It discusses the role of\nsoftware in NSF's CIF21 vision and the SI2 program, which is intended to\nsupport that goal. SI2 primarily supports software projects that are proposed\nin response to solicitations, and some of the criteria used by the\npeer-reviewers and by NSF in evaluating these projects depend on predicting\nscientific impact. This paper discusses some ideas on how the prediction of\nscientific impact can be improved."
},{
    "category": "cs.SE", 
    "doi": "10.1007/3-540-36209-6_9", 
    "link": "http://arxiv.org/pdf/1402.5267v1", 
    "title": "Simulation-Based Risk Reduction for Planning Inspections", 
    "arxiv-id": "1402.5267v1", 
    "author": "Andreas Wirsen", 
    "publish": "2014-02-21T11:59:25Z", 
    "summary": "Organizations that develop software have recognized that software process\nmodels are particularly useful for maintaining a high standard of quality. In\nthe last decade, simulations of software processes were used in several\nsettings and environments. This paper gives a short overview of the benefits of\nsoftware process simulation and describes the development of a discrete-event\nmodel, a technique rarely used before in that field. The model introduced in\nthis paper captures the behavior of a detailed code inspection process. It aims\nat reducing the risks inherent in implementing inspection processes and\ntechniques in the overall development process. The determination of the\nunderlying cause-effect relations using data mining techniques and empirical\ndata is explained. Finally, the paper gives an outlook on our future work."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ACSAT.2012.47", 
    "link": "http://arxiv.org/pdf/1402.5592v1", 
    "title": "Web Service Composition - BPEL vs cCSP Process Algebra", 
    "arxiv-id": "1402.5592v1", 
    "author": "Aoyan Barua", 
    "publish": "2014-02-23T09:55:51Z", 
    "summary": "Web services technology provides a platform on which we can develop\ndistributed services. The interoperability among these services is achieved by\nvarious standard protocols. In recent years, several researches suggested that\nprocess algebras provide a satisfactory assistance to the whole process of web\nservices development. Business transactions, on the other hand, involve the\ncoordination and interaction between multiple partners. With the emergence of\nweb services, business transactions are conducted using these services. The\ncoordination among the business processes is crucial, so is the handling of\nfaults that can arise at any stage of a transaction. BPEL models the behavior\nof business process interaction by providing a XML based grammar to describe\nthe control logic required to coordinate the web services participating in a\nprocess flow. However BPEL lacks a proper formal description where the\ncomposition of business processes cannot be formally verified. Process algebra,\non the other hand, facilitates a formal foundation for rigorous verification of\nthe composition. This paper presents a comparison of web service composition\nbetween BPEL and process algebra, cCSP."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ACSEAC.2012.14", 
    "link": "http://arxiv.org/pdf/1402.5595v1", 
    "title": "Logic Verification of Product-Line Variant Requirements", 
    "arxiv-id": "1402.5595v1", 
    "author": "Mehidee Hassan", 
    "publish": "2014-02-23T10:17:53Z", 
    "summary": "Formal verification of variant requirements has gained much interest in the\nsoftware product line (SPL) community. Feature diagrams are widely used to\nmodel product line variants. However, there is a lack of precisely defined\nformal notation for representing and verifying such models. This paper presents\nan approach to modeling and verifying SPL variant feature diagrams using\nfirst-order logic. It provides a precise and rigorous formal interpretation of\nthe feature diagrams. Logical expressions can be built by modeling variants and\ntheir dependencies by using propositional connectives. These expressions can\nthen be validated by any suitable verification tool. A case study of a Computer\nAided Dispatch (CAD) system variant feature model is presented to illustrate\nthe verification process."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.ijmedinf.2013.05.005", 
    "link": "http://arxiv.org/pdf/1402.5749v1", 
    "title": "Providing Traceability for Neuroimaging Analyses", 
    "arxiv-id": "1402.5749v1", 
    "author": "the neuGRID Consortium", 
    "publish": "2014-02-24T08:44:49Z", 
    "summary": "With the increasingly digital nature of biomedical data and as the complexity\nof analyses in medical research increases, the need for accurate information\ncapture, traceability and accessibility has become crucial to medical\nresearchers in the pursuance of their research goals. Grid- or Cloud-based\ntechnologies, often based on so-called Service Oriented Architectures (SOA),\nare increasingly being seen as viable solutions for managing distributed data\nand algorithms in the bio-medical domain. For neuroscientific analyses,\nespecially those centred on complex image analysis, traceability of processes\nand datasets is essential but up to now this has not been captured in a manner\nthat facilitates collaborative study. Over the past decade, we have been\nworking with mammographers, paediatricians and neuroscientists in three\ngenerations of projects to provide the data management and provenance services\nnow required for 21st century medical research. This paper outlines the finding\nof a requirements study and a resulting system architecture for the production\nof services to support neuroscientific studies of biomarkers for Alzheimers\nDisease. The paper proposes a software infrastructure and services that provide\nthe foundation for such support. It introduces the use of the CRISTAL software\nto provide provenance management as one of a number of services delivered on a\nSOA, deployed to manage neuroimaging projects that have been studying\nbiomarkers for Alzheimers disease."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.5753v1", 
    "title": "CRISTAL : A Practical Study in Designing Systems to Cope with Change", 
    "arxiv-id": "1402.5753v1", 
    "author": "Jetendr Shamdasani", 
    "publish": "2014-02-24T08:59:34Z", 
    "summary": "Software engineers frequently face the challenge of developing systems whose\nrequirements are likely to change in order to adapt to organizational\nreconfigurations or other external pressures. Evolving requirements present\ndifficulties, especially in environments in which business agility demands\nshorter development times and responsive prototyping. This paper uses a study\nfrom CERN in Geneva to address these research questions by employing a\ndescription-driven approach that is responsive to changes in user requirements\nand that facilitates dynamic system reconfiguration. The study describes how\nhandling descriptions of objects in practice alongside their instances (making\nthe objects self-describing) can mediate the effects of evolving user\nrequirements on system development. This paper reports on and draws lessons\nfrom the practical use of a description-driven system over time. It also\nidentifies lessons that can be learned from adopting such a self-describing\ndescription-driven approach in future software development."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.5764v1", 
    "title": "Designing Reusable Systems that Can Handle Change - Description-Driven   Systems : Revisiting Object-Oriented Principles", 
    "arxiv-id": "1402.5764v1", 
    "author": "Jetendr Shamdasani", 
    "publish": "2014-02-24T09:48:15Z", 
    "summary": "In the age of the Cloud and so-called Big Data systems must be increasingly\nflexible, reconfigurable and adaptable to change in addition to being developed\nrapidly. As a consequence, designing systems to cater for evolution is becoming\ncritical to their success. To be able to cope with change, systems must have\nthe capability of reuse and the ability to adapt as and when necessary to\nchanges in requirements. Allowing systems to be self-describing is one way to\nfacilitate this. To address the issues of reuse in designing evolvable systems,\nthis paper proposes a so-called description-driven approach to systems design.\nThis approach enables new versions of data structures and processes to be\ncreated alongside the old, thereby providing a history of changes to the\nunderlying data models and enabling the capture of provenance data. The\nefficacy of the description-driven approach is exemplified by the CRISTAL\nproject. CRISTAL is based on description-driven design principles; it uses\nversions of stored descriptions to define various versions of data which can be\nstored in diverse forms. This paper discusses the need for capturing holistic\nsystem description when modelling large-scale distributed systems."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.5768v1", 
    "title": "Model Driven Engineering for Science Gateways", 
    "arxiv-id": "1402.5768v1", 
    "author": "Herve Verjus", 
    "publish": "2014-02-24T09:56:34Z", 
    "summary": "From n-Tier client/server applications, to more complex academic Grids, or\neven the most recent and promising industrial Clouds, the last decade has\nwitnessed significant developments in distributed computing. In spite of this\nconceptual heterogeneity, Service-Oriented Architectures (SOA) seem to have\nemerged as the common underlying abstraction paradigm. Suitable access to data\nand applications resident in SOAs via so-called Science Gateways has thus\nbecome a pressing need in various fields of science, in order to realize the\nbenefits of Grid and Cloud infrastructures. In this context, authors have\nconsolidated work from three complementary experiences in European projects,\nwhich have developed and deployed large-scale production quality\ninfrastructures as Science Gateways to support research in breast cancer,\npaediatric diseases and neurodegenerative pathologies respectively. In\nanalysing the requirements from these biomedical applications the authors were\nable to elaborate on commonly faced Grid development issues, while proposing an\nadaptable and extensible engineering framework for Science Gateways. This paper\nthus proposes the application of an architecture-centric Model-Driven\nEngineering (MDE) approach to service-oriented developments, making it possible\nto define Science Gateways that satisfy quality of service requirements,\nexecution platform and distribution criteria at design time. An novel\ninvestigation is presented on the applicability of the resulting grid MDE\n(gMDE) to specific examples, and conclusions are drawn on the benefits of this\napproach and its possible application to other areas, in particular that of\nDistributed Computing Infrastructures (DCI) interoperability."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.5953v1", 
    "title": "A Description Driven Approach for Flexible Metadata Tracking", 
    "arxiv-id": "1402.5953v1", 
    "author": "Richard McClatchey", 
    "publish": "2014-02-24T10:09:30Z", 
    "summary": "Evolving user requirements presents a considerable software engineering\nchallenge, all the more so in an environment where data will be stored for a\nvery long time, and must remain usable as the system specification evolves\naround it. Capturing the description of the system addresses this issue since a\ndescription-driven approach enables new versions of data structures and\nprocesses to be created alongside the old, thereby providing a history of\nchanges to the underlying data models and enabling the capture of provenance\ndata. This description-driven approach is advocated in this paper in which a\nsystem called CRISTAL is presented. CRISTAL is based on description-driven\nprinciples; it can use previous versions of stored descriptions to define\nvarious versions of data which can be stored in various forms. To demonstrate\nthe efficacy of this approach the history of the project at CERN is presented\nwhere CRISTAL was used to track data and process definitions and their\nassociated provenance data in the construction of the CMS ECAL detector, how it\nwas applied to handle analysis tracking and data index provenance in the\nneuGRID and N4U projects, and how it will be matured further in the CRISTAL-ISE\nproject. We believe that the CRISTAL approach could be invaluable in handling\nthe evolution, indexing and tracking of large datasets, and are keen to apply\nit further in this direction."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.6045v1", 
    "title": "Multi-Dimensional Customization Modelling Based On Metagraph For Saas   Multi-Tenant Applications", 
    "arxiv-id": "1402.6045v1", 
    "author": "Ashraf A. Shahin", 
    "publish": "2014-02-25T03:44:09Z", 
    "summary": "Software as a Service (SaaS) is a new software delivery model in which\npre-built applications are delivered to customers as a service. SaaS providers\naim to attract a large number of tenants (users) with minimal system\nmodifications to meet economics of scale. To achieve this aim, SaaS\napplications have to be customizable to meet requirements of each tenant.\nHowever, due to the rapid growing of the SaaS, SaaS applications could have\nthousands of tenants with a huge number of ways to customize applications.\nModularizing such customizations still is a highly complex task. Additionally,\ndue to the big variation of requirements for tenants, no single customization\nmodel is appropriate for all tenants. In this paper, we propose a\nmulti-dimensional customization model based on metagraph. The proposed mode\naddresses the modelling variability among tenants, describes customizations and\ntheir relationships, and guarantees the correctness of SaaS customizations made\nby tenants."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.6046v1", 
    "title": "Towards rational and minimal change propagation in model evolution", 
    "arxiv-id": "1402.6046v1", 
    "author": "Aditya Ghose", 
    "publish": "2014-02-25T03:59:47Z", 
    "summary": "A critical issue in the evolution of software models is change propagation:\ngiven a primary change that is made to a model in order to meet a new or\nchanged requirement, what additional secondary changes are needed to maintain\nconsistency within the model, and between the model and other models in the\nsystem? In practice, there are many ways of propagating changes to fix a given\ninconsistency, and how to justify and automate the selection between such\nchange options remains a critical challenge. In this paper, we propose a number\nof postulates, inspired by the mature belief revision theory, that a change\npropagation process should satisfy to be considered rational and minimal. Such\npostulates enable us to reason about selecting alternative change options, and\nconsequently to develop a machinery that automatically performs this task. We\nfurther argue that a possible implementation of such a change propagation\nprocess can be considered as a classical state space search in which each state\nrepresents a snapshot of the model in the process. This view naturally reflects\nthe cascading nature of change propagation, where each change can require\nfurther changes to be made."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.is.2013.12.009", 
    "link": "http://arxiv.org/pdf/1402.6478v1", 
    "title": "Estimating verification time", 
    "arxiv-id": "1402.6478v1", 
    "author": "Pablo Gonz\u00e1lez de Aledo", 
    "publish": "2014-02-26T10:13:36Z", 
    "summary": "This essay is divided in four parts: In section (I) I explain why I think\ndetecting hot-spots in verification is complicated and in particular, more\ncomplicated than detection when developing crude software. In section (II) I\nintroduce the factors I think mostly affect performance in verification. In\nsection (III) I propose a method to find functions that are most promising to\nbe optimized. Finally, in section (IV) I draw some conclusions and discuss\npros/cons of the proposed solution."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2635868.2635901", 
    "link": "http://arxiv.org/pdf/1404.0417v3", 
    "title": "Mining Idioms from Source Code", 
    "arxiv-id": "1404.0417v3", 
    "author": "Charles Sutton", 
    "publish": "2014-04-01T23:04:12Z", 
    "summary": "We present the first method for automatically mining code idioms from a\ncorpus of previously written, idiomatic software projects. We take the view\nthat a code idiom is a syntactic fragment that recurs across projects and has a\nsingle semantic role. Idioms may have metavariables, such as the body of a for\nloop. Modern IDEs commonly provide facilities for manually defining idioms and\ninserting them on demand, but this does not help programmers to write idiomatic\ncode in languages or using libraries with which they are unfamiliar. We present\nHAGGIS, a system for mining code idioms that builds on recent advanced\ntechniques from statistical natural language processing, namely, nonparametric\nBayesian probabilistic tree substitution grammars. We apply HAGGIS to several\nof the most popular open source projects from GitHub. We present a wide range\nof evidence that the resulting idioms are semantically meaningful,\ndemonstrating that they do indeed recur across software projects and that they\noccur more frequently in illustrative code examples collected from a Q&A site.\nManual examination of the most common idioms indicate that they describe\nimportant program concepts, including object creation, exception handling, and\nresource management."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147", 
    "link": "http://arxiv.org/pdf/1404.0436v1", 
    "title": "Proceedings 11th International Workshop on Formal Engineering approaches   to Software Components and Architectures", 
    "arxiv-id": "1404.0436v1", 
    "author": "Jan Kofro\u0148", 
    "publish": "2014-04-02T02:52:51Z", 
    "summary": "The aim of the FESCA workshop is to bring together both young and senior\nresearchers from formal methods, software engineering, and industry interested\nin the development and application of formal modelling approaches as well as\nassociated analysis and reasoning techniques with practical benefits for\ncomponent-based software engineering.\n  Component-based software design has received considerable attention in\nindustry and academia in the past decade. In recent years, with the emergence\nof new platforms (such as smartphones), new areas advocating software\ncorrectness along with new challenges have appeared. These include development\nof new methods and adapting existing ones to accommodate unique features of the\nplatforms, such as inherent distribution, openness, and continuous migration.\nOn the other hand, with the growing power of computers, more and more is\npossible with respect to practical applicability of modelling and specification\nmethods as well as verification tools to real-life software, i.e, to scale to\nmore complex systems.\n  FESCA aims to address the open question of how formal methods can be applied\neffectively to these new contexts and challenges. The workshop is interested in\nboth the development and application of formal methods in component-based\ndevelopment and tries to cross-fertilize their research and application."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147", 
    "link": "http://arxiv.org/pdf/1404.0698v1", 
    "title": "Adaptability Checking in Multi-Level Complex Systems", 
    "arxiv-id": "1404.0698v1", 
    "author": "Luca Tesei", 
    "publish": "2014-04-02T20:37:09Z", 
    "summary": "A hierarchical model for multi-level adaptive systems is built on two basic\nlevels: a lower behavioural level B accounting for the actual behaviour of the\nsystem and an upper structural level S describing the adaptation dynamics of\nthe system. The behavioural level is modelled as a state machine and the\nstructural level as a higher-order system whose states have associated logical\nformulas (constraints) over observables of the behavioural level. S is used to\ncapture the global and stable features of B, by a defining set of allowed\nbehaviours. The adaptation semantics is such that the upper S level imposes\nconstraints on the lower B level, which has to adapt whenever it no longer can\nsatisfy them. In this context, we introduce weak and strong adaptabil- ity,\ni.e. the ability of a system to adapt for some evolution paths or for all\npossible evolutions, respectively. We provide a relational characterisation for\nthese two notions and we show that adaptability checking, i.e. deciding if a\nsystem is weak or strong adaptable, can be reduced to a CTL model checking\nproblem. We apply the model and the theoretical results to the case study of\nmotion control of autonomous transport vehicles."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147.1", 
    "link": "http://arxiv.org/pdf/1404.0846v1", 
    "title": "Towards Verifying Safety Properties of Real-Time Probabilistic Systems", 
    "arxiv-id": "1404.0846v1", 
    "author": "Heinz Schmidt", 
    "publish": "2014-04-03T10:43:37Z", 
    "summary": "Using probabilities in the formal-methods-based development of\nsafety-critical software has quickened interests in academia and industry. We\naddress this area by our model-driven engineering method for reactive systems\nSPACE and its tool-set Reactive Blocks that provide an extension to support the\nmodeling and verification of real-time behaviors. The approach facilitates the\ncomposition of system models from reusable building blocks as well as the\nverification of functional and real-time properties and the automatic\ngeneration of Java code.\n  In this paper, we describe the extension of the tool-set to enable the\nmodeling and verification of probabilistic real-time system behavior with the\nfocus on spatial properties that ensure system safety. In particular, we\nincorporate descriptions of probabilistic behavior into our Reactive Blocks\nmodels and integrate the model checker PRISM which allows to verify that a\nreal-time system satisfies certain safety properties with a given probability.\nMoreover, we consider the spatial implication of probabilistic system\nspecifications by integrating the spatial verification tool BeSpaceD and give\nan automatic approach to translate system specifications to the input languages\nof PRISM and BeSpaceD. The approach is highlighted by an example."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147.4", 
    "link": "http://arxiv.org/pdf/1404.0849v1", 
    "title": "Comprehensive Monitor-Oriented Compensation Programming", 
    "arxiv-id": "1404.0849v1", 
    "author": "Gordon J. Pace", 
    "publish": "2014-04-03T10:44:09Z", 
    "summary": "Compensation programming is typically used in the programming of web service\ncompositions whose correct implementation is crucial due to their handling of\nsecurity-critical activities such as financial transactions. While traditional\nexception handling depends on the state of the system at the moment of failure,\ncompensation programming is significantly more challenging and dynamic because\nit is dependent on the runtime execution flow - with the history of behaviour\nof the system at the moment of failure affecting how to apply compensation. To\naddress this dynamic element, we propose the use of runtime monitors to\nfacilitate compensation programming, with monitors enabling the modeller to be\nable to implicitly reason in terms of the runtime control flow, thus separating\nthe concerns of system building and compensation modelling. Our approach is\ninstantiated into an architecture and shown to be applicable to a case study."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147.10", 
    "link": "http://arxiv.org/pdf/1404.0855v1", 
    "title": "Transformation of UML Behavioral Diagrams to Support Software Model   Checking", 
    "arxiv-id": "1404.0855v1", 
    "author": "Nandamudi Lankalapalli Vijaykumar", 
    "publish": "2014-04-03T10:45:10Z", 
    "summary": "Unified Modeling Language (UML) is currently accepted as the standard for\nmodeling (object-oriented) software, and its use is increasing in the aerospace\nindustry. Verification and Validation of complex software developed according\nto UML is not trivial due to complexity of the software itself, and the several\ndifferent UML models/diagrams that can be used to model behavior and structure\nof the software. This paper presents an approach to transform up to three\ndifferent UML behavioral diagrams (sequence, behavioral state machines, and\nactivity) into a single Transition System to support Model Checking of software\ndeveloped in accordance with UML. In our approach, properties are formalized\nbased on use case descriptions. The transformation is done for the NuSMV model\nchecker, but we see the possibility in using other model checkers, such as\nSPIN. The main contribution of our work is the transformation of a non-formal\nlanguage (UML) to a formal language (language of the NuSMV model checker)\ntowards a greater adoption in practice of formal methods in software\ndevelopment."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.147.10", 
    "link": "http://arxiv.org/pdf/1404.1279v3", 
    "title": "Event-Flow Graphs for Efficient Path-Sensitive Analyses", 
    "arxiv-id": "1404.1279v3", 
    "author": "Suresh Kothari", 
    "publish": "2014-04-04T15:01:46Z", 
    "summary": "Efficient and accurate path-sensitive analyses pose the challenges of: (a)\nanalyzing an exponentially-increasing number of paths in a control-flow graph\n(CFG), and (b) checking feasibility of paths in a CFG. We address these\nchallenges by introducing an equivalence relation on the CFG paths to partition\nthem into equivalence classes. It is then sufficient to perform analysis on\nthese equivalence classes rather than on the individual paths in a CFG. This\ntechnique has two major advantages: (a) although the number of paths in a CFG\ncan be exponentially large, the essential information to be analyzed is\ncaptured by a small number of equivalence classes, and (b) checking path\nfeasibility becomes simpler. The key challenge is how to efficiently compute\nequivalence classes of paths in a CFG without examining each path in the CFG?\nIn this paper, we present a linear-time algorithm to form equivalence classes\nwithout the need for examination of each path in a CFG. The key to this\nalgorithm is construction of an event-flow graph (EFG), a compact derivative of\nthe CFG, in which each path represents an equivalence class of paths in the\ncorresponding CFG. EFGs are defined with respect to the set of events that are\nin turn defined by the analyzed property. The equivalence classes are thus\nguaranteed to preserve all the event traces in the original CFG. We present an\nempirical evaluation of the Linux kernel (v3.12). The EFGs in our evaluation\nare defined with respect to events of the spin safe-synchronization property.\nEvaluation results show that there are many fewer EFG-based equivalence classes\ncompared to the corresponding number of paths in a CFG. This reduction is close\nto 99% for CFGs with a large number of paths. Moreover, our controlled\nexperiment results show that EFGs are human comprehensible and compact compared\nto their corresponding CFGs."
},{
    "category": "cs.SE", 
    "doi": "10.3233/978-1-61499-411-4-37", 
    "link": "http://arxiv.org/pdf/1404.1621v3", 
    "title": "Proposal of a multiagent-based smart environment for the IoT", 
    "arxiv-id": "1404.1621v3", 
    "author": "Leszek Kotulski", 
    "publish": "2014-04-06T21:03:45Z", 
    "summary": "This work relates to context-awareness of things that belong to IoT networks.\nPreferences understood as a priority in selection are considered, and dynamic\npreference models for such systems are built. Preference models are based on\nformal logic, and they are built on-the-fly by software agents observing the\nbehavior of users/inhabitants, and gathering knowledge about preferences\nexpressed in terms of logical specifications. A 3-level structure of agents has\nbeen introduced to support IoT inference. These agents cooperate with each\nother basing on the graph representation of the system knowledge. An example of\nsuch a system is presented."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.jvlc.2012.05.002", 
    "link": "http://arxiv.org/pdf/1404.2155v1", 
    "title": "An Efficient Solution for Model Checking Abstract State Machine Using   Bogor", 
    "arxiv-id": "1404.2155v1", 
    "author": "Saeed Doostali", 
    "publish": "2014-04-07T06:47:45Z", 
    "summary": "Nowadays, publish subscribe and event based architecture are frequently used\nfor developing loosely coupled distributed systems. Hence, it is desirable to\nfind a proper solution to specify different systems through these\narchitectures. Abstract state machine (ASM) is a useful means to visually and\nformally model publish subscribe and event based architectures. However,\nmodeling per se is not enough since the designers want to be able to verify the\ndesigned models. As the model checking is a proper approach to verify software\nand hardware systems. In this paper, we present an approach to verify ASM\nmodels specified in terms of AsmetaL language using Bogor. In our approach, the\nAsmetaL specification is automatically encoded to BIR, the input language of\nthe Bogor."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5204", 
    "link": "http://arxiv.org/pdf/1404.2713v1", 
    "title": "Transaction Handling in COM, EJB and .NET", 
    "arxiv-id": "1404.2713v1", 
    "author": "Miraldi Fifo", 
    "publish": "2014-04-10T07:26:07Z", 
    "summary": "The technology evolution has shown a very impressive performance in the last\nyears by introducing several technologies that are based on the concept of\ncomponent. As time passes, new versions of Component- Based technologies are\nreleased in order to improve services provided by previous ones. One important\nissue that regards these technologies is transactional activity. Transactions\nare important because they consist in sending different small amounts of\ninformation collected properly in a single combined unit which makes the\nprocess simpler, less expensive and also improves the reliability of the whole\nsystem, reducing its chances to go through possible failures. Different\nComponent-Based technologies offer different ways of handling transactions. In\nthis paper, we will review and discuss how transactions are handled in three of\nthem: COM, EJB and .NET. It can be expected that .NET offers more efficient\nmechanisms due to the fact of being released later than the other two\ntechnologies. Nevertheless, COM and EJB are still present in the market and\ntheir services are still widely used. Comparing transaction handling in these\ntechnologies will be helpful to analyze the advantages and disadvantages of\neach of them. This comparison and evaluation will be seen in two main\nperspectives: performance and security."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5204", 
    "link": "http://arxiv.org/pdf/1404.2737v2", 
    "title": "User Centered Development of Agent-based Business Process Models and   Notations", 
    "arxiv-id": "1404.2737v2", 
    "author": "Robert Singer", 
    "publish": "2014-04-10T08:48:55Z", 
    "summary": "We discuss questions about user centric development of business process\nmodeling notations. In the center of our research there is a fully featured\nmulti-enterprise business process platform (ME-BPP) based on the concepts of\nagent-based business processes, which builds on the formal foundations of the\nsubject-oriented business process management methodology (S-BPM). The platform\nis implemented based on cloud technology using commercial services.\nAdditionally we developed a \"block modeling\" technique to find a semantically\ntransparent modeling notation which can be used by novice users to model\nsubject-oriented business process (S-BPM) models. As this is ongoing research\nthere are still serious open questions. But, the presented approach breaks with\nsome of the rules of typical process modeling notations and hopefully\nstimulates innovation. Additionally we want to continue our research towards\nthe enhancement of our modeling approach towards a user centric \"syntax and\nsemantic free\" modeling technique to develop user and domain specific modeling\nnotations."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.2939v1", 
    "title": "Modeling Enterprise Architecture Using Timed Colored PETRI Net: Single   Processor Scheduling", 
    "arxiv-id": "1404.2939v1", 
    "author": "Elham Abdolrahimi Niyari", 
    "publish": "2014-04-10T20:16:06Z", 
    "summary": "The purpose of modeling enterprise architecture and analysis of it is to ease\ndecision making about architecture of information systems. Planning is one of\nthe most important tasks in an organization and has a major role in increasing\nthe productivity of it. Scope of this paper is scheduling processes in the\nenterprise architecture. Scheduling is decision making on execution start time\nof processes that are used in manufacturing and service systems. Different\nmethods and tools have been proposed for modeling enterprise architecture.\nColored Petri net is extension of traditional Petri net that its modeling\ncapability has grown dramatically. A developed model with Colored Petri net is\nsuitable for verification of operational aspects and performance evaluation of\ninformation systems. With having ability of hierarchical modeling, colored\nPetri nets permits that using predesigned modules for smaller parts of the\nsystem and with a general algorithm, any kind of enterprise architecture can be\nmodeled. A two level hierarchical model is presented as a building block for\nmodeling architecture of Transaction Processing Systems (TPS) in this paper.\nThis model schedules and runs processes based on a predetermined non-preemptive\nscheduling method. The model can be used for scheduling of processes with four\nnon-preemptive methods named, priority based (PR), shortest job first (SJF),\nfirst come first served (FCFS) and highest response ratio next (HRRN). The\npresented model is designed such can be used as one of the main components in\nmodeling any type of enterprise architecture. Most enterprise architectures can\nbe modeled by putting together appropriate number of these modules and proper\ncomposition of them."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.3056v2", 
    "title": "Principles of Antifragile Software", 
    "arxiv-id": "1404.3056v2", 
    "author": "Martin Monperrus", 
    "publish": "2014-04-11T10:04:12Z", 
    "summary": "The goal of this paper is to study and define the concept of \"antifragile\nsoftware\". For this, I start from Taleb's statement that antifragile systems\nlove errors, and discuss whether traditional software dependability fits into\nthis class. The answer is somewhat negative, although adaptive fault tolerance\nis antifragile: the system learns something when an error happens, and always\nimrpoves. Automatic runtime bug fixing is changing the code in response to\nerrors, fault injection in production means injecting errors in business\ncritical software. I claim that both correspond to antifragility. Finally, I\nhypothesize that antifragile development processes are better at producing\nantifragile software systems."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.3186v1", 
    "title": "Automatic Repair of Buggy If Conditions and Missing Preconditions with   SMT", 
    "arxiv-id": "1404.3186v1", 
    "author": "Martin Monperrus", 
    "publish": "2014-04-11T18:57:52Z", 
    "summary": "We present Nopol, an approach for automatically repairing buggy if conditions\nand missing preconditions. As input, it takes a program and a test suite which\ncontains passing test cases modeling the expected behavior of the program and\nat least one failing test case embodying the bug to be repaired. It consists of\ncollecting data from multiple instrumented test suite executions, transforming\nthis data into a Satisfiability Modulo Theory (SMT) problem, and translating\nthe SMT result -- if there exists one -- into a source code patch. Nopol\nrepairs object oriented code and allows the patches to contain nullness checks\nas well as specific method calls."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.3280v1", 
    "title": "An Ontology Oriented Architecture for Context Aware Services Adaptation", 
    "arxiv-id": "1404.3280v1", 
    "author": "Abdelaziz Kriouile", 
    "publish": "2014-04-12T11:01:26Z", 
    "summary": "In the field of ubiquitous computing, a class of applications called\ncontext-aware services attracted great interest especially since the emergence\nof wireless technologies and mobile devices. Context-aware application can\ndynamically capture a range of information from its environment and this\ninformation represents a context, the application adapts its execution\naccording to this context. An important challenge in ubiquitous computing is\ndealing with context. Ontologies presents the most promising instrument for\ncontext modeling and managing due to their high and formal expressiveness and\nthe possibilities for applying ontology reasoning techniques. In this paper, we\npresent an ontology based approach for the development of context aware\nservices."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.3382v1", 
    "title": "An Approach for Computing Dynamic Slice of Concurrent Aspect-Oriented   Programs", 
    "arxiv-id": "1404.3382v1", 
    "author": "Durga Prasad Mohapatra", 
    "publish": "2014-04-13T13:48:04Z", 
    "summary": "We propose a dynamic slicing algorithm to compute the slice of concurrent\naspect-oriented programs. We use a dependence based intermediate program\nrepresentation called Concurrent Aspect-oriented System Dependence Graph\n(CASDG) to represent a concurrent aspect-oriented program. The CASDG of an\naspect-oriented program consists of a system dependence graph (SDG) for the\nnon-aspect code, a group of dependence graphs for aspect code and some\nadditional dependence edges used to connect the system dependence graph for the\nnon-aspect code to dependence graph for aspect code. The proposed dynamic\nslicing al-gorithm is an extended version of NMDS algorithm for concurrent\nobject-oriented programs, which is based on marking and unmarking of the\nexecuted nodes in CASDG appropriately during run-time."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.3537v1", 
    "title": "BeSpaceD: Towards a Tool Framework and Methodology for the Specification   and Verification of Spatial Behavior of Distributed Software Component   Systems", 
    "arxiv-id": "1404.3537v1", 
    "author": "Heinz Schmidt", 
    "publish": "2014-04-14T10:57:29Z", 
    "summary": "In this report, we present work towards a framework for modeling and checking\nbehavior of spatially distributed component systems. Design goals of our\nframework are the ability to model spatial behavior in a component oriented,\nsimple and intuitive way, the possibility to automatically analyse and verify\nsystems and integration possibilities with other modeling and verification\ntools. We present examples and the verification steps necessary to prove\nproperties such as range coverage or the absence of collisions between\ncomponents and technical details."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijmpict.2014.5101", 
    "link": "http://arxiv.org/pdf/1404.4100v1", 
    "title": "Locating Crashing Faults based on Crash Stack Traces", 
    "arxiv-id": "1404.4100v1", 
    "author": "Sunghun Kim", 
    "publish": "2014-04-15T22:32:21Z", 
    "summary": "Software crashes due to its increasing complexity. Once a crash happens, a\ncrash report could be sent to software developers for investigation upon user\npermission. Because of the large number of crash reports and limited\ninformation, debugging for crashes is often a tedious and labor-intensive task.\nIn this paper, we propose a statistical fault localization framework to help\ndevelopers locate functions that contain crashing faults. We generate the\nexecution traces for the failing traces based on the crash stack, and the\npassing traces from normal executions. We form program spectra by combining\ngenerated passing and failing trace, and then apply statistical fault\nlocalization techniques such as Ochiai to locate the crashing faults. We also\npropose two heuristics to improve the fault localization performance. We\nevaluate our approach using the real-world Firefox crash report data. The\nresults show that the performance of our method is promising. Our approach\npermits developers to locate 63.9% crashing faults by examining only 5% Firefox\n3.6 functions in the spectra."
},{
    "category": "cs.SE", 
    "doi": "10.5220/0004640000470054", 
    "link": "http://arxiv.org/pdf/1404.4713v1", 
    "title": "Knowledge-Driven Game Design by Non-Programmers", 
    "arxiv-id": "1404.4713v1", 
    "author": "Avinoam Alfia", 
    "publish": "2014-04-18T07:37:52Z", 
    "summary": "Game extension is an entertaining activity that offers an opportunity to test\nnew design approaches by non-programmers. The real challenge is to enable this\nactivity by means of a suitable infrastructure. We propose a knowledge-driven\napproach with natural game-player concepts. These concepts, found in game\nontologies, include game abstractions and rules for game moves. The approach\nhas been implemented and tested for board games. These include tic-tac-toe as a\nsimplest example, enabling extensions of tic-tac- toe, say to a four-by-four\nboard and Sudoku, a single player game of a very different nature."
},{
    "category": "cs.SE", 
    "doi": "10.5220/0004640000470054", 
    "link": "http://arxiv.org/pdf/1404.4970v1", 
    "title": "Prediction of rate of improvement of software quality and development   effort on the basis of Degreeof excellence with respect to number of lines of   code", 
    "arxiv-id": "1404.4970v1", 
    "author": "Vandana Bhattacherjee", 
    "publish": "2014-04-19T16:47:55Z", 
    "summary": "The objective of this research work is to improve the degree of excellence by\nremoving the number of exceptions from the software. The modern age is more\nconcerned with the quality of software. Extensive research is being carried out\nin this direction. The rate of improvement of quality of software largely\ndepends on the development time. This development time is chiefly calculated in\nclock hours. However development time does not reflect the effort put in by the\ndeveloper. A better parameter can be the rate of improvement of quality level\nor the rate of improvement of the degree of excellence with respect to time.\nNow this parameter needs the prediction of error level and degree of excellence\nat a particular stage of development of the software. This paper explores an\nattempt to develop a system to predict rate of improvement of the software\nquality at a particular point of time with respect to the number of lines of\ncode present in the software. Having calculated the error level and degree of\nexcellence at two points in time, we can move forward towards the estimation of\nthe rate of improvement of the software quality with respect to time. This\nparameter can estimate the effort put in while development of the software and\ncan add a new dimension to the understanding of software quality in software\nengineering domain. In order to obtain the results we have used an indigenous\ntool for software quality prediction and for graphical representation of data,\nwe have used Microsoft office 2007 graphical chart."
},{
    "category": "cs.SE", 
    "doi": "10.5220/0004640000470054", 
    "link": "http://arxiv.org/pdf/1404.5034v2", 
    "title": "Sustaining IT PMOs during Cycles of Global Recession", 
    "arxiv-id": "1404.5034v2", 
    "author": "Musheer Ahmad", 
    "publish": "2014-04-20T12:03:08Z", 
    "summary": "Growth in the number of PMOs established by the industry over last decade and\never growing body of literature on PMO related research in academia is a clear\nindication that there is very clear interest of researchers, practitioners and\nindustries across the globe to understand and explore value propositions of\nPMO. However, there is still a lack of consensus on many critical aspects of\nPMOs. While there are many PMOs being established, but there are also many\nbeing closed and disbanded, which is definitely a matter of concern. In\nindustry environment, a narrow majority of PMOs are well-regarded by their\norganizations and are seen as contributing business value, many of the others\nare still struggling to show value for money and some are failing, causing a\nhigh mortality rate among PMOs. This paper is the result of a study undertaken\nto get a deeper understanding of factors that may be causing mortality and\nfailure of PMOs. Post Implementation Reviews of 4-failed & 3-challenged PMOs in\nIT-Industry were carried out with concerned Project Managers & PMO-staff, using\ngrounded theory research method, with support from the concerned enterprise\nfrom IT-Industry."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6603v1", 
    "title": "Who watches the watchers: Validating the ProB Validation Tool", 
    "arxiv-id": "1404.6603v1", 
    "author": "Michael Leuschel", 
    "publish": "2014-04-26T05:32:16Z", 
    "summary": "Over the years, ProB has moved from a tool that complemented proving, to a\ndevelopment environment that is now sometimes used instead of proving for\napplications, such as exhaustive model checking or data validation. This has\nled to much more stringent requirements on the integrity of ProB. In this paper\nwe present a summary of our validation efforts for ProB, in particular within\nthe context of the norm EN 50128 and safety critical applications in the\nrailway domain."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6743v1", 
    "title": "A Formal Approach to System Integration Testing", 
    "arxiv-id": "1404.6743v1", 
    "author": "Martin Elshuber", 
    "publish": "2014-04-27T12:06:14Z", 
    "summary": "System integration testing is the process of testing a system by the stepwise\nintegration of sub-components. Usually these sub-components are already\nverified to guarantee their correct functional behavior. By integration of\nthese verified subcomponents into the overall system, emergent behavior may\noccur, i.e. behavior that evolves by the assembling of the subcomponents. For\nsystem integration testing, both, the correct functional behavior of the\noverall system, and, the proper functioning of the sub-components in their\nsystem environment, have to be verified. In this work we present the idea of an\napproach for system integration testing based on formal verification. The\nsystem components are modeled in SystemC. In a first step these components are\nformally verified. Then a model of the overall system is built. In a second\nstep this system model is formally verified. The novelty of this approach is\ngiven by two aspects: First, up to now the available verification frameworks\nfor SystemC-models are more a proof of concept than really applicable to real\nindustrial case studies. Secondly, although formal verification techniques are\na common technique for the verification of software and hardware, by now they\nhave only marginally considered for system integration testing."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6801v1", 
    "title": "Nothing is Certain but Doubt and Tests", 
    "arxiv-id": "1404.6801v1", 
    "author": "John A. McDermid", 
    "publish": "2014-04-27T18:10:44Z", 
    "summary": "Effective software safety standards will contribute to confidence, or\nassurance, in the safety of the systems in which the software is used. It is\ninfeasible to demonstrate a correlation between standards and accidents, but\nthere is an alternative view that makes standards \"testable\". Software projects\nare subject to uncertainty; good standards reduce uncertainty more than poor\nones. Similarly assurance or integrity levels in standards should define an\nuncertainty gradient. The paper proposes an argument -based method of reasoning\nabout uncertainty that can be used as a basis for conducting experiments\n(tests) to evaluate standards."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6802v1", 
    "title": "Formalism of Requirements for Safety-Critical Software: Where Does the   Benefit Come From?", 
    "arxiv-id": "1404.6802v1", 
    "author": "Andrew Rae", 
    "publish": "2014-04-27T18:28:31Z", 
    "summary": "Safety and assurance standards often rely on the principle that requirements\nerrors can be minimised by expressing the requirements more formally. Although\nnumerous case studies have shown that the act of formalising previously\ninformal requirements finds requirements errors, this principle is really just\na hypothesis. An industrially persuasive causal relationship between\nformalisation and better requirements has yet to be established. We describe\nmultiple competing explanations for this hypothesis, in terms of the levels of\nprecision, re-formulation, expertise, effort and automation that are typically\nassociated with formalising requirements. We then propose an experiment to\ndistinguish between these explanations, without necessarily excluding the\npossibility that none of them are correct."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6803v1", 
    "title": "Towards Assessing Necessary Competence", 
    "arxiv-id": "1404.6803v1", 
    "author": "Chris W. Johnson", 
    "publish": "2014-04-27T18:31:37Z", 
    "summary": "We sketch a series of studies and experiments designed to provide empirical\nevidence about the truth or falsity of claims that non-prescriptive approaches\nto standards demand greater competence from regulators than prescriptive\napproaches require."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6804v1", 
    "title": "The Efficacy of DO-178B", 
    "arxiv-id": "1404.6804v1", 
    "author": "Dewi Daniels", 
    "publish": "2014-04-27T18:34:38Z", 
    "summary": "DO-178B was based on the consensus of the avionic software community as it\nexisted in 1992. Twenty two years after publication, we have no publically\navailable experimental data as to its efficacy. It appears to work extremely\nwell, since there have been no hull loss accidents in passenger service\nascribed to software failure. This is a comforting and surprising result.\nHowever, if we don't know why DO-178B works so well, there is a danger that we\ncould stop doing something that really matters, which could lead to an\naccident."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6805v1", 
    "title": "On the efficacy of safety-related software standards", 
    "arxiv-id": "1404.6805v1", 
    "author": "Giuseppe Lami", 
    "publish": "2014-04-27T18:39:17Z", 
    "summary": "Difficulty of safety-related software standards to help producing software\nfor safe systems is discussed. Some research activity and other actions are\nproposed to focus on and possibly resolve long-lasting related problems."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6830v1", 
    "title": "Orthogonal Fault Tolerance for Dynamically Adaptive Systems", 
    "arxiv-id": "1404.6830v1", 
    "author": "Sobia K Khan", 
    "publish": "2014-04-27T21:34:55Z", 
    "summary": "In dynamic systems that adapt to users' needs and changing environments,\ndependability needs cannot be avoided. This paper proposes an orthogonal fault\ntolerance model as a means to manage and reason about multiple fault tolerance\nmechanisms that co-exist in dynamically adaptive systems. One of the key\nchallenges associated with dynamically evolving fault tolerance needs is the\nfeature interaction problem arising from the integration of fault tolerance\nfeatures. The proposed approach provides a separation of fault tolerance\nconcerns to study the effects of integrated fault tolerance on the system. This\napproach uses state machine and operational semantics to reason about these\ninteractions and inconsistencies. The proposed approach is supported by the\ntool NuSMV to simulate and verify the state machines against logic statements."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6833v1", 
    "title": "Unit verification procedure as a test of real time messaging-based   processes", 
    "arxiv-id": "1404.6833v1", 
    "author": "Miklos Taliga", 
    "publish": "2014-04-27T21:44:54Z", 
    "summary": "The article presents the first results of a PhD study connected to testing of\nsafety critical medical devices: a systematically executed case study at a\nHungarian manufacturer of medical devices. The article shortly describes the\nprocess of testing currently being used. Elements of the testing approach less\ncommonly applied in software industry are emphasized . The ending point of the\nactual testing process in the case study is the starting point for further\nresearch: the automated analysis of the testing results. The author started to\ndevelop a new approach, using a combination of tools, and modeling a\nmodel-based test generating tool - something that is both novel and intensive\nas an area of research."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6844v1", 
    "title": "Evaluating the Assessment of Software Fault-Freeness", 
    "arxiv-id": "1404.6844v1", 
    "author": "Lorenzo Strigini", 
    "publish": "2014-04-28T00:38:10Z", 
    "summary": "We propose to validate experimentally a theory of software certification that\nproceeds from assessment of confidence in fault-freeness (due to standards) to\nconservative prediction of failure-free operation."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.6846v1", 
    "title": "An Approach to Maintaining Safety Case Evidence After A System Change", 
    "arxiv-id": "1404.6846v1", 
    "author": "Iain Bate", 
    "publish": "2014-04-28T00:53:00Z", 
    "summary": "Developers of some safety critical systems construct a safety case.\nDevelopers changing a system during development or after release must analyse\nthe change's impact on the safety case. Evidence might be invalidated by\nchanges to the system design, operation, or environmental context. Assumptions\nvalid in one context might be invalid elsewhere. The impact of change might not\nbe obvious. This paper proposes a method to facilitate safety case maintenance\nby highlighting the impact of changes."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.7260v1", 
    "title": "Refinement-Based Specification: Requirements and Architecture", 
    "arxiv-id": "1404.7260v1", 
    "author": "Maria Spichkova", 
    "publish": "2014-04-29T07:12:29Z", 
    "summary": "This paper presents the methodology for the system requirements and\narchitecture w.r.t. their decomposition and refinement. It also introduces\nideas of refinement layers and of refinement-based verification."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.149.3", 
    "link": "http://arxiv.org/pdf/1404.7265v1", 
    "title": "Do we really need to write documentation for a system? CASE tool   add-ons: generator+editor for a precise documentation", 
    "arxiv-id": "1404.7265v1", 
    "author": "Dongyue Mou", 
    "publish": "2014-04-29T07:49:47Z", 
    "summary": "One of the common problems of system development projects is that the system\ndocumentation is often outdated and does not describe the latest version of the\nsystem. The situation is even more complicated if we are speaking not about a\nnatural language description of the system, but about its formal specification.\nIn this paper we discuss how the problem could be solved by updating the\ndocumentation automatically, by generating a new formal specification from the\nmodel if the model is frequently changed."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7414v2", 
    "title": "Summary of the First Workshop on Sustainable Software for Science:   Practice and Experiences (WSSSPE1)", 
    "arxiv-id": "1404.7414v2", 
    "author": "Colin Venters", 
    "publish": "2014-04-29T16:04:40Z", 
    "summary": "Challenges related to development, deployment, and maintenance of reusable\nsoftware for science are becoming a growing concern. Many scientists' research\nincreasingly depends on the quality and availability of software upon which\ntheir works are built. To highlight some of these issues and share experiences,\nthe First Workshop on Sustainable Software for Science: Practice and\nExperiences (WSSSPE1) was held in November 2013 in conjunction with the SC13\nConference. The workshop featured keynote presentations and a large number (54)\nof solicited extended abstracts that were grouped into three themes and\npresented via panels. A set of collaborative notes of the presentations and\ndiscussion was taken during the workshop.\n  Unique perspectives were captured about issues such as comprehensive\ndocumentation, development and deployment practices, software licenses and\ncareer paths for developers. Attribution systems that account for evidence of\nsoftware contribution and impact were also discussed. These include mechanisms\nsuch as Digital Object Identifiers, publication of \"software papers\", and the\nuse of online systems, for example source code repositories like GitHub.\n  This paper summarizes the issues and shared experiences that were discussed,\nincluding cross-cutting issues and use cases. It joins a nascent literature\nseeking to understand what drives software work in science, and how it is\nimpacted by the reward systems of science. These incentives can determine the\nextent to which developers are motivated to build software for the long-term,\nfor the use of others, and whether to work collaboratively or separately. It\nalso explores community building, leadership, and dynamics in relation to\nsuccessful scientific software."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7509v1", 
    "title": "On Cloud-Based Engineering of Dependable Systems", 
    "arxiv-id": "1404.7509v1", 
    "author": "Sami Alajrami", 
    "publish": "2014-04-29T20:01:55Z", 
    "summary": "The cloud computing paradigm is being adopted by many organizations in\ndifferent application domains as it is cost effective and offers a virtually\nunlimited pool of resources. Engineering critical systems can benefit from\nclouds in attaining all dependability means: fault tolerance, fault prevention,\nfault removal and fault forecasting. Our research aims to investigate the\npotential of supporting engineering of dependable software systems with cloud\ncomputing and proposes an open, extensible, and elastic cloud-based software\nengineering workflow system which represents and executes software processes to\nimprove collaboration, reliability and quality assurance, and automation in\nsoftware projects."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7513v2", 
    "title": "A formal approach for correct-by-construction system substitution", 
    "arxiv-id": "1404.7513v2", 
    "author": "Guillaume Babin", 
    "publish": "2014-04-29T20:11:05Z", 
    "summary": "The substitution of a system with another one may occur in several situations\nlike system adaptation, system failure management, system resilience, system\nreconfiguration, etc. It consists in replacing a running system by another one\nwhen given conditions hold. This contribution summarizes our proposal to define\na formal setting for proving the correctness of system substitution. It relies\non refinement and on the Event-B method."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7528v2", 
    "title": "The Utility and Practicality of Quantifying Software Reliability", 
    "arxiv-id": "1404.7528v2", 
    "author": "Rob Ashmore", 
    "publish": "2014-04-29T20:55:28Z", 
    "summary": "We argue that quantifying software reliability is important in demonstrating\nthat system-level risks are As Low As Reasonably Practicable (ALARP).\nFurthermore, we demonstrate that such quantification is possible in at least\none meaningful case. It is, however, unlikely to be practical in every case.\nThis means it is unlikely to be included as an explicit objective in standards.\nHence, for those cases where software reliability can be quantified, merely\nfollowing a standard may lead to risk-reduction opportunities being missed."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7542v1", 
    "title": "A formal experiment to assess the efficacy of certification standards", 
    "arxiv-id": "1404.7542v1", 
    "author": "Virginie Wiels", 
    "publish": "2014-04-29T21:39:04Z", 
    "summary": "Proving the efficacy of certification standards"
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7558v1", 
    "title": "Monitoring service quality: methods and solutions to implement a   managerial dash-board to improve software development", 
    "arxiv-id": "1404.7558v1", 
    "author": "Salvatore D'Antonio", 
    "publish": "2014-04-30T00:28:43Z", 
    "summary": "The software used for running and handling the inter-bank network framework\nprovides services with extremely strict uptime (above 99.98 percent) and\nquality requirements, thus tools to trace and manage changes as well as metrics\nto measure process quality are essential. Having conducted a two year long\ncampaign of data collection and activity monitoring it has been possible to\nanalyze a huge amount of process data from which many aggregated indicators\nwere derived, selected and evaluated for providing a managerial dash-board to\nmonitor software development."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7560v1", 
    "title": "Integrated Maintenance: analysis and perspective of innovation in   railway sector", 
    "arxiv-id": "1404.7560v1", 
    "author": "Roberto Nappi", 
    "publish": "2014-04-30T00:36:55Z", 
    "summary": "The high costs for the management of the modern and complex industrial\ncontrol systems make it necessary to enhance the current maintenance processes.\nTherefore, the need arises to clearly define the goals of the maintenance, in\norder to evolve and continuously enhance the management methods, to efficiently\nintegrate the maintenance activities with the ones related to the production,\nthe service provisioning, and the operation, and to use smart computer-based\nmaintenance systems. This paper proposes a general maintenance approach and its\nspecific application to the railway sector."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7564v1", 
    "title": "End-users needs and requirements for tools to support critical   infrastructures protection", 
    "arxiv-id": "1404.7564v1", 
    "author": "Witold Holubowicz", 
    "publish": "2014-04-30T00:48:29Z", 
    "summary": "The role of the services described in this paper is to support decisions in\nthe Critical Infrastructure Protection (CIP) domain. Those services are\nperceived as the most fundamental functionalities, that will serve as a basis\nfor the planned European simulation centre for modelling the behaviour of\nCritical Infrastructures (CI). The proposed services are: CI-related data\naccessing and gathering, threat forecasting and visualisation, consequence\nanalysis, crowd management, as well as resources and capability management. In\ngeneral, services proposed in the current paper will contribute to reducing the\nproblem of overwhelming decision makers by too large amount of information. In\nthe crisis, their decisions are made on the basis of the large amount of data\nrelated to the current situation, such as the status of CI, localisation of\ncapabilities, weather and threat forecasts etc. The design of the services has\nbeen established with the help of the future end-users. The work presented in\nthis paper is the result of preliminary activities performed in the FP7 project\nCIPRNet."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7565v2", 
    "title": "Investigating SCADA Failures in Interdependent Critical Infrastructure   Systems", 
    "arxiv-id": "1404.7565v2", 
    "author": "Razgar Ebrahimy", 
    "publish": "2014-04-30T00:52:03Z", 
    "summary": "This paper is based on the initial ideas of a PhD proposal which will\ninvestigate SCADA failures in physical infrastructure systems. The results will\nbe used to develop a new notation to help risk assessment using dependable\ncomputing concepts. SCADA systems are widely used within critical\ninfrastructures to perform system controls and deliver services to linked and\ndependent systems. Failures in SCADA systems will be investigated to help us\nunderstand and prevent cascading failures in future."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7618v2", 
    "title": "Modeling and Execution of Multienterprise Business Processes", 
    "arxiv-id": "1404.7618v2", 
    "author": "Stefan Ra\u00df", 
    "publish": "2014-04-30T07:40:14Z", 
    "summary": "We discuss a fully featured multienterprise business process plattform\n(ME-BPP) based on the concepts of agent-based business processes. Using the\nconcepts of the subject-oriented business process (S-BPM) methodology we\ndeveloped an architecture to realize a platform for the execution of\ndistributed business processes. The platform is implemented based on cloud\ntechnology using commercial services. For our discussion we used the well known\nService Interaction Patterns, as they are empirically developed from typical\nbusiness-to-business interactions. We can demonstrate that all patterns can be\neasily modeled and executed based on our architecture. We propose therefore a\nchange from a control flow based to an agent based view to model and enact\nbusiness processes."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7763v2", 
    "title": "Deployment Calculation and Analysis for a Fail-Operational Automotive   Platform", 
    "arxiv-id": "1404.7763v2", 
    "author": "Michael Armbruster", 
    "publish": "2014-04-30T15:35:15Z", 
    "summary": "In domains like automotive, safety-critical features are increasingly\nrealized by software. Some features might even require fail-operational\nbehavior, so that they must be provided even in the presence of random hardware\nfailures. A new fault-tolerant SW/HW architecture for electric vehicles\nprovides inherent safety capabilities that enable fail-operational features. In\nthis paper we introduce a formal model of this architecture and an approach to\ncalculate valid deployments of mixed-critical software-components to the\nexecution nodes, while ensuring fail-operational behavior of certain\ncomponents. Calculated redeployments cover the cases in which faulty execution\nnodes have to be isolated. This allows to formally analyze which set of\nfeatures can be provided under decreasing available execution resources."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7775v4", 
    "title": "Fault Modelling in System-of-Systems Contracts", 
    "arxiv-id": "1404.7775v4", 
    "author": "Klaus Kristensen", 
    "publish": "2014-04-30T16:05:51Z", 
    "summary": "The nature of Systems of Systems (SoSs), large complex systems composed of\nindependent, geographically distributed and continuously evolving constituent\nsystems, means that faults are unavoidable. Previous work on defining\ncontractual specifications of the constituent systems of SoSs does not provide\nany explicit consideration for faults. In this paper we address that gap by\nextending an existing pattern for modelling contracts with fault modelling\nconcepts. The proposed extensions are introduced with respect to an Audio\nVisual SoS case study from Bang and Olufsen, before discussing how they relate\nto previous work on modelling faults in SoSs."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7778v2", 
    "title": "SoS Fault Modelling at the Architectural Level in an Emergency Response   Case Study", 
    "arxiv-id": "1404.7778v2", 
    "author": "Afra Alrbaiyan", 
    "publish": "2014-04-30T16:17:16Z", 
    "summary": "Systems of systems (SoSs) are particularly vulnerable to faults and other\nthreats to their dependability, but frequently inhabit domains that demand high\nlevels of dependability. For this reason fault tolerance analysis is important\nin SoS engineering. The COMPASS project has previously proposed a Fault\nTolerance Architecture Framework (FMAF), consisting of a collection of\nviewpoints that support systematic reasoning about faults in an SoS at the\narchitectural level. The FMAF has been demonstrated previously with an analysis\nof an example fault in an emergency response SoS. In this paper we present\nfurther examples of the FMAF's practical use, by analysing different types of\nfaults drawn from the same emergency response case study. These example faults\nexercise different aspects of the FMAF, demonstrate its use in more complex\nfault modelling scenarios, and raise new questions for further development."
},{
    "category": "cs.SE", 
    "doi": "10.5334/jors.an", 
    "link": "http://arxiv.org/pdf/1404.7792v2", 
    "title": "Towards Verification of Constituent Systems through Automated Proof", 
    "arxiv-id": "1404.7792v2", 
    "author": "Richard Payne", 
    "publish": "2014-04-30T16:43:25Z", 
    "summary": "This paper explores verification of constituent systems within the context of\nthe Symphony tool platform for Systems of Systems (SoS). Our SoS modelling\nlanguage, CML, supports various contractual specification elements, such as\nstate invariants and operation preconditions, which can be used to specify\ncontractual obligations on the constituent systems of a SoS. To support\nverification of these obligations we have developed a proof obligation\ngenerator and theorem prover plugin for Symphony. The latter uses the\nIsabelle/HOL theorem prover to automatically discharge the proof obligations\narising from a CML model. Our hope is that the resulting proofs can then be\nused to formally verify the conformance of each constituent system, which is\nturn would result in a dependable SoS."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5201", 
    "link": "http://arxiv.org/pdf/1404.7814v1", 
    "title": "A tlm-based platform to specify and verify component-based real-time   systems", 
    "arxiv-id": "1404.7814v1", 
    "author": "Zolfy Lighvan Mina", 
    "publish": "2014-04-30T17:57:44Z", 
    "summary": "This paper is about modeling and verification languages with their pros and\ncons. Modeling is dynamic part of system development process before\nrealization. The cost and risky situations obligate designer to model system\nbefore production and modeling gives designer more flexible and dynamic image\nof realized system. Formal languages and modeling methods are the ways to model\nand verify systems but they have their own difficulties in specifying systems.\nSome of them are very precise but hard to specify complex systems like TRIO,\nand others do not support object oriented design and hardware/software\nco-design in real-time systems. In this paper we are going to introduce systemC\nand the more abstracted method called TLM 2.0 that solved all mentioned\nproblems."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5201", 
    "link": "http://arxiv.org/pdf/1406.0117v1", 
    "title": "EACOF: A Framework for Providing Energy Transparency to enable   Energy-Aware Software Development", 
    "arxiv-id": "1406.0117v1", 
    "author": "Kerstin Eder", 
    "publish": "2014-05-31T22:55:09Z", 
    "summary": "Making energy consumption data accessible to software developers is an\nessential step towards energy efficient software engineering. The presence of\nvarious different, bespoke and incompatible, methods of instrumentation to\nobtain energy readings is currently limiting the widespread use of energy data\nin software development. This paper presents EACOF, a modular Energy-Aware\nComputing Framework that provides a layer of abstraction between sources of\nenergy data and the applications that exploit them. EACOF replaces platform\nspecific instrumentation through two APIs - one accepts input to the framework\nwhile the other provides access to application software. This allows developers\nto profile their code for energy consumption in an easy and portable manner\nusing simple API calls. We outline the design of our framework and provide\ndetails of the API functionality. In a use case, where we investigate the\nimpact of data bit width on the energy consumption of various sorting\nalgorithms, we demonstrate that the data obtained using EACOF provides\ninteresting, sometimes counter-intuitive, insights. All the code is available\nonline under an open source license. http://github.com/eacof"
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5201", 
    "link": "http://arxiv.org/pdf/1406.2822v1", 
    "title": "A Framework for the Evaluation of SaaS Impact", 
    "arxiv-id": "1406.2822v1", 
    "author": "Manuel Perez Cota", 
    "publish": "2014-06-11T08:43:03Z", 
    "summary": "Nowadays the technological progress allows us to have highly flexible\nsolutions, easily accessible with lower levels of investment, which leads to\nmany companies adopting SaaS (Software-as-a-Service) to support their business\nprocesses. Associated with this movement and considering the advantages of\nSaaS, it is important to understand whether work is being developed that is\nunderutilized because companies are not taking advantage of it, and in this\ncase it is necessary to understand the reasons thereof. This knowledge is\nimportant even for people who do not use or do not develop/provide SaaS, since\nsooner or later it will be unavoidable due to current trends. In the near\nfuture, nearly all decision-makers of IT strategies will be forced to consider\nadopting SaaS as an IT solution for the convenience benefits associated with\ntechnology or market competition. At that time they will have to know how to\nevaluate impacts and decide. Often, decision-makers of business strategies\nconsider only the attractive incentives of using SaaS ignoring the impacts\nassociated with new technologies. The need for tools and processes to assess\nthese impacts before adopting a SaaS solution is crucial to ensure the\nsustainability of the information system, reduce uncertainty and facilitate\ndecision making. This article presents a framework for evaluating impacts of\nSaaS called SIE (SaaS Impact Evaluation) which in addition to guidance for the\npresent research, aims to provide guidelines for the collection, data analysis,\nimpact assessment and decision making about including SaaS on the organizations\nstrategic plans."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5201", 
    "link": "http://arxiv.org/pdf/1406.2824v1", 
    "title": "Some Ideas for Program Verifier Tactics", 
    "arxiv-id": "1406.2824v1", 
    "author": "Gudmund Grov", 
    "publish": "2014-06-11T08:46:13Z", 
    "summary": "A program verifier is a tool that can be used to verify that a \"contract\" for\na program holds - i.e. given a precondition the program guarantees that a given\npostcondition holds - by only working at the level of the annotated program. An\nalternative approach is to use an interactive theorem prover, which enables\nusers to encode common proof patterns as special programs called \"tactics\".\nThis offers more flexibility than program verifiers, but at the expense of\nskills required by the user. Here, we add such flexibility to program verifiers\nby developing \"tactics\" as a form of program refactoring called DTacs. A formal\ncharacterisation and set of examples are given, illustrated with a case study\nfrom NASA."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.3554v1", 
    "title": "Methodological Societies", 
    "arxiv-id": "1406.3554v1", 
    "author": "Ammar Lahlouhi", 
    "publish": "2014-06-13T14:52:30Z", 
    "summary": "The evolution of self-adaptive systems poses the problems of their coherence\nand the resume of the systems' functioning taking into account the accomplished\nwork. While they are the base of the self-adaptive systems, these two aspects\nare not considered in the related works. In this paper, we propose a\nmethodological based approach. In such approach, the adaptive system's\nevolution is thought at its model level where its execution is made on the\nsystem by exploiting a methodological process. For its concretization, we use\ncolored Petri nets to describe the agents' individual tasks. To handle the\nsystem's functioning resume, we exploit the property of Petri nets on which the\ncontrol flow depends on last marking only."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.3661v1", 
    "title": "Trace checking of Metric Temporal Logic with Aggregating Modalities   using MapReduce", 
    "arxiv-id": "1406.3661v1", 
    "author": "Srdan Krstic", 
    "publish": "2014-06-13T22:28:54Z", 
    "summary": "Modern complex software systems produce a large amount of execution data,\noften stored in logs. These logs can be analyzed using trace checking\ntechniques to check whether the system complies with its requirements\nspecifications. Often these specifications express quantitative properties of\nthe system, which include timing constraints as well as higher-level\nconstraints on the occurrences of significant events, expressed using aggregate\noperators. In this paper we present an algorithm that exploits the MapReduce\nprogramming model to check specifications expressed in a metric temporal logic\nwith aggregating modalities, over large execution traces. The algorithm\nexploits the structure of the formula to parallelize the evaluation, with a\nsignificant gain in time. We report on the assessment of the implementation -\nbased on the Hadoop framework - of the proposed algorithm and comment on its\nscalability."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.3727v1", 
    "title": "A methodology to identify the level of reuse using template factors", 
    "arxiv-id": "1406.3727v1", 
    "author": "Chandra Mohan", 
    "publish": "2014-06-14T13:26:39Z", 
    "summary": "To build large scale software systems, Component Based Software Engineering\n(CBSE) has played a vital role. The current practices of software industry\ndemands more development of a software within time and budget which is highly\nproductive to them. It became so necessary to achieve how effectively the\nsoftware component is reusable. In order to meet this, the component level\nreuse, in terms of both class and method level can be possibly done. The\ntraditional approaches are presented in the literature upto the level of extent\nof achievement of reuse. Any how still effective reuse is a challenging issue\nas a part. In this paper, a methodology has proposed for the identification of\nreuse level which has been considered by the using reuse metrics such as the\nClass Template Factor(CTF) and Method Template Factor(MTF). By considering\nthese measures makes easy to identify the level of reuse so that helps in the\ngrowth the productivity in the organization."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.3728v1", 
    "title": "Component Based Software Development: A State of Art", 
    "arxiv-id": "1406.3728v1", 
    "author": "Salman Abdul Moiz", 
    "publish": "2014-06-14T13:27:51Z", 
    "summary": "One of the goals of Software design is to model a system in such a way that\nit is reused. Actively reusing designs or code allows taking advantage of the\ninvestment made on reusable components. However development of domain specific\ncomponents and its impact on effort in terms of cost and time is still a\nchallenging issue. The component based technology has transformed over a period\nof time from a simple component to the domain specific components. This paper\npresents a state of art of the drastic change in component technology from\ncomponent engineering to domain engineering."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.4123v1", 
    "title": "A strategy to identify components using clustering approach for   component reusability", 
    "arxiv-id": "1406.4123v1", 
    "author": "Chandra Mohan", 
    "publish": "2014-06-14T13:33:16Z", 
    "summary": "Component Based Software Engineering (CBSE) has played a very important role\nfor building larger software systems The current practices of software industry\ndemands development of a software within time and budget which is highly\nproductive. It is necessary to achieve how much effectively the software\ncomponent is reusable. To achieve this, the component identification is\nmandatory. The traditional approaches are presented in the literature. However\neffective reuse is still a challenging issue. In this paper, a strategy has\nbeen proposed for the identification of a business component using clustering\nmethodology. This approach will be useful in identifying the reusable\ncomponents for different domains. The proposed approach has identified the\nreconfigured component using the CBO measure to reduce the coupling between the\nobjects. By considering this proposed strategy, the productivity can be\nincreased in the organization."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.4692v1", 
    "title": "A Quality Framework for Agile Requirements: A Practitioner's Perspective", 
    "arxiv-id": "1406.4692v1", 
    "author": "Andy Zaidman", 
    "publish": "2014-06-18T12:16:43Z", 
    "summary": "Verification activities are necessary to ensure that the requirements are\nspecified in a correct way. However, until now requirements verification\nresearch has focused on traditional up-front requirements. Agile or\njust-in-time requirements are by definition incomplete, not specific and might\nbe ambiguous when initially specified, indicating a different notion of\n'correctness'. We analyze how verification of agile requirements quality should\nbe performed, based on literature of traditional and agile requirements. This\nleads to an agile quality framework, instantiated for the specific requirement\ntypes of feature requests in open source projects and user stories in agile\nprojects. We have performed an initial qualitative validation of our framework\nfor feature requests with eight practitioners from the Dutch agile community,\nreceiving overall positive feedback."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.5708v1", 
    "title": "Runtime Enforcement for Component-Based Systems", 
    "arxiv-id": "1406.5708v1", 
    "author": "Mohamad Jaber", 
    "publish": "2014-06-22T11:49:11Z", 
    "summary": "Runtime enforcement is an increasingly popular and effective dynamic\nvalidation technique aiming to ensure the correct runtime behavior (w.r.t. a\nformal specification) of systems using a so-called enforcement monitor. In this\npaper we introduce runtime enforcement of specifications on component-based\nsystems (CBS) modeled in the BIP (Behavior, Interaction and Priority)\nframework. BIP is a powerful and expressive component-based framework for\nformal construction of heterogeneous systems. However, because of BIP\nexpressiveness, it remains difficult to enforce at design-time complex\nbehavioral properties.\n  First we propose a theoretical runtime enforcement framework for CBS where we\ndelineate a hierarchy of sets of enforceable properties (i.e., properties that\ncan be enforced) according to the number of observational steps a system is\nallowed to deviate from the property (i.e., the notion of k-step\nenforceability). To ensure the observational equivalence between the correct\nexecutions of the initial system and the monitored system, we show that i) only\nstutter-invariant properties should be enforced on CBS with our monitors, ii)\nsafety properties are 1-step enforceable. Given an abstract enforcement monitor\n(as a finite-state machine) for some 1-step enforceable specification, we\nformally instrument (at relevant locations) a given BIP system to integrate the\nmonitor. At runtime, the monitor observes and automatically avoids any error in\nthe behavior of the system w.r.t. the specification. Our approach is fully\nimplemented in an available tool that we used to i) avoid deadlock occurrences\non a dining philosophers benchmark, and ii) ensure the correct placement of\nrobots on a map."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.5731v1", 
    "title": "Guidelines to minimize cost of software quality in agile scrum process", 
    "arxiv-id": "1406.5731v1", 
    "author": "Gopinath Ganapathy", 
    "publish": "2014-06-22T14:30:15Z", 
    "summary": "This paper presents a case study of Agile Scrum process followed in Retail\nDomain project. This paper also reveals the impacts of Cost of Software\nQuality, when agile scrum process is not followed efficiently. While analyzing\nthe case study, the gaps were found and guidelines for process improvements\nwere also suggested in this paper."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.6622v2", 
    "title": "Managing LTL properties in Event-B refinement", 
    "arxiv-id": "1406.6622v2", 
    "author": "David Williams", 
    "publish": "2014-06-25T16:09:09Z", 
    "summary": "Refinement in Event-B supports the development of systems via proof based\nstep-wise refinement of events. This refinement approach ensures safety\nproperties are preserved, but additional reasoning is required in order to\nestablish liveness and fairness properties.\n  In this paper we present results which allow a closer integration of two\nformal methods, Event-B and linear temporal logic. In particular we show how a\nclass of temporal logic properties can carry through a refinement chain of\nmachines. Refinement steps can include introduction of new events, event\nrenaming and event splitting. We also identify a general liveness property that\nholds for the events of the initial system of a refinement chain. The approach\nwill aid developers in enabling them to verify linear temporal logic properties\nat early stages of a development, knowing they will be preserved at later\nstages. We illustrate the results via a simple case study."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijfcst.2014.4305", 
    "link": "http://arxiv.org/pdf/1406.6834v1", 
    "title": "A Model-Based Approach to Impact Analysis Using Model Differencing", 
    "arxiv-id": "1406.6834v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-06-26T10:34:20Z", 
    "summary": "Impact analysis is concerned with the identification of consequences of\nchanges and is therefore an important activity for software evolution. In\nmodelbased software development, models are core artifacts, which are often\nused to generate essential parts of a software system. Changes to a model can\nthus substantially affect different artifacts of a software system. In this\npaper, we propose a modelbased approach to impact analysis, in which explicit\nimpact rules can be specified in a domain specific language (DSL). These impact\nrules define consequences of designated UML class diagram changes on software\nartifacts and the need of dependent activities such as data evolution. The UML\nclass diagram changes are identified automatically using model differencing.\nThe advantage of using explicit impact rules is that they enable the\nformalization of knowledge about a product. By explicitly defining this\nknowledge, it is possible to create a checklist with hints about development\nsteps that are (potentially) necessary to manage the evolution. To validate the\nfeasibility of our approach, we provide results of a case study."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.simpat.2014.07.003", 
    "link": "http://arxiv.org/pdf/1406.6937v2", 
    "title": "A Family of Simulation Criteria to Guide DEVS Models Validation   Rigorously, Systematically and Semi-Automatically", 
    "arxiv-id": "1406.6937v2", 
    "author": "Claudia Frydman", 
    "publish": "2014-06-26T16:28:56Z", 
    "summary": "The most common method to validate a DEVS model against the requirements is\nto simulate it several times under different conditions, with some simulation\ntool. The behavior of the model is compared with what the system is supposed to\ndo. The number of different scenarios to simulate is usually infinite,\ntherefore, selecting them becomes a crucial task. This selection, actually, is\nmade following the experience or intuition of an engineer. Here we present a\nfamily of criteria to conduct DEVS model simulations in a disciplined way and\ncovering the most significant simulations to increase the confidence on the\nmodel. This is achieved by analyzing the mathematical representation of the\nDEVS model and, thus, part of the validation process can be automatized."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.simpat.2014.07.003", 
    "link": "http://arxiv.org/pdf/1406.7000v1", 
    "title": "Towards a Pattern-based Automatic Generation of Logical Specifications   for Software Models", 
    "arxiv-id": "1406.7000v1", 
    "author": "Radoslaw Klimek", 
    "publish": "2014-06-26T19:57:36Z", 
    "summary": "The work relates to the automatic generation of logical specifications,\nconsidered as sets of temporal logic formulas, extracted directly from\ndeveloped software models. The extraction process is based on the assumption\nthat the whole developed model is structured using only predefined workflow\npatterns. A method of automatic transformation of workflow patterns to logical\nspecifications is proposed. Applying the presented concepts enables bridging\nthe gap between the benefits of deductive reasoning for the correctness\nverification process and the difficulties in obtaining complete logical\nspecifications for this process."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2568225.2568237", 
    "link": "http://arxiv.org/pdf/1406.7136v1", 
    "title": "Verifying Component and Connector Models against Crosscutting Structural   Views", 
    "arxiv-id": "1406.7136v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-06-27T10:05:46Z", 
    "summary": "The structure of component and connector (C&C) models, which are used in many\napplication domains of software engineering, consists of components at\ndifferent containment levels, their typed input and output ports, and the\nconnectors between them. C&C views, presented in [24], can be used to specify\nstructural properties of C&C models in an expressive and intuitive way. In this\nwork we address the verification of a C&C model against a C&C view and present\nefficient (polynomial) algorithms to decide satisfaction. A unique feature of\nour work, not present in existing approaches to checking structural properties\nof C&C models, is the generation of witnesses for satisfaction/non-satisfaction\nand of short naturallanguage texts, which serve to explain and formally justify\nthe verification results and point the engineer to its causes. A prototype tool\nand an evaluation over four example systems with multiple views, performance\nand scalability experiments, as well as a user study of the usefulness of the\nwitnesses for engineers, demonstrate the contribution of our work to the\nstate-of-the-art in component and connector modeling and analysis."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.157.9", 
    "link": "http://arxiv.org/pdf/1406.7608v3", 
    "title": "Parameterized Synthesis Case Study: AMBA AHB (extended version)", 
    "arxiv-id": "1406.7608v3", 
    "author": "Ayrat Khalimov", 
    "publish": "2014-06-30T05:41:54Z", 
    "summary": "We revisit the AMBA AHB case study that has been used as a benchmark for\nseveral reactive syn- thesis tools. Synthesizing AMBA AHB implementations that\ncan serve a large number of masters is still a difficult problem. We\ndemonstrate how to use parameterized synthesis in token rings to obtain an\nimplementation for a component that serves a single master, and can be arranged\nin a ring of arbitrarily many components. We describe new tricks -- property\ndecompositional synthesis, and direct encoding of simple GR(1) -- that together\nwith previously described optimizations allowed us to synthesize the model with\n14 states in 30 minutes."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.157.9", 
    "link": "http://arxiv.org/pdf/1408.1068v1", 
    "title": "The Size of Software Projects Developed by Mexican Companies", 
    "arxiv-id": "1408.1068v1", 
    "author": "Jose Figueroa", 
    "publish": "2014-08-05T18:53:59Z", 
    "summary": "Currently, most software projects around the world are small rather than\nlarge. Despite this, there are more methodologies, tools, frameworks,\nprocesses, and so on, for developing and managing large software projects than\nfor small ones. Small software projects are important because they generate\nconsiderable resources. For example: apps (small mobile applications) generate\naround $25 billion dollars of revenue. This paper shows our findings regarding\nthe size of the projects built by Mexican software development companies. We\nsurveyed 107 Mexican companies and found that 92% of their developed projects\nare micro and small, and 8% are medium or large. In addition, according to our\nresearch, 84.1% of companies in Mexico are micro or small businesses."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.157.9", 
    "link": "http://arxiv.org/pdf/1408.1150v1", 
    "title": "Early Development of UVM based Verification Environment of Image Signal   Processing Designs using TLM Reference Model of RTL", 
    "arxiv-id": "1408.1150v1", 
    "author": "Krishna Kumar", 
    "publish": "2014-08-06T00:20:34Z", 
    "summary": "With semiconductor industry trend of smaller the better, from an idea to a\nfinal product, more innovation on product portfolio and yet remaining\ncompetitive and profitable are few criteria which are culminating into pressure\nand need for more and more innovation for CAD flow, process management and\nproject execution cycle. Project schedules are very tight and to achieve first\nsilicon success is key for projects. This necessitates quicker verification\nwith better coverage matrix. Quicker Verification requires early development of\nthe verification environment with wider test vectors without waiting for RTL to\nbe available.\n  In this paper, we are presenting a novel approach of early development of\nreusable multi-language verification flow, by addressing four major activities\nof verification like Early creation of Executable Specification, Early creation\nof Verification Environment, Early development of test vectors and Better and\nincreased Re-use of blocks.\n  Although this paper focuses on early development of UVM based Verification\nEnvironment of Image Signal Processing designs using TLM Reference Model of\nRTL, same concept can be extended for non-image signal processing designs.\n  Main Keywords are SystemVerilog, SystemC, Transaction Level Modeling,\nUniversal Verification Methodology (UVM), Processor model, Universal\nVerification Component (UVC), Reference Model."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1673", 
    "link": "http://arxiv.org/pdf/1408.1293v2", 
    "title": "Do feelings matter? On the correlation of affects and the self-assessed   productivity in software engineering", 
    "arxiv-id": "1408.1293v2", 
    "author": "Pekka Abrahamsson", 
    "publish": "2014-08-06T14:31:15Z", 
    "summary": "Background: software engineering research (SE) lacks theory and methodologies\nfor addressing human aspects in software development. Development tasks are\nundertaken through cognitive processing activities. Affects (emotions, moods,\nfeelings) have a linkage to cognitive processing activities and the\nproductivity of individuals. SE research needs to incorporate affect\nmeasurements to valorize human factors and to enhance management styles.\n  Objective: analyze the affects dimensions of valence, arousal, and dominance\nof software developers and their real-time correlation with their self-assessed\nproductivity (sPR).\n  Method: repeated measurements design with 8 participants (4 students, 4\nprofessionals), conveniently sampled and studied individually over 90 minutes\nof programming. The analysis was performed by fitting a linear mixed- effects\n(LME) model.\n  Results: valence and dominance are positively correlated with the sPR. The\nmodel was able to express about 38% of deviance from the sPR. Many lessons were\nlearned when employing psychological measurements in SE and for fitting LME.\n  Conclusion: this article demonstrates the value of applying psychological\ntests in SE and echoes a call to valorize the human, individualized aspects of\nsoftware developers. It reports a body of knowledge about affects, their\nclassification, their measurement, and the best practices to perform\npsychological measurements in SE with LME models."
},{
    "category": "cs.SE", 
    "doi": "10.1002/smr.1673", 
    "link": "http://arxiv.org/pdf/1408.1776v1", 
    "title": "Context-awareness of the IoT through the on-the-fly preference modeling", 
    "arxiv-id": "1408.1776v1", 
    "author": "Leszek Kotulski", 
    "publish": "2014-08-08T07:47:46Z", 
    "summary": "The context-awareness of things that belong to IoT networks have to be\nconsidered in a distributed computation paradigm. In the paper we suggest the\nuse of graph transformations and temporal logic as a formal framework for a\nknowledge representation of user/inhabitant behaviors in multi-agent systems.\nIoT networks are considered as graph structures. Dynamic preference models,\nunderstood as a priority in the selecting, is also introduced. Preference\nmodels as a result of observed behaviors base on formal logic, and they are\nbuilt on-the-fly by software agents. Software agents gather knowledge about\nuser preferences expressed in terms of logical specifications as well as\nsuggest on-the-fly future behavior basing on the logical inference process\nusing the semantic tableaux method. The predictive processes are result of some\nnew and important events in the context of IoT systems that should meet a\nresponse. Due to the ubiquitous availability of cyber systems that interact\nwith physical environments, there is a great need to develop technologies that\ntarget the whole IoT system as a context-awareness system. Formal approach\nincreases the trustworthy of a system. A simple yet illustrative example is\nprovided."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2568225.2568324", 
    "link": "http://arxiv.org/pdf/1408.2103v1", 
    "title": "A Critical Review of \"Automatic Patch Generation Learned from   Human-Written Patches\": Essay on the Problem Statement and the Evaluation of   Automatic Software Repair", 
    "arxiv-id": "1408.2103v1", 
    "author": "Martin Monperrus", 
    "publish": "2014-08-09T14:23:06Z", 
    "summary": "At ICSE'2013, there was the first session ever dedicated to automatic program\nrepair. In this session, Kim et al. presented PAR, a novel template-based\napproach for fixing Java bugs. We strongly disagree with key points of this\npaper. Our critical review has two goals. First, we aim at explaining why we\ndisagree with Kim and colleagues and why the reasons behind this disagreement\nare important for research on automatic software repair in general. Second, we\naim at contributing to the field with a clarification of the essential ideas\nbehind automatic software repair. In particular we discuss the main evaluation\ncriteria of automatic software repair: understandability, correctness and\ncompleteness. We show that depending on how one sets up the repair scenario,\nthe evaluation goals may be contradictory. Eventually, we discuss the nature of\nfix acceptability and its relation to the notion of software correctness."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2568225.2568324", 
    "link": "http://arxiv.org/pdf/1408.2607v1", 
    "title": "What Java Developers Know About Compatibility, And Why This Matters", 
    "arxiv-id": "1408.2607v1", 
    "author": "Premek Brada", 
    "publish": "2014-08-12T02:59:42Z", 
    "summary": "Real-world programs are neither monolithic nor static -- they are constructed\nusing platform and third party libraries, and both programs and libraries\ncontinuously evolve in response to change pressure. In case of the Java\nlanguage, rules defined in the Java Language and Java Virtual Machine\nSpecifications define when library evolution is safe. These rules distinguish\nbetween three types of compatibility - binary, source and behavioural. We claim\nthat some of these rules are counter intuitive and not well-understood by many\ndevelopers. We present the results of a survey where we quizzed developers\nabout their understanding of the various types of compatibility. 414 developers\nresponded to our survey. We find that while most programmers are familiar with\nthe rules of source compatibility, they generally lack knowledge about the\nrules of binary and behavioural compatibility. This can be problematic when\norganisations switch from integration builds to technologies that require\ndynamic linking, such as OSGi. We have assessed the gravity of the problem by\nstudying how often linkage-related problems are referenced in issue tracking\nsystems, and find that they are common."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5402", 
    "link": "http://arxiv.org/pdf/1408.2687v1", 
    "title": "A Noble Methodology for Users Work Process Driven Software Requirements   for Smart Handheld Devices", 
    "arxiv-id": "1408.2687v1", 
    "author": "M. Rokonuzzaman", 
    "publish": "2014-08-12T10:50:35Z", 
    "summary": "Requirement engineering is a key ingredient for software development to be\neffective. Apart from the traditional software requirement which is not much\nappropriate for new emerging software such as smart handheld device based\nsoftware. In many perspectives of requirement engineering, traditional and new\nemerging software are not similar. Whereas requirement engineering of\ntraditional software needs more research, it is obvious that new emerging\nsoftware needs methodically and in-depth research for improved productivity,\nquality, risk management and validity. In particular, the result of this paper\nshows that how effective requirement engineering can improve in project\nnegotiation, project planning, managing feature creep, testing, defect, rework\nand product quality. This paper also shows a new methodology which is focused\non users work process applicable for eliciting the requirement of traditional\nsoftware and any new type software of smart handheld device such as iPad. As an\nexample, the paper shows how the methodology will be applied as a software\nrequirement of iPad-based software for play-group students."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijsea.2014.5407", 
    "link": "http://arxiv.org/pdf/1408.2969v1", 
    "title": "An Extended Stable Marriage Problem Algorithm for Clone Detection", 
    "arxiv-id": "1408.2969v1", 
    "author": "Helge Janicke", 
    "publish": "2014-08-13T10:47:22Z", 
    "summary": "Code cloning negatively affects industrial software and threatens\nintellectual property. This paper presents a novel approach to detecting cloned\nsoftware by using a bijective matching technique. The proposed approach focuses\non increasing the range of similarity measures and thus enhancing the precision\nof the detection. This is achieved by extending a well-known stable-marriage\nproblem (SMP) and demonstrating how matches between code fragments of different\nfiles can be expressed. A prototype of the proposed approach is provided using\na proper scenario, which shows a noticeable improvement in several features of\nclone detection such as scalability and accuracy."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2559627.2559633", 
    "link": "http://arxiv.org/pdf/1408.3231v1", 
    "title": "Simulations on Consumer Tests: A Perspective for Driver Assistance   Systems", 
    "arxiv-id": "1408.3231v1", 
    "author": "Vladislavs Serebro", 
    "publish": "2014-08-14T09:42:56Z", 
    "summary": "This article discusses new challenges for series development regarding the\nvehicle safety that arise from the recently published AEB test protocol by the\nconsumer-test-organisation EuroNCAP for driver assistance systems [6]. The\ntests from the test protocol are of great significance for an OEM that sells\nmillions of cars each year, due to the fact that a positive rating of the\nvehicle-under-test (VUT) in safety relevant aspects is important for the\nreputation of a car manufacturer. The further intensification and aggravation\nof the test requirements for those systems is one of the challenges, that has\nto be mastered in order to continuously make significant contributions to\nsafety for high-volume cars. Therefore, it is to be shown how a simulation\napproach may support the development process, especially with tolerance\nanalysis. This article discusses the current stage of work, steps that are\nplanned for the future and results that can be expected at the end of such an\nanalysis."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.3253v2", 
    "title": "A measurement based software quality framework", 
    "arxiv-id": "1408.3253v2", 
    "author": "Zolt\u00e1n Badinka", 
    "publish": "2014-08-14T12:06:21Z", 
    "summary": "In this report we propose a solution to problem of the dependency on the\nexperience of the software project quality assurance personnel by providing a\ntransparent, objective and measurement based quality framework. The framework\nhelps the quality assurance experts making objective and comparable decisions\nin software projects by defining and assessing measurable quality goals and\nthresholds, directly relating these to an escalation mechanism. First results\nof applying the proposed measurement based software quality framework in a real\nlife case study are also addressed in this report."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.3976v1", 
    "title": "Static Analysis for Extracting Permission Checks of a Large Scale   Framework: The Challenges And Solutions for Analyzing Android", 
    "arxiv-id": "1408.3976v1", 
    "author": "Yves Le Traon", 
    "publish": "2014-08-18T11:16:41Z", 
    "summary": "A common security architecture is based on the protection of certain\nresources by permission checks (used e.g., in Android and Blackberry). It has\nsome limitations, for instance, when applications are granted more permissions\nthan they actually need, which facilitates all kinds of malicious usage (e.g.,\nthrough code injection). The analysis of permission-based framework requires a\nprecise mapping between API methods of the framework and the permissions they\nrequire. In this paper, we show that naive static analysis fails miserably when\napplied with off-the-shelf components on the Android framework. We then present\nan advanced class-hierarchy and field-sensitive set of analyses to extract this\nmapping. Those static analyses are capable of analyzing the Android framework.\nThey use novel domain specific optimizations dedicated to Android."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.4523v1", 
    "title": "The Correlation among Software Complexity Metrics with Case Study", 
    "arxiv-id": "1408.4523v1", 
    "author": "Bassam Arkok", 
    "publish": "2014-08-20T05:08:32Z", 
    "summary": "People demand for software quality is growing increasingly, thus different\nscales for the software are growing fast to handle the quality of software. The\nsoftware complexity metric is one of the measurements that use some of the\ninternal attributes or characteristics of software to know how they effect on\nthe software quality. In this paper, we cover some of more efficient software\ncomplexity metrics such as Cyclomatic complexity, line of code and Hallstead\ncomplexity metric. This paper presents their impacts on the software quality.\nIt also discusses and analyzes the correlation between them. It finally reveals\ntheir relation with the number of errors using a real dataset as a case study."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.4565v1", 
    "title": "Cloud WorkBench - Infrastructure-as-Code Based Cloud Benchmarking", 
    "arxiv-id": "1408.4565v1", 
    "author": "Harald Gall", 
    "publish": "2014-08-20T09:04:57Z", 
    "summary": "To optimally deploy their applications, users of Infrastructure-as-a-Service\nclouds are required to evaluate the costs and performance of different\ncombinations of cloud configurations to find out which combination provides the\nbest service level for their specific application. Unfortunately, benchmarking\ncloud services is cumbersome and error-prone. In this paper, we propose an\narchitecture and concrete implementation of a cloud benchmarking Web service,\nwhich fosters the definition of reusable and representative benchmarks. In\ndistinction to existing work, our system is based on the notion of\nInfrastructure-as-Code, which is a state of the art concept to define IT\ninfrastructure in a reproducible, well-defined, and testable way. We\ndemonstrate our system based on an illustrative case study, in which we measure\nand compare the disk IO speeds of different instance and storage types in\nAmazon EC2."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.4644v1", 
    "title": "Developer Belief vs. Reality: The Case of the Commit Size Distribution", 
    "arxiv-id": "1408.4644v1", 
    "author": "Michel A. Salim", 
    "publish": "2014-08-20T13:24:26Z", 
    "summary": "The design of software development tools follows from what the developers of\nsuch tools believe is true about software development. A key aspect of such\nbeliefs is the size of code contributions (commits) to a software project. In\nthis paper, we show that what tool developers think is true about the size of\ncode contributions is different by more than an order of magnitude from\nreality. We present this reality, called the commit size distribution, for a\nlarge sample of open source and selected closed source projects. We suggest\nthat these new empirical insights will help improve software development tools\nby aligning underlying design assumptions closer with reality."
},{
    "category": "cs.SE", 
    "doi": "10.13140/2.1.3484.3528", 
    "link": "http://arxiv.org/pdf/1408.4899v1", 
    "title": "Software Cloning in Extreme Programming Environment", 
    "arxiv-id": "1408.4899v1", 
    "author": "Ashima", 
    "publish": "2014-08-21T06:44:03Z", 
    "summary": "Software systems are evolving by adding new functions and modifying existing\nfunctions over time. Through the evolution, the structure of software is\nbecoming more complex and so the understandability and maintainability of\nsoftware systems is deteriorating day by day. These are not only important but\none of the most expensive activities in software development. Refactoring has\noften been applied to the software to improve them. One of the targets of\nrefactoring is to limit Code Cloning because it hinders software maintenance\nand affects its quality. And in order to cope with the constant changes,\nrefactoring is seen as an essential component of Extreme Programming. Agile\nMethods use refactoring as important key practice and are first choice for\ndeveloping clone-free code. This paper summarizes my overview talk on software\ncloning analysis. It first discusses the notion of code cloning, types of\nclones, reasons, its consequences and analysis. It highlights Code Cloning in\nExtreme Programming Environment and finds Clone Detection as effective tool for\nRefactoring."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-642-35843-2_6", 
    "link": "http://arxiv.org/pdf/1408.4974v1", 
    "title": "A Model of the Commit Size Distribution of Open Source", 
    "arxiv-id": "1408.4974v1", 
    "author": "Michel A. Salim", 
    "publish": "2014-08-21T12:31:11Z", 
    "summary": "A fundamental unit of work in programming is the code contribution (\"commit\")\nthat a developer makes to the code base of the project in work. We use\nstatistical methods to derive a model of the probabilistic distribution of\ncommit sizes in open source projects and we show that the model is applicable\nto different project sizes. We use both graphical as well as statistical\nmethods to validate the goodness of fit of our model. By measuring and modeling\na fundamental dimension of programming we help improve software development\ntools and our understanding of software development."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.4978v1", 
    "title": "The Empirical Commit Frequency Distribution of Open Source Projects", 
    "arxiv-id": "1408.4978v1", 
    "author": "Michel A. Salim", 
    "publish": "2014-08-21T12:45:55Z", 
    "summary": "A fundamental unit of work in programming is the code contribution (\"commit\")\nthat a developer makes to the code base of the project in work. An author's\ncommit frequency describes how often that author commits. Knowing the\ndistribution of all commit frequencies is a fundamental part of understanding\nsoftware development processes. This paper presents a detailed quantitative\nanalysis of commit frequencies in open-source software development. The\nanalysis is based on a large sample of open source projects, and presents the\noverall distribution of commit frequencies. We analyze the data to show the\ndifferences between authors and projects by project size; we also includes a\ncomparison of successful and non successful projects and we derive an activity\nindicator from these analyses. By measuring a fundamental dimension of\nprogramming we help improve software development tools and our understanding of\nsoftware development. We also validate some fundamental assumptions about\nsoftware development."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.4986v1", 
    "title": "Objektorientierte Graphendarstellung von Simulink-Modellen zur einfachen   Analyse und Transformation", 
    "arxiv-id": "1408.4986v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-21T13:08:53Z", 
    "summary": "In software and hardware development MATLAB and Simulink are used to model\ncyber physical systems for many years, , especially in automation technology\nand the automotive industry. Compliance with the required product quality and\nproject efficiency is facilitated by analyzing and transforming Simulink\nmodels. The existing API, provided by MATLAB is only suitable for programmatic\nchanging of Simulink models. We show using our own tool which is used in\nindustry, how such as a Simulink model can be edited more easily. For this\npurpose the model, is converted to an object-oriented class structure that\nprovides convenient access and editing APIs and allows applying well-known\nalgorithms and analyses from graph theory directly. It is also designed as a\nbi-directional tool, so it transforms a Simulink model into a graph\nrepresentation and vice versa.\n  -----\n  In der Software- und Hardwareentwicklung wird seit Jahren verst\\\"arkt MATLAB\nund Simulink f\\\"ur die Modellierung von cyberphysikalischen Systemen,\ninsbesondere in der Automatisierungstechnik und der Automobilindustrie\neingesetzt. Die Einhaltung der notwendigen Produktqualit\\\"at und\nProjekteffizienz wird durch Analysen und Transformationen auf Simulink-Modellen\nerleichtert. Die bestehende, von MATLAB bereitgestellte, API ist f\\\"ur die\nprogrammatische Ver\\\"anderung von Simulink-Modellen nur bedingt geeignet. Wir\nzeigen deshalb anhand eines eigenen, im industriellen Einsatz befindlichen\nWerkzeugs, wie ein Simulink-Modell leichter bearbeitet werden kann. Dazu wird\nes in eine objektorientierte Klassenstruktur \\\"uberf\\\"uhrt, die einen\nkomfortablen Zugang und Bearbeitungs-APIs bietet und es erlaubt bekannte\nAlgorithmen und Analysen aus der Graphentheorie direkt anzuwenden. Das Werkzeug\nist bidirektional entworfen, es transformiert also ein Simulink-Modell in eine\nGraphenrepresentation und umgekehrt."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5511v1", 
    "title": "Validation of the development methodologies", 
    "arxiv-id": "1408.5511v1", 
    "author": "Ammar Lahlouhi", 
    "publish": "2014-08-23T16:15:37Z", 
    "summary": "This paper argues that modelling the development methodologies can improve\nthe multi-agents systems software engineering. Such modelling allows applying\nmethods, techniques and practices used in the software development to the\nmethodologies themselves. The paper discusses then the advantages of the\nmodelling of development methodologies. It describes a model of development\nmethodologies, uses such a model to develop a system of their partial\nvalidation, and applies such a system to multi-agent methodologies. Several\nbenefits can be gained from such modelling, such as the improvement of the\nworks on the development, evaluation and comparison of multi-agent development\nmethodologies."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5690v1", 
    "title": "From Software Architecture Structure and Behavior Modeling to   Implementations of Cyber-Physical Systems", 
    "arxiv-id": "1408.5690v1", 
    "author": "Andreas Wortmann", 
    "publish": "2014-08-25T09:11:11Z", 
    "summary": "Software development for Cyber-Physical Systems (CPS) is a sophisticated\nactivity as these systems are inherently complex. The engineering of CPS\nrequires composition and interaction of diverse distributed software modules.\nDescribing both, a systems architecture and behavior in integrated models,\nyields many advantages to cope with this complexity: the models are platform\nindependent, can be decomposed to be developed independently by experts of the\nrespective fields, are highly reusable and may be subjected to formal analysis.\nIn this paper, we introduce a code generation framework for the\nMontiArcAutomaton modeling language. CPS are modeled as Component & Connector\narchitectures with embedded I/O! automata. During development, these models can\nbe analyzed using formal methods, graphically edited, and deployed to various\nplatforms. For this, we present four code generators based on the MontiCore\ncode generation framework, that implement the transformation from\nMontiArcAutomaton models to Mona (formal analysis), EMF Ecore (graphical\nediting), and Java and Python (deployment. Based on these prototypes, we\ndiscuss their commonalities and differences as well as language and application\nspecific challenges focusing on code generator development."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5691v1", 
    "title": "Meta-Metrics for Simulations in Software Engineering on the Example of   Integral Safety Systems", 
    "arxiv-id": "1408.5691v1", 
    "author": "Torsten Strutz", 
    "publish": "2014-08-25T09:17:31Z", 
    "summary": "Vehicles passengers and other traffic participants are protected more and\nmore by integral safety systems. They continuously perceive the vehicles\nenvironment to prevent dangerous situations by e.g. emergency braking systems.\nFurthermore, increasingly intelligent vehicle functions are still of major\ninterest in research and development to reduce the risk of accidents. However,\nthe development and testing of these functions should not rely only on\nvalidations on proving grounds and on long-term test-runs in real traffic;\ninstead, they should be extended by virtual testing approaches to model\npotentially dangerous situations or to re-run specific traffic situations\neasily. This article outlines meta-metrics as one of todays challenges for the\nsoftware engineering of these cyber-physical systems to provide guidance during\nthe system development: For example, unstable results of simulation test-runs\nover the vehicle functions revision history are elaborated as an indicating\nmetric where to focus on with real or further virtual test-runs; furthermore,\nvarying acting time points for the same virtual traffic situation are\nindicating problems with the reliability to interpret the specific situation.\nIn this article, several of such meta-metrics are discussed and assigned both\nto different phases during the series development and to different levels of\ndetailedness of virtual testing approaches."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5692v1", 
    "title": "A Case Study on Model-Based Development of Robotic Systems using   MontiArc with Embedded Automata", 
    "arxiv-id": "1408.5692v1", 
    "author": "Andreas Wortmann", 
    "publish": "2014-08-25T09:20:38Z", 
    "summary": "Software development for service robotics is inherently complex. Even a\nsingle robot requires the composition of several sensors, actuators, and\nsoftware modules. The systems are usually developed by groups of domain\nexperts, rarely software engineering experts. Thus the resulting software\nsystems are monolithic programs solving a single problem on a single platform.\nWe claim modeling of both structure and behavior of robots in a modular way\nleads to better reusable software. We report on a study about the modeling of\nrobotics software with the structure and behavior modeling language\nMontiArcAutomaton. This study assesses the benefits and difficulties of\nmodel-based robotics software development using MontiArc-Automaton. Our\nfindings are based on a survey, discussions with the participants, and key\nfigures from their development behavior. We present the project, our study,\nlessons learned, and future work based on the insights gained"
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5693v1", 
    "title": "Model Matching Challenge: Benchmarks for Ecore and BPMN Diagrams", 
    "arxiv-id": "1408.5693v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:26:14Z", 
    "summary": "In the last couple of years, Model Driven Engineering (MDE) gained a\nprominent role in the context of software engineering. In the MDE paradigm,\nmodels are considered first level artifacts which are iteratively developed by\nteams of programmers over a period of time. Because of this, dedicated tools\nfor versioning and management of models are needed. A central functionality\nwithin this group of tools is model comparison and differencing. In two\ndisjunct research projects, we identified a group of general matching problems\nwhere state-of-the-art comparison algorithms delivered low quality results. In\nthis article, we will present five edit operations which are the cause for\nthese low quality results. The reasons why the algorithms fail, as well as\npossible solutions, are also discussed. These examples can be used as\nbenchmarks by model developers to assess the quality and applicability of a\nmodel comparison tool for a given model type."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491055.2491073", 
    "link": "http://arxiv.org/pdf/1408.5695v1", 
    "title": "Using Lightweight Activity Diagrams for Modeling and Generation of Web   Information Systems", 
    "arxiv-id": "1408.5695v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:28:30Z", 
    "summary": "The development process of web information systems nowadays improved a lot\nregarding effectiveness and tool support, but still contains many redundant\nsteps for similar tasks. In order to overcome this, we use a model-driven\napproach to specify a web information system in an agile way and generate a\nfull- edged and runnable application from a set of models. The covered aspects\nof the system comprise data structure, page structure including view on data,\npage- and workflow within the system as well as overall application structure\nand user rights management. Appropriate tooling allows transforming these\nmodels to complete systems and thus gives us opportunity for a lightweight\ndevelopment process based on models. In this paper, we describe how we approach\nthe page- and workflow aspect by using activity diagrams as part of the agile\nmodeling approach MontiWIS. We give an overview of the defined syntax, describe\nthe supported forms of action contents and finally explain how the behavior is\nrealized in the generated application."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491411.2491414", 
    "link": "http://arxiv.org/pdf/1408.5696v1", 
    "title": "Synthesis of Component and Connector Models from Crosscutting Structural   Views", 
    "arxiv-id": "1408.5696v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:30:53Z", 
    "summary": "We present component and connector (C&C) views, which specify structural\nproperties of component and connector models in an expressive and intuitive\nway. C&C views provide means to abstract away direct hierarchy, direct\nconnectivity, port names and types, and thus can crosscut the traditional\nboundaries of the implementation-oriented hierarchical decomposition of systems\nand sub-systems, and reflect the partial knowledge available to different\nstakeholders involved in a system's design. As a primary application for C&C\nviews we investigate the synthesis problem: given a C&C views specification,\nconsisting of mandatory, alternative, and negative views, construct a concrete\nsatisfying C&C model, if one exists. We show that the problem is NP-hard and\nsolve it, in a bounded scope, using a reduction to SAT, via Alloy. We further\nextend the basic problem with support for library components, specification\npatterns, and architectural styles. The result of synthesis can be used for\nfurther exploration, simulation, and refinement of the C&C model or, as the\ncomplete, final model itself, for direct code generation. A prototype tool and\nan evaluation over four example systems with multiple specifications show\npromising results and suggest interesting future research directions towards a\ncomprehensive development environment for the structure of component and\nconnector designs."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491411.2491414", 
    "link": "http://arxiv.org/pdf/1408.5698v1", 
    "title": "Report on the Aachen OCL Meeting", 
    "arxiv-id": "1408.5698v1", 
    "author": "Burkhart Wolff", 
    "publish": "2014-08-25T09:33:16Z", 
    "summary": "As a continuation of the OCL workshop during the MODELS 2013 conference in\nOctober 2013, a number of OCL experts decided to meet in November 2013 in\nAachen for two days to discuss possible short term improvements of OCL for an\nupcoming OMG meeting and to envision possible future long-term developments of\nthe language. This paper is a sort of \\minutes of the meeting\" and intended to\nquickly inform the OCL community about the discussion topics."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s11219-015-9298-y", 
    "link": "http://arxiv.org/pdf/1408.5699v1", 
    "title": "Proactive Quality Guidance for Model Evolution in Model Libraries", 
    "arxiv-id": "1408.5699v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:34:50Z", 
    "summary": "Model evolution in model libraries differs from general model evolution. It\nlimits the scope to the manageable and allows to develop clear concepts,\napproaches, solutions, and methodologies. Looking at model quality in evolving\nmodel libraries, we focus on quality concerns related to reusability. In this\npaper, we put forward our proactive quality guidance approach for model\nevolution in model libraries. It uses an editing-time assessment linked to a\nlightweight quality model, corresponding metrics, and simplified reviews. All\nof which help to guide model evolution by means of quality gates fostering\nmodel reusability."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s11219-015-9298-y", 
    "link": "http://arxiv.org/pdf/1408.5703v1", 
    "title": "Report on the First Workshop On the Globalization of Modeling Languages", 
    "arxiv-id": "1408.5703v1", 
    "author": "Martin Schindler", 
    "publish": "2014-08-25T09:38:36Z", 
    "summary": "The first edition of GEMOC workshop was co-located with the MODELS 2013\nconference in Miami, FL, USA. The workshop provided an open forum for sharing\nexperiences, problems and solutions related to the challenges of using of\nmultiple modeling languages in the development of complex software based\nsystems. During the workshop, concrete language composition artifacts,\napproaches, and mechanisms were presented and discussed, ideas and opinions\nexchanged, and constructive feedback provided to authors of accepted papers. A\nmajor objective was to encourage collaborations and to start building a\ncommunity that focused on providing solutions that support what we refer to as\nthe globalization of domain-specific modeling languages, that is, support\ncoordinated use of multiple languages throughout the development of complex\nsystems. This report summarizes the presentations and discussions that took\nplace in the first GEMOC 2013 workshop."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s11219-015-9298-y", 
    "link": "http://arxiv.org/pdf/1408.5705v1", 
    "title": "Modeling Cloud Architectures as Interactive Systems", 
    "arxiv-id": "1408.5705v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:44:00Z", 
    "summary": "The development and maintenance of cloud software is complicated by complex\nbut crucial technological requirements that are tightly coupled with each other\nand with the softwares actual business functionality. Consequently, the\ncomplexity of design, implementation, deployment, and maintenance activities\nincreases. We present an architecture description language that raises the\nlevel of technological abstraction by modeling cloud software as interactive\nsystems. We show how its models correspond to an architecture style that\nparticularly meets the requirements of cloud-based cyber-physical systems. The\nresult provides a basis for an architecture-driven model-based methodology for\nengineering cloud software."
},{
    "category": "cs.SE", 
    "doi": "10.1007/s11219-015-9298-y", 
    "link": "http://arxiv.org/pdf/1408.5707v1", 
    "title": "Staged Evolution with Quality Gates for Model Libraries", 
    "arxiv-id": "1408.5707v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-08-25T09:46:07Z", 
    "summary": "Model evolution is widely considered as a subject under research. Despite its\nrole in research, common purpose concepts, approaches, solutions, and\nmethodologies are missing. Limiting the scope to model libraries makes model\nevolution and related quality concerns manageable, as we show below. In this\npaper, we put forward our quality staged model evolution theory for model\nlibraries. It is founded on evolution graphs, which offer a structure for model\nevolution in model libraries through evolution steps. These evolution steps\neventually form a sequence, which can be partitioned into stages by quality\ngates. Each quality gate is defined by a lightweight quality model and\nrespective characteristics fostering reusability."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2430502.2430508", 
    "link": "http://arxiv.org/pdf/1408.5751v1", 
    "title": "First-Class Variability Modeling in Matlab/Simulink", 
    "arxiv-id": "1408.5751v1", 
    "author": "Ina Schaefer", 
    "publish": "2014-08-25T13:27:33Z", 
    "summary": "Modern cars exist in an vast number of variants. Thus, variability has to be\ndealt with in all phases of the development process, in particular during\nmodel-based development of software-intensive functionality using\nMatlab/Simulink. Currently, variability is often encoded within a functional\nmodel leading to so called 150%-models which easily become very complex and do\nnot scale for larger product lines. To counter these problems, we propose a\nmodular variability modeling approach for Matlab/Simulink based on the concept\nof delta modeling [8, 9, 24]. A functional variant is described by a delta\nencapsulating a set of modifications. A sequence of deltas can be applied to a\ncore product to derive the desired variant. We present a prototypical\nimplementation, which is integrated into Matlab/Simulink and offers graphical\nediting of delta models."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.5756v1", 
    "title": "Engineering Delta Modeling Languages", 
    "arxiv-id": "1408.5756v1", 
    "author": "Ina Schaefer", 
    "publish": "2014-08-25T13:32:26Z", 
    "summary": "Delta modeling is a modular, yet flexible approach to capture spatial and\ntemporal variability by explicitly representing the differences between system\nvariants or versions. The conceptual idea of delta modeling is\nlanguage-independent. But, in order to apply delta modeling for a concrete\nlanguage, so far, a delta language had to be manually developed on top of the\nbase language leading to a large variety of heterogeneous language concepts. In\nthis paper, we present a process that allows deriving a delta language from the\ngrammar of a given base language. Our approach relies on an automatically\ngenerated language extension that can be manually adapted to meet\ndomain-specific needs. We illustrate our approach using delta modeling on a\ntextual variant of statecharts."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.5786v1", 
    "title": "A comparison of model view controller and model view presenter", 
    "arxiv-id": "1408.5786v1", 
    "author": "Fatima Sabir", 
    "publish": "2014-08-25T15:04:26Z", 
    "summary": "Web application frameworks are managed by using different design strategies.\nDesign strategies are applied by using different design processes. In each\ndesign process, requirement specifications are changed in to different design\nmodel that describe the detail of different data structure, system\narchitecture, interface and components. Web application frame work is\nimplemented by using Model View Controller (MVC) and Model View Presenter\n(MVP). These web application models are used to provide standardized view for\nweb applications. This paper mainly focuses on different design aspect of MVC\nand MVP. Generally we present different methodologies that are related to the\nimplementation of MVC and MVP and implementation of appropriate platform and\nsuitable environment for MVC and MVP."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.6120v1", 
    "title": "The proposal of a novel software testing framework", 
    "arxiv-id": "1408.6120v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2014-08-26T14:12:22Z", 
    "summary": "Software testing is normally used to check the validity of a program. Test\noracle performs an important role in software testing. The focus in this\nresearch is to perform class level test by introducing a testing framework. A\ntechnique is developed to generate test oracle for specification-based software\ntesting using Vienna Development Method (VDM++) formal language. A three stage\ntranslation process, of VDM++ specifications of container classes to C++ test\noracle classes, is described in this paper. It is also presented that how\nderived test oracle is integrated into a proposed functional testing framework.\nThis technique caters object oriented features such as inheritance and\naggregation, but concurrency is not considered in this work. Translation\nissues, limitations and evaluation of the technique are also discussed. The\nproposed approach is illustrated with the help of popular triangle problem case\nstudy."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.6130v1", 
    "title": "Significance of the teamwork in agile software engineering", 
    "arxiv-id": "1408.6130v1", 
    "author": "Fatima Sabir", 
    "publish": "2014-08-26T14:41:17Z", 
    "summary": "A Software Engineering project depends significantly on team performance, as\ndoes any activity that involves human interaction. In the last years, the\ntraditional perspective on software development is changing and agile methods\nhave received considerable attention. Among other attributes, the ageists claim\nthat fostering creativity is one of the keys to response to common problems and\nchallenges of software development today. The development of new software\nproducts requires the generation of novel and useful ideas. It is a conceptual\nframework introduced in the Agile Manifesto in 2001. This paper is written in\nsupport of agile practices in terms of significance of teamwork for the success\nof software projects. Survey is used as a research method to know the\nsignificance of teamwork."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.6142v1", 
    "title": "Scrum of scrums solution for large size teams using scrum methodology", 
    "arxiv-id": "1408.6142v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2014-08-26T15:04:40Z", 
    "summary": "Scrum is a structured framework to support complex product development.\nHowever, Scrum methodology faces a challenge of managing large teams. To\naddress this challenge, in this paper we propose a solution called Scrum of\nScrums. In Scrum of Scrums, we divide the Scrum team into teams of the right\nsize, and then organize them hierarchically into a Scrum of Scrums. The main\ngoals of the proposed solution are to optimize communication between teams in\nScrum of Scrums; to make the system work after integration of all parts; to\nreduce the dependencies between the parts of system; and to prevent the\nduplication of parts in the system."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2491627.2491632", 
    "link": "http://arxiv.org/pdf/1408.6147v1", 
    "title": "The proposal of improved inexact isomorphic graph algorithm to detect   design patterns", 
    "arxiv-id": "1408.6147v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2014-08-26T15:13:39Z", 
    "summary": "Design patterns being applied more and more to solve the software engineering\ndifficulties in the object oriented software design procedures. So, the design\npattern detection is widely used by software industries. Currently, many\nsolutions presented to detect the design pattern in the system design. In this\npaper, we will propose a new one which first; we will use the graph\nimplementation to implement both the system design UML diagram and the design\npattern UML diagram. Second, we will implement the edges for each one of the\nboth two graphs in a set of 4-tuple elements. Then, we will apply a new inexact\ngraph isomorphic algorithm to detect the design pattern in the system design."
},{
    "category": "cs.SE", 
    "doi": "10.7763/IJMLC.2013.V3.346", 
    "link": "http://arxiv.org/pdf/1408.6228v1", 
    "title": "Estimation of the new agile XP process model for medium-scale projects   using industrial case studies", 
    "arxiv-id": "1408.6228v1", 
    "author": "M. Rizwan Jameel Qureshi", 
    "publish": "2014-08-26T14:07:34Z", 
    "summary": "Agile is one of the terms with which software professionals are quite\nfamiliar. Agile models promote fast development to develop high quality\nsoftware. XP process model is one of the most widely used and most documented\nagile models. XP model is meant for small-scale projects. Since XP model is a\ngood model, therefore there is need of its extension for the development of\nmedium and large-scale projects. XP model has certain drawbacks such as weak\ndocumentation and poor performance while adapting it for the development of\nmedium and large-scale projects having large teams. A new XP model is proposed\nin this paper to cater the needs of software development companies for\nmedium-scale projects having large teams. This research may prove to be step\nforward for adaptation of the proposed new XP model for the development of\nlarge-scale projects. Two independent industrial case studies are conducted to\nvalidate the proposed new XP model handling for small and medium scale software\nprojects, one case study for each type of project."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSE.2015.90", 
    "link": "http://arxiv.org/pdf/1409.0252v4", 
    "title": "A Comparative Study of Programming Languages in Rosetta Code", 
    "arxiv-id": "1409.0252v4", 
    "author": "Carlo A. Furia", 
    "publish": "2014-08-31T19:04:01Z", 
    "summary": "Sometimes debates on programming languages are more religious than\nscientific. Questions about which language is more succinct or efficient, or\nmakes developers more productive are discussed with fervor, and their answers\nare too often based on anecdotes and unsubstantiated beliefs. In this study, we\nuse the largely untapped research potential of Rosetta Code, a code repository\nof solutions to common programming tasks in various languages, to draw a fair\nand well-founded comparison. Rosetta Code offers a large data set for analysis.\nOur study is based on 7087 solution programs corresponding to 745 tasks in 8\nwidely used languages representing the major programming paradigms (procedural:\nC and Go; object-oriented: C# and Java; functional: F# and Haskell; scripting:\nPython and Ruby). Our statistical analysis reveals, most notably, that:\nfunctional and scripting languages are more concise than procedural and\nobject-oriented languages; C is hard to beat when it comes to raw speed on\nlarge inputs, but performance differences over inputs of moderate size are less\npronounced and allow even interpreted languages to be competitive; compiled\nstrongly-typed languages, where more defects can be caught at compile time, are\nless prone to runtime failures than interpreted or weakly-typed languages. We\ndiscuss implications of these results for developers, language designers, and\neducators."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSE.2015.90", 
    "link": "http://arxiv.org/pdf/1409.0384v1", 
    "title": "An Interim Summary on Semantic Model Differencing", 
    "arxiv-id": "1409.0384v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-09-01T12:28:51Z", 
    "summary": "This position paper provides an interim summary on the goals and current\nstate of our ongoing research project on semantic model differencing for\nsoftware evolution. We describe the basics of semantic model differencing, give\ntwo examples from our recent work, and discuss future challenges in taking full\nadvantage of the potential of semantic differencing techniques in the context\nof models' evolution."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSE.2015.90", 
    "link": "http://arxiv.org/pdf/1409.0385v1", 
    "title": "Cyber-Physical Systems -- eine Herausforderung an die   Automatisierungstechnik?", 
    "arxiv-id": "1409.0385v1", 
    "author": "Andre Stollenwerk", 
    "publish": "2014-09-01T12:31:05Z", 
    "summary": "We discuss challenges to control systems engineering arising from the advent\nof cyber-physical systems (CPS). After discussing the terminology, general,\nIT-related issues are treated which need cooperation with computer science, in\nparticular software engineering. Then we study those challenges that require\nspecific core competencies from control systems engineering. We sketch solution\napproaches for the exemplary problem of dealing with changes in the physical\nenvironment of a CPS.\n  ----\n  Der Beitrag befasst sich mit den methodischen Herausforderungen, die durch\ndie Verbreitung der Cyber-Physical Systems (CPS) in der Automatisierungstechnik\nentstehen, und stellt L\\\"osungsans\\\"atze vor. Nach einer Behandlung des\nBegriffs CPS werden zun\\\"achst die allgemeinen, IT-bezogenen Fragestellungen\nangesprochen, die gemeinsam mit der Informatik gel\\\"ost werden m\\\"ussen. Danach\ngehen wir auf die Herausforderungen ein, deren Behandlung spezifisch\nautomatisierungstechnische Kernkompetenzen erfordern und skizzieren f\\\"ur eine\nbeispielhafte Problemstellung, den Umgang mit \\\"Anderungen in der\nphysikalischen Umgebung, wie entsprechende L\\\"osungen aussehen k\\\"onnen."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSE.2015.90", 
    "link": "http://arxiv.org/pdf/1409.0394v1", 
    "title": "A Requirements Modeling Language for the Component Behavior of Cyber   Physical Robotics Systems", 
    "arxiv-id": "1409.0394v1", 
    "author": "Andreas Wortmann", 
    "publish": "2014-09-01T12:55:56Z", 
    "summary": "Software development for robotics applications is a sophisticated endeavor as\nrobots are inherently complex. Explicit modeling of the architecture and\nbehavior of robotics application yields many advantages to cope with this\ncomplexity by identifying and separating logically and physically independent\ncomponents and by hierarchically structuring the system under development. On\ntop of component and connector models we propose modeling the requirements on\nthe behavior of robotics software components using I/O! automata. This approach\nfacilitates early simulation of requirements model, allows to subject these to\nformal analysis and to generate the software from them. In this paper, we\nintroduce an extension of the architecture description language MontiArc to\nmodel the requirements on components with I/O!automata, which are defined in\nthe spirit of Martin Glinz Statecharts for requirements modeling [10]. We\nfurthermore present a case study based on a robotics application generated for\nthe Lego NXT robotic platform."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICSE.2015.90", 
    "link": "http://arxiv.org/pdf/1409.0400v1", 
    "title": "Modeling Cyber-Physical Systems: Model-Driven Specification of Energy   Efficient Buildings", 
    "arxiv-id": "1409.0400v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-09-01T13:03:22Z", 
    "summary": "A lot of current buildings are operated energy inefficient and offer a great\npotential to reduce the overall energy consumption and CO2 emission. Detecting\nthese inefficiencies is a complicated task and needs domain experts that are\nable to identify them. Most approaches try to support detection by focussing on\nmonitoring the building's operation and visualizing data. Instead our approach\nfocuses on using techniques taken from the cyber-physical systems' modeling\ndomain. We create a model of the building and show how we constrain the model\nby OCL-like rules to support a sound specification which can be matched against\nmonitoring results afterwards. The paper presents our domain-specific language\nfor modeling buildings and technical facilities that is implemented in a\nsoftware-based tool used by domain experts and thus hopefully providing a\nsuitable contribution to modeling the cyber-physical world."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0402v1", 
    "title": "Supporting acceptance testing in distributed software projects with   integrated feedback systems: Experiences and requirements", 
    "arxiv-id": "1409.0402v1", 
    "author": "Kurt Schneider", 
    "publish": "2014-09-01T13:05:26Z", 
    "summary": "During acceptance testing customers assess whether a system meets their\nexpectations and often identify issues that should be improved. These findings\nhave to be communicated to the developers a task we observed to be error prone,\nespecially in distributed teams. Here, it is normally not possible to have\ndeveloper representatives from every site attend the test. Developers who were\nnot present might misunderstand insufficiently documented findings. This\nhinders fixing the issues and endangers customer satisfaction. Integrated\nfeedback systems promise to mitigate this problem. They allow to easily capture\nfindings and their context. Correctly applied, this technique could improve\nfeedback, while reducing customer effort. This paper collects our experiences\nfrom comparing acceptance testing with and without feedback systems in a\ndistributed project. Our results indicate that this technique can improve\nacceptance testing if certain requirements are met. We identify key\nrequirements feedback systems should meet to support acceptance testing."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0415v1", 
    "title": "SSELab: A Plug-In-Based Framework for Web-Based Project Portals", 
    "arxiv-id": "1409.0415v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-09-01T13:24:14Z", 
    "summary": "Tools are an essential part of every software engineering project. But the\nnumber of tools that are used in all phases of the software development\nlife-cycle and their complexity is growing continually. Consequently, the setup\nand maintenance of current tool chains and development environments requires\nmuch effort and consumes a lot of time. One approach to counter this, is to\nemploy web-based systems for development tasks, because centralized systems\nsimplify the administration and the deployment of new features. But desktop\nIDEs play an important role in software development projects today, and will\nnot be replaced entirely by web-based environments in the near future.\nTherefore, supporting a mixture of hosted tools and tools integrated into\ndesktop IDEs is a sensible approach. In this paper, we present the SSELab, a\nframework for web- based project portals that attempts to migrate more software\ndevelopment tools from desktop to server environments, but still allows their\nintegration into modern desktop IDEs. It supports the deployment of tools as\nhosted services using plug-in systems on the server-side. Additionally, it\nprovides access to these tools by a set of clients that can be used in\ndifferent contexts, either from the command line, from within IDEs such as\nEclipse, or from web pages. In the paper, we discuss the architecture and the\nextensibility of the SSELab framework. Furthermore, we share our experiences\nwith creating an instance of the framework and integrating various tools for\nour own software development projects."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0416v1", 
    "title": "The Energy Navigator - A Web-Platform for Performance Design and   Management", 
    "arxiv-id": "1409.0416v1", 
    "author": "Bernhard Rumpe", 
    "publish": "2014-09-01T13:27:11Z", 
    "summary": "Over the last three decades comprehensive research has been carried out\ntrying to improve commissioning processes with powerful modeling tools and\nmethodologies for data analysis and visualization. Typically addressed\napplication scenarios are facilities management, contracting, special\nconsulting services and measurement & verification as part of a certification\nprocess. The results are all but convincing: Monitoring of building operation\nhas so far not become a regular service for buildings. We have identified a\nlack of process integration as a significant barrier for market success. Most\nmethodologies have so far caused additional initial invest and transaction\ncost: they added new services instead of improving existing ones. The Energy\nNavigator, developed by synavision GmbH in cooperation with leading research\ninstitutes of the Technical University Braunschweig and the RWTH Aachen\nUniversity, presents a new methodology with several new approaches. Its\nsoftware platform uses state graphs and a domain specific language to describe\nbuilding functions offering an alternative to the software that is so far most\nwidely used for this task: Microsoft Word. The Energy Navigators so called\nActive Functional Specification (AFS) is used for the technical specification\nof building services in the design phase. After construction it is completed by\nthe supplier of the BMS (Building Management System) with the relevant sensors\ndata as documentation of his service. Operation data can then automatically be\nchecked for initial and continuous commissioning on whether it meets the\ncriteria of the specification."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0526v1", 
    "title": "Dynamic Component Composition", 
    "arxiv-id": "1409.0526v1", 
    "author": "Efim Grinkrug", 
    "publish": "2014-08-30T14:55:55Z", 
    "summary": "This paper presents an approach to dynamic component composition that\nfacilitates creating new composed components using existing ones at runtime and\nwithout any code generation. The dynamic abilities are supported by extended\ntype notion and implementation based on additional superstructure provided with\nits Java API and corresponding JavaBeans components. The new component\ncomposition is performed by building the composed prototype object that can be\ndynamically transformed into the new instantiable type (component). That\napproach demonstrates interrelations between prototype-based and class-based\ncomponent-oriented programming. The component model proposed can be used when\nimplementing user-defined types in declarative languages for event-driven\napplications programming."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0759v2", 
    "title": "Enablers and Impediments for Collaborative Research in Software Testing:   An Empirical Exploration", 
    "arxiv-id": "1409.0759v2", 
    "author": "Adnan Causevic", 
    "publish": "2014-09-02T15:36:35Z", 
    "summary": "When it comes to industrial organizations, current collaboration efforts in\nsoftware engineering research are very often kept in-house, depriving these\norganizations off the skills necessary to build independent collaborative\nresearch. The current trend, towards empirical software engineering research,\nrequires certain standards to be established which would guide these\ncollaborative efforts in creating a strong partnership that promotes\nindependent, evidence-based, software engineering research. This paper examines\nkey enabling factors for an efficient and effective industry-academia\ncollaboration in the software testing domain. A major finding of the research\nwas that while technology is a strong enabler to better collaboration, it must\nbe complemented with industrial openness to disclose research results and the\nuse of a dedicated tooling platform. We use as an example an automated test\ngeneration approach that has been developed in the last two years\ncollaboratively with Bombardier Transportation AB in Sweden."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.0982v1", 
    "title": "Taming the Concurrency: Controlling Concurrent Behavior while Testing   Multithreaded Software", 
    "arxiv-id": "1409.0982v1", 
    "author": "Amiram Yehudai", 
    "publish": "2014-09-03T08:14:26Z", 
    "summary": "Developing multithreaded software is an extremely challenging task, even for\nexperienced programmers. The challenge does not end after the code is written.\nThere are other tasks associated with a development process that become\nexceptionally hard in a multithreaded environment. A good example of this is\ncreating unit tests for concurrent data structures. In addition to the desired\ntest logic, such a test contains plenty of synchronization code that makes it\nhard to understand and maintain.\n  In our work we propose a novel approach for specifying and executing\nschedules for multithreaded tests. It allows explicit specification of desired\nthread scheduling for some unit test and enforces it during the test execution,\ngiving the developer an ability to construct deterministic and repeatable unit\ntests. This goal is achieved by combining a few basic tools available in every\nmodern runtime/IDE and does not require dedicated runtime environment, new\nspecification language or code under test modifications."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.1656v2", 
    "title": "An Aspect-Oriented Approach for SaaS Application Customization", 
    "arxiv-id": "1409.1656v2", 
    "author": "Abdelaziz Khamis", 
    "publish": "2014-09-05T03:21:52Z", 
    "summary": "Multi-tenancy is one of the most important concepts for any Software as a\nService (SaaS) application. Multi-tenant SaaS application serves a large number\nof tenants with one single application instance. Complex SaaS application that\nserves significant number of tenants could have a huge number of customizations\nwith complicated relationships, which increases the customization complexity\nand reduces the customization understandability. Modeling such customizations,\nvalidating each tenant's customization, and adapting SaaS applications on the\nfly based on each tenant's requirements become very complex tasks. To mitigate\nthese challenges, we propose an aspect-oriented approach that makes use of the\nOrthogonal Variability Model (OVM) and Metagraphs. The OVM is used to provide\nthe tenants with simple and understandable customization model. A\nMetagraph-based algorithm has been developed to validate tenants'\ncustomizations. On the other hand, the aspect-oriented approach offers a high\nlevel of runtime adaptability."
},{
    "category": "cs.SE", 
    "doi": "10.1109/ICGSE.2012.34", 
    "link": "http://arxiv.org/pdf/1409.1793v3", 
    "title": "The Handbook of Engineering Self-Aware and Self-Expressive Systems", 
    "arxiv-id": "1409.1793v3", 
    "author": "Lukas Esterle", 
    "publish": "2014-09-05T13:42:30Z", 
    "summary": "When faced with the task of designing and implementing a new self-aware and\nself-expressive computing system, researchers and practitioners need a set of\nguidelines on how to use the concepts and foundations developed in the\nEngineering Proprioception in Computing Systems (EPiCS) project. This report\nprovides such guidelines on how to design self-aware and self-expressive\ncomputing systems in a principled way. We have documented different categories\nof self-awareness and self-expression level using architectural patterns. We\nhave also documented common architectural primitives, their possible candidate\ntechniques and attributes for architecting self-aware and self-expressive\nsystems. Drawing on the knowledge obtained from the previous investigations, we\nproposed a pattern driven methodology for engineering self-aware and\nself-expressive systems to assist in utilising the patterns and primitives\nduring design. The methodology contains detailed guidance to make decisions\nwith respect to the possible design alternatives, providing a systematic way to\nbuild self-aware and self-expressive systems. Then, we qualitatively and\nquantitatively evaluated the methodology using two case studies. The results\nreveal that our pattern driven methodology covers the main aspects of\nengineering self-aware and self-expressive systems, and that the resulted\nsystems perform significantly better than the non-self-aware systems."
},lol]